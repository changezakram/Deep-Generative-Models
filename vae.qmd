---
title: "Variational Autoencoders"  
---

## Autoencoders vs Variational Autoencoders

Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:

- They lack **generative capabilities** â€” they cannot sample new data effectively
- The **latent space is unstructured**, offering little control or interpretation
- There is no **probabilistic modeling**, limiting uncertainty estimation

Variational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.

### Probabilistic Framework

More formally, VAEs assume the data is generated by a two-step process:

1. Sample a latent variable $\mathbf{z} \sim \mathcal{N}(0, I)$
2. Generate the observation $\mathbf{x}$ from:
   $$
   p(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mu_\theta(\mathbf{z}), \Sigma_\theta(\mathbf{z}))
   $$
   where $\mu_\theta$ and $\Sigma_\theta$ are neural networks parameterized by $\theta$

### Key Properties

- $\mathbf{z}$ is a **hidden variable**, unobserved during training
- Defines a *mixture of infinitely many Gaussians*
- The marginal likelihood requires integration:
  $$
  p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) \, d\mathbf{z}
  $$

### Computational Challenges

This integral requires integrating over:
- All possible values of $\mathbf{z}$ (often high-dimensional)
- Non-linear transformations through neural networks

**Result:** Exact computation is intractable, motivating variational inference techniques like ELBO (developed next).

## Estimation Techniques

### Naive Monte Carlo Estimation

One natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:

$$
p(x) \approx \frac{1}{K} \sum_{j=1}^K p_\theta(x, z_j), \quad z_j \sim \text{Uniform}
$$

However, this fails in practice. For most values of $z$, the joint probability $p_\theta(x, z)$ is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has:

- **High variance** (sparse meaningful samples)
- **Low hit rate** (rarely samples likely $z$ values)

### Importance Sampling

To address this, we use **importance sampling**, introducing a proposal distribution $q(z)$:

$$
p(x) = \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right]
$$

**Key Properties:**
- Unbiased estimator when $\text{supp}(q) \supseteq \text{supp}(p_\theta)$
- Optimal $q(z) \approx p_\theta(z|x)$ (the true posterior)
- More efficient sampling in high-probability regions

#### The Log-Likelihood Challenge

For optimization, we need the log-likelihood, but:

$$
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \neq \mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]
$$

**Jensen's Inequality** reveals the relationship:

$$
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \geq \underbrace{\mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]}_{\text{ELBO}}
$$

This gives us the **Evidence Lower Bound (ELBO)**, which is lower bounds the true log-likelihood.


## Why Variational Inference?

### The Intractability Problem
In latent variable models, we need to compute:

1. **Posterior Distribution**:
   \begin{equation}
   p(z|x; \theta) = \frac{p(z,x;\theta)}{p(x;\theta)}
   \end{equation}
2. **Marginal Likelihood**:
   \begin{equation}
   p(x;\theta) = \int p(x,z;\theta) dz
   \end{equation}

These are intractable because:
- High-dimensional integration in (2)
- Complex dependencies in (1)

### Variational Solution
Introduce an **approximate distribution** $q(z;\phi)$ with:
- Tractable form (e.g., Gaussian)
- Parameters $\phi$ optimized to match $p(z|x;\theta)$

## Core Derivation

### Step 1: KL Divergence Objective
\begin{equation}
D_{KL}(q(z)\|p(z|x; \theta)) = \sum_z q(z) \log \frac{q(z)}{p(z|x; \theta)}
\end{equation}

### Step 2: Apply Bayes' Rule
Substitute $p(z|x; \theta) = \frac{p(z,x;\theta)}{p(x;\theta)}$:
\begin{equation}
= \sum_z q(z) \log \left( \frac{q(z) \cdot p(x; \theta)}{p(z, x; \theta)} \right)
\end{equation}

### Step 3: Decompose Terms
\begin{align}
&= \sum_z q(z) \log q(z) + \sum_z q(z) \log p(x; \theta) \nonumber \\
&\quad - \sum_z q(z) \log p(z, x; \theta) \\
&= -H(q) + \log p(x; \theta) - \mathbb{E}_q[\log p(z,x;\theta)]
\end{align}

### Step 4: Rearrange for ELBO
\begin{equation}
\log p(x;\theta) = \underbrace{\mathbb{E}_q[\log p(z,x;\theta)] + H(q)}_{\text{ELBO}} + D_{KL}(q\|p)
\end{equation}

## Key Results

1. **Evidence Lower Bound (ELBO)**:
   \begin{equation}
   \mathcal{L}(\theta,\phi) = \mathbb{E}_{q(z;\phi)}[\log p(x,z;\theta)] + H(q(z;\phi))
   \end{equation}

2. **Optimization**:
   \begin{equation}
   \max_{\theta,\phi} \mathcal{L}(\theta,\phi) \Rightarrow 
   \begin{cases}
   \text{Maximizes data likelihood} \\
   \text{Minimizes } D_{KL}(q\|p)
   \end{cases}
   \end{equation}

## Practical Implications

- **For $q(z)$**: Choose simple distributions (e.g., Gaussian)
- **For $\phi$**: Use gradient ascent on $\mathcal{L}$
- **For VAEs**: $q(z|x;\phi)$ becomes the encoder network
