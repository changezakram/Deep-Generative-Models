---
title: "Variational Autoencoders"  
---

## Autoencoders vs Variational Autoencoders

Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:

- They lack **generative capabilities** — they cannot sample new data effectively
- The **latent space is unstructured**, offering little control or interpretation
- There is no **probabilistic modeling**, limiting uncertainty estimation

Variational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.

### Probabilistic Framework

More formally, VAEs assume the data is generated by a two-step process:

1. Sample a latent variable $\mathbf{z} \sim \mathcal{N}(0, I)$
2. Generate the observation $\mathbf{x}$ from:
   $$
   p(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mu_\theta(\mathbf{z}), \Sigma_\theta(\mathbf{z}))
   $$
   where $\mu_\theta$ and $\Sigma_\theta$ are neural networks parameterized by $\theta$

Here, $\mathbf{z}$ acts as a hidden or latent variable, which is **unobserved during training**. The model thus defines a \textit{mixture of infinitely many Gaussians} — one for each $\mathbf{z}$.

To compute the likelihood of a data point $\mathbf{x}$, we must marginalize over all possible latent variables:
  $$
  p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) \, d\mathbf{z}
  $$

This integral requires integrating over all possible values of the latent variable $\mathbf{z}$, which is often high-dimensional and enters the likelihood non-linearly through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of variational inference techniques like ELBO, which will be developed in the following sections.


### Computational Challenge

This integral requires integrating over:

- All possible values of $\mathbf{z}$ (often high-dimensional)
- Non-linear transformations through neural networks

**Result:** Exact computation is intractable, motivating variational inference techniques like ELBO (developed next).

## Estimation Techniques

### Naive Monte Carlo Estimation

One natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:

$$
p(x) \approx \frac{1}{K} \sum_{j=1}^K p_\theta(x, z_j), \quad z_j \sim \text{Uniform}
$$

However, this fails in practice. For most values of $z$, the joint probability $p_\theta(x, z)$ is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely “hits” likely values of $z$.

### Importance Sampling

To address this, we use **importance sampling**, introducing a proposal distribution $q(z)$:

$$
p(x) = \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right]
$$

This gives an **unbiased estimator** of $p(x)$ if $q(z)$ is well-chosen (ideally close to $p_\theta(z|x)$). Intuitively, we sample $z$ more frequently in regions where $p_\theta(x, z)$ is high.

However, our goal is to optimize the **log-likelihood**, and the log of an expectation is not the same as the expectation of the log. That is,

$$
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \neq \mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]
$$

This discrepancy is captured by **Jensen’s Inequality**, which tells us:

$$
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \geq \underbrace{\mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]}_{\text{ELBO}}
$$

The right-hand side provides a tractable lower bound on the log-likelihood and is referred to as the **Evidence Lower Bound (ELBO)**. Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood. In the next section, we derive this bound formally and explore its components in detail.


## Why Variational Inference?



Computing the true posterior distribution \( p(\mathbf{z} \mid \mathbf{x}; \theta) \) is intractable in most cases because it requires evaluating the marginal likelihood:

\[
p(\mathbf{x}; \theta) = \int p(\mathbf{x}, \mathbf{z}; \theta) \, d\mathbf{z}
\]

This integral poses three fundamental challenges:

1. **High-dimensional integration**: The latent space $\mathbf{z}$ is often high-dimensional (e.g., 100+ dimensions)
2. **Non-linear dependencies**: The mapping $p(\mathbf{x}|\mathbf{z})$ involves neural networks with complex, non-linear transformations
3. **Exponential complexity**: Exact computation would require enumerating all possible latent configurations

Variational inference addresses these issues by:

1. **Approximation**: Introducing a tractable family of distributions $q(\mathbf{z}; \phi)$ (typically Gaussian) parameterized by $\phi$
2. **Optimization**: Minimizing the KL divergence between $q$ and the true posterior:

\[
D_{\mathrm{KL}}\big(q(\mathbf{z}; \phi) \parallel p(\mathbf{z} \mid \mathbf{x}; \theta)\big)
\]

3. **Reformulation**: Converting the inference problem into an optimization problem through the ELBO:

\[
\log p(\mathbf{x}; \theta) \geq \underbrace{\mathbb{E}_{q(\mathbf{z}; \phi)}\big[\log p(\mathbf{x}, \mathbf{z}; \theta)\big] - \mathbb{E}_{q(\mathbf{z}; \phi)}\big[\log q(\mathbf{z}; \phi)\big]}_{\text{ELBO}(\theta, \phi)}
\]

Key advantages of this approach:

- **Computational tractability**: Replaces integration with optimization
- **Flexibility**: $q(\mathbf{z}; \phi)$ can be any parametric distribution (often chosen as Gaussian)
- **Scalability**: Compatible with stochastic gradient descent
- **Theoretical guarantees**: Maximizing ELBO simultaneously:
  \begin{itemize}
  \item Increases the data likelihood $\log p(\mathbf{x}; \theta)$
  \item Decreases $D_{\mathrm{KL}}(q \parallel p)$
  \end{itemize}





## Core Derivation

### Step 1: KL Divergence Objective
\begin{equation}
D_{KL}(q(z)\|p(z|x; \theta)) = \sum_z q(z) \log \frac{q(z)}{p(z|x; \theta)}
\end{equation}

### Step 2: Apply Bayes' Rule
Substitute $p(z|x; \theta) = \frac{p(z,x;\theta)}{p(x;\theta)}$:
\begin{equation}
= \sum_z q(z) \log \left( \frac{q(z) \cdot p(x; \theta)}{p(z, x; \theta)} \right)
\end{equation}

### Step 3: Decompose Terms
\begin{align}
&= \sum_z q(z) \log q(z) + \sum_z q(z) \log p(x; \theta) \nonumber \\
&\quad - \sum_z q(z) \log p(z, x; \theta) \\
&= -H(q) + \log p(x; \theta) - \mathbb{E}_q[\log p(z,x;\theta)]
\end{align}

### Step 4: Rearrange for ELBO
\begin{equation}
\log p(x;\theta) = \underbrace{\mathbb{E}_q[\log p(z,x;\theta)] + H(q)}_{\text{ELBO}} + D_{KL}(q\|p)
\end{equation}

## Key Results

1. **Evidence Lower Bound (ELBO)**:
   \begin{equation}
   \mathcal{L}(\theta,\phi) = \mathbb{E}_{q(z;\phi)}[\log p(x,z;\theta)] + H(q(z;\phi))
   \end{equation}

2. **Optimization**:
   \begin{equation}
   \max_{\theta,\phi} \mathcal{L}(\theta,\phi) \Rightarrow 
   \begin{cases}
   \text{Maximizes data likelihood} \\
   \text{Minimizes } D_{KL}(q\|p)
   \end{cases}
   \end{equation}

## Practical Implications

- **For $q(z)$**: Choose simple distributions (e.g., Gaussian)
- **For $\phi$**: Use gradient ascent on $\mathcal{L}$
- **For VAEs**: $q(z|x;\phi)$ becomes the encoder network

\section*{Applications of VAEs}

Variational Autoencoders are widely used in:

## Applications of VAEs

- **Image Generation**: VAEs can generate new images similar to the training data (e.g., MNIST digits)  
- **Anomaly Detection**: High reconstruction error flags unusual data points  
- **Representation Learning**: Latent space captures features for downstream tasks 