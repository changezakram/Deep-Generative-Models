---
title: "Variational Autoencoders"  
---

## Autoencoders vs Variational Autoencoders

Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:

- They lack **generative capabilities** — they cannot sample new data effectively
- The **latent space is unstructured**, offering little control or interpretation
- There is no **probabilistic modeling**, limiting uncertainty estimation

Variational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.

### Probabilistic Framework

More formally, VAEs assume the data is generated by a two-step process:

1. Sample a latent variable $\mathbf{z} \sim \mathcal{N}(0, I)$
2. Generate the observation $\mathbf{x}$ from:
   $$
   p(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mu_\theta(\mathbf{z}), \Sigma_\theta(\mathbf{z}))
   $$
   where $\mu_\theta$ and $\Sigma_\theta$ are neural networks parameterized by $\theta$

Here, $\mathbf{z}$ acts as a hidden or latent variable, which is **unobserved during training**. The model thus defines a \textit{mixture of infinitely many Gaussians} — one for each $\mathbf{z}$.

To compute the likelihood of a data point $\mathbf{x}$, we must marginalize over all possible latent variables:
  $$
  p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) \, d\mathbf{z}
  $$

This integral requires integrating over all possible values of the latent variable $\mathbf{z}$, which is often high-dimensional and enters the likelihood non-linearly through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of variational inference techniques like ELBO, which will be developed in the following sections.


### Computational Challenge

This integral requires integrating over:

- All possible values of $\mathbf{z}$ (often high-dimensional)
- Non-linear transformations through neural networks

**Result:** Exact computation is intractable, motivating variational inference techniques like ELBO (developed next).

## Estimation Techniques

### Naive Monte Carlo Estimation

One natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:

$$
p(x) \approx \frac{1}{K} \sum_{j=1}^K p_\theta(x, z_j), \quad z_j \sim \text{Uniform}
$$

However, this fails in practice. For most values of $z$, the joint probability $p_\theta(x, z)$ is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely “hits” likely values of $z$.

### Importance Sampling

To address this, we use **importance sampling**, introducing a proposal distribution $q(z)$:

$$
p(x) = \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right]
$$

This gives an **unbiased estimator** of $p(x)$ if $q(z)$ is well-chosen (ideally close to $p_\theta(z|x)$). Intuitively, we sample $z$ more frequently in regions where $p_\theta(x, z)$ is high.

However, our goal is to optimize the **log-likelihood**, and the log of an expectation is not the same as the expectation of the log. That is,

$$
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \neq \mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]
$$

This discrepancy is captured by **Jensen’s Inequality**, which tells us:

$$
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \geq \underbrace{\mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]}_{\text{ELBO}}
$$

The right-hand side provides a tractable lower bound on the log-likelihood and is referred to as the **Evidence Lower Bound (ELBO)**. Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood. In the next section, we derive this bound formally and explore its components in detail.


## Why Variational Inference?

Computing the true posterior distribution $p(z|x)$ is intractable in most cases because it requires evaluating the marginal likelihood:
\[
    p(x) = \int p(x, z) \, dz
\]
This integral is hard to compute due to the high dimensionality and complexity of $p(x, z)$.

Variational inference tackles this problem by introducing a tractable, parameterized distribution $q(z)$ to approximate $p(z|x)$. The goal is to make $q(z)$ as close as possible to the true posterior by minimizing their KL divergence:
\[
    D_{KL}(q(z) \| p(z|x))
\]

This approach converts the inference problem into an optimization problem. A key result from this framework is the Evidence Lower Bound (ELBO), which provides a lower bound on the log-likelihood of the data:

\[
    \text{ELBO} = \mathbb{E}_{q_\phi(z|x)} \left[ \log p_\theta(x|z) \right] - \text{KL}(q_\phi(z|x) \| p(z))
\]

This ELBO can be optimized as a surrogate objective to approximate the log-likelihood.



\subsection*{Interpretation of the ELBO}

The ELBO has two parts:

\begin{itemize}
    \item \textbf{Reconstruction Term:} $\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$ — encourages the decoder to reconstruct input $x$ accurately from the latent variable $z$.
    \item \textbf{Regularization Term:} $\mathrm{KL}(q_\phi(z|x) \| p(z))$ — ensures that the approximate posterior stays close to the prior, preventing overfitting and encouraging smooth latent spaces.
\end{itemize}

In simple terms, the ELBO helps VAEs learn a balance between making good reconstructions and maintaining a well-behaved, compact latent space.


## Core Derivation

### Step 1: KL Divergence Objective
\begin{equation}
D_{KL}(q(z)\|p(z|x; \theta)) = \sum_z q(z) \log \frac{q(z)}{p(z|x; \theta)}
\end{equation}

### Step 2: Apply Bayes' Rule
Substitute $p(z|x; \theta) = \frac{p(z,x;\theta)}{p(x;\theta)}$:
\begin{equation}
= \sum_z q(z) \log \left( \frac{q(z) \cdot p(x; \theta)}{p(z, x; \theta)} \right)
\end{equation}

### Step 3: Decompose Terms
\begin{align}
&= \sum_z q(z) \log q(z) + \sum_z q(z) \log p(x; \theta) \nonumber \\
&\quad - \sum_z q(z) \log p(z, x; \theta) \\
&= -H(q) + \log p(x; \theta) - \mathbb{E}_q[\log p(z,x;\theta)]
\end{align}

### Step 4: Rearrange for ELBO
\begin{equation}
\log p(x;\theta) = \underbrace{\mathbb{E}_q[\log p(z,x;\theta)] + H(q)}_{\text{ELBO}} + D_{KL}(q\|p)
\end{equation}

## Key Results

1. **Evidence Lower Bound (ELBO)**:
   \begin{equation}
   \mathcal{L}(\theta,\phi) = \mathbb{E}_{q(z;\phi)}[\log p(x,z;\theta)] + H(q(z;\phi))
   \end{equation}

2. **Optimization**:
   \begin{equation}
   \max_{\theta,\phi} \mathcal{L}(\theta,\phi) \Rightarrow 
   \begin{cases}
   \text{Maximizes data likelihood} \\
   \text{Minimizes } D_{KL}(q\|p)
   \end{cases}
   \end{equation}

## Practical Implications

- **For $q(z)$**: Choose simple distributions (e.g., Gaussian)
- **For $\phi$**: Use gradient ascent on $\mathcal{L}$
- **For VAEs**: $q(z|x;\phi)$ becomes the encoder network

\section*{Applications of VAEs}

Variational Autoencoders are widely used in:

## Applications of VAEs

- **Image Generation**: VAEs can generate new images similar to the training data (e.g., MNIST digits)  
- **Anomaly Detection**: High reconstruction error flags unusual data points  
- **Representation Learning**: Latent space captures features for downstream tasks 