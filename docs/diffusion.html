<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Diffusion Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> 
<span class="menu-text">Changez Akram</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-generative-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Generative Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-generative-models">    
        <li>
    <a class="dropdown-item" href="./vae.html">
 <span class="dropdown-text">Variational Autoencoders (VAEs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./flows.html">
 <span class="dropdown-text">Normalizing Flows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ebm.html">
 <span class="dropdown-text">Energy-Based Models (EBMs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./diffusion.html">
 <span class="dropdown-text">Diffusion Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-large-language-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Large Language Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-large-language-models">    
        <li>
    <a class="dropdown-item" href="./coming-soon.html">
 <span class="dropdown-text">Recurrent Neural Networks [Coming Soon]</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./coming-soon.html">
 <span class="dropdown-text">Transformers [Coming Soon]</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math-review" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math Review</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math-review">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/calculus.html">
 <span class="dropdown-text">Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/probability.html">
 <span class="dropdown-text">Probability</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#forward-diffusion-process" id="toc-forward-diffusion-process" class="nav-link" data-scroll-target="#forward-diffusion-process"><span class="header-section-number">2</span> Forward Diffusion Process</a></li>
  <li><a href="#reverse-diffusion-process" id="toc-reverse-diffusion-process" class="nav-link" data-scroll-target="#reverse-diffusion-process"><span class="header-section-number">3</span> Reverse Diffusion Process</a></li>
  <li><a href="#training-understanding-the-elbo" id="toc-training-understanding-the-elbo" class="nav-link" data-scroll-target="#training-understanding-the-elbo"><span class="header-section-number">4</span> Training: Understanding the ELBO</a></li>
  <li><a href="#sampling-from-the-reverse-process" id="toc-sampling-from-the-reverse-process" class="nav-link" data-scroll-target="#sampling-from-the-reverse-process"><span class="header-section-number">5</span> Sampling from the Reverse Process</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Diffusion Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Diffusion models are a powerful class of generative models that learn to create data—such as images—by reversing a gradual noising process. During training, real data is progressively corrupted by adding small amounts of Gaussian noise over many steps until it becomes nearly indistinguishable from pure noise. A neural network is then trained to learn the reverse process: transforming noise back into realistic samples, one step at a time.</p>
<p>This approach has enabled state-of-the-art results in image generation, powering tools like <strong>DALL·E 2</strong>, <strong>Imagen</strong>, and <strong>Stable Diffusion</strong>. One of the key advantages of diffusion models lies in their training stability and output quality, especially when compared to earlier generative approaches:</p>
<ul>
<li><strong>GANs</strong> generate sharp images but rely on adversarial training, which can be unstable and prone to mode collapse.</li>
<li><strong>VAEs</strong> are more stable but often produce blurry outputs due to their reliance on Gaussian assumptions and variational approximations.</li>
<li><strong>Normalizing Flows</strong> provide exact log-likelihoods and stable training but require invertible architectures, which limit model expressiveness.</li>
<li><strong>Diffusion models</strong> avoid adversarial dynamics and use a simple denoising objective. This makes them easier to train and capable of producing highly detailed and diverse samples.</li>
</ul>
<p>This combination of <strong>theoretical simplicity</strong>, <strong>training robustness</strong>, and <strong>high-quality outputs</strong> has made diffusion models one of the most effective generative modeling techniques in use today.</p>
<section id="connection-to-vaes" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="connection-to-vaes"><span class="header-section-number">1.1</span> Connection to VAEs</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 36%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>VAEs</th>
<th>Diffusion Models</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Forward process</td>
<td>Learned encoder <span class="math inline">\(q_\phi(z \mid x)\)</span></td>
<td>Fixed noising process <span class="math inline">\(q(x_t \mid x_{t-1})\)</span></td>
</tr>
<tr class="even">
<td>Reverse process</td>
<td>Learned decoder <span class="math inline">\(p_\theta(x \mid z)\)</span></td>
<td>Learned denoising network <span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span></td>
</tr>
<tr class="odd">
<td>Training objective</td>
<td>Optimize ELBO over latent variables</td>
<td>Optimize ELBO via noise prediction loss</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="forward-diffusion-process" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="forward-diffusion-process"><span class="header-section-number">2</span> Forward Diffusion Process</h2>
<p>The forward diffusion process gradually turns a data sample (such as an image) into pure noise by adding a little bit of random noise at each step. This process is a Markov chain, meaning each step depends only on the previous one.</p>
<section id="start-with-a-data-sample" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="start-with-a-data-sample"><span class="header-section-number">2.1</span> Start with a Data Sample</h3>
<p>Begin with a data point <span class="math inline">\(x_0\)</span>, sampled from dataset (such as a real image). The goal is to slowly corrupt <span class="math inline">\(x_0\)</span> by adding noise over many steps, until it becomes indistinguishable from random Gaussian noise.<br>
We’ll later see that it’s also possible to sample <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span>, without simulating every step.</p>
</section>
<section id="add-noise-recursively" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="add-noise-recursively"><span class="header-section-number">2.2</span> Add Noise Recursively</h3>
<p>At each time step <span class="math inline">\(t\)</span>, the process is defined as: <span class="math display">\[
q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) I\right)
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\alpha_t = 1 - \beta_t\)</span>, where <span class="math inline">\(\beta_t\)</span> a small positive number controlling the noise level at step <span class="math inline">\(t\)</span>, while <span class="math inline">\(\alpha_t\)</span> emphasizes the <strong>amount of original signal retained</strong>.</li>
<li><span class="math inline">\(I\)</span> is the identity matrix, so noise is added independently to each component.</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Intuition:</strong> At each step, we shrink the signal and add new Gaussian noise. Over many steps, the image becomes blurrier and more like random noise.</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Why keep <span class="math inline">\(\beta_t\)</span> small?</strong><br>
Keeping <span class="math inline">\(\beta_t\)</span> small ensures that noise is added gradually. This allows the model to retain structure across steps and converge slowly to pure noise. Large values of <span class="math inline">\(\beta_t\)</span> would destroy the signal too quickly, making it harder for the reverse model to reconstruct the data. The design of the forward process balances signal decay (via <span class="math inline">\(\sqrt{\alpha_t}\)</span>) and noise growth (via <span class="math inline">\(\sqrt{1 - \alpha_t}\)</span>) to ensure a smooth, learnable transition.</p>
</blockquote>
</section>
<section id="the-markov-chain" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="the-markov-chain"><span class="header-section-number">2.3</span> The Markov Chain</h3>
<p>The full sequence is:</p>
<p><span class="math display">\[
x_0 \rightarrow x_1 \rightarrow x_2 \rightarrow \ldots \rightarrow x_T
\]</span></p>
<p>The joint probability of the sequence is:</p>
<p><span class="math display">\[
q(x_{1:T} \mid x_0) = \prod_{t=1}^{T} q(x_t \mid x_{t-1})
\]</span></p>
<p>This means we can sample the whole chain by repeatedly applying the noise step.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Insight:</strong> While the forward process defines a full Markov chain from <span class="math inline">\(x_0\)</span> to <span class="math inline">\(x_T\)</span>, we’ll soon see that it’s also possible to sample any <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span> using a closed-form Gaussian — without simulating each intermediate step.</p>
</div>
</div>
</div>
</section>
<section id="deriving-the-marginal-distribution-qx_t-mid-x_0" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="deriving-the-marginal-distribution-qx_t-mid-x_0"><span class="header-section-number">2.4</span> Deriving the Marginal Distribution <span class="math inline">\(q(x_t \mid x_0)\)</span></h3>
<p> How do we get the formula that lets us sample <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span> (without simulating all the intermediate steps)?</p>
<p></p>
<p>Let’s see how <span class="math inline">\(x_t\)</span> is built up from <span class="math inline">\(x_0\)</span>:</p>
<p>For <span class="math inline">\(t = 1\)</span>: <span class="math display">\[
x_1 = \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_1, \qquad \epsilon_1 \sim \mathcal{N}(0, I)
\]</span></p>
<p>For <span class="math inline">\(t = 2\)</span>: <span class="math display">\[
x_2 = \sqrt{\alpha_2} x_1 + \sqrt{1 - \alpha_2} \epsilon_2
\]</span> Substitute <span class="math inline">\(x_1\)</span>: <span class="math display">\[
x_2 = \sqrt{\alpha_2} \left( \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_1 \right) + \sqrt{1 - \alpha_2} \epsilon_2
\]</span> <span class="math display">\[
= \sqrt{\alpha_2 \alpha_1} x_0 + \sqrt{\alpha_2 (1 - \alpha_1)} \epsilon_1 + \sqrt{1 - \alpha_2} \epsilon_2
\]</span></p>
<p>For general <span class="math inline">\(t\)</span>, recursively expanding gives: <span class="math display">\[
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sum_{i=1}^t \left( \sqrt{ \left( \prod_{j=i+1}^t \alpha_j \right) (1 - \alpha_i) } \, \epsilon_i \right)
\]</span> where <span class="math inline">\(\bar{\alpha}_t = \prod_{i=1}^t \alpha_i\)</span>.</p>
<p>Each <span class="math inline">\(\epsilon_i\)</span> is independent Gaussian noise. The sum of independent Gaussians (each scaled by a constant) is still a Gaussian, with variance equal to the sum of the variances: <span class="math display">\[
\text{Total variance} = \sum_{i=1}^t \left( \prod_{j=i+1}^t \alpha_j \right) (1 - \alpha_i)
\]</span> This sum simplifies to: <span class="math display">\[
1 - \bar{\alpha}_t
\]</span></p>
<p>This can be proved by induction or by telescoping the sum.</p>
<p>All the little bits of noise added at each step combine into one big Gaussian noise term, with variance <span class="math inline">\(1 - \bar{\alpha}_t\)</span>.</p>
</section>
<section id="the-final-marginal-distribution" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="the-final-marginal-distribution"><span class="header-section-number">2.5</span> The Final Marginal Distribution</h3>
<p>So, we can sample <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span> using: <span class="math display">\[
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I)
\]</span></p>
<p>This lets us sample <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span>, without recursively computing all previous steps <span class="math inline">\(x_1, x_2, \dots, x_{t-1}\)</span>.</p>
<p>This means: <span class="math display">\[
q(x_t \mid x_0) = \mathcal{N}\left(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I\right)
\]</span></p>
<p>As <span class="math inline">\(t\)</span> increases, <span class="math inline">\(\bar{\alpha}_t\)</span> shrinks toward zero. Eventually, <span class="math inline">\(x_t\)</span> becomes pure noise:</p>
<p><span class="math display">\[
x_T \sim \mathcal{N}(0, I)
\]</span></p>
</section>
<section id="recap-forward-diffusion-steps" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="recap-forward-diffusion-steps"><span class="header-section-number">2.6</span> Recap: Forward Diffusion Steps</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 33%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Step</strong></th>
<th><strong>Formula</strong></th>
<th><strong>Explanation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(x_0\)</span></td>
<td>Original data sample</td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(q(x_t \mid x_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} x_{t-1}, (1-\alpha_t) I)\)</span></td>
<td>Add noise at each step</td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon\)</span></td>
<td>Directly sample <span class="math inline">\(x_t\)</span> from <span class="math inline">\(x_0\)</span> using noise <span class="math inline">\(\epsilon\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td><span class="math inline">\(q(x_t \mid x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t) I)\)</span></td>
<td>Marginal distribution at step <span class="math inline">\(t\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td><span class="math inline">\(x_T \sim \mathcal{N}(0, I)\)</span></td>
<td>After many steps, pure noise</td>
</tr>
</tbody>
</table>
</section>
<section id="key-takeaways" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">2.7</span> Key Takeaways</h3>
<ul>
<li>The forward diffusion process is just repeatedly adding noise to your data.</li>
<li>Thanks to properties of Gaussian noise, you can describe the result as the original data scaled down plus one cumulative chunk of Gaussian noise.<br>
</li>
<li>After enough steps, the data becomes indistinguishable from random noise.</li>
</ul>
<hr>
</section>
</section>
<section id="reverse-diffusion-process" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="reverse-diffusion-process"><span class="header-section-number">3</span> Reverse Diffusion Process</h2>
<p>Let’s break down the reverse diffusion process step by step. This is the <strong>generative phase</strong> of diffusion models, where we learn to turn pure noise back into data. For clarity, we’ll use the same notation as in the forward process:</p>
<ul>
<li><strong>Forward process</strong>: Gradually adds noise to data via <span class="math inline">\(q(x_t \mid x_{t-1})\)</span></li>
<li><strong>Reverse process</strong>: Gradually removes noise via <span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span>, learned by a neural network</li>
</ul>
<p><strong>The Goal of the Reverse Process</strong></p>
<p><strong>Objective</strong>: Given a noisy sample <span class="math inline">\(x_t\)</span>, we want to estimate the conditional distribution <span class="math inline">\(q(x_{t-1} \mid x_t)\)</span>. However, this is <strong>intractable</strong> because it would require knowing the true data distribution.</p>
<p>Instead, we train a neural network to approximate it: <span class="math display">\[
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
\]</span></p>
<p>Here, <span class="math inline">\(\mu_\theta(x_t, t)\)</span> is the predicted mean and <span class="math inline">\(\Sigma_\theta(x_t, t)\)</span> is the predicted covariance (often diagonal) of the reverse Gaussian distribution.</p>
<p>In practice, many diffusion models do not directly predict <span class="math inline">\(\mu_\theta\)</span> or <span class="math inline">\(x_0\)</span>, but instead predict the noise <span class="math inline">\(\epsilon\)</span> added in the forward process. This makes the objective simpler and more effective, as we’ll see in the next section.</p>
<p><strong>Key Insight from the Forward Process</strong></p>
<p>If the noise added in the forward process is small (i.e., <span class="math inline">\(\beta_t \ll 1\)</span>), then the reverse conditional <span class="math inline">\(q(x_{t-1} \mid x_t)\)</span> is also Gaussian: <span class="math display">\[
q(x_{t-1} \mid x_t) \approx \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t), \tilde{\beta}_t I)
\]</span></p>
<p>This approximation works because the forward process adds Gaussian noise in small increments at each step. The Markov chain formed by these small Gaussian transitions ensures that local conditionals (like <span class="math inline">\(q(x_{t-1} \mid x_t)\)</span>) remain Gaussian under mild assumptions.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Glossary of Symbols">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Glossary of Symbols
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong><span class="math inline">\(\alpha_t\)</span></strong>: Variance-preserving noise coefficient at step <span class="math inline">\(t\)</span></li>
<li><strong><span class="math inline">\(\bar{\alpha}_t\)</span></strong>: Cumulative product of <span class="math inline">\(\alpha_t\)</span>, i.e., <span class="math inline">\(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\)</span></li>
<li><strong><span class="math inline">\(\beta_t\)</span></strong>: Variance of the noise added at step <span class="math inline">\(t\)</span>, typically <span class="math inline">\(\beta_t = 1 - \alpha_t\)</span></li>
<li><strong><span class="math inline">\(x_0\)</span></strong>: Original clean data sample (e.g., image)</li>
<li><strong><span class="math inline">\(x_t\)</span></strong>: Noisy version of <span class="math inline">\(x_0\)</span> at timestep <span class="math inline">\(t\)</span></li>
<li><strong><span class="math inline">\(\epsilon\)</span></strong>: Standard Gaussian noise sampled from <span class="math inline">\(\mathcal{N}(0, I)\)</span></li>
<li><strong><span class="math inline">\(\tilde{\mu}_t\)</span></strong>: Mean of the reverse process distribution at time <span class="math inline">\(t\)</span></li>
<li><strong><span class="math inline">\(\tilde{\beta}_t\)</span></strong>: Variance of the reverse process distribution at time <span class="math inline">\(t\)</span></li>
</ul>
</div>
</div>
<section id="deriving-qx_t-1-mid-x_t-x_0-using-bayes-rule" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="deriving-qx_t-1-mid-x_t-x_0-using-bayes-rule"><span class="header-section-number">3.1</span> Deriving <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span> Using Bayes’ Rule</h3>
<p>We can’t directly evaluate <span class="math inline">\(q(x_{t-1} \mid x_t)\)</span>, but we can derive the <strong>posterior</strong> <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span> using Bayes’ rule:</p>
<p><span class="math display">\[
q(x_{t-1} \mid x_t, x_0) = \frac{q(x_t \mid x_{t-1}, x_0) \cdot q(x_{t-1} \mid x_0)}{q(x_t \mid x_0)}
\]</span></p>
<p>From the forward process, we know:</p>
<ul>
<li><span class="math inline">\(q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1},\, \beta_t I)\)</span><br>
</li>
<li><span class="math inline">\(q(x_{t-1} \mid x_0) = \mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} x_0,\, (1 - \bar{\alpha}_{t-1}) I)\)</span><br>
</li>
<li><span class="math inline">\(q(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0,\, (1 - \bar{\alpha}_t) I)\)</span></li>
</ul>
<p>To derive a usable form of the posterior, we substitute the <strong>Gaussian densities</strong> into Bayes’ rule. The multivariate normal density is:</p>
<p><span class="math display">\[
\mathcal{N}(x \mid \mu, \Sigma) \propto \exp\left( -\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu) \right)
\]</span></p>
<p>Since all covariances here are multiples of the identity matrix, <span class="math inline">\(\Sigma = \sigma^2 I\)</span>, the formula simplifies to:</p>
<p><span class="math display">\[
\mathcal{N}(x \mid \mu, \sigma^2 I) \propto \exp\left( -\frac{1}{2\sigma^2} \|x - \mu\|^2 \right)
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Understanding the squared norm">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Understanding the squared norm
</div>
</div>
<div class="callout-body-container callout-body">
<p>The expression <span class="math inline">\(\|x - \mu\|^2\)</span> is the squared distance between two vectors. In 1D, it’s just <span class="math inline">\((x - \mu)^2\)</span>, but in higher dimensions, it becomes:</p>
<p><span class="math display">\[
\|x - \mu\|^2 = \sum_{i=1}^d (x_i - \mu_i)^2
\]</span></p>
<p>This term appears in the exponent of the Gaussian and represents how far the sample is from the center (mean), scaled by the variance.</p>
</div>
</div>
<p>Applying this to the forward process terms:</p>
<ul>
<li><span class="math inline">\(q(x_t \mid x_{t-1}) \propto \exp\left( -\frac{1}{2\beta_t} \| x_t - \sqrt{\alpha_t} x_{t-1} \|^2 \right)\)</span><br>
</li>
<li><span class="math inline">\(q(x_{t-1} \mid x_0) \propto \exp\left( -\frac{1}{2(1 - \bar{\alpha}_{t-1})} \| x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0 \|^2 \right)\)</span></li>
</ul>
<p>We can ignore <span class="math inline">\(q(x_t \mid x_0)\)</span> in the denominator, since it is independent of <span class="math inline">\(x_{t-1}\)</span> and will be absorbed into a proportionality constant.</p>
<p>Putting these together:</p>
<p><span class="math display">\[
q(x_{t-1} \mid x_t, x_0) \propto \exp\left(
-\frac{1}{2} \left[
\frac{ \|x_t - \sqrt{\alpha_t} x_{t-1} \|^2 }{\beta_t} +
\frac{ \| x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0 \|^2 }{1 - \bar{\alpha}_{t-1}}
\right]
\right)
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Why does the product of Gaussians give another Gaussian?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why does the product of Gaussians give another Gaussian?
</div>
</div>
<div class="callout-body-container callout-body">
<p>When we multiply two Gaussian distributions over the same variable, the result is also a Gaussian.</p>
<p>Here, we are multiplying two Gaussians in <span class="math inline">\(x_{t-1}\)</span>:<br>
- One centered at <span class="math inline">\(\sqrt{\alpha_t} x_t\)</span><br>
- One centered at <span class="math inline">\(\sqrt{\bar{\alpha}_{t-1}} x_0\)</span></p>
<p>The product is another Gaussian in <span class="math inline">\(x_{t-1}\)</span>, with a new mean that is a <strong>weighted average</strong> of both.<br>
We’ll derive this explicitly by completing the square in the exponent.</p>
</div>
</div>
<blockquote class="blockquote">
<p>Although we won’t use this posterior directly during sampling, this closed-form expression is essential for defining the ELBO used in training. It gives us a precise target that the reverse model attempts to approximate.</p>
</blockquote>
<p>We now complete the square to put the expression into standard Gaussian form.</p>
<hr>
</section>
<section id="complete-the-square" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="complete-the-square"><span class="header-section-number">3.2</span> Complete the square</h3>
<p>To express the exponent in Gaussian form, we’ll complete the square using the identity:<br>
<span class="math display">\[
a x^2 - 2 b x = a \left( x - \frac{b}{a} \right)^2 - \frac{b^2}{a}
\]</span></p>
<p>From earlier, we arrived at this expression for the exponent of the posterior:<br>
<span class="math display">\[
-\frac{1}{2} \left[
\frac{(x_t - \sqrt{\alpha_t} \, x_{t-1})^2}{\beta_t} +
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \, x_0)^2}{1 - \bar{\alpha}_{t-1}}
\right]
\]</span></p>
<p>We expand both terms:</p>
<p><strong>First term:</strong><br>
<span class="math display">\[
\frac{(x_t - \sqrt{\alpha_t} \, x_{t-1})^2}{\beta_t}
= \frac{x_t^2 - 2 \sqrt{\alpha_t} \, x_t x_{t-1} + \alpha_t x_{t-1}^2}{\beta_t}
\]</span></p>
<p><strong>Second term:</strong><br>
<span class="math display">\[
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \, x_0)^2}{1 - \bar{\alpha}_{t-1}}
= \frac{x_{t-1}^2 - 2 \sqrt{\bar{\alpha}_{t-1}} \, x_{t-1} x_0 + \bar{\alpha}_{t-1} x_0^2}{1 - \bar{\alpha}_{t-1}}
\]</span></p>
<p><strong>Group like terms</strong></p>
<p>Now we collect all the terms involving <span class="math inline">\(x_{t-1}\)</span>:</p>
<p><strong>Coefficient of <span class="math inline">\(x_{t-1}^2\)</span>:</strong><br>
<span class="math display">\[
a = \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}
\]</span></p>
<p><strong>Coefficient of <span class="math inline">\(x_{t-1}\)</span> (the full linear term):</strong><br>
<span class="math display">\[
-2 \left(
\frac{ \sqrt{\alpha_t} \, x_t }{ \beta_t } + \frac{ \sqrt{\bar{\alpha}_{t-1}} \, x_0 }{ 1 - \bar{\alpha}_{t-1} }
\right)
\]</span></p>
<p>So we define:<br>
<span class="math display">\[
b = \frac{ \sqrt{\alpha_t} \, x_t }{ \beta_t } + \frac{ \sqrt{\bar{\alpha}_{t-1}} \, x_0 }{ 1 - \bar{\alpha}_{t-1} }
\]</span></p>
<p>Remaining terms (like <span class="math inline">\(x_t^2\)</span> and <span class="math inline">\(x_0^2\)</span>) are independent of <span class="math inline">\(x_{t-1}\)</span> and can be absorbed into a constant.</p>
<p>We are modeling the conditional distribution <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span>, which means both <span class="math inline">\(x_t\)</span> and <span class="math inline">\(x_0\)</span> are known and fixed. So any expression involving only <span class="math inline">\(x_t\)</span> or <span class="math inline">\(x_0\)</span> behaves like a constant and does not influence the shape of the Gaussian over <span class="math inline">\(x_{t-1}\)</span>.</p>
<p>The exponent now has the form:<br>
<span class="math display">\[
-\frac{1}{2} \left( a x_{t-1}^2 - 2 b x_{t-1} \right) + \text{(constants)}
\]</span></p>
<p><strong>Apply the identity</strong></p>
<p>Using the identity: <span class="math display">\[
a x^2 - 2 b x = a \left( x - \frac{b}{a} \right)^2 - \frac{b^2}{a}
\]</span></p>
<p>we rewrite the exponent: <span class="math display">\[
-\frac{1}{2} \left( a x_{t-1}^2 - 2 b x_{t-1} \right)
= -\frac{1}{2} \left[ a \left( x_{t-1} - \frac{b}{a} \right)^2 - \frac{b^2}{a} \right]
\]</span></p>
<p>We drop the constant term <span class="math inline">\(\frac{b^2}{a}\)</span> under proportionality. This transforms the exponent into the Gaussian form: <span class="math display">\[
q(x_{t-1} \mid x_t, x_0) \propto \exp\left(
- \frac{1}{2 \tilde{\beta}_t} \| x_{t-1} - \tilde{\mu}_t \|^2
\right)
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Note: This matches the standard Gaussian">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note: This matches the standard Gaussian
</div>
</div>
<div class="callout-body-container callout-body">
<p>The standard Gaussian is written as: <span class="math display">\[
\mathcal{N}(x \mid \mu, \sigma^2 I) \propto \exp\left(
- \frac{1}{2\sigma^2} \| x - \mu \|^2
\right)
\]</span></p>
<p>So in our case:</p>
<ul>
<li><span class="math inline">\(\tilde{\mu}_t = \frac{b}{a}\)</span> is the <strong>mean</strong></li>
<li><span class="math inline">\(\tilde{\beta}_t = \frac{1}{a}\)</span> is the <strong>variance</strong></li>
</ul>
<p>We keep the notation <span class="math inline">\(\tilde{\beta}_t\)</span> instead of <span class="math inline">\(\sigma^2\)</span> because it connects directly to the <strong>noise schedule</strong> (<span class="math inline">\(\beta_t\)</span>, <span class="math inline">\(\bar{\alpha}_t\)</span>) used in the diffusion model. This helps tie everything back to how the forward and reverse processes relate.</p>
</div>
</div>
<p><strong>Final expressions</strong></p>
<p>Now we can directly read off the expressions for the mean and variance from the completed square.</p>
<p>We had: <span class="math display">\[
a = \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}, \quad
b = \frac{\sqrt{\alpha_t} \, x_t}{\beta_t} + \frac{\sqrt{\bar{\alpha}_{t-1}} \, x_0}{1 - \bar{\alpha}_{t-1}}
\]</span></p>
<p>From the identity: <span class="math display">\[
q(x_{t-1} \mid x_t, x_0) \propto \exp\left(
- \frac{1}{2 \tilde{\beta}_t} \| x_{t-1} - \tilde{\mu}_t \|^2
\right)
\]</span></p>
<p>we identify: - <span class="math inline">\(\tilde{\mu}_t = \frac{b}{a}\)</span>, - <span class="math inline">\(\tilde{\beta}_t = \frac{1}{a}\)</span></p>
<p>Let’s compute these explicitly:</p>
<p><strong>Mean:</strong> <span class="math display">\[
\tilde{\mu}_t = \frac{b}{a} =
\frac{
\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t +
\sqrt{\bar{\alpha}_{t-1}} \beta_t x_0
}{
1 - \bar{\alpha}_t
}
\]</span></p>
<p><strong>Variance:</strong> <span class="math display">\[
\tilde{\beta}_t = \frac{1}{a}
= \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t
\]</span></p>
<p>So the final expression for the posterior becomes: <span class="math display">\[
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(x_{t-1};\, \tilde{\mu}_t,\, \tilde{\beta}_t I)
\]</span></p>
</section>
<section id="parameterizing-the-reverse-process" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="parameterizing-the-reverse-process"><span class="header-section-number">3.3</span> Parameterizing the Reverse Process</h3>
<p>During training, we can compute the posterior exactly because <span class="math inline">\(x_0\)</span> is known. But at sampling time, we don’t have access to <span class="math inline">\(x_0\)</span>, so we must express everything in terms of the current noisy sample <span class="math inline">\(x_t\)</span> and the model’s prediction of noise <span class="math inline">\(\epsilon\)</span>.</p>
<p>We start from the forward noising equation:</p>
<p><span class="math display">\[
x_t = \sqrt{\bar{\alpha}_t} \, x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon
\]</span></p>
<p>This expresses how noise is added to the clean image <span class="math inline">\(x_0\)</span> to produce the noisy observation <span class="math inline">\(x_t\)</span>.</p>
<p>We rearrange this to solve for <span class="math inline">\(x_0\)</span> in terms of <span class="math inline">\(x_t\)</span> and <span class="math inline">\(\epsilon\)</span>:</p>
<p><span class="math display">\[
x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \, \epsilon}{\sqrt{\bar{\alpha}_t}}
\]</span></p>
<p>Now we substitute this into the posterior mean expression <span class="math inline">\(\tilde{\mu}_t\)</span>, which originally depended on <span class="math inline">\(x_0\)</span>:</p>
<p><span class="math display">\[
\tilde{\mu}_t =
\frac{
\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t +
\sqrt{\bar{\alpha}_{t-1}} \beta_t x_0
}{
1 - \bar{\alpha}_t
}
\]</span></p>
<p>Substituting <span class="math inline">\(x_0\)</span> into this gives:</p>
<p><span class="math display">\[
\tilde{\mu}_t =
\frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \, \epsilon \right)
\]</span></p>
<p>This allows us to compute the mean of the reverse process using only <span class="math inline">\(x_t\)</span>, <span class="math inline">\(\epsilon\)</span>, and known scalars from the noise schedule.</p>
<ul>
<li><span class="math inline">\(\epsilon\)</span> is the noise that was added to <span class="math inline">\(x_0\)</span> to get <span class="math inline">\(x_t\)</span></li>
<li>At test time, we use the model’s prediction <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span> in its place</li>
</ul>
</section>
<section id="recap-reverse-diffusion-steps" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="recap-reverse-diffusion-steps"><span class="header-section-number">3.4</span> Recap: Reverse Diffusion Steps</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 32%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Step</th>
<th>Formula</th>
<th>Explanation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span></td>
<td>True posterior used during training (when <span class="math inline">\(x_0\)</span> is known)</td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(\tilde{\mu}_t = \dfrac{1}{\sqrt{\alpha_t}} \left( x_t - \dfrac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \, \epsilon \right)\)</span></td>
<td>Posterior mean rewritten using <span class="math inline">\(x_t\)</span> and noise</td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(\epsilon \approx \epsilon_\theta(x_t, t)\)</span></td>
<td>At test time, model predicts the noise</td>
</tr>
<tr class="even">
<td>4</td>
<td><span class="math inline">\(p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(\tilde{\mu}_t, \tilde{\beta}_t I)\)</span></td>
<td>Reverse step sampled from model’s predicted mean and fixed variance</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="training-understanding-the-elbo" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="training-understanding-the-elbo"><span class="header-section-number">4</span> Training: Understanding the ELBO</h2>
<p><strong>What is the Goal?</strong> The ultimate goal in diffusion models is to train the neural network so that it can reverse the noising process. In other words, we want the network to learn how to turn random noise back into realistic data (like images). But how do we actually train the network? We need a loss function—a way to measure how good or bad the network’s predictions are, so we can improve it.</p>
<section id="what-is-the-elbo" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="what-is-the-elbo"><span class="header-section-number">4.1</span> What is the ELBO?</h3>
<p>The ELBO is a <strong>lower bound</strong> on the log-likelihood of the data. Maximizing the ELBO is equivalent to maximizing the likelihood that the model can generate the training data. For diffusion models, the ELBO ensures that the reverse process (denoising) aligns with the forward process (noising).</p>
</section>
<section id="deriving-the-elbo-for-diffusion-models" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="deriving-the-elbo-for-diffusion-models"><span class="header-section-number">4.2</span> Deriving the ELBO for Diffusion Models</h3>
<p><strong>Goal:</strong><br>
We want to maximize the log-likelihood of the data:</p>
<p><span class="math display">\[
\log p_\theta(x_0)
\]</span></p>
<p>where <span class="math inline">\(x_0\)</span> is a clean data sample (e.g., an image).</p>
<p><strong>Problem:</strong><br>
Computing <span class="math inline">\(\log p_\theta(x_0)\)</span> directly is <strong>intractable</strong> because it involves integrating over all possible noisy intermediate states <span class="math inline">\(x_{1:T}\)</span>.</p>
<p><strong>Solution:</strong><br>
Use <strong>Jensen’s Inequality</strong> to derive a lower bound (the ELBO) that we can optimize instead.</p>
</section>
<section id="full-derivation-step-by-step" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="full-derivation-step-by-step"><span class="header-section-number">4.3</span> Full Derivation (Step-by-Step)</h3>
<p><strong>Step 1: Start with the log-likelihood</strong></p>
<p><span class="math display">\[
\log p_\theta(x_0) = \log \int p_\theta(x_{0:T}) \, dx_{1:T}
\]</span></p>
<p><strong>Step 2: Introduce the forward process <span class="math inline">\(q(x_{1:T} \mid x_0)\)</span></strong></p>
<p>Multiply and divide by the fixed forward process:</p>
<p><span class="math display">\[
\log p_\theta(x_0) = \log \int \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} q(x_{1:T} \mid x_0) \, dx_{1:T}
\]</span></p>
<p><strong>Step 3: Rewrite as an expectation</strong></p>
<p><span class="math display">\[
\log p_\theta(x_0) = \log \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} \right]
\]</span></p>
<p><strong>Step 4: Apply Jensen’s Inequality</strong></p>
<p><span class="math display">\[
\log p_\theta(x_0) \geq \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \log \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} \right]
\]</span></p>
<p><strong>Step 5: Expand <span class="math inline">\(p_\theta(x_{0:T})\)</span> and <span class="math inline">\(q(x_{1:T} \mid x_0)\)</span></strong></p>
<p>The reverse (generative) process is:</p>
<p><span class="math display">\[
p_\theta(x_{0:T}) = p(x_T) \cdot \prod_{t=1}^T p_\theta(x_{t-1} \mid x_t)
\]</span></p>
<p>The forward (noising) process is:</p>
<p><span class="math display">\[
q(x_{1:T} \mid x_0) = \prod_{t=1}^T q(x_t \mid x_{t-1})
\]</span></p>
<p>Substitute both into the ELBO:</p>
<p><span class="math display">\[
\text{ELBO} = \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[
\log \left(
\frac{p(x_T) \cdot \prod_{t=1}^T p_\theta(x_{t-1} \mid x_t)}
     {\prod_{t=1}^T q(x_t \mid x_{t-1})}
\right)
\right]
\]</span></p>
<p>Split the logarithm:</p>
<p><span class="math display">\[
\text{ELBO} = \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[
\log p(x_T)
+ \sum_{t=1}^T \log p_\theta(x_{t-1} \mid x_t)
- \sum_{t=1}^T \log q(x_t \mid x_{t-1})
\right]
\]</span></p>
<p>Group the terms:</p>
<p><span class="math display">\[
\text{ELBO} = \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[
\log p(x_T)
+ \sum_{t=1}^T \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})}
\right]
\]</span></p>
<p><strong>Step 6: Decompose the ELBO</strong></p>
<p>We now break down the <strong>Evidence Lower Bound (ELBO)</strong> into three interpretable components:</p>
<ul>
<li>The <strong>prior loss</strong> — how well the final noisy sample matches the prior<br>
</li>
<li>The <strong>denoising KL terms</strong> — how well the model learns to denoise at each timestep<br>
</li>
<li>The <strong>reconstruction loss</strong> — how well the model recovers the original input</li>
</ul>
<p><strong>ELBO Expression from Previous Step</strong></p>
<p><span class="math display">\[
= \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[
\log p(x_T) + \sum_{t=1}^T \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})}
\right]
\]</span></p>
<p><strong>Isolating the Reconstruction Term</strong></p>
<p>The case for <span class="math inline">\(t = 1\)</span> is special: it’s the step where the model tries to reconstruct <span class="math inline">\(x_0\)</span> from <span class="math inline">\(x_1\)</span>. So we isolate it from the rest of the trajectory-based KL terms.</p>
<p><span class="math display">\[
= \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[
\log p(x_T)
+ \sum_{t=2}^T \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})}
+ \log \frac{p_\theta(x_0 \mid x_1)}{q(x_1 \mid x_0)}
\right]
\]</span></p>
<p><strong>Rewriting Using the Known Forward Process</strong></p>
<p>The forward process gives us a complete description of how noise is added to data. Because of this, we can calculate the exact probability of earlier steps given later ones. In particular, since both <span class="math inline">\(x_t\)</span> and <span class="math inline">\(x_0\)</span> are known during training, we can compute the true backward distribution <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span>. This lets us directly compare it to the model’s learned reverse process <span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span>.</p>
<p>This gives:</p>
<p><span class="math display">\[
= \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[
\log p(x_T)
+ \sum_{t=2}^T \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_{t-1} \mid x_t, x_0)}
+ \log p_\theta(x_0 \mid x_1)
- \log q(x_1 \mid x_0)
\right]
\]</span></p>
<p>The last term, <span class="math inline">\(\log q(x_1 \mid x_0)\)</span>, comes from the known forward process and does not depend on the model parameters. Since it stays constant during training, we drop it from the objective and retain the remaining three terms.</p>
<p>The first two log-ratios can now be rewritten as KL divergences, and the third term becomes a standard reconstruction loss.</p>
<p><strong>Rewriting the First Term as a KL Divergence</strong></p>
<p>We begin with the first term from the ELBO expression:</p>
<p><span class="math display">\[
\mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \log p(x_T) \right]
\]</span></p>
<p>Since this expectation only involves <span class="math inline">\(x_T\)</span>, we can simplify it as:</p>
<p><span class="math display">\[
\mathbb{E}_{q(x_T \mid x_0)} \left[ \log p(x_T) \right]
\]</span></p>
<p>Now recall the definition of KL divergence between two distributions <span class="math inline">\(q(x)\)</span> and <span class="math inline">\(p(x)\)</span>:</p>
<p><span class="math display">\[
D_{\text{KL}}(q(x) \,\|\, p(x)) = \mathbb{E}_{q(x)} \left[ \log \frac{q(x)}{p(x)} \right]
= \mathbb{E}_{q(x)} [\log q(x)] - \mathbb{E}_{q(x)} [\log p(x)]
\]</span></p>
<p>Rearranging this gives:</p>
<p><span class="math display">\[
\mathbb{E}_{q(x)} [\log p(x)] = -D_{\text{KL}}(q(x) \,\|\, p(x)) + \mathbb{E}_{q(x)} [\log q(x)]
= -D_{\text{KL}}(q(x) \,\|\, p(x)) + \mathbb{H}[q(x)]
\]</span></p>
<p>Applying this identity to <span class="math inline">\(q(x_T \mid x_0)\)</span> — which is analytically tractable due to the known forward process — and the prior <span class="math inline">\(p(x_T)\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}_{q(x_T \mid x_0)} [\log p(x_T)] = -D_{\text{KL}}(q(x_T \mid x_0) \,\|\, p(x_T)) + \mathbb{H}[q(x_T \mid x_0)]
\]</span></p>
<p>Since <span class="math inline">\(q(x_T \mid x_0)\)</span> is part of the fixed forward process, its entropy <span class="math inline">\(\mathbb{H}[q(x_T \mid x_0)]\)</span> is independent of model parameters and can be ignored during training. So we drop it:</p>
<p><span class="math display">\[
\mathbb{E}_{q(x_T \mid x_0)} [\log p(x_T)]
\approx -D_{\text{KL}}(q(x_T \mid x_0) \parallel p(x_T))
\quad \text{(ignoring constant entropy term)}
\]</span></p>
<p>This shows that the first term in the ELBO corresponds to <span class="math inline">\(D_{\text{KL}}(q(x_T \mid x_0) \,\|\, p(x_T))\)</span>, comparing the forward process at time <span class="math inline">\(T\)</span> to the model’s prior.</p>
<p><strong>Rewriting the Second Terms as KL Divergences</strong></p>
<p>Next, we consider the sum of log-ratio terms from the ELBO expression:</p>
<p><span class="math display">\[
\sum_{t=2}^T \mathbb{E}_q \left[ \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_{t-1} \mid x_t, x_0)} \right]
\]</span></p>
<p>This expression compares two distributions:</p>
<ul>
<li><span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span>: the model’s learned reverse (denoising) process<br>
</li>
<li><span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span>: the true posterior over <span class="math inline">\(x_{t-1}\)</span> given <span class="math inline">\(x_t\)</span> and the original data <span class="math inline">\(x_0\)</span><br>
(this is computable in closed-form since the forward process is known)</li>
</ul>
<p>Now recall the definition of KL divergence:</p>
<p><span class="math display">\[
D_{\text{KL}}(q(x) \,\|\, p(x)) = \mathbb{E}_{q(x)} \left[ \log \frac{q(x)}{p(x)} \right]
\]</span></p>
<p>If we flip the log-ratio, we get:</p>
<p><span class="math display">\[
\mathbb{E}_{q(x)} \left[ \log \frac{p(x)}{q(x)} \right] = - D_{\text{KL}}(q(x) \,\|\, p(x))
\]</span></p>
<p>So each log term becomes the <strong>negative KL divergence</strong>:</p>
<p><span class="math display">\[
\mathbb{E}_q \left[ \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_{t-1} \mid x_t, x_0)} \right]
= - D_{\text{KL}}(q(x_{t-1} \mid x_t, x_0) \,\|\, p_\theta(x_{t-1} \mid x_t))
\]</span></p>
<p>Applying this for every timestep from <span class="math inline">\(t = 2\)</span> to <span class="math inline">\(T\)</span>, we get:</p>
<p><span class="math display">\[
\sum_{t=2}^T \mathbb{E}_q \left[ \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_{t-1} \mid x_t, x_0)} \right]
= - \sum_{t=2}^T D_{\text{KL}}(q(x_{t-1} \mid x_t, x_0) \,\|\, p_\theta(x_{t-1} \mid x_t))
\]</span></p>
<p>This shows that the middle terms in the ELBO can be rewritten as a sum of KL divergences between the true posterior and the model’s learned reverse process at each timestep.</p>
<p><strong>Rewriting the Third Term as a Reconstruction Loss</strong></p>
<p>The last part of the ELBO expression is:</p>
<p><span class="math display">\[
\mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) \right]
\]</span></p>
<p>This term does <strong>not</strong> involve any KL divergence — it directly corresponds to the model’s attempt to reconstruct the original input <span class="math inline">\(x_0\)</span> from <span class="math inline">\(x_1\)</span>.</p>
<ul>
<li><span class="math inline">\(x_1\)</span> is a slightly noisy version of <span class="math inline">\(x_0\)</span> (after one step of the forward process).</li>
<li><span class="math inline">\(p_\theta(x_0 \mid x_1)\)</span> is the model’s decoder — it tries to map the noisy input <span class="math inline">\(x_1\)</span> back to the clean data <span class="math inline">\(x_0\)</span>.</li>
</ul>
<p>During training, this term is treated as a <strong>standard log-likelihood</strong> loss. Since we want to maximize the ELBO, we want to maximize this log-probability — which is equivalent to <strong>minimizing the negative log-likelihood</strong>:</p>
<p><span class="math display">\[
- \log p_\theta(x_0 \mid x_1)
\]</span></p>
<p>This is why the reconstruction term appears with a <strong>positive sign in the loss</strong> (as a value we minimize), but a <strong>negative sign inside the ELBO</strong>.</p>
<p>This is referred to as the <strong>reconstruction loss</strong> in diffusion models.</p>
<p>If <span class="math inline">\(p_\theta(x_0 \mid x_1)\)</span> is modeled as a Gaussian, this term becomes a <strong>mean squared error</strong> between the predicted and true <span class="math inline">\(x_0\)</span> values.</p>
<p><strong>ELBO vs.&nbsp;Loss</strong></p>
<p>We write the ELBO as:</p>
<p><span class="math display">\[
\text{ELBO} =
\underbrace{- D_{\text{KL}}(q(x_T \mid x_0) \parallel p(x_T))}_{\mathcal{L}_T}
\quad
\underbrace{- \sum_{t=2}^T D_{\text{KL}}(q(x_{t-1} \mid x_t, x_0) \parallel p_\theta(x_{t-1} \mid x_t))}_{\mathcal{L}_{1:T-1}}
\quad
\underbrace{+ \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) \right]}_{\mathcal{L}_0}
\]</span></p>
<p>Since we minimize loss instead of maximizing ELBO, we flip the sign.</p>
<p>We write the loss as:</p>
<p><span class="math display">\[
\text{Loss} =
\underbrace{+ D_{\text{KL}}(q(x_T \mid x_0) \parallel p(x_T))}_{\mathcal{L}_T}
\quad
\underbrace{+ \sum_{t=2}^T D_{\text{KL}}(q(x_{t-1} \mid x_t, x_0) \parallel p_\theta(x_{t-1} \mid x_t))}_{\mathcal{L}_{1:T-1}}
\quad
\underbrace{- \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) \right]}_{\mathcal{L}_0}
\]</span></p>
</section>
<section id="interpreting-each-term-in-the-elbo" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="interpreting-each-term-in-the-elbo"><span class="header-section-number">4.4</span> Interpreting Each Term in the ELBO**</h3>
<p><strong>Reconstruction Loss (<span class="math inline">\(\mathcal{L}_0\)</span>)</strong><br>
- Encourages the model to reconstruct <span class="math inline">\(x_0\)</span> from the first noisy sample <span class="math inline">\(x_1\)</span><br>
- Comes from the log-probability term <span class="math inline">\(\log p_\theta(x_0 \mid x_1)\)</span><br>
- Treated as a negative log-likelihood (e.g., MSE if modeled as Gaussian)</p>
<p><strong>Prior Matching Loss (<span class="math inline">\(\mathcal{L}_T\)</span>)</strong><br>
- Penalizes mismatch between the final noisy sample <span class="math inline">\(x_T\)</span> and the prior <span class="math inline">\(p(x_T)\)</span><br>
- Comes from the KL divergence <span class="math inline">\(D_{\text{KL}}(q(x_T \mid x_0) \parallel p(x_T))\)</span><br>
- Ensures the generative process starts from pure noise</p>
<p><strong>Denoising KL Terms (<span class="math inline">\(\mathcal{L}_{1:T-1}\)</span>)</strong><br>
- Encourage the model to learn the correct reverse step at each <span class="math inline">\(t = 2\)</span> to <span class="math inline">\(T\)</span><br>
- Each term compares <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span> to the learned <span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span><br>
- Drives step-by-step denoising behavior</p>
</section>
<section id="practical-training-simplification-ddpm" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="practical-training-simplification-ddpm"><span class="header-section-number">4.5</span> Practical Training Simplification (DDPM)</h3>
<p>In practice, training with the full ELBO can be computationally expensive, and several terms can be simplified or approximated without significantly impacting performance.</p>
<ul>
<li>The prior term <span class="math inline">\(\mathcal{L}_T\)</span> is often treated as a constant (since <span class="math inline">\(p(x_T) = \mathcal{N}(0, I)\)</span> is fixed).</li>
<li>The reconstruction term <span class="math inline">\(\mathcal{L}_0\)</span> can be small and is sometimes dropped entirely.</li>
<li>The denoising KL terms <span class="math inline">\(\mathcal{L}_{1:T-1}\)</span> can be simplified by making certain modeling assumptions.</li>
</ul>
<p>These assumptions allow us to rewrite the training objective as a simple <strong>mean squared error (MSE)</strong> between the <strong>true noise <span class="math inline">\(\epsilon\)</span></strong> and the <strong>predicted noise <span class="math inline">\(\epsilon_\theta\)</span></strong>, giving us the DDPM training loss:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{simple}} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
\]</span></p>
<p>Where:<br>
- <span class="math inline">\(\epsilon\)</span> is the actual noise<br>
- <span class="math inline">\(\epsilon_\theta\)</span> is the predicted noise by the neural network</p>
<p><strong>Why?</strong> If the network can predict the noise <span class="math inline">\(\epsilon\)</span>, it can denoise <span class="math inline">\(x_t\)</span> and reverse the diffusion.</p>
<div class="callout callout-style-default callout-note callout-titled" title="How the ELBO Helps Us Understand the MSE Loss">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How the ELBO Helps Us Understand the MSE Loss
</div>
</div>
<div class="callout-body-container callout-body">
<p>Even though we end up training with a simplified MSE loss in DDPM, deriving the full ELBO helps us understand <strong>what the model is truly learning</strong> — and <strong>why</strong> the simplification works.</p>
<ul>
<li>The ELBO gives us a <strong>principled, variational objective</strong><br>
</li>
<li>It tells us that training involves matching noise distributions and reconstructing data step-by-step<br>
</li>
<li>The simplified DDPM loss is an <strong>approximation</strong> of the denoising KL terms (<span class="math inline">\(\mathcal{L}_{1:T-1}\)</span>)<br>
</li>
<li>Other terms (like <span class="math inline">\(\mathcal{L}_T\)</span> and <span class="math inline">\(\mathcal{L}_0\)</span>) are often treated as constants or dropped for efficiency<br>
</li>
<li>Many advanced diffusion models return to the ELBO to add back or rethink these terms</li>
</ul>
<p>So the ELBO is like the blueprint — and the DDPM loss is an optimized shortcut that works because we understand the full path.</p>
</div>
</div>
</section>
<section id="the-noise-prediction-network" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="the-noise-prediction-network"><span class="header-section-number">4.6</span> The Noise Prediction Network</h3>
<p>Now that we’ve seen how the ELBO leads to a simplified MSE loss in DDPM, let’s understand the network that’s trained to minimize it — the noise predictor <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span>.</p>
<p><span class="math display">\[
\mathcal{L}_{\text{simple}} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
\]</span></p>
<p>But what exactly is this <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span>?</p>
<p><strong>What the Network Learns</strong></p>
<ul>
<li>During training, we <strong>know the true noise</strong> <span class="math inline">\(\epsilon\)</span> used to generate the noisy sample <span class="math inline">\(x_t\)</span> from <span class="math inline">\(x_0\)</span>.</li>
<li>The network <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span> is trained to <strong>predict this noise</strong>.</li>
<li>Once trained, it can “undo” the noise and help reconstruct <span class="math inline">\(x_0\)</span> at test time.</li>
</ul>
<p><strong>Why Predicting Noise Works</strong></p>
<p>Recall the forward process:</p>
<p><span class="math display">\[
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon
\]</span></p>
<p>Rearranging this gives us an estimate of the original (clean) image:</p>
<p><span class="math display">\[
x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( x_t - \sqrt{1 - \bar{\alpha}_t} \, \epsilon \right)
\]</span></p>
<p>So if the model can predict the noise <span class="math inline">\(\epsilon\)</span>, we can subtract it from the noisy input <span class="math inline">\(x_t\)</span> and recover the clean image <span class="math inline">\(x_0\)</span>.</p>
<p>This denoised estimate can then be used to compute the mean of the posterior distribution <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span> — which is the key to reversing the diffusion process step by step.</p>
</section>
<section id="architecture-note" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="architecture-note"><span class="header-section-number">4.7</span> Architecture Note</h3>
<p>The network <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span> is typically implemented as a <strong>U-Net</strong>, which takes: - A noisy image <span class="math inline">\(x_t\)</span> - A timestep <span class="math inline">\(t\)</span> (encoded using sinusoidal or learned embeddings)</p>
<p>It outputs the predicted noise <span class="math inline">\(\epsilon\)</span>.</p>
<hr>
</section>
<section id="takeaways" class="level3" data-number="4.8">
<h3 data-number="4.8" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">4.8</span> Takeaways</h3>
<ul>
<li>The ELBO provides a tractable lower bound on the data likelihood <span class="math inline">\(\log p_\theta(x_0)\)</span>, and serves as the theoretical training objective.</li>
<li>It decomposes into loss terms that align the learned reverse process with the fixed forward noising process, step by step.</li>
<li>In practice (as in DDPM), training is simplified to minimizing the mean squared error between the <strong>true noise</strong> <span class="math inline">\(\epsilon\)</span> and the <strong>predicted noise</strong> <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span>.</li>
</ul>
<hr>
</section>
</section>
<section id="sampling-from-the-reverse-process" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="sampling-from-the-reverse-process"><span class="header-section-number">5</span> Sampling from the Reverse Process</h2>
<p>Now that we’ve learned how to train the model using the ELBO, let’s understand how it generates new data at test time.</p>
<section id="recap-what-are-we-trying-to-do" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="recap-what-are-we-trying-to-do"><span class="header-section-number">5.1</span> Recap: What Are We Trying to Do?</h3>
<p>In the <strong>forward diffusion process</strong>, we gradually add noise to a data sample (like an image) over <span class="math inline">\(T\)</span> steps, eventually turning it into nearly pure Gaussian noise.</p>
<p>In the <strong>reverse process</strong>, we start from that noise and apply a learned denoising step at each time step to recover a realistic data sample. This is the generation phase.</p>
</section>
<section id="the-reverse-process-as-a-markov-chain" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="the-reverse-process-as-a-markov-chain"><span class="header-section-number">5.2</span> The Reverse Process as a Markov Chain</h3>
<p>At sampling time, we generate a new sample using the reverse process defined by:</p>
<p><span class="math display">\[
p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} \mid x_t)
\]</span></p>
<ul>
<li><span class="math inline">\(p(x_T)\)</span> is a standard Gaussian <span class="math inline">\(\mathcal{N}(0, I)\)</span>.</li>
<li>Each <span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span> is modeled as a Gaussian whose mean <span class="math inline">\(\mu_\theta(x_t, t)\)</span> is predicted by the neural network.</li>
<li>The variance may be fixed or learned (e.g., <span class="math inline">\(\beta_t\)</span>, <span class="math inline">\(\Sigma_\theta\)</span>).</li>
</ul>
<p>Each reverse step is:</p>
<p><span class="math display">\[
x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z, \quad z \sim \mathcal{N}(0, I)
\]</span></p>
<p>This recursive process transforms noise into a structured sample like an image.</p>
</section>
<section id="what-does-the-neural-network-predict" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="what-does-the-neural-network-predict"><span class="header-section-number">5.3</span> What Does the Neural Network Predict?</h3>
<p>During training, we derived the true posterior mean <span class="math inline">\(\tilde{\mu}_t\)</span> as:</p>
<p><span class="math display">\[
\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \, \epsilon \right)
\]</span></p>
<p>At test time, since we don’t have access to the true noise <span class="math inline">\(\epsilon\)</span>, the network predicts it as <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span> and substitutes it into the expression:</p>
<p><span class="math display">\[
\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \, \epsilon_\theta(x_t, t) \right)
\]</span></p>
<p>This lets the model estimate the mean of the reverse Gaussian using only <span class="math inline">\(x_t\)</span>, <span class="math inline">\(t\)</span>, and the predicted noise.</p>
</section>
<section id="sampling-procedure" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="sampling-procedure"><span class="header-section-number">5.4</span> Sampling Procedure</h3>
<p>To generate a sample, we do the following:</p>
<ol type="1">
<li><p>Sample initial noise:<br>
<span class="math inline">\(x_T \sim \mathcal{N}(0, I)\)</span></p></li>
<li><p>For <span class="math inline">\(t = T, T-1, \dots, 1\)</span>:</p>
<ul>
<li>Predict <span class="math inline">\(\mu_\theta(x_t, t)\)</span><br>
</li>
<li>Optionally use fixed or learned variance <span class="math inline">\(\sigma_t^2\)</span><br>
</li>
<li>Sample from the reverse step:<br>
<span class="math inline">\(x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z\)</span>, where <span class="math inline">\(z \sim \mathcal{N}(0, I)\)</span></li>
</ul></li>
<li><p>Return <span class="math inline">\(x_0\)</span> as the final generated sample.</p></li>
</ol>
<p>This is how diffusion models synthesize data — by gradually denoising random noise using the learned reverse process.</p>
</section>
<div class="grid">
<section id="algorithm-1-training-ddpm" class="level3 g-col-6" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="algorithm-1-training-ddpm"><span class="header-section-number">5.5</span> 5.5 Algorithm 1: Training (DDPM)</h3>
<pre><code>repeat:
    x₀ ~ q(x₀)
    t ~ Uniform({1, ..., T})
    ε ~ 𝒩(0, I)
    Take gradient step on:
        ∇θ ‖ε − εθ(√ᾱₜ x₀ + √(1−ᾱₜ) ε, t)‖²
until converged</code></pre>
<p><strong>Explanation</strong><br>
- Start with real data sample <span class="math inline">\(x_0\)</span><br>
- Add noise to create a corrupted version <span class="math inline">\(x_t\)</span><br>
- Train the model <span class="math inline">\(\epsilon_\theta\)</span> to predict the noise that was added<br>
- Minimize mean squared error between true noise and predicted noise</p>
</section>
<section id="algorithm-2-sampling-ddpm" class="level3 g-col-6" data-number="5.6">
<h3 data-number="5.6" class="anchored" data-anchor-id="algorithm-2-sampling-ddpm"><span class="header-section-number">5.6</span> 5.6 Algorithm 2: Sampling (DDPM)</h3>
<pre><code>x_T ~ 𝒩(0, I)
for t = T, ..., 1:
    z ~ 𝒩(0, I) if t &gt; 1 else z = 0
    x_{t-1} = 1/√αₜ (
        xₜ - (1−αₜ)/√(1−ᾱₜ) εθ(xₜ, t)
    ) + σₜ z
return x₀</code></pre>
<p><strong>Explanation</strong><br>
- Start with pure Gaussian noise <span class="math inline">\(x_T\)</span><br>
- For each timestep, predict the noise at that step<br>
- Remove predicted noise and add scaled Gaussian noise<br>
- Gradually denoise to get a clean sample <span class="math inline">\(x_0\)</span></p>
</section>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>