<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Diffusion Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> <i class="bi bi-house" role="img">
</i> 
<span class="menu-text">Changez Akram</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-generative-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Generative Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-generative-models">    
        <li>
    <a class="dropdown-item" href="./vae.html">
 <span class="dropdown-text">Variational Autoencoders (VAEs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./flows.html">
 <span class="dropdown-text">Normalizing Flows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ebm.html">
 <span class="dropdown-text">Energy-Based Models (EBMs)</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-large-language-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Large Language Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-large-language-models">    
        <li>
    <a class="dropdown-item" href="./coming-soon.html">
 <span class="dropdown-text">Recurrent Neural Networks [Coming Soon]</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./coming-soon.html">
 <span class="dropdown-text">Sequence to Sequence Models [Coming Soon]</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./coming-soon.html">
 <span class="dropdown-text">Transformers [Coming Soon]</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./coming-soon.html">
 <span class="dropdown-text">Benchmarking and Evaluation [Coming Soon]</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math-review" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math Review</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math-review">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/calculus.html">
 <span class="dropdown-text">Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/probability.html">
 <span class="dropdown-text">Probability</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#math-review" id="toc-math-review" class="nav-link" data-scroll-target="#math-review"><span class="header-section-number">2</span> Math Review</a></li>
  <li><a href="#back-to-the-diffusion-detailed-explanation" id="toc-back-to-the-diffusion-detailed-explanation" class="nav-link" data-scroll-target="#back-to-the-diffusion-detailed-explanation"><span class="header-section-number">3</span> Back to the Diffusion (Detailed Explanation)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Diffusion Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Diffusion models are a powerful class of generative models that learn to create data—such as images—by reversing a gradual noising process. During training, real data is progressively corrupted by adding small amounts of Gaussian noise over many steps until it becomes nearly indistinguishable from pure noise. A neural network is then trained to learn the reverse process: transforming noise back into realistic samples, one step at a time.</p>
<p>This approach has enabled state-of-the-art results in image generation, powering tools like <strong>DALL·E 2</strong>, <strong>Imagen</strong>, and <strong>Stable Diffusion</strong>. One of the key advantages of diffusion models lies in their training stability and output quality, especially when compared to earlier generative approaches:</p>
<ul>
<li><strong>GANs</strong> generate sharp images but rely on adversarial training, which can be unstable and prone to mode collapse.</li>
<li><strong>VAEs</strong> are more stable but often produce blurry outputs due to their reliance on Gaussian assumptions and variational approximations.</li>
<li><strong>Normalizing Flows</strong> provide exact log-likelihoods and stable training but require invertible architectures, which limit model expressiveness.</li>
<li><strong>Diffusion models</strong> avoid adversarial dynamics and use a simple denoising objective. This makes them easier to train and capable of producing highly detailed and diverse samples.</li>
</ul>
<p>This combination of <strong>theoretical simplicity</strong>, <strong>training robustness</strong>, and <strong>high-quality outputs</strong> has made diffusion models one of the most effective generative modeling techniques in use today.</p>
</section>
<section id="math-review" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="math-review"><span class="header-section-number">2</span> Math Review</h2>
<section id="forward-diffusion-process" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="forward-diffusion-process"><span class="header-section-number">2.1</span> Forward Diffusion Process</h3>
<p>The forward diffusion process gradually turns a data sample (such as an image) into pure noise by adding a little bit of random noise at each step. This process is a Markov chain, meaning each step depends only on the previous one.</p>
<section id="start-with-a-data-sample" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="start-with-a-data-sample"><span class="header-section-number">2.1.1</span> Start with a Data Sample</h4>
<p>Begin with a data point <span class="math inline">\(x_0\)</span>, sampled from dataset (such as a real image). The goal is to slowly corrupt <span class="math inline">\(x_0\)</span> by adding noise over many steps, until it becomes indistinguishable from random Gaussian noise.<br>
We’ll later see that it’s also possible to sample <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span>, without simulating every step.</p>
</section>
<section id="add-noise-recursively" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="add-noise-recursively"><span class="header-section-number">2.1.2</span> Add Noise Recursively</h4>
<p>At each time step <span class="math inline">\(t\)</span>, the process is defined as: <span class="math display">\[
q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) I\right)
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\alpha_t = 1 - \beta_t\)</span>, where <span class="math inline">\(\beta_t\)</span> a small positive number controlling the noise level at step <span class="math inline">\(t\)</span>, while <span class="math inline">\(\alpha_t\)</span> emphasizes the <strong>amount of original signal retained</strong>.</li>
<li><span class="math inline">\(I\)</span> is the identity matrix, so noise is added independently to each component.</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Intuition:</strong> At each step, we shrink the signal and add new Gaussian noise. Over many steps, the image becomes blurrier and more like random noise.</p>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Why keep <span class="math inline">\(\beta_t\)</span> small?</strong><br>
Keeping <span class="math inline">\(\beta_t\)</span> small ensures that noise is added gradually. This allows the model to retain structure across steps and converge slowly to pure noise. Large values of <span class="math inline">\(\beta_t\)</span> would destroy the signal too quickly, making it harder for the reverse model to reconstruct the data. The design of the forward process balances signal decay (via <span class="math inline">\(\sqrt{\alpha_t}\)</span>) and noise growth (via <span class="math inline">\(\sqrt{1 - \alpha_t}\)</span>) to ensure a smooth, learnable transition.</p>
</blockquote>
</section>
<section id="the-markov-chain" class="level4" data-number="2.1.3">
<h4 data-number="2.1.3" class="anchored" data-anchor-id="the-markov-chain"><span class="header-section-number">2.1.3</span> The Markov Chain</h4>
<p>The full sequence is:</p>
<p><span class="math display">\[
x_0 \rightarrow x_1 \rightarrow x_2 \rightarrow \ldots \rightarrow x_T
\]</span></p>
<p>The joint probability of the sequence is:</p>
<p><span class="math display">\[
q(x_{1:T} \mid x_0) = \prod_{t=1}^{T} q(x_t \mid x_{t-1})
\]</span></p>
<p>This means we can sample the whole chain by repeatedly applying the noise step.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Insight:</strong> While the forward process defines a full Markov chain from <span class="math inline">\(x_0\)</span> to <span class="math inline">\(x_T\)</span>, we’ll soon see that it’s also possible to sample any <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span> using a closed-form Gaussian — without simulating each intermediate step.</p>
</div>
</div>
</div>
</section>
<section id="deriving-the-marginal-distribution-qx_t-mid-x_0" class="level4" data-number="2.1.4">
<h4 data-number="2.1.4" class="anchored" data-anchor-id="deriving-the-marginal-distribution-qx_t-mid-x_0"><span class="header-section-number">2.1.4</span> Deriving the Marginal Distribution <span class="math inline">\(q(x_t \mid x_0)\)</span></h4>
<p> How do we get the formula that lets us sample <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span> (without simulating all the intermediate steps)?</p>
<p></p>
<p>Let’s see how <span class="math inline">\(x_t\)</span> is built up from <span class="math inline">\(x_0\)</span>:</p>
<p>For <span class="math inline">\(t = 1\)</span>: <span class="math display">\[
x_1 = \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_1, \qquad \epsilon_1 \sim \mathcal{N}(0, I)
\]</span></p>
<p>For <span class="math inline">\(t = 2\)</span>: <span class="math display">\[
x_2 = \sqrt{\alpha_2} x_1 + \sqrt{1 - \alpha_2} \epsilon_2
\]</span> Substitute <span class="math inline">\(x_1\)</span>: <span class="math display">\[
x_2 = \sqrt{\alpha_2} \left( \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_1 \right) + \sqrt{1 - \alpha_2} \epsilon_2
\]</span> <span class="math display">\[
= \sqrt{\alpha_2 \alpha_1} x_0 + \sqrt{\alpha_2 (1 - \alpha_1)} \epsilon_1 + \sqrt{1 - \alpha_2} \epsilon_2
\]</span></p>
<p>For general <span class="math inline">\(t\)</span>, recursively expanding gives: <span class="math display">\[
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sum_{i=1}^t \left( \sqrt{ \left( \prod_{j=i+1}^t \alpha_j \right) (1 - \alpha_i) } \, \epsilon_i \right)
\]</span> where <span class="math inline">\(\bar{\alpha}_t = \prod_{i=1}^t \alpha_i\)</span>.</p>
<p>Each <span class="math inline">\(\epsilon_i\)</span> is independent Gaussian noise. The sum of independent Gaussians (each scaled by a constant) is still a Gaussian, with variance equal to the sum of the variances: <span class="math display">\[
\text{Total variance} = \sum_{i=1}^t \left( \prod_{j=i+1}^t \alpha_j \right) (1 - \alpha_i)
\]</span> This sum simplifies to: <span class="math display">\[
1 - \bar{\alpha}_t
\]</span></p>
<p>This can be proved by induction or by telescoping the sum.</p>
<p>All the little bits of noise added at each step combine into one big Gaussian noise term, with variance <span class="math inline">\(1 - \bar{\alpha}_t\)</span>.</p>
</section>
<section id="the-final-marginal-distribution" class="level4" data-number="2.1.5">
<h4 data-number="2.1.5" class="anchored" data-anchor-id="the-final-marginal-distribution"><span class="header-section-number">2.1.5</span> The Final Marginal Distribution</h4>
<p>So, we can sample <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span> using: <span class="math display">\[
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I)
\]</span></p>
<p>This lets us sample <span class="math inline">\(x_t\)</span> directly from <span class="math inline">\(x_0\)</span>, without recursively computing all previous steps <span class="math inline">\(x_1, x_2, \dots, x_{t-1}\)</span>.</p>
<p>This means: <span class="math display">\[
q(x_t \mid x_0) = \mathcal{N}\left(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I\right)
\]</span></p>
<p>As <span class="math inline">\(t\)</span> increases, <span class="math inline">\(\bar{\alpha}_t\)</span> shrinks toward zero. Eventually, <span class="math inline">\(x_t\)</span> becomes pure noise:</p>
<p><span class="math display">\[
x_T \sim \mathcal{N}(0, I)
\]</span></p>
</section>
<section id="recap-forward-diffusion-steps" class="level4" data-number="2.1.6">
<h4 data-number="2.1.6" class="anchored" data-anchor-id="recap-forward-diffusion-steps"><span class="header-section-number">2.1.6</span> Recap: Forward Diffusion Steps</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 33%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Step</strong></th>
<th><strong>Formula</strong></th>
<th><strong>Explanation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(x_0\)</span></td>
<td>Original data sample</td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(q(x_t \mid x_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} x_{t-1}, (1-\alpha_t) I)\)</span></td>
<td>Add noise at each step</td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon\)</span></td>
<td>Directly sample <span class="math inline">\(x_t\)</span> from <span class="math inline">\(x_0\)</span> using noise <span class="math inline">\(\epsilon\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td><span class="math inline">\(q(x_t \mid x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t) I)\)</span></td>
<td>Marginal distribution at step <span class="math inline">\(t\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td><span class="math inline">\(x_T \sim \mathcal{N}(0, I)\)</span></td>
<td>After many steps, pure noise</td>
</tr>
</tbody>
</table>
</section>
<section id="key-takeaways" class="level4" data-number="2.1.7">
<h4 data-number="2.1.7" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">2.1.7</span> Key Takeaways</h4>
<ul>
<li>The forward diffusion process is just repeatedly adding noise to your data.</li>
<li>Thanks to properties of Gaussian noise, you can describe the result as the original data scaled down plus one cumulative chunk of Gaussian noise.<br>
</li>
<li>After enough steps, the data becomes indistinguishable from random noise.</li>
</ul>
<hr>
</section>
</section>
<section id="reverse-diffusion-process" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="reverse-diffusion-process"><span class="header-section-number">2.2</span> Reverse Diffusion Process</h3>
<p>Let’s break down the reverse diffusion process step by step. This is the <strong>generative phase</strong> of diffusion models, where we learn to turn pure noise back into data. For clarity, we’ll use the same notation as in the forward process:</p>
<ul>
<li><strong>Forward process</strong>: Gradually adds noise to data via <span class="math inline">\(q(x_t \mid x_{t-1})\)</span></li>
<li><strong>Reverse process</strong>: Gradually removes noise via <span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span>, learned by a neural network</li>
</ul>
<section id="the-goal-of-the-reverse-process" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="the-goal-of-the-reverse-process"><span class="header-section-number">2.2.1</span> The Goal of the Reverse Process</h4>
<p><strong>Objective</strong>: Given a noisy sample <span class="math inline">\(x_t\)</span>, we want to estimate the conditional distribution <span class="math inline">\(q(x_{t-1} \mid x_t)\)</span>. However, this is <strong>intractable</strong> because it would require knowing the true data distribution.</p>
<p>Instead, we train a neural network to approximate it: <span class="math display">\[
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
\]</span></p>
</section>
<section id="key-insight-from-the-forward-process" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="key-insight-from-the-forward-process"><span class="header-section-number">2.2.2</span> Key Insight from the Forward Process</h4>
<p>If the noise added in the forward process is small (i.e., <span class="math inline">\(\beta_t \ll 1\)</span>), then the reverse conditional <span class="math inline">\(q(x_{t-1} \mid x_t)\)</span> is also Gaussian: <span class="math display">\[
q(x_{t-1} \mid x_t) \approx \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t), \tilde{\beta}_t I)
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Glossary of Symbols">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Glossary of Symbols
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong><span class="math inline">\(\alpha_t\)</span></strong>: Variance-preserving noise coefficient at step <span class="math inline">\(t\)</span></li>
<li><strong><span class="math inline">\(\bar{\alpha}_t\)</span></strong>: Cumulative product of <span class="math inline">\(\alpha_t\)</span>, i.e., <span class="math inline">\(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\)</span></li>
<li><strong><span class="math inline">\(\beta_t\)</span></strong>: Variance of the noise added at step <span class="math inline">\(t\)</span>, typically <span class="math inline">\(\beta_t = 1 - \alpha_t\)</span></li>
<li><strong><span class="math inline">\(x_0\)</span></strong>: Original clean data sample (e.g., image)</li>
<li><strong><span class="math inline">\(x_t\)</span></strong>: Noisy version of <span class="math inline">\(x_0\)</span> at timestep <span class="math inline">\(t\)</span></li>
<li><strong><span class="math inline">\(\epsilon\)</span></strong>: Standard Gaussian noise sampled from <span class="math inline">\(\mathcal{N}(0, I)\)</span></li>
<li><strong><span class="math inline">\(\tilde{\mu}_t\)</span></strong>: Mean of the reverse process distribution at time <span class="math inline">\(t\)</span></li>
<li><strong><span class="math inline">\(\tilde{\beta}_t\)</span></strong>: Variance of the reverse process distribution at time <span class="math inline">\(t\)</span></li>
</ul>
</div>
</div>
</section>
</section>
<section id="deriving-qx_t-1-mid-x_t-x_0-using-bayes-rule" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="deriving-qx_t-1-mid-x_t-x_0-using-bayes-rule"><span class="header-section-number">2.3</span> Deriving <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span> Using Bayes’ Rule</h3>
<p>We can’t directly evaluate <span class="math inline">\(q(x_{t-1} \mid x_t)\)</span>, but we can derive the <strong>posterior</strong> <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span> using Bayes’ rule:</p>
<p><span class="math display">\[
q(x_{t-1} \mid x_t, x_0) = \frac{q(x_t \mid x_{t-1}, x_0) \cdot q(x_{t-1} \mid x_0)}{q(x_t \mid x_0)}
\]</span></p>
<p>From the forward process, we know:</p>
<ul>
<li><span class="math inline">\(q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1},\, \beta_t I)\)</span><br>
</li>
<li><span class="math inline">\(q(x_{t-1} \mid x_0) = \mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} x_0,\, (1 - \bar{\alpha}_{t-1}) I)\)</span><br>
</li>
<li><span class="math inline">\(q(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0,\, (1 - \bar{\alpha}_t) I)\)</span></li>
</ul>
<p>To derive a usable form of the posterior, we substitute the <strong>Gaussian densities</strong> into Bayes’ rule. The multivariate normal density is:</p>
<p><span class="math display">\[
\mathcal{N}(x \mid \mu, \Sigma) \propto \exp\left( -\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu) \right)
\]</span></p>
<p>Since all covariances here are multiples of the identity matrix, <span class="math inline">\(\Sigma = \sigma^2 I\)</span>, the formula simplifies to:</p>
<p><span class="math display">\[
\mathcal{N}(x \mid \mu, \sigma^2 I) \propto \exp\left( -\frac{1}{2\sigma^2} \|x - \mu\|^2 \right)
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Understanding the squared norm">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Understanding the squared norm
</div>
</div>
<div class="callout-body-container callout-body">
<p>The expression <span class="math inline">\(\|x - \mu\|^2\)</span> is the squared distance between two vectors. In 1D, it’s just <span class="math inline">\((x - \mu)^2\)</span>, but in higher dimensions, it becomes:</p>
<p><span class="math display">\[
\|x - \mu\|^2 = \sum_{i=1}^d (x_i - \mu_i)^2
\]</span></p>
<p>This term appears in the exponent of the Gaussian and represents how far the sample is from the center (mean), scaled by the variance.</p>
</div>
</div>
<p>Applying this to the forward process terms:</p>
<ul>
<li><span class="math inline">\(q(x_t \mid x_{t-1}) \propto \exp\left( -\frac{1}{2\beta_t} \| x_t - \sqrt{\alpha_t} x_{t-1} \|^2 \right)\)</span><br>
</li>
<li><span class="math inline">\(q(x_{t-1} \mid x_0) \propto \exp\left( -\frac{1}{2(1 - \bar{\alpha}_{t-1})} \| x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0 \|^2 \right)\)</span></li>
</ul>
<p>We can ignore <span class="math inline">\(q(x_t \mid x_0)\)</span> in the denominator, since it is independent of <span class="math inline">\(x_{t-1}\)</span> and will be absorbed into a proportionality constant.</p>
<p>Putting these together:</p>
<p><span class="math display">\[
q(x_{t-1} \mid x_t, x_0) \propto \exp\left(
-\frac{1}{2} \left[
\frac{ \|x_t - \sqrt{\alpha_t} x_{t-1} \|^2 }{\beta_t} +
\frac{ \| x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0 \|^2 }{1 - \bar{\alpha}_{t-1}}
\right]
\right)
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Why does the product of Gaussians give another Gaussian?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why does the product of Gaussians give another Gaussian?
</div>
</div>
<div class="callout-body-container callout-body">
<p>When we multiply two Gaussian distributions over the same variable, the result is also a Gaussian.</p>
<p>Here, we are multiplying two Gaussians in <span class="math inline">\(x_{t-1}\)</span>:<br>
- One centered at <span class="math inline">\(\sqrt{\alpha_t} x_t\)</span><br>
- One centered at <span class="math inline">\(\sqrt{\bar{\alpha}_{t-1}} x_0\)</span></p>
<p>The product is another Gaussian in <span class="math inline">\(x_{t-1}\)</span>, with a new mean that is a <strong>weighted average</strong> of both.<br>
We’ll derive this explicitly by completing the square in the exponent.</p>
</div>
</div>
<p>Completing the square in the exponent, we find that <span class="math inline">\(q(x_{t-1} \mid x_t, x_0)\)</span> is Gaussian with the following mean and variance:</p>
<p><span class="math display">\[
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}\left(x_{t-1};\, \tilde{\mu}_t(x_t, x_0),\, \tilde{\beta}_t I\right)
\]</span></p>
<hr>
</section>
<section id="complete-the-square" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="complete-the-square"><span class="header-section-number">2.4</span> Complete the square</h3>
<p>To simplify the exponent into the standard form of a Gaussian, we apply the <strong>general formula for completing the square</strong>:</p>
<p><span class="math display">\[
a x^2 - 2 b x = a \left( x - \frac{b}{a} \right)^2 - \frac{b^2}{a}
\]</span></p>
<p>This identity rewrites a quadratic expression as a <strong>perfect square</strong>, which is essential for expressing a Gaussian in the form:</p>
<p><span class="math display">\[
\exp\left( -\frac{1}{2 \sigma^2} \| x - \mu \|^2 \right)
\]</span></p>
<section id="expand-the-expression" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="expand-the-expression"><span class="header-section-number">2.4.1</span> Expand the expression</h4>
<p>From earlier, we arrived at this expression for the exponent of the posterior:</p>
<p><span class="math display">\[
-\frac{1}{2} \left[
\frac{(x_t - \sqrt{\alpha_t} x_{t-1})^2}{\beta_t} +
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0)^2}{1 - \bar{\alpha}_{t-1}}
\right]
\]</span></p>
<p>We expand both terms:</p>
<p><strong>First term:</strong></p>
<p><span class="math display">\[
\frac{(x_t - \sqrt{\alpha_t} x_{t-1})^2}{\beta_t}
= \frac{x_t^2 - 2 \sqrt{\alpha_t} x_t x_{t-1} + \alpha_t x_{t-1}^2}{\beta_t}
\]</span></p>
<p><strong>Second term:</strong></p>
<p><span class="math display">\[
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0)^2}{1 - \bar{\alpha}_{t-1}}
= \frac{x_{t-1}^2 - 2 \sqrt{\bar{\alpha}_{t-1}} x_{t-1} x_0 + \bar{\alpha}_{t-1} x_0^2}{1 - \bar{\alpha}_{t-1}}
\]</span></p>
</section>
<section id="group-like-terms" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="group-like-terms"><span class="header-section-number">2.4.2</span> Group like terms</h4>
<p>Now we collect all the terms involving <span class="math inline">\(x_{t-1}\)</span>:</p>
<ul>
<li><strong>Coefficient of <span class="math inline">\(x_{t-1}\)</span>:</strong></li>
</ul>
<p><span class="math display">\[
a = \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}
\]</span></p>
<ul>
<li><strong>Coefficient of <span class="math inline">\(x_{t-1}\)</span>:</strong></li>
</ul>
<p>The full linear term is:</p>
<p><span class="math display">\[
-2 \left(
\frac{ \sqrt{\alpha_t} x_t }{ \beta_t } + \frac{ \sqrt{\bar{\alpha}_{t-1}} x_0 }{ 1 - \bar{\alpha}_{t-1} }
\right)
\]</span></p>
<p>So we define:</p>
<p><span class="math display">\[
b = \frac{ \sqrt{\alpha_t} x_t }{ \beta_t } + \frac{ \sqrt{\bar{\alpha}_{t-1}} x_0 }{ 1 - \bar{\alpha}_{t-1} }
\]</span></p>
<p>Remaining terms (like <span class="math inline">\(x_t^2\)</span> and <span class="math inline">\(x_0^2\)</span>) are independent of <span class="math inline">\(x_{t-1}\)</span> and can be absorbed into a constant.</p>
</section>
<section id="complete-the-square-1" class="level4" data-number="2.4.3">
<h4 data-number="2.4.3" class="anchored" data-anchor-id="complete-the-square-1"><span class="header-section-number">2.4.3</span> Complete the square</h4>
<p>Using the identity:</p>
<p><span class="math display">\[
a x^2 - 2 b x = a \left( x - \frac{b}{a} \right)^2 - \frac{b^2}{a}
\]</span></p>
<p>We rewrite the exponent as:</p>
<p><span class="math display">\[
-\frac{1}{2} \left[
a \left( x_{t-1} - \frac{b}{a} \right)^2 - \frac{b^2}{a}
\right]
\]</span></p>
<p>The constant term <span class="math inline">\(\frac{b^2}{a}\)</span> can be ignored under proportionality, giving:</p>
<p><span class="math display">\[
q(x_{t-1} \mid x_t, x_0) \propto \exp\left(
- \frac{1}{2 \tilde{\beta}_t} \| x_{t-1} - \tilde{\mu}_t \|^2
\right)
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="How does this become a Gaussian?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How does this become a Gaussian?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Once we complete the square, the exponent becomes:</p>
<p><span class="math display">\[
-\frac{1}{2} a \left( x_{t-1} - \frac{b}{a} \right)^2
\]</span></p>
<p>This matches the standard form of a Gaussian:</p>
<p><span class="math display">\[
\exp\left( -\frac{1}{2\sigma^2} \|x - \mu\|^2 \right)
\]</span></p>
<p>So we identify:</p>
<ul>
<li>Mean: <span class="math inline">\(\tilde{\mu}_t = \frac{b}{a}\)</span></li>
<li>Variance: <span class="math inline">\(\tilde{\beta}_t = \frac{1}{a}\)</span></li>
</ul>
<p>This gives us the compact expression:</p>
<p><span class="math display">\[
q(x_{t-1} \mid x_t, x_0) \propto \exp\left( -\frac{1}{2 \tilde{\beta}_t} \| x_{t-1} - \tilde{\mu}_t \|^2 \right)
\]</span></p>
</div>
</div>
</section>
<section id="final-expressions" class="level4" data-number="2.4.4">
<h4 data-number="2.4.4" class="anchored" data-anchor-id="final-expressions"><span class="header-section-number">2.4.4</span> Final expressions</h4>
<p>Now we can directly read off the mean and variance of the Gaussian:</p>
<p><strong>Mean:</strong></p>
<p><span class="math display">\[
\tilde{\mu}_t =
\frac{
\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t +
\sqrt{\bar{\alpha}_{t-1}} \beta_t x_0
}{
1 - \bar{\alpha}_t
}
\]</span></p>
<p><strong>Variance:</strong></p>
<p><span class="math display">\[
\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t
\]</span></p>
<p>So the full form of the posterior is:</p>
<p><span class="math display">\[
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(x_{t-1};\, \tilde{\mu}_t,\, \tilde{\beta}_t I)
\]</span></p>
<hr>
</section>
<section id="parameterizing-the-reverse-process" class="level4" data-number="2.4.5">
<h4 data-number="2.4.5" class="anchored" data-anchor-id="parameterizing-the-reverse-process"><span class="header-section-number">2.4.5</span> Parameterizing the Reverse Process</h4>
<p>Since we don’t know <span class="math inline">\(x_0\)</span> during the reverse process, we express it in terms of <span class="math inline">\(x_t\)</span> using the reparameterization trick from the forward process:</p>
<p><span class="math display">\[
x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \, \epsilon}{\sqrt{\bar{\alpha}_t}}
\]</span></p>
<p>Substituting this into <span class="math inline">\(\tilde{\mu}_t\)</span>, we get:</p>
<p><span class="math display">\[
\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \, \epsilon \right)
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is the noise added to <span class="math inline">\(x_0\)</span> to get <span class="math inline">\(x_t\)</span>.</p>
</section>
<section id="training-the-neural-network" class="level4" data-number="2.4.6">
<h4 data-number="2.4.6" class="anchored" data-anchor-id="training-the-neural-network"><span class="header-section-number">2.4.6</span> Training the Neural Network</h4>
<p>The reverse process is implemented as a neural network (often a U-Net), which is trained to <strong>predict the noise</strong> <span class="math inline">\(\epsilon\)</span> from <span class="math inline">\(x_t\)</span> at each step. The forward process is fixed and non-learned.</p>
<p>Let <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span> be the network’s prediction. Then the training objective is: <span class="math display">\[
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \epsilon \sim \mathcal{N}(0, I)}
\left[ \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2 \right]
\]</span></p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Intuition:</strong> The model learns to “denoise” each <span class="math inline">\(x_t\)</span> by predicting the noise that was added to create it.</p>
</div>
</div>
</div>
</section>
<section id="sampling-generating-new-data" class="level4" data-number="2.4.7">
<h4 data-number="2.4.7" class="anchored" data-anchor-id="sampling-generating-new-data"><span class="header-section-number">2.4.7</span> Sampling (Generating New Data)</h4>
<p>To generate a sample:</p>
<ol type="1">
<li>Sample noise: <span class="math inline">\(x_T \sim \mathcal{N}(0, I)\)</span><br>
</li>
<li>For each <span class="math inline">\(t = T, T-1, ..., 1\)</span>:
<ul>
<li>Predict noise: <span class="math inline">\(\epsilon_\theta(x_t, t)\)</span></li>
<li>Compute <span class="math inline">\(x_{t-1}\)</span> as: <span class="math display">\[
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t)\right) + \sigma_t z
\]</span> Where:</li>
</ul></li>
</ol>
<ul>
<li><span class="math inline">\(z \sim \mathcal{N}(0, I)\)</span><br>
</li>
<li><span class="math inline">\(\sigma_t^2 = \beta_t\)</span></li>
</ul>
</section>
<section id="simplifications-in-ddpm" class="level4" data-number="2.4.8">
<h4 data-number="2.4.8" class="anchored" data-anchor-id="simplifications-in-ddpm"><span class="header-section-number">2.4.8</span> Simplifications in DDPM</h4>
<p>In the <strong>Denoising Diffusion Probabilistic Models (DDPM)</strong> paper:</p>
<ul>
<li>The variance is fixed: <span class="math inline">\(\Sigma_\theta = \beta_t I\)</span></li>
<li>The stochastic term <span class="math inline">\(z\)</span> is removed for <strong>deterministic</strong> sampling</li>
</ul>
<p>This simplifies the sampling equation to: <span class="math display">\[
x_{t-1} = \mu_\theta(x_t, t)
\]</span></p>
</section>
<section id="in-summary" class="level4" data-number="2.4.9">
<h4 data-number="2.4.9" class="anchored" data-anchor-id="in-summary"><span class="header-section-number">2.4.9</span> In Summary</h4>
<ul>
<li>The forward process is a fixed schedule of adding Gaussian noise.</li>
<li>The reverse process is <strong>learned</strong> by a neural network that predicts noise <span class="math inline">\(\epsilon\)</span> at each step.</li>
<li>Sampling starts from pure noise and applies the learned denoising steps iteratively.</li>
</ul>
<hr>
</section>
</section>
<section id="learning" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="learning"><span class="header-section-number">2.5</span> Learning</h3>
<p><strong>What is the Goal?</strong> The ultimate goal in diffusion models is to train the neural network so that it can reverse the noising process. In other words, we want the network to learn how to turn random noise back into realistic data (like images). But how do we actually train the network? We need a loss function—a way to measure how good or bad the network’s predictions are, so we can improve it.</p>
<section id="detailed-explanation-of-the-elbo-in-diffusion-models" class="level4" data-number="2.5.1">
<h4 data-number="2.5.1" class="anchored" data-anchor-id="detailed-explanation-of-the-elbo-in-diffusion-models"><span class="header-section-number">2.5.1</span> Detailed Explanation of the ELBO in Diffusion Models</h4>
<p>Let’s break down the <strong>Evidence Lower Bound (ELBO)</strong> used in diffusion models, step by step. This is the core mathematical foundation for training diffusion models, and understanding it will clarify how the neural network learns to reverse the noising process.</p>
<section id="what-is-the-elbo" class="level5" data-number="2.5.1.1">
<h5 data-number="2.5.1.1" class="anchored" data-anchor-id="what-is-the-elbo"><span class="header-section-number">2.5.1.1</span> What is the ELBO?</h5>
<p>The ELBO is a <strong>lower bound</strong> on the log-likelihood of the data. Maximizing the ELBO is equivalent to maximizing the likelihood that the model can generate the training data. For diffusion models, the ELBO ensures that the reverse process (denoising) aligns with the forward process (noising).</p>
</section>
</section>
<section id="deriving-the-elbo-for-diffusion-models" class="level4" data-number="2.5.2">
<h4 data-number="2.5.2" class="anchored" data-anchor-id="deriving-the-elbo-for-diffusion-models"><span class="header-section-number">2.5.2</span> Deriving the ELBO for Diffusion Models</h4>
<p><strong>Goal:</strong><br>
We want to maximize the log-likelihood of the data:</p>
<p><span class="math display">\[
\log p_\theta(x_0)
\]</span></p>
<p>where <span class="math inline">\(x_0\)</span> is a clean data sample (e.g., an image).</p>
<p><strong>Problem:</strong><br>
Computing <span class="math inline">\(\log p_\theta(x_0)\)</span> directly is <strong>intractable</strong> because it involves integrating over all possible noisy intermediate states <span class="math inline">\(x_{1:T}\)</span>.</p>
<p><strong>Solution:</strong><br>
Use <strong>Jensen’s Inequality</strong> to derive a lower bound (the ELBO) that we can optimize instead.</p>
<section id="full-derivation-step-by-step" class="level5" data-number="2.5.2.1">
<h5 data-number="2.5.2.1" class="anchored" data-anchor-id="full-derivation-step-by-step"><span class="header-section-number">2.5.2.1</span> Full Derivation (Step-by-Step)</h5>
<p><strong>Step 1: Start with the log-likelihood</strong></p>
<p><span class="math display">\[
\log p_\theta(x_0) = \log \int p_\theta(x_{0:T}) \, dx_{1:T}
\]</span></p>
<p><strong>Step 2: Introduce the forward process <span class="math inline">\(q(x_{1:T} \mid x_0)\)</span></strong></p>
<p>Multiply and divide by the fixed forward process:</p>
<p><span class="math display">\[
\log p_\theta(x_0) = \log \int \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} q(x_{1:T} \mid x_0) \, dx_{1:T}
\]</span></p>
<p><strong>Step 3: Rewrite as an expectation</strong></p>
<p><span class="math display">\[
\log p_\theta(x_0) = \log \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} \right]
\]</span></p>
<p><strong>Step 4: Apply Jensen’s Inequality</strong></p>
<p><span class="math display">\[
\log p_\theta(x_0) \geq \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \log \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} \right] = \text{ELBO}
\]</span></p>
<p><strong>Step 5: Expand <span class="math inline">\(p_\theta(x_{0:T})\)</span> and <span class="math inline">\(q(x_{1:T} \mid x_0)\)</span></strong></p>
<p>Forward process: <span class="math display">\[
q(x_{1:T} \mid x_0) = \prod_{t=1}^T q(x_t \mid x_{t-1})
\]</span></p>
<p>Reverse process: <span class="math display">\[
p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} \mid x_t)
\]</span></p>
<p>Substitute into the ELBO:</p>
<p><span class="math display">\[
\text{ELBO} = \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \log p(x_T) + \sum_{t=1}^T \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})} \right]
\]</span></p>
<p><strong>Step 6: Decompose the ELBO</strong></p>
<p><span class="math display">\[
\text{ELBO} = \mathbb{E}_{q} \left[ \log p(x_T) \right] + \sum_{t=2}^T \mathbb{E}_q \left[ \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})} \right] + \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) \right]
\]</span></p>
</section>
<section id="interpreting-each-term" class="level5" data-number="2.5.2.2">
<h5 data-number="2.5.2.2" class="anchored" data-anchor-id="interpreting-each-term"><span class="header-section-number">2.5.2.2</span> Interpreting Each Term</h5>
<p><strong>Reconstruction Term:</strong><br>
- <span class="math inline">\(\mathbb{E}_q[\log p_\theta(x_0 \mid x_1)]\)</span> measures how well the model can reconstruct <span class="math inline">\(x_0\)</span> from the first noisy sample <span class="math inline">\(x_1\)</span>.</p>
<p><strong>Prior Matching Term:</strong><br>
- <span class="math inline">\(\mathbb{E}_q[\log p(x_T)]\)</span> encourages the final state <span class="math inline">\(x_T\)</span> to match the prior <span class="math inline">\(\mathcal{N}(0, I)\)</span>.</p>
<p><strong>Consistency Terms:</strong><br>
- <span class="math inline">\(\sum \mathbb{E}_q[\log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})}]\)</span> ensures each denoising step approximates the forward noising step.</p>
</section>
<section id="practical-training-simplification-ddpm" class="level5" data-number="2.5.2.3">
<h5 data-number="2.5.2.3" class="anchored" data-anchor-id="practical-training-simplification-ddpm"><span class="header-section-number">2.5.2.3</span> Practical Training Simplification (DDPM)</h5>
<p>In DDPM:</p>
<ul>
<li><strong><span class="math inline">\(p(x_T) = \mathcal{N}(0, I)\)</span></strong> is fixed and known.<br>
</li>
<li>The reconstruction term is small and often ignored.<br>
</li>
<li>The KL terms are approximated via <strong>noise prediction</strong>:</li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{\text{simple}} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
\]</span></p>
<p>Where: - <span class="math inline">\(\epsilon\)</span> is the actual noise<br>
- <span class="math inline">\(\epsilon_\theta\)</span> is the predicted noise by the neural network</p>
<p><strong>Why?</strong> If the network can predict the noise <span class="math inline">\(\epsilon\)</span>, it can denoise <span class="math inline">\(x_t\)</span> and reverse the diffusion.</p>
</section>
<section id="connection-to-vaes" class="level5" data-number="2.5.2.4">
<h5 data-number="2.5.2.4" class="anchored" data-anchor-id="connection-to-vaes"><span class="header-section-number">2.5.2.4</span> Connection to VAEs</h5>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 36%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>VAEs</th>
<th>Diffusion Models</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Forward process</td>
<td>Learned encoder <span class="math inline">\(q_\phi(z \mid x)\)</span></td>
<td>Fixed noising process <span class="math inline">\(q(x_t \mid x_{t-1})\)</span></td>
</tr>
<tr class="even">
<td>Reverse process</td>
<td>Learned decoder <span class="math inline">\(p_\theta(x \mid z)\)</span></td>
<td>Learned denoising network <span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span></td>
</tr>
<tr class="odd">
<td>Training objective</td>
<td>Optimize ELBO over latent variables</td>
<td>Optimize ELBO via noise prediction loss</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="takeaways" class="level5" data-number="2.5.2.5">
<h5 data-number="2.5.2.5" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">2.5.2.5</span> Takeaways</h5>
<ul>
<li>The ELBO is a tractable lower bound on <span class="math inline">\(\log p_\theta(x_0)\)</span>.<br>
</li>
<li>It aligns the reverse (learned) process with the forward (fixed) noising process.<br>
</li>
<li>In practice, training reduces to minimizing the difference between predicted and true noise.</li>
</ul>
<hr>
</section>
</section>
</section>
</section>
<section id="back-to-the-diffusion-detailed-explanation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="back-to-the-diffusion-detailed-explanation"><span class="header-section-number">3</span> Back to the Diffusion (Detailed Explanation)</h2>
<section id="recap-what-is-diffusion-in-this-context" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="recap-what-is-diffusion-in-this-context"><span class="header-section-number">3.1</span> Recap: What Is Diffusion in This Context?</h3>
<p>Diffusion models work by gradually adding noise to data (like images) in small steps until the data becomes nearly pure noise. This is called the <strong>forward diffusion process</strong>. The challenge lies in reversing this process: starting from noise and recovering realistic data. This is known as the <strong>reverse diffusion process</strong>.</p>
</section>
<section id="the-reverse-diffusion-process-step-by-step" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="the-reverse-diffusion-process-step-by-step"><span class="header-section-number">3.2</span> The Reverse Diffusion Process: Step by Step</h3>
<section id="the-goal" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="the-goal"><span class="header-section-number">3.2.1</span> 1. The Goal</h4>
<p>The reverse diffusion process aims to start from random noise and, through a sequence of learned steps, remove the noise and reconstruct a realistic data sample (e.g., an image).</p>
</section>
<section id="why-is-this-hard" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="why-is-this-hard"><span class="header-section-number">3.2.2</span> 2. Why Is This Hard?</h4>
<ul>
<li>The forward process is simple and analytically defined.</li>
<li>The reverse process requires “undoing” the added noise, which is complex and intractable for real data.</li>
<li>To solve this, we use a neural network to learn how to denoise the sample step-by-step.</li>
</ul>
</section>
<section id="how-is-the-reverse-process-modeled" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="how-is-the-reverse-process-modeled"><span class="header-section-number">3.2.3</span> 3. How Is the Reverse Process Modeled?</h4>
<ul>
<li>The reverse process is represented as a Markov chain of conditional Gaussians:</li>
</ul>
<p><span class="math display">\[
p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} \mid x_t)
\]</span></p>
<ul>
<li>Here, <span class="math inline">\(p(x_T)\)</span> is a standard normal distribution.</li>
<li>Each <span class="math inline">\(p_\theta(x_{t-1} \mid x_t)\)</span> is a Gaussian with parameters predicted by a neural network.</li>
</ul>
</section>
<section id="why-use-gaussians" class="level4" data-number="3.2.4">
<h4 data-number="3.2.4" class="anchored" data-anchor-id="why-use-gaussians"><span class="header-section-number">3.2.4</span> 4. Why Use Gaussians?</h4>
<ul>
<li>If the forward noise schedule uses small variances <span class="math inline">\(\beta_t\)</span>, the reverse conditional distributions can be well-approximated by Gaussians.</li>
<li>This makes training and sampling tractable using standard probabilistic tools.</li>
</ul>
</section>
<section id="what-does-the-neural-network-actually-do" class="level4" data-number="3.2.5">
<h4 data-number="3.2.5" class="anchored" data-anchor-id="what-does-the-neural-network-actually-do"><span class="header-section-number">3.2.5</span> 5. What Does the Neural Network Actually Do?</h4>
<ul>
<li>Given a noisy input <span class="math inline">\(x_t\)</span> and the time step <span class="math inline">\(t\)</span>, the neural network predicts:
<ul>
<li>The mean <span class="math inline">\(\mu_\theta(x_t, t)\)</span> of the reverse Gaussian</li>
<li>The added noise <span class="math inline">\(\epsilon\)</span></li>
<li>Or another related signal, depending on how the model is parameterized</li>
</ul></li>
</ul>
</section>
<section id="the-sampling-generation-process" class="level4" data-number="3.2.6">
<h4 data-number="3.2.6" class="anchored" data-anchor-id="the-sampling-generation-process"><span class="header-section-number">3.2.6</span> 6. The Sampling (Generation) Process</h4>
<p>To generate a new sample:</p>
<ol type="1">
<li>Sample noise: <span class="math inline">\(x_T \sim \mathcal{N}(0, I)\)</span><br>
</li>
<li>For <span class="math inline">\(t = T, T-1, \dots, 1\)</span>:
<ul>
<li>Predict <span class="math inline">\(\mu_\theta(x_t, t)\)</span> and optionally <span class="math inline">\(\Sigma_\theta(x_t, t)\)</span></li>
<li>Sample:</li>
</ul></li>
</ol>
<p><span class="math display">\[
x_{t-1} = \mu_\theta(x_t, t) + \Sigma_\theta^{1/2}(x_t, t) \cdot z, \quad z \sim \mathcal{N}(0, I)
\]</span></p>
<ol start="3" type="1">
<li>The final output <span class="math inline">\(x_0\)</span> should resemble realistic data.</li>
</ol>
</section>
<section id="discrete-data-and-the-final-step" class="level4" data-number="3.2.7">
<h4 data-number="3.2.7" class="anchored" data-anchor-id="discrete-data-and-the-final-step"><span class="header-section-number">3.2.7</span> 7. Discrete Data and the Final Step</h4>
<ul>
<li>Real images have discrete pixel values (e.g., 0 to 255).</li>
<li>The final step often uses a discrete decoder (like a softmax over 256 bins) to convert continuous predictions into discrete outputs.</li>
</ul>
<hr>
</section>
</section>
<section id="key-mathematical-details-with-explanations" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="key-mathematical-details-with-explanations"><span class="header-section-number">3.3</span> Key Mathematical Details (With Explanations)</h3>
<ul>
<li><strong>Forward process:</strong></li>
</ul>
<p><span class="math display">\[
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon
\]</span></p>
<p>where <span class="math inline">\(\epsilon \sim \mathcal{N}(0, I)\)</span>, and <span class="math inline">\(\bar{\alpha}_t\)</span> is the cumulative product of <span class="math inline">\(\alpha_t = 1 - \beta_t\)</span>.</p>
<ul>
<li><strong>Reverse process:</strong></li>
</ul>
<p><span class="math display">\[
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
\]</span></p>
<ul>
<li><strong>Sampling equation:</strong></li>
</ul>
<p><span class="math display">\[
x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z, \quad z \sim \mathcal{N}(0, I)
\]</span></p>
<ul>
<li><strong>Final decoding:</strong> Produces discrete pixel values from continuous outputs.</li>
</ul>
</section>
<section id="why-is-this-important" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="why-is-this-important"><span class="header-section-number">3.4</span> Why Is This Important?</h3>
<ul>
<li>Connects the forward and reverse processes to the actual generative algorithm.</li>
<li>Explains how the neural network learns to remove noise step-by-step.</li>
<li>Clarifies why Gaussian assumptions and probabilistic modeling matter.</li>
</ul>
<hr>
</section>
<section id="summary" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="summary"><span class="header-section-number">3.5</span> Summary</h3>
<ul>
<li>The reverse diffusion process generates new data by progressively denoising a random noise sample.</li>
<li>It’s guided by a neural network trained to predict the direction of denoising at each step.</li>
<li>Each reverse step is modeled as a Gaussian, and the final output is discretized to match real data.</li>
<li>This process allows diffusion models to create realistic images, audio, or other types of structured data.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>