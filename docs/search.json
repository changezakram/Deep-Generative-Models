[
  {
    "objectID": "vae_mnist.html",
    "href": "vae_mnist.html",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "",
    "text": "This notebook explains a Variational Autoencoder (VAE) trained on the MNIST dataset using PyTorch.\nEach step is annotated with detailed comments to help beginners understand what‚Äôs happening.\n!pip install torch torchvision"
  },
  {
    "objectID": "vae_mnist.html#import-required-libraries",
    "href": "vae_mnist.html#import-required-libraries",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "1. Import Required Libraries",
    "text": "1. Import Required Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "href": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "2. Load and Prepare the MNIST Dataset",
    "text": "2. Load and Prepare the MNIST Dataset\n\n# We transform MNIST images into tensors.\ntransform = transforms.ToTensor()\n\n# Download and load the training data\ntrain_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n\n# DataLoader for batching and shuffling\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
  },
  {
    "objectID": "vae_mnist.html#define-the-vae-model",
    "href": "vae_mnist.html#define-the-vae-model",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "3. Define the VAE Model",
    "text": "3. Define the VAE Model\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):\n        super(VAE, self).__init__()\n        # Encoder layers: input -&gt; hidden -&gt; (mu, logvar)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # outputs mean of q(z|x)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # outputs log-variance of q(z|x)\n\n        # Decoder layers: latent -&gt; hidden -&gt; reconstruction\n        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, input_dim)\n\n    def encode(self, x):\n        # Apply a hidden layer then split into mean and logvar\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        # Apply the reparameterization trick\n        std = torch.exp(0.5 * logvar)      # standard deviation\n        eps = torch.randn_like(std)        # random normal noise\n        return mu + eps * std              # sample z\n\n    def decode(self, z):\n        # Reconstruct input from latent representation\n        h = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h))  # Output in [0, 1] range for binary MNIST\n\n    def forward(self, x):\n        # Full VAE forward pass\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        return recon_x, mu, logvar"
  },
  {
    "objectID": "vae_mnist.html#define-the-elbo-loss",
    "href": "vae_mnist.html#define-the-elbo-loss",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "4. Define the ELBO Loss",
    "text": "4. Define the ELBO Loss\n\ndef elbo_loss(recon_x, x, mu, logvar):\n    # Binary cross-entropy for reconstruction\n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n\n    # KL divergence term to regularize q(z|x) against standard normal p(z)\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    # Total loss is negative ELBO\n    return BCE + KLD"
  },
  {
    "objectID": "vae_mnist.html#train-the-vae",
    "href": "vae_mnist.html#train-the-vae",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "5. Train the VAE",
    "text": "5. Train the VAE\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = VAE().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for x, _ in train_loader:\n        x = x.view(-1, 784).to(device)               # Flatten 28x28 images into 784 vectors\n        recon_x, mu, logvar = model(x)               # Forward pass\n        loss = elbo_loss(recon_x, x, mu, logvar)     # Compute loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader.dataset):.2f}\")"
  },
  {
    "objectID": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "href": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "6. Visualize Original and Reconstructed Digits",
    "text": "6. Visualize Original and Reconstructed Digits\n\nmodel.eval()\nwith torch.no_grad():\n    x, _ = next(iter(train_loader))\n    x = x.view(-1, 784).to(device)\n    recon_x, _, _ = model(x)\n\n    # Convert back to image format\n    x = x.view(-1, 1, 28, 28).cpu()\n    recon_x = recon_x.view(-1, 1, 28, 28).cpu()\n\n    fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n    for i in range(10):\n        axs[0, i].imshow(x[i][0], cmap='gray')\n        axs[0, i].axis('off')\n        axs[1, i].imshow(recon_x[i][0], cmap='gray')\n        axs[1, i].axis('off')\n    axs[0, 0].set_ylabel(\"Original\", fontsize=12)\n    axs[1, 0].set_ylabel(\"Reconstruction\", fontsize=12)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "vae_mnist.html#visualize-latent-space",
    "href": "vae_mnist.html#visualize-latent-space",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "7. Visualize Latent Space",
    "text": "7. Visualize Latent Space\n\nimport seaborn as sns\n\nmodel.eval()\nall_z = []\nall_labels = []\n\n# Go through a few batches and collect latent representations\nwith torch.no_grad():\n    for x, y in train_loader:\n        x = x.view(-1, 784).to(device)\n        mu, _ = model.encode(x)  # use the mean as representation\n        all_z.append(mu.cpu())\n        all_labels.append(y)\n\n# Concatenate all batches\nz = torch.cat(all_z, dim=0).numpy()\nlabels = torch.cat(all_labels, dim=0).numpy()\n\n# Plot with seaborn\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=z[:, 0], y=z[:, 1], hue=labels, palette=\"tab10\", s=15)\nplt.title(\"Latent Space Visualization (using Œº)\")\nplt.xlabel(\"z[0]\")\nplt.ylabel(\"z[1]\")\nplt.legend(title=\"Digit\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#key-model-families",
    "href": "index.html#key-model-families",
    "title": "Deep Generative Models",
    "section": "Key Model Families",
    "text": "Key Model Families\n\n1. Variational Autoencoders (VAEs)\n\nProbabilistic encoder-decoder architecture\nLearn latent spaces with Gaussian distributions\nLearn more about VAEs\n\n\n\n2. Autoregressive Models\n\nGenerate sequences element-by-element\nExamples: PixelCNN, WaveNet\n\n\n\n3. Generative Adversarial Networks (GANs)\n\nAdversarial training with generator/discriminator\nHigh-quality image generation\n\n\n\n4. Normalizing Flows\n\nInvertible transformations for exact likelihood\nFlexible density estimation\n\n\n\n5. Energy-Based Models (EBMs)\n\nLearn energy functions for data distribution\nFlexible for discrete/continuous data\n\n\n\n6. Diffusion Models\n\nIterative denoising process\nState-of-the-art image generation"
  },
  {
    "objectID": "vae_faces.html",
    "href": "vae_faces.html",
    "title": "Face Generation with Convolutional VAE",
    "section": "",
    "text": "This notebook implements a convolutional variational autoencoder (VAE) trained on the CelebA face dataset using PyTorch.\nIt uses convolutional layers to encode and decode 64x64 face images, and demonstrates generation by sampling from the latent space.\n!pip install torch torchvision matplotlib\n# Enable autoreloading in Jupyter (if applicable)\n%load_ext autoreload\n%autoreload 2\n\n# Core libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import norm  # Used for visualizing latent distributions\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms, utils\n# Hyperparameters\n\nIMAGE_SIZE = 32        # Input image resolution (32x32)\nCHANNELS = 3           # Number of color channels (RGB)\nBATCH_SIZE = 128       # Batch size for training\nNUM_FEATURES = 128     # Hidden layer size or intermediate feature size\nZ_DIM = 200            # Dimensionality of the latent space\nLEARNING_RATE = 5e-4   # Learning rate for optimizer\nEPOCHS = 1             # Number of training epochs\nBETA = 2000            # Weight for KL divergence term in loss (used in beta-VAE)"
  },
  {
    "objectID": "vae_faces.html#loading-and-preprocessing-the-celeba-dataset",
    "href": "vae_faces.html#loading-and-preprocessing-the-celeba-dataset",
    "title": "Face Generation with Convolutional VAE",
    "section": "Loading and Preprocessing the CelebA Dataset",
    "text": "Loading and Preprocessing the CelebA Dataset\n\nfrom torchvision.datasets import CelebA\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Transform pipeline: resize and convert to tensor\ntransform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor()\n])\n\n# Load CelebA dataset\ntrain_data = CelebA(\n    root=\"./data\",                      # Where to download/save the data\n    split=\"train\",                      # Options: \"train\", \"valid\", \"test\", or \"all\"\n    download=True,                      # Download if not already there\n    transform=transform\n)\n\n# DataLoader\ntrain_loader = DataLoader(\n    train_data,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\n\nval_data = CelebA(\n    root=\"./data\",\n    split=\"valid\",\n    download=True,\n    transform=transform\n)\n\nval_loader = DataLoader(\n    val_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)"
  },
  {
    "objectID": "vae_faces.html#visualizing-training-data",
    "href": "vae_faces.html#visualizing-training-data",
    "title": "Face Generation with Convolutional VAE",
    "section": "Visualizing Training Data",
    "text": "Visualizing Training Data\n\ndef sample_batch(dataloader, num_samples=16):\n    \"\"\"\n    Sample one batch from the dataloader and return the first `num_samples` images.\n    \"\"\"\n    for images, _ in dataloader:\n        return images[:num_samples]\n\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\ndef display(images, cmap=None, nrow=8):\n    \"\"\"\n    Display a grid of images using matplotlib.\n    \"\"\"\n    grid = make_grid(images, nrow=nrow, padding=2, normalize=True)\n    np_img = grid.permute(1, 2, 0).cpu().numpy()\n\n    plt.figure(figsize=(10, 5))\n    plt.imshow(np_img, cmap=cmap)\n    plt.axis('off')\n    plt.show()\n\n\ntrain_sample = sample_batch(train_loader, num_samples=16)\ndisplay(train_sample)"
  },
  {
    "objectID": "vae_faces.html#reparameterization-trick",
    "href": "vae_faces.html#reparameterization-trick",
    "title": "Face Generation with Convolutional VAE",
    "section": "Reparameterization Trick",
    "text": "Reparameterization Trick\n\ndef reparameterize(z_mean, z_log_var):\n    \"\"\"\n    Reparameterization trick: z = mu + sigma * epsilon\n    where epsilon ~ N(0, I)\n    \"\"\"\n    std = torch.exp(0.5 * z_log_var)\n    eps = torch.randn_like(std)\n    return z_mean + eps * std"
  },
  {
    "objectID": "vae_faces.html#encoder",
    "href": "vae_faces.html#encoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "Encoder",
    "text": "Encoder\n\nclass Encoder(nn.Module):\n    def __init__(self, z_dim):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(CHANNELS, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 32 ‚Üí 16\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 16 ‚Üí 8\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 8 ‚Üí 4\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 4 ‚Üí 2\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n        )\n\n        self.flatten = nn.Flatten()\n        self.feature_shape = (NUM_FEATURES, IMAGE_SIZE // 16, IMAGE_SIZE // 16)\n        self.flat_dim = NUM_FEATURES * (IMAGE_SIZE // 16) ** 2\n\n        self.fc_mu = nn.Linear(self.flat_dim, z_dim)\n        self.fc_logvar = nn.Linear(self.flat_dim, z_dim)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_flat = self.flatten(x)\n        mu = self.fc_mu(x_flat)\n        logvar = self.fc_logvar(x_flat)\n        return mu, logvar"
  },
  {
    "objectID": "vae_faces.html#vae-model-wrapper-encoder-reparameterization-and-decoder",
    "href": "vae_faces.html#vae-model-wrapper-encoder-reparameterization-and-decoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "VAE Model Wrapper: Encoder, Reparameterization, and Decoder",
    "text": "VAE Model Wrapper: Encoder, Reparameterization, and Decoder\n\nclass VAE(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"\n        z = mu + sigma * epsilon\n        where epsilon ~ N(0, I)\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass:\n        - Encode input to get mu and logvar\n        - Sample z using reparameterization trick\n        - Decode z to reconstruct image\n        \"\"\"\n        mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        x_recon = self.decoder(z)\n        return x_recon, mu, logvar"
  },
  {
    "objectID": "vae_faces.html#vae-loss",
    "href": "vae_faces.html#vae-loss",
    "title": "Face Generation with Convolutional VAE",
    "section": "VAE Loss",
    "text": "VAE Loss\n\ndef vae_loss(x, x_recon, mu, logvar, beta=BETA):\n    \"\"\"\n    VAE loss = beta * reconstruction loss (MSE) + KL divergence\n    \"\"\"\n    # Reconstruction loss (MSE)\n    recon_loss = F.mse_loss(x_recon, x, reduction='mean') * beta\n\n    # KL divergence between q(z|x) and N(0, I)\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    kl_loss = kl_loss / x.size(0)  # average over batch\n\n    return recon_loss + kl_loss, recon_loss, kl_loss"
  },
  {
    "objectID": "vae_faces.html#generating-and-saving-sample-images-from-the-latent-space",
    "href": "vae_faces.html#generating-and-saving-sample-images-from-the-latent-space",
    "title": "Face Generation with Convolutional VAE",
    "section": "Generating and Saving Sample Images from the Latent Space",
    "text": "Generating and Saving Sample Images from the Latent Space\n\nfrom torchvision.utils import save_image\nimport os\n\ndef generate_images(model, epoch, z_dim, num_img=8, path=\"./output\"):\n    \"\"\"\n    Generate and save images from random latent vectors after each epoch.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    model.eval()\n    z = torch.randn(num_img, z_dim).to(next(model.parameters()).device)\n    with torch.no_grad():\n        generated = model.decoder(z)\n    for i in range(num_img):\n        save_image(generated[i], f\"{path}/generated_img_{epoch:03d}_{i}.png\")"
  },
  {
    "objectID": "vae_faces.html#save-checkpoint",
    "href": "vae_faces.html#save-checkpoint",
    "title": "Face Generation with Convolutional VAE",
    "section": "Save Checkpoint",
    "text": "Save Checkpoint\n\ndef save_checkpoint(model, epoch, loss, best_loss, path=\"./checkpoint.pt\"):\n    if loss &lt; best_loss:\n        print(f\"Saving new best model at epoch {epoch} with loss {loss:.4f}\")\n        torch.save(model.state_dict(), path)\n        return loss\n    return best_loss"
  },
  {
    "objectID": "vae_faces.html#training-step",
    "href": "vae_faces.html#training-step",
    "title": "Face Generation with Convolutional VAE",
    "section": "Training Step",
    "text": "Training Step\n\ndef train_step(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n\n    for batch in dataloader:\n        x, _ = batch if isinstance(batch, (tuple, list)) else (batch, None)\n        x = x.to(device)\n\n        optimizer.zero_grad()\n\n        x_recon, mu, logvar = model(x)\n        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_recon += recon_loss.item()\n        total_kl += kl_loss.item()\n\n    num_batches = len(dataloader)\n    return {\n        \"loss\": total_loss / num_batches,\n        \"reconstruction_loss\": total_recon / num_batches,\n        \"kl_loss\": total_kl / num_batches\n    }"
  },
  {
    "objectID": "vae_faces.html#validation-step",
    "href": "vae_faces.html#validation-step",
    "title": "Face Generation with Convolutional VAE",
    "section": "Validation Step",
    "text": "Validation Step\n\n@torch.no_grad()\ndef validate_epoch(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n\n    for batch in dataloader:\n        x, _ = batch if isinstance(batch, (tuple, list)) else (batch, None)\n        x = x.to(device)\n        x_recon, mu, logvar = model(x)\n        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar)\n        total_loss += loss.item()\n        total_recon += recon_loss.item()\n        total_kl += kl_loss.item()\n\n    num_batches = len(dataloader)\n    return {\n        \"loss\": total_loss / num_batches,\n        \"reconstruction_loss\": total_recon / num_batches,\n        \"kl_loss\": total_kl / num_batches\n    }"
  },
  {
    "objectID": "vae_faces.html#instantiate-model-optimizer-writer-device",
    "href": "vae_faces.html#instantiate-model-optimizer-writer-device",
    "title": "Face Generation with Convolutional VAE",
    "section": "Instantiate model, optimizer, writer, device",
    "text": "Instantiate model, optimizer, writer, device\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nencoder = Encoder(z_dim=Z_DIM)\ndecoder = Decoder(z_dim=Z_DIM)\nvae = VAE(encoder, decoder).to(device)\n\noptimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n\nwriter = SummaryWriter(log_dir=\"./logs\")"
  },
  {
    "objectID": "vae_faces.html#training-the-vae-model",
    "href": "vae_faces.html#training-the-vae-model",
    "title": "Face Generation with Convolutional VAE",
    "section": "Training the VAE Model",
    "text": "Training the VAE Model\n\nbest_loss = float(\"inf\")\n\nfor epoch in range(EPOCHS):\n    train_logs = train_step(vae, train_loader, optimizer, device)\n    val_logs = validate_epoch(vae, val_loader, device)\n\n    print(f\"Epoch {epoch+1:02d} | \"\n          f\"Train Loss: {train_logs['loss']:.4f} | \"\n          f\"Val Loss: {val_logs['loss']:.4f}\")\n\n    # Save best model\n    best_loss = save_checkpoint(vae, epoch, val_logs[\"loss\"], best_loss)\n\n    # Generate and save sample images\n    generate_images(vae, epoch, Z_DIM)\n\n    # Log to TensorBoard\n    writer.add_scalar(\"Loss/train\", train_logs[\"loss\"], epoch)\n    writer.add_scalar(\"Loss/val\", val_logs[\"loss\"], epoch)\n    writer.add_scalar(\"KL/train\", train_logs[\"kl_loss\"], epoch)\n    writer.add_scalar(\"Recon/train\", train_logs[\"reconstruction_loss\"], epoch)"
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities ‚Äî they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) were introduced to overcome these limitations. Rather than encoding inputs into fixed latent vectors, VAEs learn a probabilistic latent space by modeling each input as a distribution ‚Äî typically a Gaussian with a learned mean \\(\\\\mu\\) and standard deviation \\(\\\\sigma\\). This approach enables the model to sample latent variables \\(z\\) using the reparameterization trick, allowing the entire architecture to remain differentiable and trainable. By doing so, VAEs not only enable reconstruction, but also promote the learning of a continuous, interpretable latent space ‚Äî a key enabler for generation and interpolation.\nThe diagram below illustrates this process:\n\nSource: Wikimedia Commons, licensed under CC BY-SA 4.0."
  },
  {
    "objectID": "vae.html#autoencoders-vs-variational-autoencoders",
    "href": "vae.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities ‚Äî they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) were introduced to overcome these limitations. Rather than encoding inputs into fixed latent vectors, VAEs learn a probabilistic latent space by modeling each input as a distribution ‚Äî typically a Gaussian with a learned mean \\(\\\\mu\\) and standard deviation \\(\\\\sigma\\). This approach enables the model to sample latent variables \\(z\\) using the reparameterization trick, allowing the entire architecture to remain differentiable and trainable. By doing so, VAEs not only enable reconstruction, but also promote the learning of a continuous, interpretable latent space ‚Äî a key enabler for generation and interpolation.\nThe diagram below illustrates this process:\n\nSource: Wikimedia Commons, licensed under CC BY-SA 4.0."
  },
  {
    "objectID": "vae.html#probabilistic-framework",
    "href": "vae.html#probabilistic-framework",
    "title": "Variational Autoencoders",
    "section": "Probabilistic Framework",
    "text": "Probabilistic Framework\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\nHere, \\(\\mathbf{z}\\) acts as a hidden or latent variable, which is unobserved during training. The model thus defines a ‚Äî one for each \\(\\mathbf{z}\\).\nTo compute the likelihood of a data point \\(\\mathbf{x}\\), we must marginalize over all possible latent variables: \\[\n  p(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n  \\]\nThis integral requires integrating over all possible values of the latent variable \\(\\mathbf{z}\\), which is often high-dimensional and enters the likelihood non-linearly through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of variational inference techniques like ELBO, which will be developed in the following sections.\n\nComputational Challenge\nThis integral requires integrating over:\n\nAll possible values of \\(\\mathbf{z}\\) (often high-dimensional)\nNon-linear transformations through neural networks\n\nResult: Exact computation is intractable, motivating variational inference techniques like ELBO (developed next)."
  },
  {
    "objectID": "vae.html#estimating-the-marginal-likelihood",
    "href": "vae.html#estimating-the-marginal-likelihood",
    "title": "Variational Autoencoders",
    "section": "Estimating the Marginal Likelihood",
    "text": "Estimating the Marginal Likelihood\n\nNaive Monte Carlo Estimation\nOne natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nHowever, this fails in practice. For most values of \\(z\\), the joint probability \\(p_\\theta(x, z)\\) is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely ‚Äúhits‚Äù likely values of \\(z\\).\n\n\nImportance Sampling\nTo address this, we use importance sampling, introducing a proposal distribution \\(q(z)\\):\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nThis gives an unbiased estimator of \\(p(x)\\) if \\(q(z)\\) is well-chosen (ideally close to \\(p_\\theta(z|x)\\)). Intuitively, we sample \\(z\\) more frequently in regions where \\(p_\\theta(x, z)\\) is high.\n\n\n\nLog likelihood\nOur goal is to optimize the log-likelihood, and the log of an expectation is not the same as the expectation of the log. That is,\n\\[\n\\log p(x) = log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\neq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nWhile the marginal likelihood p(x) can be estimated unbiasedly using importance sampling, estimating its logarithm \\(p(x)\\) introduces bias due to the concavity of the log function. This is captured by Jensen‚Äôs Inequality, which tells us:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\geq \\underbrace{\\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]}_{\\text{ELBO}}\n\\]\nThis means that the expected log of the estimator underestimates the true log-likelihood. The right-hand side provides a tractable surrogate objective known as the Evidence Lower Bound (ELBO), which is a biased lower bound to \\(\\log p(x)\\).\nThis bias is inherent in variational approximations and reflects a trade-off between computational tractability and estimation accuracy. Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood.\nIn the next section, we formally derive this bound and explore its components in detail."
  },
  {
    "objectID": "vae.html#why-variational-inference",
    "href": "vae.html#why-variational-inference",
    "title": "Variational Autoencoders",
    "section": "Why Variational Inference?",
    "text": "Why Variational Inference?\nComputing the true posterior distribution \\(p(z|x)\\) is intractable in most cases because it requires evaluating the marginal likelihood over all possible values of z:\n\\[\np(x) = \\int p(x, z) \\, dz\n\\]\nVariational inference tackles this by introducing a tractable, parameterized distribution \\(q(z)\\) to approximate \\(p(z|x)\\). We aim to make \\(q(z)\\) as close as possible to the true posterior by minimizing the KL divergence:\n\\[\nD_{\\text{KL}}(q(z) \\| p(z|x))\n\\]\nThis turns inference into an optimization problem. A key result is the Evidence Lower Bound (ELBO). See next section."
  },
  {
    "objectID": "vae.html#training-a-vae",
    "href": "vae.html#training-a-vae",
    "title": "Variational Autoencoders",
    "section": "Training a VAE",
    "text": "Training a VAE\n\nELBO Objective\nNow that we‚Äôve introduced the challenge of approximating the intractable posterior using variational inference, we turn our attention to deriving the Evidence Lower Bound (ELBO). This derivation reveals how optimizing a surrogate objective allows us to approximate the true log-likelihood of the data while keeping the approximate posterior close to the prior. The steps below walk through this formulation.\n\nStep 1: KL Divergence Objective\n\\[\\begin{equation}\nD_{KL}(q(z)\\|p(z|x; \\theta)) = \\sum_z q(z) \\log \\frac{q(z)}{p(z|x; \\theta)}\n\\end{equation}\\]\n\n\nStep 2: Apply Bayes‚Äô Rule\nSubstitute \\(p(z|x; \\theta) = \\frac{p(z,x;\\theta)}{p(x;\\theta)}\\): \\[\\begin{equation}\n= \\sum_z q(z) \\log \\left( \\frac{q(z) \\cdot p(x; \\theta)}{p(z, x; \\theta)} \\right)\n\\end{equation}\\]\n\n\nStep 3: Decompose Terms\n\\[\\begin{align}\n&= \\sum_z q(z) \\log q(z) + \\sum_z q(z) \\log p(x; \\theta) \\nonumber \\\\\n&\\quad - \\sum_z q(z) \\log p(z, x; \\theta) \\\\\n&= -H(q) + \\log p(x; \\theta) - \\mathbb{E}_q[\\log p(z,x;\\theta)]\n\\end{align}\\]\n\nNote: The term \\(\\mathcal{H}(q)\\) represents the entropy of the variational distribution \\(q(z|x)\\). Entropy is defined as:\n\\[\n\\mathcal{H}(q) = -\\sum_z q(z) \\log q(z) = -\\mathbb{E}_{q(z)}[\\log q(z)]\n\\]\nEntropy measures the amount of uncertainty or ‚Äúspread‚Äù in a distribution. A high-entropy \\(q(z)\\) places probability mass across a wide region of the latent space, while a low-entropy \\(q(z)\\) is more concentrated. This decomposition is key to understanding the KL divergence term in the ELBO.\n\n\n\nStep 4: Rearrange for ELBO\n\\[\n\\log p(x; \\theta) =\n\\underbrace{\n    \\mathbb{E}_q[\\log p(z, x; \\theta)] + \\mathcal{H}(q)\n}_{\\text{ELBO}}\n+D_{KL}(q(z)\\|p(z|x; \\theta))\n\\]\nThis equation shows that the log-likelihood \\(\\log p(x)\\) can be decomposed into the ELBO and the KL divergence between the approximate posterior and the true posterior. Since the KL divergence is always non-negative, the ELBO serves as a lower bound to the log-likelihood. By maximizing the ELBO, we indirectly minimize the KL divergence, bringing \\(q(z)\\) closer to \\(p(z|x)\\).\n Visualizing how \\(\\log p(x)\\) decomposes into the ELBO and KL divergence.\nSource: deepgenerativemodels.github.io\n\n\nKey Results\n\nEvidence Lower Bound (ELBO): \\[\\begin{equation}\n\\mathcal{L}(\\theta,\\phi) = \\mathbb{E}_{q(z;\\phi)}[\\log p(x,z;\\theta)] + H(q(z;\\phi))\n\\end{equation}\\]\nOptimization: \\[\\begin{equation}\n\\max_{\\theta,\\phi} \\mathcal{L}(\\theta,\\phi) \\Rightarrow\n\\begin{cases}\n\\text{Maximizes data likelihood} \\\\\n\\text{Minimizes } D_{KL}(q\\|p)\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "href": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "title": "Variational Autoencoders",
    "section": "Understanding the KL Divergence Term in the VAE Loss",
    "text": "Understanding the KL Divergence Term in the VAE Loss\nIn a VAE, the KL divergence term penalizes the encoder for producing latent distributions that deviate too far from the standard normal prior. This regularization has several important benefits:\n\nIt ensures that the latent space has a consistent structure, enabling meaningful sampling and interpolation.\nIt helps avoid large gaps between clusters in latent space by encouraging the encoder to distribute representations more uniformly.\nIt pushes the model to use the space around the origin more symmetrically and efficiently.\n\n\nBalancing KL Divergence and Reconstruction\nIn a Variational Autoencoder, the loss balances two goals:\n\nReconstruction ‚Äî making the output resemble the input\nRegularization ‚Äî keeping the latent space close to a standard normal distribution\n\nThis is captured by the loss function:\n\\[\n\\mathcal{L}_{\\text{VAE}} = \\text{Reconstruction Loss} + \\beta \\cdot D_{\\text{KL}}(q(z|x) \\,\\|\\, p(z))\n\\]\nThe parameter \\(\\beta\\) controls how strongly we enforce this regularization. Getting its value right is critical.\n\nWhen \\(\\beta\\) is too low:\n\nThe model mostly ignores the KL term, behaving like a plain autoencoder\nThe latent space becomes disorganized or fragmented\nSampling from the prior \\(p(z) = \\mathcal{N}(0, I)\\) results in unrealistic or broken outputs\n\n\n\nWhen \\(\\beta\\) is too high:\n\nThe encoder is forced to keep \\(q(z|x)\\) too close to the prior\nIt encodes less information about the input\nReconstructions become blurry or generic, since the decoder gets little to work with\n\n\nChoosing \\(\\beta\\) carefully is essential for balancing generalization and fidelity.\nA well-tuned \\(\\beta\\) helps the VAE both reconstruct accurately and generate new samples that resemble the training data.\n\n\n\n\nGradient Challenge\nIn variational inference, we approximate the true posterior \\(p(z|x)\\) with a tractable distribution \\(q_\\phi(z|x)\\). This allows us to optimize the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nOur goal is to maximize this objective with respect to both \\(\\theta\\) and \\(\\phi\\). While computing the gradient with respect to \\(\\theta\\) is straightforward, optimizing with respect to \\(\\phi\\) presents a challenge.\nThe complication arises because \\(\\phi\\) appears both in the density \\(q_\\phi(z|x)\\) and in the expectation operator. That is:\n\\[\n\\nabla_\\phi \\mathbb{E}_{q_\\phi(z|x)} \\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nThis gradient is hard to compute directly because we‚Äôre sampling from a distribution that depends on the parameters we‚Äôre trying to update.\n\n\n\nThe Reparameterization Trick\nTo make this expression differentiable, we reparameterize the random variable \\(z\\) as a deterministic transformation of a parameter-free noise variable \\(\\epsilon\\):\n\\[\n\\epsilon \\sim \\mathcal{N}(0, I), \\quad z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n\\]\nThis turns the expectation into:\n\\[\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nwhere \\(z\\) is now a differentiable function of \\(\\phi\\) and \\(\\epsilon\\).\n\n\n\nReparameterization Trick Diagram\n\n\nImage source: Wikipedia (CC BY-SA 4.0)\nThis diagram illustrates how the reparameterization trick enables differentiable sampling:\n\nIn the original formulation, \\(z\\) is sampled directly from a learned distribution, breaking the gradient flow.\nIn the reparameterized formulation, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\), and compute \\(z = \\mu + \\sigma \\cdot \\epsilon\\), making the sampling path fully differentiable.\n\n\nMonte Carlo Approximation\nWe approximate the expectation using Monte Carlo sampling:\n\\[\n\\mathbb{E}_{\\epsilon}[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)] \\approx \\frac{1}{K} \\sum_{k=1}^K \\left[\\log p_\\theta(x, z^{(k)}) - \\log q_\\phi(z^{(k)}|x)\\right]\n\\]\nwith:\n\\[\nz^{(k)} = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon^{(k)}, \\quad \\epsilon^{(k)} \\sim \\mathcal{N}(0, I)\n\\]\nThis enables us to compute gradients using backpropagation.\n\n\n\nSummary\n\nVariational inference introduces a gradient challenge because \\(q_\\phi(z|x)\\) depends on \\(\\phi\\)\nThe reparameterization trick expresses \\(z\\) as a differentiable function of noise and \\(\\phi\\)\nThis allows us to use backpropagation to optimize the ELBO efficiently\n\n\n\n\n\nAmortized Inference\nIn classical variational inference, we introduce a separate set of variational parameters \\(\\phi^i\\) for each datapoint \\(x^i\\) to approximate the true posterior \\(p(z|x^i)\\). However:\n\nOptimizing a separate \\(\\phi^i\\) for every datapoint is computationally expensive and does not scale to large datasets.\n\n\n\nThe Key Idea: Amortization\nInstead of learning and storing a separate \\(\\phi^i\\) for every datapoint, we learn a single parametric function \\(f_\\phi(x)\\) ‚Äî typically a neural network ‚Äî that maps each input \\(x\\) to the parameters of the approximate posterior:\n\\[\nq_\\phi(z|x) = \\mathcal{N}\\left(\\mu_\\phi(x), \\sigma^2_\\phi(x)\\right)\n\\]\nHere, \\(\\phi\\) are the shared parameters of the encoder network, and \\(\\mu_\\phi(x), \\sigma_\\phi(x)\\) are its outputs.\nThis is like learning a regression function that predicts the optimal variational parameters for any input \\(x\\).\n\n\n\n\nTraining with Amortized Inference\nOur training objective remains the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nWe optimize both \\(\\theta\\) (decoder parameters) and \\(\\phi\\) (encoder parameters) using stochastic gradient descent.\n\nAlgorithm:\n\nInitialize \\(\\theta^{(0)}, \\phi^{(0)}\\)\nSample a datapoint \\(x^i\\)\nUse \\(f_\\phi(x^i)\\) to produce \\(\\mu^i, \\sigma^i\\)\nSample \\(z^i = \\mu^i + \\sigma^i \\cdot \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)\nEstimate the ELBO and compute gradients w.r.t. \\(\\theta, \\phi\\)\nUpdate \\(\\theta, \\phi\\) using gradient descent\nUpdate \\(\\theta\\), \\(\\phi\\) using gradient descent:\n\n\\[\n\\phi \\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\n\\[\n\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\nwhere \\(\\mathcal{B}\\) is the current minibatch and \\(\\tilde{\\nabla}\\) indicates a stochastic gradient approximation.\n\n\n\n\nSummary\n\nAmortized inference replaces per-datapoint optimization with a single learned mapping \\(f_\\phi(x)\\)\nThis makes variational inference scalable and efficient\nThe model can generalize to unseen inputs by predicting variational parameters on-the-fly\n\n\nNote: Following common practice in the literature, we use \\(\\phi\\) to denote the parameters of the encoder network, even though it now defines a function rather than individual variational parameters."
  },
  {
    "objectID": "vae.html#applications-of-vaes",
    "href": "vae.html#applications-of-vaes",
    "title": "Variational Autoencoders",
    "section": "Applications of VAEs",
    "text": "Applications of VAEs\nVariational Autoencoders are widely used in:\n\nImage Generation: VAEs can generate new images similar to the training data (e.g., MNIST digits)\n\nAnomaly Detection: High reconstruction error flags unusual data points\n\nRepresentation Learning: Latent space captures features for downstream tasks\n\n\nüòé Face Generation with Convolutional VAE\nTo complement the theory, I‚Äôve built a full PyTorch implementation of a Variational Autoencoder trained on the CelebA dataset.\nüìò The notebook walks through: - Defining the encoder, decoder, and reparameterization trick - Implementing the ELBO loss function (reconstruction + KL divergence) - Training the model on face images - Generating new faces from random latent vectors\n\nüìì View on GitHub"
  },
  {
    "objectID": "vae.html#further-reading",
    "href": "vae.html#further-reading",
    "title": "Variational Autoencoders",
    "section": "Further Reading",
    "text": "Further Reading\nFor readers interested in diving deeper into the theory and applications of variational autoencoders, the following resources are recommended:\n\nTutorial on Variational Autoencoders\nCarl Doersch (2016)\nhttps://arxiv.org/pdf/1606.05908\nAuto-Encoding Variational Bayes\nKingma & Welling (2014) ‚Äî the original VAE paper\nhttps://arxiv.org/pdf/1312.6114\nThe Challenges of Amortized Inference for Structured Prediction\nCremer, Li, & Duvenaud (2019)\nhttps://arxiv.org/pdf/1906.02691\nDeep Generative Models course notes\nhttps://deepgenerativemodels.github.io/notes/vae/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections-feel-free-to-clone-tweak-or-extend-it.",
    "href": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections-feel-free-to-clone-tweak-or-extend-it.",
    "title": "Variational Autoencoders",
    "section": "This example is designed to reinforce the theoretical concepts from earlier sections ‚Äî feel free to clone, tweak, or extend it.",
    "text": "This example is designed to reinforce the theoretical concepts from earlier sections ‚Äî feel free to clone, tweak, or extend it."
  }
]