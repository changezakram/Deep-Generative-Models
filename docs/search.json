[
  {
    "objectID": "vae_mnist.html",
    "href": "vae_mnist.html",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "",
    "text": "This notebook explains a Variational Autoencoder (VAE) trained on the MNIST dataset using PyTorch.\nEach step is annotated with detailed comments to help beginners understand what‚Äôs happening.\n!pip install torch torchvision"
  },
  {
    "objectID": "vae_mnist.html#import-required-libraries",
    "href": "vae_mnist.html#import-required-libraries",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "1. Import Required Libraries",
    "text": "1. Import Required Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "href": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "2. Load and Prepare the MNIST Dataset",
    "text": "2. Load and Prepare the MNIST Dataset\n\n# We transform MNIST images into tensors.\ntransform = transforms.ToTensor()\n\n# Download and load the training data\ntrain_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n\n# DataLoader for batching and shuffling\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
  },
  {
    "objectID": "vae_mnist.html#define-the-vae-model",
    "href": "vae_mnist.html#define-the-vae-model",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "3. Define the VAE Model",
    "text": "3. Define the VAE Model\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):\n        super(VAE, self).__init__()\n        # Encoder layers: input -&gt; hidden -&gt; (mu, logvar)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # outputs mean of q(z|x)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # outputs log-variance of q(z|x)\n\n        # Decoder layers: latent -&gt; hidden -&gt; reconstruction\n        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, input_dim)\n\n    def encode(self, x):\n        # Apply a hidden layer then split into mean and logvar\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        # Apply the reparameterization trick\n        std = torch.exp(0.5 * logvar)      # standard deviation\n        eps = torch.randn_like(std)        # random normal noise\n        return mu + eps * std              # sample z\n\n    def decode(self, z):\n        # Reconstruct input from latent representation\n        h = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h))  # Output in [0, 1] range for binary MNIST\n\n    def forward(self, x):\n        # Full VAE forward pass\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        return recon_x, mu, logvar"
  },
  {
    "objectID": "vae_mnist.html#define-the-elbo-loss",
    "href": "vae_mnist.html#define-the-elbo-loss",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "4. Define the ELBO Loss",
    "text": "4. Define the ELBO Loss\n\ndef elbo_loss(recon_x, x, mu, logvar):\n    # Binary cross-entropy for reconstruction\n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n\n    # KL divergence term to regularize q(z|x) against standard normal p(z)\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    # Total loss is negative ELBO\n    return BCE + KLD"
  },
  {
    "objectID": "vae_mnist.html#train-the-vae",
    "href": "vae_mnist.html#train-the-vae",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "5. Train the VAE",
    "text": "5. Train the VAE\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = VAE().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for x, _ in train_loader:\n        x = x.view(-1, 784).to(device)               # Flatten 28x28 images into 784 vectors\n        recon_x, mu, logvar = model(x)               # Forward pass\n        loss = elbo_loss(recon_x, x, mu, logvar)     # Compute loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader.dataset):.2f}\")"
  },
  {
    "objectID": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "href": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "6. Visualize Original and Reconstructed Digits",
    "text": "6. Visualize Original and Reconstructed Digits\n\nmodel.eval()\nwith torch.no_grad():\n    x, _ = next(iter(train_loader))\n    x = x.view(-1, 784).to(device)\n    recon_x, _, _ = model(x)\n\n    # Convert back to image format\n    x = x.view(-1, 1, 28, 28).cpu()\n    recon_x = recon_x.view(-1, 1, 28, 28).cpu()\n\n    fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n    for i in range(10):\n        axs[0, i].imshow(x[i][0], cmap='gray')\n        axs[0, i].axis('off')\n        axs[1, i].imshow(recon_x[i][0], cmap='gray')\n        axs[1, i].axis('off')\n    axs[0, 0].set_ylabel(\"Original\", fontsize=12)\n    axs[1, 0].set_ylabel(\"Reconstruction\", fontsize=12)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "vae_mnist.html#visualize-latent-space",
    "href": "vae_mnist.html#visualize-latent-space",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "7. Visualize Latent Space",
    "text": "7. Visualize Latent Space\n\nimport seaborn as sns\n\nmodel.eval()\nall_z = []\nall_labels = []\n\n# Go through a few batches and collect latent representations\nwith torch.no_grad():\n    for x, y in train_loader:\n        x = x.view(-1, 784).to(device)\n        mu, _ = model.encode(x)  # use the mean as representation\n        all_z.append(mu.cpu())\n        all_labels.append(y)\n\n# Concatenate all batches\nz = torch.cat(all_z, dim=0).numpy()\nlabels = torch.cat(all_labels, dim=0).numpy()\n\n# Plot with seaborn\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=z[:, 0], y=z[:, 1], hue=labels, palette=\"tab10\", s=15)\nplt.title(\"Latent Space Visualization (using Œº)\")\nplt.xlabel(\"z[0]\")\nplt.ylabel(\"z[1]\")\nplt.legend(title=\"Digit\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#key-model-families",
    "href": "index.html#key-model-families",
    "title": "Deep Generative Models",
    "section": "Key Model Families",
    "text": "Key Model Families\n\n1. Variational Autoencoders (VAEs)\n\nProbabilistic encoder-decoder architecture\nLearn latent spaces with Gaussian distributions\nLearn more about VAEs\n\n\n\n2. Autoregressive Models\n\nGenerate sequences element-by-element\nExamples: PixelCNN, WaveNet\n\n\n\n3. Generative Adversarial Networks (GANs)\n\nAdversarial training with generator/discriminator\nHigh-quality image generation\n\n\n\n4. Normalizing Flows\n\nInvertible transformations for exact likelihood\nFlexible density estimation\n\n\n\n5. Energy-Based Models (EBMs)\n\nLearn energy functions for data distribution\nFlexible for discrete/continuous data\n\n\n\n6. Diffusion Models\n\nIterative denoising process\nState-of-the-art image generation"
  },
  {
    "objectID": "vae_faces.html",
    "href": "vae_faces.html",
    "title": "Face Generation with Convolutional VAE",
    "section": "",
    "text": "This notebook implements a convolutional variational autoencoder (VAE) trained on the CelebA face dataset using PyTorch.\nIt uses convolutional layers to encode and decode 64x64 face images, and demonstrates generation by sampling from the latent space.\n!pip install torch torchvision matplotlib"
  },
  {
    "objectID": "vae_faces.html#import-required-libraries",
    "href": "vae_faces.html#import-required-libraries",
    "title": "Face Generation with Convolutional VAE",
    "section": "1. Import Required Libraries",
    "text": "1. Import Required Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport os"
  },
  {
    "objectID": "vae_faces.html#load-celeba-dataset",
    "href": "vae_faces.html#load-celeba-dataset",
    "title": "Face Generation with Convolutional VAE",
    "section": "2. Load CelebA Dataset",
    "text": "2. Load CelebA Dataset\n\ntransform = transforms.Compose([\n    transforms.CenterCrop(128),\n    transforms.Resize(64),\n    transforms.ToTensor()\n])\n\n# Download CelebA dataset (you must accept the license the first time)\nceleba = torchvision.datasets.CelebA(root=\"./data\", split=\"train\", download=True, transform=transform)\ndataloader = DataLoader(celeba, batch_size=128, shuffle=True)"
  },
  {
    "objectID": "vae_faces.html#define-the-convolutional-vae",
    "href": "vae_faces.html#define-the-convolutional-vae",
    "title": "Face Generation with Convolutional VAE",
    "section": "3. Define the Convolutional VAE",
    "text": "3. Define the Convolutional VAE\n\nVAE Architecture Explained\n\nEncoder: uses 4 convolutional layers to compress the image into a latent vector.\nLatent Space: the model learns a distribution (mean and variance) over latent variables.\nReparameterization: samples from this distribution to make training differentiable.\nDecoder: mirrors the encoder with transposed convolutions to reconstruct the image.\n\n\nclass ConvVAE(nn.Module):\n    def __init__(self, latent_dim=100):\n        super().__init__()\n        self.latent_dim = latent_dim\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 4, 2, 1),  # [B, 32, 32, 32]\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 4, 2, 1),  # [B, 64, 16, 16]\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 4, 2, 1), # [B, 128, 8, 8]\n            nn.ReLU(),\n            nn.Conv2d(128, 256, 4, 2, 1), # [B, 256, 4, 4]\n            nn.ReLU()\n        )\n        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n\n        # Decoder\n        self.fc_decode = nn.Linear(latent_dim, 256 * 4 * 4)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # [B, 128, 8, 8]\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # [B, 64, 16, 16]\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # [B, 32, 32, 32]\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 3, 4, 2, 1),     # [B, 3, 64, 64]\n            nn.Sigmoid()\n        )\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = h.view(h.size(0), -1)\n        return self.fc_mu(h), self.fc_logvar(h)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z):\n        h = self.fc_decode(z).view(-1, 256, 4, 4)\n        return self.decoder(h)\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar"
  },
  {
    "objectID": "vae_faces.html#define-elbo-loss",
    "href": "vae_faces.html#define-elbo-loss",
    "title": "Face Generation with Convolutional VAE",
    "section": "4. Define ELBO Loss",
    "text": "4. Define ELBO Loss\n\ndef elbo_loss(recon_x, x, mu, logvar):\n    recon_loss = F.mse_loss(recon_x, x, reduction='sum')  # MSE for real-valued images\n    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kld"
  },
  {
    "objectID": "vae_faces.html#train-the-vae",
    "href": "vae_faces.html#train-the-vae",
    "title": "Face Generation with Convolutional VAE",
    "section": "5. Train the VAE",
    "text": "5. Train the VAE\nThis section trains the convolutional variational autoencoder (VAE) on the CelebA dataset using ELBO loss. After training, we will sample from the latent space to generate new face images.\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ConvVAE(latent_dim=100).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nepochs = 5  # Adjust for better quality\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0\n    for x, _ in dataloader:\n        x = x.to(device)\n        recon_x, mu, logvar = model(x)\n        loss = elbo_loss(recon_x, x, mu, logvar)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.2f}\")\n\nEpoch 1, Loss: 29744.51\nEpoch 2, Loss: 21248.06\nEpoch 3, Loss: 20511.19\nEpoch 4, Loss: 20139.32\nEpoch 5, Loss: 19904.67"
  },
  {
    "objectID": "vae_faces.html#generate-new-faces",
    "href": "vae_faces.html#generate-new-faces",
    "title": "Face Generation with Convolutional VAE",
    "section": "6. Generate New Faces",
    "text": "6. Generate New Faces\nIn this section, we sample random latent vectors from a standard normal distribution and pass them through the decoder to generate new face images.\n\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n\n# Function to visualize generated images\ndef show_generated_images(samples, nrow=8):\n    grid = make_grid(samples, nrow=nrow)\n    plt.figure(figsize=(12, 6))\n    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n    plt.axis(\"off\")\n    plt.title(\"Generated Faces\")\n    plt.show()\n\n# Sample z ~ N(0, I) and generate faces\nmodel.eval()\nwith torch.no_grad():\n    z = torch.randn(64, model.latent_dim).to(device)\n    generated = model.decode(z)\n\n# Display\nshow_generated_images(generated)"
  },
  {
    "objectID": "vae_faces.html#reconstruction-comparison",
    "href": "vae_faces.html#reconstruction-comparison",
    "title": "Face Generation with Convolutional VAE",
    "section": "7. Reconstruction Comparison",
    "text": "7. Reconstruction Comparison\nThis section compares original images with their reconstructions to show how well the VAE preserves features.\n\nmodel.eval()\nwith torch.no_grad():\n    # Take a small batch\n    x, _ = next(iter(dataloader))\n    x = x.to(device)\n    recon_x, _, _ = model(x)\n\n    # Convert to CPU and reshape for plotting\n    x = x[:8].cpu()\n    recon_x = recon_x[:8].cpu()\n\n    from torchvision.utils import make_grid\n\n    def show_grid(images, title):\n        grid = make_grid(images, nrow=8)\n        plt.figure(figsize=(12, 3))\n        plt.imshow(grid.permute(1, 2, 0))\n        plt.title(title)\n        plt.axis('off')\n        plt.show()\n\n    show_grid(x, \"Original Images\")\n    show_grid(recon_x, \"Reconstructed Images\")"
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities ‚Äî they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation."
  },
  {
    "objectID": "vae.html#autoencoders-vs-variational-autoencoders",
    "href": "vae.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities ‚Äî they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation."
  },
  {
    "objectID": "vae.html#probabilistic-framework",
    "href": "vae.html#probabilistic-framework",
    "title": "Variational Autoencoders",
    "section": "Probabilistic Framework",
    "text": "Probabilistic Framework\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\nHere, \\(\\mathbf{z}\\) acts as a hidden or latent variable, which is unobserved during training. The model thus defines a ‚Äî one for each \\(\\mathbf{z}\\).\nTo compute the likelihood of a data point \\(\\mathbf{x}\\), we must marginalize over all possible latent variables: \\[\n  p(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n  \\]\nThis integral requires integrating over all possible values of the latent variable \\(\\mathbf{z}\\), which is often high-dimensional and enters the likelihood non-linearly through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of variational inference techniques like ELBO, which will be developed in the following sections.\n\nComputational Challenge\nThis integral requires integrating over:\n\nAll possible values of \\(\\mathbf{z}\\) (often high-dimensional)\nNon-linear transformations through neural networks\n\nResult: Exact computation is intractable, motivating variational inference techniques like ELBO (developed next)."
  },
  {
    "objectID": "vae.html#estimating-the-marginal-likelihood",
    "href": "vae.html#estimating-the-marginal-likelihood",
    "title": "Variational Autoencoders",
    "section": "Estimating the Marginal Likelihood",
    "text": "Estimating the Marginal Likelihood\n\nNaive Monte Carlo Estimation\nOne natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nHowever, this fails in practice. For most values of \\(z\\), the joint probability \\(p_\\theta(x, z)\\) is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely ‚Äúhits‚Äù likely values of \\(z\\).\n\n\nImportance Sampling\nTo address this, we use importance sampling, introducing a proposal distribution \\(q(z)\\):\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nThis gives an unbiased estimator of \\(p(x)\\) if \\(q(z)\\) is well-chosen (ideally close to \\(p_\\theta(z|x)\\)). Intuitively, we sample \\(z\\) more frequently in regions where \\(p_\\theta(x, z)\\) is high.\n\n\n\nLog likelihood\nOur goal is to optimize the log-likelihood, and the log of an expectation is not the same as the expectation of the log. That is,\n\\[\n\\log p(x) = log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\neq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nWhile the marginal likelihood p(x) can be estimated unbiasedly using importance sampling, estimating its logarithm \\(p(x)\\) introduces bias due to the concavity of the log function. This is captured by Jensen‚Äôs Inequality, which tells us:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\geq \\underbrace{\\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]}_{\\text{ELBO}}\n\\]\nThis means that the expected log of the estimator underestimates the true log-likelihood. The right-hand side provides a tractable surrogate objective known as the Evidence Lower Bound (ELBO), which is a biased lower bound to \\(\\log p(x)\\).\nThis bias is inherent in variational approximations and reflects a trade-off between computational tractability and estimation accuracy. Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood.\nIn the next section, we formally derive this bound and explore its components in detail."
  },
  {
    "objectID": "vae.html#why-variational-inference",
    "href": "vae.html#why-variational-inference",
    "title": "Variational Autoencoders",
    "section": "Why Variational Inference?",
    "text": "Why Variational Inference?\nComputing the true posterior distribution \\(p(z|x)\\) is intractable in most cases because it requires evaluating the marginal likelihood over all possible values of z:\n\\[\np(x) = \\int p(x, z) \\, dz\n\\]\nVariational inference tackles this by introducing a tractable, parameterized distribution \\(q(z)\\) to approximate \\(p(z|x)\\). We aim to make \\(q(z)\\) as close as possible to the true posterior by minimizing the KL divergence:\n\\[\nD_{\\text{KL}}(q(z) \\| p(z|x))\n\\]\nThis turns inference into an optimization problem. A key result is the Evidence Lower Bound (ELBO). See next section."
  },
  {
    "objectID": "vae.html#training-a-vae",
    "href": "vae.html#training-a-vae",
    "title": "Variational Autoencoders",
    "section": "Training a VAE",
    "text": "Training a VAE\n\nELBO Objective\nNow that we‚Äôve introduced the challenge of approximating the intractable posterior using variational inference, we turn our attention to deriving the Evidence Lower Bound (ELBO). This derivation reveals how optimizing a surrogate objective allows us to approximate the true log-likelihood of the data while keeping the approximate posterior close to the prior. The steps below walk through this formulation.\n\nStep 1: KL Divergence Objective\n\\[\\begin{equation}\nD_{KL}(q(z)\\|p(z|x; \\theta)) = \\sum_z q(z) \\log \\frac{q(z)}{p(z|x; \\theta)}\n\\end{equation}\\]\n\n\nStep 2: Apply Bayes‚Äô Rule\nSubstitute \\(p(z|x; \\theta) = \\frac{p(z,x;\\theta)}{p(x;\\theta)}\\): \\[\\begin{equation}\n= \\sum_z q(z) \\log \\left( \\frac{q(z) \\cdot p(x; \\theta)}{p(z, x; \\theta)} \\right)\n\\end{equation}\\]\n\n\nStep 3: Decompose Terms\n\\[\\begin{align}\n&= \\sum_z q(z) \\log q(z) + \\sum_z q(z) \\log p(x; \\theta) \\nonumber \\\\\n&\\quad - \\sum_z q(z) \\log p(z, x; \\theta) \\\\\n&= -H(q) + \\log p(x; \\theta) - \\mathbb{E}_q[\\log p(z,x;\\theta)]\n\\end{align}\\]\n\nNote: The term \\(\\mathcal{H}(q)\\) represents the entropy of the variational distribution \\(q(z|x)\\). Entropy is defined as:\n\\[\n\\mathcal{H}(q) = -\\sum_z q(z) \\log q(z) = -\\mathbb{E}_{q(z)}[\\log q(z)]\n\\]\nEntropy measures the amount of uncertainty or ‚Äúspread‚Äù in a distribution. A high-entropy \\(q(z)\\) places probability mass across a wide region of the latent space, while a low-entropy \\(q(z)\\) is more concentrated. This decomposition is key to understanding the KL divergence term in the ELBO.\n\n\n\nStep 4: Rearrange for ELBO\n\\[\n\\log p(x; \\theta) =\n\\underbrace{\n    \\mathbb{E}_q[\\log p(z, x; \\theta)] + \\mathcal{H}(q)\n}_{\\text{ELBO}}\n+D_{KL}(q(z)\\|p(z|x; \\theta))\n\\]\nThis equation shows that the log-likelihood \\(\\log p(x)\\) can be decomposed into the ELBO and the KL divergence between the approximate posterior and the true posterior. Since the KL divergence is always non-negative, the ELBO serves as a lower bound to the log-likelihood. By maximizing the ELBO, we indirectly minimize the KL divergence, bringing \\(q(z)\\) closer to \\(p(z|x)\\).\n\n\nKey Results\n\nEvidence Lower Bound (ELBO): \\[\\begin{equation}\n\\mathcal{L}(\\theta,\\phi) = \\mathbb{E}_{q(z;\\phi)}[\\log p(x,z;\\theta)] + H(q(z;\\phi))\n\\end{equation}\\]\nOptimization: \\[\\begin{equation}\n\\max_{\\theta,\\phi} \\mathcal{L}(\\theta,\\phi) \\Rightarrow\n\\begin{cases}\n\\text{Maximizes data likelihood} \\\\\n\\text{Minimizes } D_{KL}(q\\|p)\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "href": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "title": "Variational Autoencoders",
    "section": "Understanding the KL Divergence Term in the VAE Loss",
    "text": "Understanding the KL Divergence Term in the VAE Loss\nIn a VAE, the KL divergence term penalizes the encoder for producing latent distributions that deviate too far from the standard normal prior. This regularization has several important benefits:\n\nIt ensures that the latent space has a consistent structure, enabling meaningful sampling and interpolation.\nIt helps avoid large gaps between clusters in latent space by encouraging the encoder to distribute representations more uniformly.\nIt pushes the model to use the space around the origin more symmetrically and efficiently.\n\n\nBalancing KL Divergence and Reconstruction\nIn a Variational Autoencoder, the loss balances two goals:\n\nReconstruction ‚Äî making the output resemble the input\nRegularization ‚Äî keeping the latent space close to a standard normal distribution\n\nThis is captured by the loss function:\n\\[\n\\mathcal{L}_{\\text{VAE}} = \\text{Reconstruction Loss} + \\beta \\cdot D_{\\text{KL}}(q(z|x) \\,\\|\\, p(z))\n\\]\nThe parameter \\(\\beta\\) controls how strongly we enforce this regularization. Getting its value right is critical.\n\n\n\nüîª When \\(\\beta\\) is too low:\n\nThe model mostly ignores the KL term, behaving like a plain autoencoder\nThe latent space becomes disorganized or fragmented\nSampling from the prior \\(p(z) = \\mathcal{N}(0, I)\\) results in unrealistic or broken outputs\n\n\n\n\nüî∫ When \\(\\beta\\) is too high:\n\nThe encoder is forced to keep \\(q(z|x)\\) too close to the prior\nIt encodes less information about the input\nReconstructions become blurry or generic, since the decoder gets little to work with\n\n\n\nChoosing \\(\\beta\\) carefully is essential for balancing generalization and fidelity.\nA well-tuned \\(\\beta\\) helps the VAE both reconstruct accurately and generate new samples that resemble the training data.\n\n\n\nGradient Challenge\nIn variational inference, we approximate the true posterior \\(p(z|x)\\) with a tractable distribution \\(q_\\phi(z|x)\\). This allows us to optimize the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nOur goal is to maximize this objective with respect to both \\(\\theta\\) and \\(\\phi\\). While computing the gradient with respect to \\(\\theta\\) is straightforward, optimizing with respect to \\(\\phi\\) presents a challenge.\nThe complication arises because \\(\\phi\\) appears both in the density \\(q_\\phi(z|x)\\) and in the expectation operator. That is:\n\\[\n\\nabla_\\phi \\mathbb{E}_{q_\\phi(z|x)} \\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nThis gradient is hard to compute directly because we‚Äôre sampling from a distribution that depends on the parameters we‚Äôre trying to update.\n\n\n\nThe Reparameterization Trick\nTo make this expression differentiable, we reparameterize the random variable \\(z\\) as a deterministic transformation of a parameter-free noise variable \\(\\epsilon\\):\n\\[\n\\epsilon \\sim \\mathcal{N}(0, I), \\quad z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n\\]\nThis turns the expectation into:\n\\[\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nwhere \\(z\\) is now a differentiable function of \\(\\phi\\) and \\(\\epsilon\\).\n\n\nMonte Carlo Approximation\nWe approximate the expectation using Monte Carlo sampling:\n\\[\n\\mathbb{E}_{\\epsilon}[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)] \\approx \\frac{1}{K} \\sum_{k=1}^K \\left[\\log p_\\theta(x, z^{(k)}) - \\log q_\\phi(z^{(k)}|x)\\right]\n\\]\nwith:\n\\[\nz^{(k)} = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon^{(k)}, \\quad \\epsilon^{(k)} \\sim \\mathcal{N}(0, I)\n\\]\nThis enables us to compute gradients using backpropagation.\n\n\n\nSummary\n\nVariational inference introduces a gradient challenge because \\(q_\\phi(z|x)\\) depends on \\(\\phi\\)\nThe reparameterization trick expresses \\(z\\) as a differentiable function of noise and \\(\\phi\\)\nThis allows us to use backpropagation to optimize the ELBO efficiently\n\n\n\n\n\nAmortized Inference\nIn classical variational inference, we introduce a separate set of variational parameters \\(\\phi^i\\) for each datapoint \\(x^i\\) to approximate the true posterior \\(p(z|x^i)\\). However:\n\nOptimizing a separate \\(\\phi^i\\) for every datapoint is computationally expensive and does not scale to large datasets.\n\n\n\nThe Key Idea: Amortization\nInstead of learning and storing a separate \\(\\phi^i\\) for every datapoint, we learn a single parametric function \\(f_\\phi(x)\\) ‚Äî typically a neural network ‚Äî that maps each input \\(x\\) to the parameters of the approximate posterior:\n\\[\nq_\\phi(z|x) = \\mathcal{N}\\left(\\mu_\\phi(x), \\sigma^2_\\phi(x)\\right)\n\\]\nHere, \\(\\phi\\) are the shared parameters of the encoder network, and \\(\\mu_\\phi(x), \\sigma_\\phi(x)\\) are its outputs.\nThis is like learning a regression function that predicts the optimal variational parameters for any input \\(x\\).\n\n\n\n\nTraining with Amortized Inference\nOur training objective remains the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nWe optimize both \\(\\theta\\) (decoder parameters) and \\(\\phi\\) (encoder parameters) using stochastic gradient descent.\n\nAlgorithm:\n\nInitialize \\(\\theta^{(0)}, \\phi^{(0)}\\)\nSample a datapoint \\(x^i\\)\nUse \\(f_\\phi(x^i)\\) to produce \\(\\mu^i, \\sigma^i\\)\nSample \\(z^i = \\mu^i + \\sigma^i \\cdot \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)\nEstimate the ELBO and compute gradients w.r.t. \\(\\theta, \\phi\\)\nUpdate \\(\\theta, \\phi\\) using gradient descent\nUpdate \\(\\theta\\), \\(\\phi\\) using gradient descent:\n\n\\[\n\\phi \\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\n\\[\n\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\nwhere \\(\\mathcal{B}\\) is the current minibatch and \\(\\tilde{\\nabla}\\) indicates a stochastic gradient approximation.\n\n\n\n\nSummary\n\nAmortized inference replaces per-datapoint optimization with a single learned mapping \\(f_\\phi(x)\\)\nThis makes variational inference scalable and efficient\nThe model can generalize to unseen inputs by predicting variational parameters on-the-fly\n\n\nNote: Following common practice in the literature, we use \\(\\phi\\) to denote the parameters of the encoder network, even though it now defines a function rather than individual variational parameters."
  },
  {
    "objectID": "vae.html#applications-of-vaes",
    "href": "vae.html#applications-of-vaes",
    "title": "Variational Autoencoders",
    "section": "Applications of VAEs",
    "text": "Applications of VAEs\nVariational Autoencoders are widely used in:\n\nImage Generation: VAEs can generate new images similar to the training data (e.g., MNIST digits)\n\nAnomaly Detection: High reconstruction error flags unusual data points\n\nRepresentation Learning: Latent space captures features for downstream tasks\n\n\nüß† VAE on MNIST (Beginner Tutorial)\nA beginner-friendly VAE implementation trained on the MNIST digits dataset.\nIncludes extensive comments, training loop, and latent space visualization.\n\nüìì View on GitHub\n\n\n\nüòé Face Generation with Convolutional VAE\nA convolutional VAE trained on the CelebA dataset.\nLearn how to generate realistic faces from latent samples.\n\nüìì View on GitHub ‚Äî"
  },
  {
    "objectID": "vae.html#further-reading",
    "href": "vae.html#further-reading",
    "title": "Variational Autoencoders",
    "section": "Further Reading",
    "text": "Further Reading\nFor readers interested in diving deeper into the theory and applications of variational autoencoders, the following resources are recommended:\n\nTutorial on Variational Autoencoders\nCarl Doersch (2016)\nhttps://arxiv.org/pdf/1606.05908\nAuto-Encoding Variational Bayes\nKingma & Welling (2014) ‚Äî the original VAE paper\nhttps://arxiv.org/pdf/1312.6114\nThe Challenges of Amortized Inference for Structured Prediction\nCremer, Li, & Duvenaud (2019)\nhttps://arxiv.org/pdf/1906.02691\nDeep Generative Models course notes\nhttps://deepgenerativemodels.github.io/notes/vae/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]