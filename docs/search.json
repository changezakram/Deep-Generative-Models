[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.\n\n\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\n\n\n\n\n\\(\\mathbf{z}\\) is a hidden variable, unobserved during training\nDefines a mixture of infinitely many Gaussians\nThe marginal likelihood requires integration: \\[\np(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n\\]\n\n\n\n\nThis integral requires integrating over: - All possible values of \\(\\mathbf{z}\\) (often high-dimensional) - Non-linear transformations through neural networks\nResult: Exact computation is intractable, motivating variational inference techniques like ELBO (developed next)."
  },
  {
    "objectID": "vae.html#autoencoders-vs-variational-autoencoders",
    "href": "vae.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.\n\n\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\n\n\n\n\n\\(\\mathbf{z}\\) is a hidden variable, unobserved during training\nDefines a mixture of infinitely many Gaussians\nThe marginal likelihood requires integration: \\[\np(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n\\]\n\n\n\n\nThis integral requires integrating over: - All possible values of \\(\\mathbf{z}\\) (often high-dimensional) - Non-linear transformations through neural networks\nResult: Exact computation is intractable, motivating variational inference techniques like ELBO (developed next)."
  },
  {
    "objectID": "vae.html#estimation-techniques",
    "href": "vae.html#estimation-techniques",
    "title": "Variational Autoencoders",
    "section": "Estimation Techniques",
    "text": "Estimation Techniques\n\nNaive Monte Carlo Estimation\nOne natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nHowever, this fails in practice. For most values of \\(z\\), the joint probability \\(p_\\theta(x, z)\\) is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has:\n\nHigh variance (sparse meaningful samples)\nLow hit rate (rarely samples likely \\(z\\) values)\n\n\n\nImportance Sampling\nTo address this, we use importance sampling, introducing a proposal distribution \\(q(z)\\):\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nKey Properties: - Unbiased estimator when \\(\\text{supp}(q) \\supseteq \\text{supp}(p_\\theta)\\) - Optimal \\(q(z) \\approx p_\\theta(z|x)\\) (the true posterior) - More efficient sampling in high-probability regions\n\nThe Log-Likelihood Challenge\nFor optimization, we need the log-likelihood, but:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\neq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nJensen’s Inequality reveals the relationship:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\geq \\underbrace{\\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]}_{\\text{ELBO}}\n\\]\nThis gives us the Evidence Lower Bound (ELBO), which is lower bounds the true log-likelihood."
  },
  {
    "objectID": "vae.html#why-variational-inference",
    "href": "vae.html#why-variational-inference",
    "title": "Variational Autoencoders",
    "section": "Why Variational Inference?",
    "text": "Why Variational Inference?\n\nThe Intractability Problem\nIn latent variable models, we need to compute:\n\nPosterior Distribution: \\[\\begin{equation}\np(z|x; \\theta) = \\frac{p(z,x;\\theta)}{p(x;\\theta)}\n\\end{equation}\\]\nMarginal Likelihood: \\[\\begin{equation}\np(x;\\theta) = \\int p(x,z;\\theta) dz\n\\end{equation}\\]\n\nThese are intractable because: - High-dimensional integration in (2) - Complex dependencies in (1)\n\n\nVariational Solution\nIntroduce an approximate distribution \\(q(z;\\phi)\\) with: - Tractable form (e.g., Gaussian) - Parameters \\(\\phi\\) optimized to match \\(p(z|x;\\theta)\\)"
  },
  {
    "objectID": "vae.html#core-derivation",
    "href": "vae.html#core-derivation",
    "title": "Variational Autoencoders",
    "section": "Core Derivation",
    "text": "Core Derivation\n\nStep 1: KL Divergence Objective\n\\[\\begin{equation}\nD_{KL}(q(z)\\|p(z|x; \\theta)) = \\sum_z q(z) \\log \\frac{q(z)}{p(z|x; \\theta)}\n\\end{equation}\\]\n\n\nStep 2: Apply Bayes’ Rule\nSubstitute \\(p(z|x; \\theta) = \\frac{p(z,x;\\theta)}{p(x;\\theta)}\\): \\[\\begin{equation}\n= \\sum_z q(z) \\log \\left( \\frac{q(z) \\cdot p(x; \\theta)}{p(z, x; \\theta)} \\right)\n\\end{equation}\\]\n\n\nStep 3: Decompose Terms\n\\[\\begin{align}\n&= \\sum_z q(z) \\log q(z) + \\sum_z q(z) \\log p(x; \\theta) \\nonumber \\\\\n&\\quad - \\sum_z q(z) \\log p(z, x; \\theta) \\\\\n&= -H(q) + \\log p(x; \\theta) - \\mathbb{E}_q[\\log p(z,x;\\theta)]\n\\end{align}\\]\n\n\nStep 4: Rearrange for ELBO\n\\[\\begin{equation}\n\\log p(x;\\theta) = \\underbrace{\\mathbb{E}_q[\\log p(z,x;\\theta)] + H(q)}_{\\text{ELBO}} + D_{KL}(q\\|p)\n\\end{equation}\\]"
  },
  {
    "objectID": "vae.html#key-results",
    "href": "vae.html#key-results",
    "title": "Variational Autoencoders",
    "section": "Key Results",
    "text": "Key Results\n\nEvidence Lower Bound (ELBO): \\[\\begin{equation}\n\\mathcal{L}(\\theta,\\phi) = \\mathbb{E}_{q(z;\\phi)}[\\log p(x,z;\\theta)] + H(q(z;\\phi))\n\\end{equation}\\]\nOptimization: \\[\\begin{equation}\n\\max_{\\theta,\\phi} \\mathcal{L}(\\theta,\\phi) \\Rightarrow\n\\begin{cases}\n\\text{Maximizes data likelihood} \\\\\n\\text{Minimizes } D_{KL}(q\\|p)\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "vae.html#practical-implications",
    "href": "vae.html#practical-implications",
    "title": "Variational Autoencoders",
    "section": "Practical Implications",
    "text": "Practical Implications\n\nFor \\(q(z)\\): Choose simple distributions (e.g., Gaussian)\nFor \\(\\phi\\): Use gradient ascent on \\(\\mathcal{L}\\)\nFor VAEs: \\(q(z|x;\\phi)\\) becomes the encoder network"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#key-model-families",
    "href": "index.html#key-model-families",
    "title": "Deep Generative Models",
    "section": "Key Model Families",
    "text": "Key Model Families\n\n1. Variational Autoencoders (VAEs)\n\nProbabilistic encoder-decoder architecture\nLearn latent spaces with Gaussian distributions\nLearn more about VAEs\n\n\n\n2. Autoregressive Models\n\nGenerate sequences element-by-element\nExamples: PixelCNN, WaveNet\n\n\n\n3. Generative Adversarial Networks (GANs)\n\nAdversarial training with generator/discriminator\nHigh-quality image generation\n\n\n\n4. Normalizing Flows\n\nInvertible transformations for exact likelihood\nFlexible density estimation\n\n\n\n5. Energy-Based Models (EBMs)\n\nLearn energy functions for data distribution\nFlexible for discrete/continuous data\n\n\n\n6. Diffusion Models\n\nIterative denoising process\nState-of-the-art image generation"
  }
]