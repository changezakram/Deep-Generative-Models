[
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Variational Autoencoders (VAEs) combine the power of neural networks with probabilistic inference to model complex data distributions. This blog unpacks the intuition, math, and implementation of VAEs — from KL divergence and the ELBO to PyTorch code that generates to generate new images."
  },
  {
    "objectID": "vae.html#autoencoders-vs-variational-autoencoders",
    "href": "vae.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational Autoencoders",
    "section": "1 Autoencoders vs Variational Autoencoders",
    "text": "1 Autoencoders vs Variational Autoencoders\nTraditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) were introduced to overcome these limitations. Rather than encoding inputs into fixed latent vectors, VAEs learn a probabilistic latent space by modeling each input as a distribution — typically a Gaussian with a learned mean \\(\\\\mu\\) and standard deviation \\(\\\\sigma\\). This approach enables the model to sample latent variables \\(z\\) using the reparameterization trick, allowing the entire architecture to remain differentiable and trainable. By doing so, VAEs not only enable reconstruction, but also promote the learning of a continuous, interpretable latent space — a key enabler for generation and interpolation.\nThe diagram below illustrates this process:\n\nSource: Wikimedia Commons, licensed under CC BY-SA 4.0."
  },
  {
    "objectID": "vae.html#probabilistic-framework",
    "href": "vae.html#probabilistic-framework",
    "title": "Variational Autoencoders",
    "section": "2 Probabilistic Framework",
    "text": "2 Probabilistic Framework\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\nHere, \\(\\mathbf{z}\\) acts as a hidden or latent variable, which is unobserved during training. The model thus defines a mixture of infinitely many Gaussians — one for each \\(\\mathbf{z}\\).\nTo compute the likelihood of a data point \\(\\mathbf{x}\\), we must marginalize over all possible latent variables: \\[\n  p(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n  \\]\nThis integral requires integrating over all possible values of the latent variable \\(\\mathbf{z}\\), which is often high-dimensional and affects the likelihood in a non-linear way through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of techniques like variational inference and ELBO.\n\n2.1 Computational Challenge\nThis integral requires integrating over:\n\nAll possible values of \\(\\mathbf{z}\\) (often high-dimensional)\nNon-linear transformations through neural networks\n\nResult: Exact computation is intractable, motivating techniques like variational inference and ELBO (developed next)."
  },
  {
    "objectID": "vae.html#estimating-the-marginal-likelihood",
    "href": "vae.html#estimating-the-marginal-likelihood",
    "title": "Variational Autoencoders",
    "section": "3 Estimating the Marginal Likelihood",
    "text": "3 Estimating the Marginal Likelihood\n\n3.1 Naive Monte Carlo Estimation\nOne natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nHowever, this fails in practice. For most values of \\(z\\), the joint probability \\(p_\\theta(x, z)\\) is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely “hits” likely values of \\(z\\).\n\n\n3.2 Importance Sampling\nTo address this, we use importance sampling, introducing a proposal distribution \\(q(z)\\):\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nThis gives an unbiased estimator of \\(p(x)\\) if \\(q(z)\\) is well-chosen (ideally close to \\(p_\\theta(z|x)\\)). Intuitively, we sample \\(z\\) more frequently in regions where \\(p_\\theta(x, z)\\) is high.\n\n\n\n3.3 Log likelihood\nOur goal is to optimize the log-likelihood, and the log of an expectation is not the same as the expectation of the log. That is,\n\\[\n\\log p(x) = log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\neq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nWhile the marginal likelihood p(x) can be estimated unbiasedly using importance sampling, estimating its logarithm \\(p(x)\\) introduces bias due to the concavity of the log function. This is captured by Jensen’s Inequality, which tells us:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\geq \\underbrace{\\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]}_{\\text{ELBO}}\n\\]\nThis means that the expected log of the estimator underestimates the true log-likelihood. The right-hand side provides a tractable surrogate objective known as the Evidence Lower Bound (ELBO), which is a biased lower bound to \\(\\log p(x)\\). Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood.\nIn the next section, we formally derive this bound and explore its components in detail."
  },
  {
    "objectID": "vae.html#why-variational-inference",
    "href": "vae.html#why-variational-inference",
    "title": "Variational Autoencoders",
    "section": "4 Why Variational Inference?",
    "text": "4 Why Variational Inference?\nComputing the true posterior distribution \\(p(z \\mid x)\\) is intractable in most cases, because it requires evaluating the marginal likelihood \\(p(x)\\), which involves integrating over all possible values of \\(z\\):\n\\[\np(x) = \\int p(x, z) \\, dz\n\\]\nVariational inference tackles this by introducing a tractable, parameterized distribution \\(q(z)\\) to approximate \\(p(z|x)\\). We aim to make \\(q(z)\\) as close as possible to the true posterior by minimizing the KL divergence:\n\\[\nD_{\\text{KL}}(q(z) \\| p(z|x))\n\\]\nThis turns inference into an optimization problem. A key result is the Evidence Lower Bound (ELBO). See next section."
  },
  {
    "objectID": "vae.html#training-a-vae",
    "href": "vae.html#training-a-vae",
    "title": "Variational Autoencoders",
    "section": "5 Training a VAE",
    "text": "5 Training a VAE\n\n5.1 ELBO Objective\nNow that we’ve introduced the challenge of approximating the intractable posterior using variational inference, we turn our attention to deriving the Evidence Lower Bound (ELBO). This derivation reveals how optimizing a surrogate objective allows us to approximate the true log-likelihood of the data while keeping the approximate posterior close to the prior. The steps below walk through this formulation.\n\n5.1.1 KL Divergence Objective\n\\[\\begin{equation}\nD_{KL}(q(z)\\|p(z|x; \\theta)) = \\sum_z q(z) \\log \\frac{q(z)}{p(z|x; \\theta)}\n\\end{equation}\\]\n\n\n5.1.2 Apply Bayes’ Rule\nSubstitute \\(p(z|x; \\theta) = \\frac{p(z,x;\\theta)}{p(x;\\theta)}\\): \\[\\begin{equation}\n= \\sum_z q(z) \\log \\left( \\frac{q(z) \\cdot p(x; \\theta)}{p(z, x; \\theta)} \\right)\n\\end{equation}\\]\n\n\n5.1.3 Decompose Terms\n\\[\\begin{align}\n&= \\sum_z q(z) \\log q(z) + \\sum_z q(z) \\log p(x; \\theta) \\nonumber \\\\\n&\\quad - \\sum_z q(z) \\log p(z, x; \\theta) \\\\\n&= -H(q) + \\log p(x; \\theta) - \\mathbb{E}_q[\\log p(z,x;\\theta)]\n\\end{align}\\]\n\nNote: The term \\(\\mathcal{H}(q)\\) represents the entropy of the variational distribution \\(q(z|x)\\). Entropy is defined as:\n\\[\n\\mathcal{H}(q) = -\\sum_z q(z) \\log q(z) = -\\mathbb{E}_{q(z)}[\\log q(z)]\n\\]\nEntropy measures the amount of uncertainty or “spread” in a distribution. A high-entropy \\(q(z)\\) places probability mass across a wide region of the latent space, while a low-entropy \\(q(z)\\) is more concentrated. This decomposition is key to understanding the KL divergence term in the ELBO.\n\n\n\n5.1.4 Rearrange for ELBO\n\\[\n\\log p(x; \\theta) =\n\\underbrace{\n    \\mathbb{E}_q[\\log p(z, x; \\theta)] + \\mathcal{H}(q)\n}_{\\text{ELBO}}\n+D_{KL}(q(z)\\|p(z|x; \\theta))\n\\]\nThis equation shows that the log-likelihood \\(\\log p(x)\\) can be decomposed into the ELBO and the KL divergence between the approximate posterior and the true posterior. Since the KL divergence is always non-negative, the ELBO serves as a lower bound to the log-likelihood. By maximizing the ELBO, we indirectly minimize the KL divergence, bringing \\(q(z)\\) closer to \\(p(z|x)\\).\n Visualizing how \\(\\log p(x)\\) decomposes into the ELBO and KL divergence.\nSource: deepgenerativemodels.github.io\n\n\n5.1.5 Key Results\n\nEvidence Lower Bound (ELBO): \\[\\begin{equation}\n\\mathcal{L}(\\theta,\\phi) = \\mathbb{E}_{q(z;\\phi)}[\\log p(x,z;\\theta)] + H(q(z;\\phi))\n\\end{equation}\\]\nOptimization: \\[\\begin{equation}\n\\max_{\\theta,\\phi} \\mathcal{L}(\\theta,\\phi) \\Rightarrow\n\\begin{cases}\n\\text{Maximizes data likelihood} \\\\\n\\text{Minimizes } D_{KL}(q\\|p)\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "href": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "title": "Variational Autoencoders",
    "section": "6 Understanding the KL Divergence Term in the VAE Loss",
    "text": "6 Understanding the KL Divergence Term in the VAE Loss\nIn a VAE, the KL divergence term penalizes the encoder for producing latent distributions that deviate too far from the standard normal prior. This regularization has several important benefits:\n\nIt ensures that the latent space has a consistent structure, enabling meaningful sampling and interpolation.\nIt helps avoid large gaps between clusters in latent space by encouraging the encoder to distribute representations more uniformly.\nIt pushes the model to use the space around the origin more symmetrically and efficiently.\n\n\n6.1 Balancing KL Divergence and Reconstruction\nIn a Variational Autoencoder, the loss balances two goals:\n\nReconstruction — making the output resemble the input\nRegularization — keeping the latent space close to a standard normal distribution\n\nThis is captured by the loss function:\n\\[\n\\mathcal{L}_{\\text{VAE}} = \\text{Reconstruction Loss} + \\beta \\cdot D_{\\text{KL}}(q(z|x) \\,\\|\\, p(z))\n\\]\nThe parameter \\(\\beta\\) controls how strongly we enforce this regularization. Getting its value right is critical.\n\n6.1.1 When \\(\\beta\\) is too low:\n\nThe model mostly ignores the KL term, behaving like a plain autoencoder\nThe latent space becomes disorganized or fragmented\nSampling from the prior \\(p(z) = \\mathcal{N}(0, I)\\) results in unrealistic or broken outputs\n\n\n\n6.1.2 When \\(\\beta\\) is too high:\n\nThe encoder is forced to keep \\(q(z|x)\\) too close to the prior\nIt encodes less information about the input\nReconstructions become blurry or generic, since the decoder gets little to work with\n\n\nChoosing \\(\\beta\\) carefully is essential for balancing generalization and fidelity.\nA well-tuned \\(\\beta\\) helps the VAE both reconstruct accurately and generate new samples that resemble the training data.\n\n\n\n\n6.2 Gradient Challenge\nIn variational inference, we approximate the true posterior \\(p(z|x)\\) with a tractable distribution \\(q_\\phi(z)\\). This allows us to optimize the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q(z; \\phi)} \\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nOur goal is to maximize this objective with respect to both \\(\\theta\\) and \\(\\phi\\). While computing the gradient with respect to \\(\\theta\\) is straightforward, optimizing with respect to \\(\\phi\\) presents a challenge.\nThe complication arises because \\(\\phi\\) appears both in the density \\(q_\\phi(z|x)\\) and in the expectation operator. That is:\n\\[\n\\nabla_\\phi \\mathbb{E}_{q(z; \\phi)} \\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nThis gradient is hard to compute directly because we’re sampling from a distribution that depends on the parameters we’re trying to update.\n\n\n\n6.3 The Reparameterization Trick\nTo make this expression differentiable, we reparameterize the random variable \\(z\\) as a deterministic transformation of a parameter-free noise variable \\(\\epsilon\\):\n\\[\n\\epsilon \\sim \\mathcal{N}(0, I), \\quad z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n\\]\nThis turns the expectation into:\n\\[\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nwhere \\(z\\) is now a differentiable function of \\(\\phi\\).\n\n\n\nReparameterization Trick Diagram\n\n\nImage source: Wikipedia (CC BY-SA 4.0)\nThis diagram illustrates how the reparameterization trick enables differentiable sampling:\n\nIn the original formulation, \\(z\\) is sampled directly from a learned distribution, breaking the gradient flow.\nIn the reparameterized formulation, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\), and compute \\(z = \\mu + \\sigma \\cdot \\epsilon\\), making the sampling path fully differentiable.\n\n\n6.3.1 Monte Carlo Approximation\nWe approximate the expectation using Monte Carlo sampling:\n\\[\n\\mathbb{E}_{\\epsilon}[\\log p_\\theta(x, z) - \\log q_\\phi(z)] \\approx \\frac{1}{K} \\sum_{k=1}^K \\left[\\log p_\\theta(x, z^{(k)}) - \\log q_\\phi(z^{(k)})\\right]\n\\]\nwith:\n\\[\nz^{(k)} = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon^{(k)}, \\quad \\epsilon^{(k)} \\sim \\mathcal{N}(0, I)\n\\]\nThis enables us to compute gradients using backpropagation.\n\n\n\n6.3.2 Summary\n\nVariational inference introduces a gradient challenge because \\(q_\\phi(z)\\) depends on \\(\\phi\\)\nThe reparameterization trick expresses \\(z\\) as a differentiable function of noise and \\(\\phi\\)\nThis allows us to use backpropagation to optimize the ELBO efficiently\n\n\n\n\n\n6.4 Amortized Inference\nIn classical variational inference, we introduce a separate set of variational parameters \\(\\phi^i\\) for each datapoint \\(x^i\\) to approximate the true posterior \\(p(z|x^i)\\). However:\n\nOptimizing a separate \\(\\phi^i\\) for every datapoint is computationally expensive and does not scale to large datasets.\n\n\n\n6.4.1 The Key Idea: Amortization\nInstead of learning and storing a separate \\(\\phi^i\\) for every datapoint, we learn a single parametric function \\(f_\\phi(x)\\) — typically a neural network — that maps each input \\(x\\) to the parameters of the approximate posterior:\n\\[\nq_\\phi(z|x) = \\mathcal{N}\\left(\\mu_\\phi(x), \\sigma^2_\\phi(x)\\right)\n\\]\nHere, \\(\\phi\\) are the shared parameters of the encoder network, and \\(\\mu_\\phi(x), \\sigma_\\phi(x)\\) are its outputs.\nThis is like learning a regression function that predicts the optimal variational parameters for any input \\(x\\).\n\n\n\n\n6.5 Training with Amortized Inference\nOur training objective remains the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nWe optimize both \\(\\theta\\) (decoder parameters) and \\(\\phi\\) (encoder parameters) using stochastic gradient descent.\n\n6.5.1 Algorithm:\n\nInitialize \\(\\theta^{(0)}, \\phi^{(0)}\\)\nSample a datapoint \\(x^i\\)\nUse \\(f_\\phi(x^i)\\) to produce \\(\\mu^i, \\sigma^i\\)\nSample \\(z^i = \\mu^i + \\sigma^i \\cdot \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)\nEstimate the ELBO and compute gradients w.r.t. \\(\\theta, \\phi\\)\nUpdate \\(\\theta, \\phi\\) using gradient descent\nUpdate \\(\\theta\\), \\(\\phi\\) using gradient descent:\n\n\\[\n\\phi \\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\n\\[\n\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\nwhere \\(\\mathcal{B}\\) is the current minibatch and \\(\\tilde{\\nabla}\\) indicates a stochastic gradient approximation.\n\n\n\n\n6.6 Summary\n\nAmortized inference replaces per-datapoint optimization with a single learned mapping \\(f_\\phi(x)\\)\nThis makes variational inference scalable and efficient\nThe model can generalize to unseen inputs by predicting variational parameters on-the-fly\n\n\nNote: Following common practice in the literature, we use \\(\\phi\\) to denote the parameters of the encoder network, even though it now defines a function rather than individual variational parameters."
  },
  {
    "objectID": "vae.html#applications-of-vaes",
    "href": "vae.html#applications-of-vaes",
    "title": "Variational Autoencoders",
    "section": "7 Applications of VAEs",
    "text": "7 Applications of VAEs\nVariational Autoencoders are widely used in:\n\nImage Generation: VAEs can generate new images similar to the training data (e.g., MNIST digits)\n\nAnomaly Detection: High reconstruction error flags unusual data points\n\nRepresentation Learning: Latent space captures features for downstream tasks\n\n\n7.1 😎 Face Generation with Convolutional VAE\nTo complement the theory, I’ve built a full PyTorch implementation of a Variational Autoencoder trained on the CelebA dataset.\n📘 The notebook walks through:\n\nDefining the encoder, decoder, and reparameterization trick\n\nImplementing the ELBO loss function (reconstruction + KL divergence)\n\nTraining the model on face images\n\nGenerating new faces from random latent vectors\n\n\n📓 View on GitHub"
  },
  {
    "objectID": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections.",
    "href": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections.",
    "title": "Variational Autoencoders",
    "section": "8 This example is designed to reinforce the theoretical concepts from earlier sections.",
    "text": "8 This example is designed to reinforce the theoretical concepts from earlier sections."
  },
  {
    "objectID": "vae.html#further-reading",
    "href": "vae.html#further-reading",
    "title": "Variational Autoencoders",
    "section": "9 Further Reading",
    "text": "9 Further Reading\nFor readers interested in diving deeper into the theory and applications of variational autoencoders, the following resources are recommended:\n\nTutorial on Variational Autoencoders\nCarl Doersch (2016)\nhttps://arxiv.org/pdf/1606.05908\nAuto-Encoding Variational Bayes\nKingma & Welling (2014) — the original VAE paper\nhttps://arxiv.org/pdf/1312.6114\nThe Challenges of Amortized Inference for Structured Prediction\nCremer, Li, & Duvenaud (2019)\nhttps://arxiv.org/pdf/1906.02691\nDeep Generative Models course notes\nhttps://deepgenerativemodels.github.io/notes/vae/"
  },
  {
    "objectID": "ebm.html",
    "href": "ebm.html",
    "title": "Energy Based Models (EBM)",
    "section": "",
    "text": "Modern generative models impose strict architectural requirements:\n\nVAEs need encoder-decoder pairs\n\nGANs require adversarial networks\n\nNormalizing flows must use invertible transforms\n\nWhile each model family has strengths, they also carry trade-offs:\n\nVAEs often produce blurry samples due to reliance on Gaussian assumptions.\n\nGANs can generate high-quality images but are notoriously unstable and suffer from mode collapse.\n\nNormalizing Flows guarantee exact likelihoods but restrict architecture design due to invertibility constraints.\n\n\nEnergy-Based Models (EBMs) break this pattern by:\n\nLearning a scoring function (energy) where it assigns low energy to likely or observed data (\\(x \\sim p_{data}\\)), and high energy to unlikely or unobserved inputs.\nLinking to probability via: \\(p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z(\\theta)}, \\quad Z(\\theta) = \\int e^{-E_\\theta(x)}\\,dx\\)\nwhere low energy = high probability\nUsing generic neural nets—no specialized architectures needed\n\nThis approach provides unique advantages:\n✓ Simpler setup - works with any neural net\n✓ Explicit scoring - \\(p(x) \\propto e^{-E(x)}\\) (energy)\n✓ Stable training - No adversarial balancing act like GANs\n\n\n\n\n\n\nCore Principle\n\n\n\nEBMs replace architectural constraints with a single principle:\n“Learn to distinguish real data from noise through energy scoring.”\n\n\nTo understand how this works mathematically, we must first answer: What defines a valid probability distribution?\nThe next section derives EBMs’ theoretical foundation—revealing how they bypass traditional limitations while introducing new challenges (like partition function estimation)."
  },
  {
    "objectID": "ebm.html#introduction",
    "href": "ebm.html#introduction",
    "title": "Energy Based Models (EBM)",
    "section": "",
    "text": "Modern generative models impose strict architectural requirements:\n\nVAEs need encoder-decoder pairs\n\nGANs require adversarial networks\n\nNormalizing flows must use invertible transforms\n\nWhile each model family has strengths, they also carry trade-offs:\n\nVAEs often produce blurry samples due to reliance on Gaussian assumptions.\n\nGANs can generate high-quality images but are notoriously unstable and suffer from mode collapse.\n\nNormalizing Flows guarantee exact likelihoods but restrict architecture design due to invertibility constraints.\n\n\nEnergy-Based Models (EBMs) break this pattern by:\n\nLearning a scoring function (energy) where it assigns low energy to likely or observed data (\\(x \\sim p_{data}\\)), and high energy to unlikely or unobserved inputs.\nLinking to probability via: \\(p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z(\\theta)}, \\quad Z(\\theta) = \\int e^{-E_\\theta(x)}\\,dx\\)\nwhere low energy = high probability\nUsing generic neural nets—no specialized architectures needed\n\nThis approach provides unique advantages:\n✓ Simpler setup - works with any neural net\n✓ Explicit scoring - \\(p(x) \\propto e^{-E(x)}\\) (energy)\n✓ Stable training - No adversarial balancing act like GANs\n\n\n\n\n\n\nCore Principle\n\n\n\nEBMs replace architectural constraints with a single principle:\n“Learn to distinguish real data from noise through energy scoring.”\n\n\nTo understand how this works mathematically, we must first answer: What defines a valid probability distribution?\nThe next section derives EBMs’ theoretical foundation—revealing how they bypass traditional limitations while introducing new challenges (like partition function estimation)."
  },
  {
    "objectID": "ebm.html#math-review",
    "href": "ebm.html#math-review",
    "title": "Energy Based Models (EBM)",
    "section": "2 Math Review",
    "text": "2 Math Review\n\n2.1 Understanding the Probability Foundation Behind EBMs\nIn generative modeling, a valid probability distribution \\(p(x)\\) must satisfy:\n\nNon-negativity:\n\\[\np(x) \\geq 0\n\\]\nNormalization:\n\\[\n\\int p(x)\\, dx = 1\n\\]\n\nWhile it’s easy to define a function that satisfies \\(p(x) \\geq 0\\) (e.g., using exponentials), ensuring that it also sums to 1 — i.e., \\(\\int p(x)\\, dx = 1\\) — is much more difficult, especially for flexible functions like neural networks.\n\n\n\n2.2 Why do we introduce \\(g(x)\\)?\nInstead of modeling \\(p(x)\\) directly, we define a non-negative function \\(g(x) \\geq 0\\) and turn it into a probability distribution by normalizing:\n\\[\np_\\theta(x) = \\frac{g_\\theta(x)}{Z(\\theta)}, \\quad \\text{where} \\quad Z(\\theta) = \\int g_\\theta(x)\\, dx\n\\]\nThis trick simplifies the problem by separating the two requirements:\n\n\\(g_\\theta(x)\\) ensures non-negativity\n\n\\(Z(\\theta)\\) enforces normalization\n\nThe normalization constant \\(Z(\\theta)\\) is also known as the partition function.\nThis trick allows us to use any expressive function for \\(g_\\theta(x)\\) — including deep neural networks.\n\n\n2.2.1 Intuition\nThink of \\(g_\\theta(x)\\) as a scoring function:\n\nHigher \\(g_\\theta(x)\\) means more likely\n\nDividing by \\(Z(\\theta)\\) rescales these scores to form a valid probability distribution\n\n\n\n\n\n2.3 Energy-Based Parameterization\nWe’ve just seen how \\(g_\\theta(x)\\) can be any non-negative function. A common and powerful choice is to define it using an exponential transformation:\n\\[\ng_\\theta(x) = \\exp(f_\\theta(x))\n\\]\nThis ensures that \\(g_\\theta(x) \\geq 0\\) and allows us to interpret \\(f_\\theta(x)\\) as an unnormalized log-probability score.\nWe then normalize using the partition function ( Z() ) to obtain a valid probability distribution:\n\\[\np_\\theta(x) = \\frac{g_\\theta(x)}{Z(\\theta)} = \\frac{\\exp(f_\\theta(x))}{Z(\\theta)},\n\\quad \\text{where} \\quad Z(\\theta) = \\int \\exp(f_\\theta(x)) \\, dx\n\\]\nThis leads to the energy-based formulation, where \\(f_\\theta(x)\\) is interpreted as the negative energy. We can equivalently write:\n\\[\nE_\\theta(x) = -f_\\theta(x)\n\\quad \\Rightarrow \\quad\np_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z(\\theta)}\n\\]\nEnergy-Based Models (EBMs) follow this foundational idea: define a scoring function \\(f_\\theta(x)\\) that assigns high values to likely data points, then convert those scores into probabilities using exponentiation and normalization.\nThis perspective offers four key advantages:\n\nIt allows us to use flexible models (like deep neural networks) to assign unnormalized scores.\n\nIt separates concerns: one function ensures non-negativity (via exponentiation), and the partition function enforces normalization.\n\nIt lets us interpret \\(f_\\theta(x)\\) as an unnormalized log-probability, improving interpretability.\n\nIt connects naturally to well-known distributions (e.g., exponential family, Boltzmann distribution), making the formulation more general.\n\nThis formulation gives us the freedom to model complex distributions with any differentiable scoring function \\(f_\\theta(x)\\), while only requiring that we can compute or approximate its gradients.\n\n\n2.3.1 Summary\nWhether we express scores directly via \\(f_\\theta(x)\\) or indirectly through energy \\(E_\\theta(x)\\), the core idea remains the same:\n\nUse a flexible model to assign unnormalized scores\n\nNormalize those scores using the partition function \\(Z(\\theta)\\)\n\nThis gives EBMs the freedom to model complex data distributions using any differentiable function for \\(f_\\theta(x)\\) — without requiring invertibility or exact likelihoods.\n\n\n\n2.3.2 Visualizing Energy Functions\nLet’s now build visual intuition for what these energy landscapes look like.\nEnergy-based models capture compatibility between variables using an energy function. In this example (adapted from Atcold, 2020), the energy function assigns lower values to pairs of variables \\((x, y)\\) that are more likely to co-occur — and higher energy elsewhere.\n\n\n\nVisualizing Energy Field\n\n\nVisualizing the energy landscape \\(E(x_1, x_2)\\) — lower energy corresponds to higher probability.\nThis shows how EBMs encode knowledge: valleys indicate high-likelihood (real) data, while peaks represent unlikely (noise) regions.\nThis kind of energy landscape is especially useful in practical tasks like image generation, where the model learns to assign low energy to realistic images and high energy to unrealistic ones. During inference, the model can generate new samples by exploring low-energy regions of this landscape."
  },
  {
    "objectID": "ebm.html#practical-applications",
    "href": "ebm.html#practical-applications",
    "title": "Energy Based Models (EBM)",
    "section": "3 Practical Applications",
    "text": "3 Practical Applications\nEnergy-Based Models (EBMs) offer unique benefits in scenarios where traditional models struggle. Below are two practical applications that demonstrate how EBMs shine in real-world settings.\n\n3.1 When You Don’t Need the Partition Function\nIn general, evaluating the full probability \\(p_\\theta(x)\\) requires computing the partition function \\(Z(\\theta)\\):\n\\[\np_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp(f_\\theta(x))\n\\]\n\n\n\n\n\n\nKey Insight\nIn some applications, we don’t need the exact probability — we only need to compare scores. This allows EBMs to be useful even when the partition function is intractable.\n\n\n\nWhen comparing two samples \\(x\\) and \\(x'\\), we can compute the ratio of their probabilities:\n\\[\n\\frac{p_\\theta(x)}{p_\\theta(x')} = \\exp(f_\\theta(x) - f_\\theta(x'))\n\\]\nThis lets us determine which input is more likely — without ever computing \\(Z(\\theta)\\) — a powerful advantage of EBMs.\nPractical Applications:\n\nAnomaly detection: Identify inputs with unusually low likelihood.\nDenoising: Prefer cleaner versions of corrupted data by comparing likelihoods.\nObject recognition: Assign the most likely label to an input image.\nSequence labeling: Predict tags (e.g., part-of-speech) for input tokens.\nImage restoration: Recover clean images from noisy inputs.\n\n\n\n\n\n\n\nNote\n\n\n\nReal-World Example\nIn anomaly detection, EBMs have been used to identify outliers in high-dimensional sensor data without computing exact probabilities — simply by scoring input configurations and flagging those with unusually high energy.\n\n\n\n\n3.2 Product of Experts (Compositional Generation)\nIn some cases, we want to combine multiple expert models that each score different attributes of an input \\(\\mathbf{x}\\) — for example, age, gender, or hairstyle. This is where Energy-Based Models (EBMs) shine through Product of Experts (PoE).\nSuppose you have three trained expert models \\(f_{\\theta_1}(x)\\), \\(f_{\\theta_2}(x)\\), and \\(f_{\\theta_3}(x)\\). A tempting idea is to combine their scores additively and exponentiate:\n\\[\n\\exp\\left(f_{\\theta_1}(x) + f_{\\theta_2}(x) + f_{\\theta_3}(x)\\right)\n\\]\nTo make this a valid probability distribution, we normalize:\n\\[\np_{\\theta_1, \\theta_2, \\theta_3}(x) = \\frac{1}{Z(\\theta_1, \\theta_2, \\theta_3)} \\exp\\left(f_{\\theta_1}(x) + f_{\\theta_2}(x) + f_{\\theta_3}(x)\\right)\n\\]\nThis behaves like a logical AND: if any expert assigns low score, the overall likelihood drops. This contrasts with mixture models (like Mixture of Gaussians), which behave more like OR.\n\n\n\n\n\n\nNote\n\n\n\nReal-World Example\nIn the figure below (Du et al., 2020), EBMs were used to model attributes like “young”, “female”, “smiling”, and “wavy hair”. By combining these via Product of Experts, the model generated faces that satisfied multiple specific attributes simultaneously.\n\n\n\n\n\nSource: Du et al., 2020. Compositional Visual Generation with Energy Based Models"
  },
  {
    "objectID": "ebm.html#benefits-and-limitations-of-ebms",
    "href": "ebm.html#benefits-and-limitations-of-ebms",
    "title": "Energy Based Models (EBM)",
    "section": "4 Benefits and Limitations of EBMs",
    "text": "4 Benefits and Limitations of EBMs\n\n4.1 Key Benefits\nVery flexible model architectures\nNo need for invertibility, autoregressive factorization, or adversarial design.\nStable training\nCompared to GANs, EBMs can be more robust and easier to optimize.\nHigh sample quality\nCapable of modeling complex, multi-modal data distributions.\nFlexible composition\nEnergies can be combined to support multi-task objectives or structured learning.\n\n\n\n4.2 Limitations\nDespite their strengths, EBMs come with notable challenges:\nSampling is expensive\nNo direct way to sample from \\(p_\\theta(x)\\); MCMC methods are slow and scale poorly.\nLikelihood is intractable\nPartition function \\(Z(\\theta)\\) is hard to compute, and we can’t directly evaluate log-likelihood.\nTraining is indirect\nLearning requires reducing energy of incorrect samples, not just increasing for correct ones.\nNo feature learning (by default)\nEBMs don’t learn latent features unless explicitly structured (e.g., RBMs)."
  },
  {
    "objectID": "ebm.html#training-energy-based-models",
    "href": "ebm.html#training-energy-based-models",
    "title": "Energy Based Models (EBM)",
    "section": "5 Training Energy-Based Models",
    "text": "5 Training Energy-Based Models\nEBMs are trained to assign higher scores (lower energy) to observed data points, and lower scores (higher energy) to unobserved ones. This corresponds to maximizing the likelihood of training data.\n\\[\np_\\theta(x_{\\text{train}}) = \\frac{\\exp(f_\\theta(x_{\\text{train}}))}{Z(\\theta)}\n\\]\nThis expression tells us that increasing the score for \\(x_{\\text{train}}\\) is not enough — we must also decrease scores for other \\(x\\) to reduce \\(Z(\\theta)\\) and make the probability higher relatively.\n\n\n5.1 Log-Likelihood and Its Gradient\nTo train EBMs, we compute the gradient of the log-likelihood with respect to parameters \\(\\theta\\).\nWe start by writing the log-likelihood:\n\\[\n\\log p_\\theta(x_{\\text{train}}) = f_\\theta(x_{\\text{train}}) - \\log Z(\\theta)\n\\]\nTaking the gradient with respect to \\(\\theta\\):\n\\[\n\\nabla_\\theta \\log p_\\theta(x_{\\text{train}}) = \\nabla_\\theta f_\\theta(x_{\\text{train}}) - \\nabla_\\theta \\log Z(\\theta)\n\\]\nTo compute this, we need the gradient of the log partition function:\n\\[\nZ(\\theta) = \\int \\exp(f_\\theta(x))\\, dx\n\\]\nApplying the chain rule:\n\\[\n\\nabla_\\theta \\log Z(\\theta)\n= \\frac{1}{Z(\\theta)} \\int \\exp(f_\\theta(x)) \\nabla_\\theta f_\\theta(x)\\, dx\n= \\mathbb{E}_{x \\sim p_\\theta} \\left[ \\nabla_\\theta f_\\theta(x) \\right]\n\\]\nSubstitute this back:\n\\[\n\\nabla_\\theta \\log p_\\theta(x_{\\text{train}})\n= \\nabla_\\theta f_\\theta(x_{\\text{train}}) - \\mathbb{E}_{x \\sim p_\\theta} \\left[ \\nabla_\\theta f_\\theta(x) \\right]\n\\]\nThe first term is straightforward — it’s the gradient of the model’s score on the training point.\nBut the second term, the expectation over model samples, is difficult. It requires drawing samples from \\(p_\\theta(x)\\), which in turn depends on the intractable normalization constant \\(Z(\\theta)\\).\nTo deal with this, we use sampling-based approximations like Contrastive Divergence.\n\n\n\n5.2 Contrastive Divergence\nSince computing the true gradient requires sampling from \\(p_\\theta(x)\\), which is intractable due to the partition function \\(Z(\\theta)\\), we use Contrastive Divergence as an efficient approximation.\nWe approximate the expectation:\n\\[\n\\mathbb{E}_{x \\sim p_\\theta} \\left[ \\nabla_\\theta f_\\theta(x) \\right] \\approx \\nabla_\\theta f_\\theta(x_{\\text{sample}})\n\\]\nThis gives:\n\\[\n\\nabla_\\theta \\log p_\\theta(x_{\\text{train}}) \\approx \\nabla_\\theta f_\\theta(x_{\\text{train}}) - \\nabla_\\theta f_\\theta(x_{\\text{sample}})\n= \\nabla_\\theta \\left( f_\\theta(x_{\\text{train}}) - f_\\theta(x_{\\text{sample}}) \\right)\n\\]\nContrastive Divergence Algorithm:\n\nSample \\(x_{\\text{sample}} \\sim p_\\theta\\) (typically via MCMC)\nTake a gradient step on:\n\n\\[\n\\nabla_\\theta \\left( f_\\theta(x_{\\text{train}}) - f_\\theta(x_{\\text{sample}}) \\right)\n\\]\nThis encourages the model to increase the score of the training sample and decrease the score of samples it currently believes are likely.\n\n\n\n\n\n\nEBM Training Recap\n- Want: High scores (low energy) for real data\n- Avoid: High scores for incorrect data\n- Can’t compute exact gradient due to \\(Z(\\theta)\\)\n- So: Approximate using Monte Carlo sample \\(\\sim p_\\theta(x)\\)\n\n\n\n\n\n5.2.1 Intuition Recap\n\nPull up: Increase the score (lower the energy) of the real training sample \\(\\nabla_\\theta f_\\theta(x_{\\text{train}})\\)\nPush down: Decrease the score of samples from the model \\(\\nabla_\\theta f_\\theta(x_{\\text{sample}})\\)\nThis sharpens the model’s belief in real data and corrects high-scoring regions where it is currently overconfident.\n\n During training, EBMs increase the score of correct samples and decrease the score of incorrect ones.\n\nSource: course material from CS236: Deep Generative Models"
  },
  {
    "objectID": "ebm.html#sampling-from-energy-based-models",
    "href": "ebm.html#sampling-from-energy-based-models",
    "title": "Energy Based Models (EBM)",
    "section": "6 Sampling from Energy-Based Models",
    "text": "6 Sampling from Energy-Based Models\nRecall that EBMs define a probability distribution as:\n\\[\np_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp(f_\\theta(x))\n\\]\nUnlike autoregressive or flow models, there is no direct way to sample from \\(p_\\theta(x)\\) because we cannot easily compute how likely each possible sample is. That’s because the normalization term \\(Z(\\theta)\\) is intractable.\nHowever, we can still compare two samples using a key insight:\n\n\n\n\n\n\nKey Insight\nWe can still compare two samples \\(x\\) and \\(x'\\) without needing \\(Z(\\theta)\\):\n\\[\n\\frac{p_\\theta(x)}{p_\\theta(x')} = \\exp(f_\\theta(x) - f_\\theta(x'))\n\\]\nThis property is useful for tasks like ranking, anomaly detection, and denoising.\n\n\n\nWhile we can’t sample from \\(p_\\theta(x)\\) directly due to the intractable \\(Z(\\theta)\\), we can still generate approximate samples using Markov Chain Monte Carlo (MCMC) methods.\n\n\n6.1 Metropolis-Hastings (MH) MCMC\nMetropolis-Hastings proposes samples and accepts or rejects them based on how much they improve the energy score.\nTo sample from \\(p_\\theta(x)\\), we use an iterative approach like MCMC:\n\nInitialize \\(x^0\\) randomly\n\nPropose a new sample: \\(x' = x^t + \\text{noise}\\)\n\nAccept or reject based on scores:\n\nIf \\(f_\\theta(x') &gt; f_\\theta(x^t)\\), set \\(x^{t+1} = x'\\)\nElse set \\(x^{t+1} = x'\\) with probability \\(\\exp(f_\\theta(x') - f_\\theta(x^t))\\)\nOtherwise, set \\(x^{t+1} = x^t\\)\n\nRepeat this process until the chain converges\n\nPros:\n- General-purpose\n- Guaranteed to converge to \\(p_\\theta(x)\\) under mild conditions\nCons:\n- Can take a very long time to convergence - Sensitive to proposal distribution\n- Computationally expensive in high dimensions\n\n\n6.2 Unadjusted Langevin MCMC (ULA)\nULA improves over random-walk MH by using gradient information to guide proposals.\nTo sample from \\(p_\\theta(x)\\), Unadjusted Langevin MCMC uses gradient information to guide proposals:\n\nInitialize \\(x^0 \\sim \\pi(x)\\)\n\nRepeat for \\(t = 0, 1, 2, \\dots, T - 1\\):\n\nSample \\(z^t \\sim \\mathcal{N}(0, I)\\)\n\nUpdate: \\(x^{t+1} = x^t + \\epsilon \\nabla_x \\log p_\\theta(x^t) + \\sqrt{2\\epsilon} z^t\\)\n\n\nFor EBMs, since \\(\\nabla_x \\log p_\\theta(x) = \\nabla_x f_\\theta(x)\\) the update becomes:\n\\[\nx^{t+1} = x^t + \\epsilon \\nabla_x f_\\theta(x^t) + \\sqrt{2\\epsilon} z^t\n\\]\nPros:\n- Uses gradient to improve proposal\n- Often faster mixing than random-walk methods\nCons:\n- Still requires many steps for good convergence\n- Sensitive to step size \\(\\epsilon\\)\n\n\n6.3 Adjusted Langevin MCMC (ALA)\nALA fixes the bias in ULA by using Metropolis-Hastings-style acceptance to ensure correct sampling.\nTo sample from \\(p_\\theta(x)\\), Adjusted Langevin MCMC applies a step after each Langevin update to ensure samples follow the correct stationary distribution.\nThis makes it a corrected version of ULA with proper stationary distribution.\n\n \\(x^0 \\sim \\pi(x)\\)\n\n for \\(t = 0, 1, 2, \\dots, T - 1\\):\n\nSample \\(z^t \\sim \\mathcal{N}(0, I)\\)\n\nPropose: \\(x' = x^t + \\epsilon \\nabla_x \\log p_\\theta(x^t) + \\sqrt{2\\epsilon} z^t\\)\nForward proposal: \\(q(x^{t+1} \\mid x^t) = \\mathcal{N}\\left(x^{t+1} \\mid x^t + \\epsilon \\nabla_x \\log p_\\theta(x^t),\\ 2\\epsilon I\\right)\\)\nReverse proposal: \\(q(x^t \\mid x^{t+1}) = \\mathcal{N}\\left(x^t \\mid x^{t+1} + \\epsilon \\nabla_x \\log p_\\theta(x^{t+1}),\\ 2\\epsilon I\\right)\\)\nAccept \\(x\\) with probability \\(\\alpha = \\min\\left(1, \\frac{p_\\theta(x') \\cdot q(x^t \\mid x')}{p_\\theta(x^t) \\cdot q(x' \\mid x^t)}\\right)\\)\n\nIf accepted: \\(x^{t+1} = x'\\)\n\nOtherwise: \\(x^{t+1} = x^t\\)\n\n\n\nFor EBMs, since\n\\[\n\\nabla_x \\log p_\\theta(x) = \\nabla_x f_\\theta(x)\n\\] and \\[  \nq(x' \\mid x^t) = \\mathcal{N}\\left(x' \\mid x^t + \\epsilon \\nabla_x f_\\theta(x^t), 2\\epsilon I\\right)\n\\]\nthe proposal becomes:\n\\[\nx' = x^t + \\epsilon \\nabla_x f_\\theta(x^t) + \\sqrt{2\\epsilon} z^t\n\\]\n\n\n\n6.4 Summary\nAll these methods aim to sample from \\(p_\\theta(x)\\), but differ in how they explore the space:\n\nMH is simple but can be inefficient.\n\nULA is gradient-guided and faster but biased.\n\nAdjusted Langevin corrects ULA using MH-style acceptance.\n\n\n\n\n\n\n\n\n\nMethod\nKey Advantage\nKey Limitation\n\n\n\n\nMH (Metropolis-Hastings)\nGeneral-purpose; unbiased under mild conditions\nCan be inefficient and slow to converge\n\n\nULA (Unadjusted Langevin)\nGradient-guided; faster mixing than MH\nBiased sampling; sensitive to step size\n\n\nALA (Adjusted Langevin)\nCombines ULA with MH for correctness\nMore complex and computationally expensive\n\n\n\nSampling is a core challenge in EBMs — especially because we need to sample during every training step when using contrastive divergence."
  },
  {
    "objectID": "ebm.html#ebm-recap",
    "href": "ebm.html#ebm-recap",
    "title": "Energy Based Models (EBM)",
    "section": "7 🧩 EBM Recap",
    "text": "7 🧩 EBM Recap\nEnergy-Based Models (EBMs) offer a flexible and elegant alternative to traditional generative models by focusing on scoring functions rather than prescribing fixed model architectures. By defining an unnormalized score over inputs and converting it into a probability via a partition function, EBMs avoid the structural constraints of VAEs, GANs, or flows.\nKey ideas: - EBMs define probabilities as:\n\\[\n  p_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp(f_\\theta(x))\n  \\] - The energy function \\(E_\\theta(x) = -f_\\theta(x)\\) creates a landscape where low energy = high probability.\n- EBMs excel when exact likelihood is unnecessary (e.g., denoising, anomaly detection, structured generation).\n- They support modular composition through techniques like Product of Experts (PoE).\n- Training involves score-based contrastive learning, and sampling is powered by MCMC methods.\nDespite challenges like intractable sampling and likelihood computation, EBMs continue to gain traction due to their conceptual clarity, compositional flexibility, and alignment with physics-inspired learning paradigms."
  },
  {
    "objectID": "ebm.html#references",
    "href": "ebm.html#references",
    "title": "Energy Based Models (EBM)",
    "section": "8 📚 References",
    "text": "8 📚 References\n[1] Atcold, Y. (2020). NYU Deep Learning Spring 2020 – Week 07: Energy-Based Models. Retrieved from https://atcold.github.io/NYU-DLSP20/en/week07/07-1/\n[2] LeCun, Y., Hinton, G., & Bengio, Y. (2021). A Path Towards Autonomous Machine Intelligence. arXiv. Retrieved from https://arxiv.org/pdf/2101.03288\n[3] MIT. (2022). Energy-Based Models – MIT Class Project. Retrieved from https://energy-based-model.github.io/Energy-based-Model-MIT/\n[4] University of Amsterdam. (2021). Deep Energy Models – UvA DL Notebooks. Retrieved from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html\n[5] MIT. (2022). Compositional Generation and Inference with Energy-Based Models. Retrieved from https://energy-based-model.github.io/compositional-generation-inference/"
  },
  {
    "objectID": "flows.html",
    "href": "flows.html",
    "title": "Normalizing Flow Models",
    "section": "",
    "text": "In generative modeling, the objective is to learn a probability distribution over data that allows us to both generate new examples and evaluate the likelihood of observed ones. For a model to be practically useful, it must support efficient sampling and enable exact or tractable likelihood computation during training.\nA Variational Autoencoder (VAE) is a type of generative model that introduces latent variables \\(z\\), allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood \\(p(x)\\) is often intractable. To address this, VAEs use variational inference to approximate the posterior \\(p(z \\mid x)\\) and optimize a surrogate objective known as the Evidence Lower Bound (ELBO). This is made possible by the reparameterization trick, which enables gradients to flow through stochastic latent variables during training.\nNormalizing flows address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point \\(x\\) is generated by applying a function \\(x = f(z)\\) to a latent variable \\(z\\) sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so \\(z\\) can be exactly recovered as \\(z = f^{-1}(x)\\). This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.\nThis structure offers several advantages. First, each \\(x\\) maps to a unique \\(z\\), eliminating the need to marginalize over latent variables as in VAEs. Second, the change-of-variables formula enables exact computation of the likelihood, rather than approximations. Third, sampling is straightforward: draw \\(z \\sim p_Z(z)\\) from the base distribution and apply the transformation \\(x = f(z)\\).\nDespite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn lower-dimensional latent representations, flows require the latent and data spaces to have equal dimensionality to preserve invertibility. This means flow-based models do not perform dimensionality reduction, which can be a disadvantage in tasks where compact representations are important.\n\n\n\nComparison of VAE and Flow-based Models\n\n\nVAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.\nTo understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We’ll begin with the 1D case and build up to the multivariate formulation."
  },
  {
    "objectID": "flows.html#introduction",
    "href": "flows.html#introduction",
    "title": "Normalizing Flow Models",
    "section": "",
    "text": "In generative modeling, the objective is to learn a probability distribution over data that allows us to both generate new examples and evaluate the likelihood of observed ones. For a model to be practically useful, it must support efficient sampling and enable exact or tractable likelihood computation during training.\nA Variational Autoencoder (VAE) is a type of generative model that introduces latent variables \\(z\\), allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood \\(p(x)\\) is often intractable. To address this, VAEs use variational inference to approximate the posterior \\(p(z \\mid x)\\) and optimize a surrogate objective known as the Evidence Lower Bound (ELBO). This is made possible by the reparameterization trick, which enables gradients to flow through stochastic latent variables during training.\nNormalizing flows address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point \\(x\\) is generated by applying a function \\(x = f(z)\\) to a latent variable \\(z\\) sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so \\(z\\) can be exactly recovered as \\(z = f^{-1}(x)\\). This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.\nThis structure offers several advantages. First, each \\(x\\) maps to a unique \\(z\\), eliminating the need to marginalize over latent variables as in VAEs. Second, the change-of-variables formula enables exact computation of the likelihood, rather than approximations. Third, sampling is straightforward: draw \\(z \\sim p_Z(z)\\) from the base distribution and apply the transformation \\(x = f(z)\\).\nDespite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn lower-dimensional latent representations, flows require the latent and data spaces to have equal dimensionality to preserve invertibility. This means flow-based models do not perform dimensionality reduction, which can be a disadvantage in tasks where compact representations are important.\n\n\n\nComparison of VAE and Flow-based Models\n\n\nVAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.\nTo understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We’ll begin with the 1D case and build up to the multivariate formulation."
  },
  {
    "objectID": "flows.html#math-review",
    "href": "flows.html#math-review",
    "title": "Normalizing Flow Models",
    "section": "2 Math Review",
    "text": "2 Math Review\nThis section builds the mathematical foundation for understanding flow models, starting with change-of-variable and extending to multivariate transformations and Jacobians.\n\n2.1 Change of Variables in 1D\nSuppose we have a random variable \\(z\\) with a known distribution \\(p_Z(z)\\), and we define a new variable:\n\\[\nx = f(z)\n\\]\nwhere \\(f\\) is a monotonic, differentiable function with an inverse:\n\\[\nz = f^{-1}(x) = h(x)\n\\]\nOur goal is to compute the probability density function (PDF) of \\(x\\), denoted \\(p_X(x)\\), in terms of the known PDF \\(p_Z(z)\\).\n\n2.1.1 Cumulative Distribution Function (CDF)\nWe begin with the cumulative distribution function of \\(x\\):\n\\[\nF_X(x) = P(X \\leq x) = P(f(Z) \\leq x)\n\\]\nSince \\(f\\) is monotonic and invertible, this becomes:\n\\[\nP(f(Z) \\leq x) = P(Z \\leq f^{-1}(x)) = F_Z(h(x))\n\\]\n\n\n2.1.2 Deriving the PDF via Chain Rule\nTo obtain the PDF, we differentiate the CDF:\n\\[\np_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx} F_Z(h(x))\n\\]\nApplying the chain rule:\n\\[\np_X(x) = F_Z'(h(x)) \\cdot h'(x) = p_Z(h(x)) \\cdot h'(x)\n\\]\n\n\n2.1.3 Rewrite in Terms of \\(z\\)\nFrom the previous step:\n\\[\np_X(x) = p_Z(h(x)) \\cdot h'(x)\n\\]\nSince \\(z = h(x)\\), we can rewrite:\n\\[\np_X(x) = p_Z(z) \\cdot h'(x)\n\\]\nNow, using the inverse function theorem, we express \\(h'(x)\\) as:\n\\[\nh'(x) = \\frac{d}{dx} f^{-1}(x) = \\frac{1}{f'(z)}\n\\]\nSo the final expression becomes:\n\\[\np_X(x) = p_Z(z) \\cdot \\left| \\frac{1}{f'(z)} \\right|\n\\]\nThe absolute value ensures the density remains non-negative, as required for any valid probability distribution.\nThis is the fundamental concept normalizing flows use to model complex distributions by transforming simple ones.\n\n\n\n2.2 Geometry: Determinants and Volume Changes\nTo further understand the multivariate change-of-variable formula, it’s helpful to first explore how linear transformations affect volume in high-dimensional spaces.\nLet \\(\\mathbf{Z}\\) be a random vector uniformly distributed in the unit cube \\([0,1]^n\\), and let \\(\\mathbf{X} = A\\mathbf{Z}\\), where \\(A\\) is a square, invertible matrix. Geometrically, the matrix \\(A\\) maps the unit hypercube to a parallelogram in 2D or a parallelotope in higher dimensions.\nThe determinant of a square matrix tells us how the transformation scales volume. For instance, if the determinant of a \\(2 \\times 2\\) matrix is 3, applying that matrix will stretch the area of a region by a factor of 3. A negative determinant indicates a reflection, meaning the transformation also flips the orientation. When measuring volume, we care about the absolute value of the determinant.\nThe volume of the resulting parallelotope is given by:\n\\[\n\\text{Volume} = |\\det(A)|\n\\]\nThis expression tells us how much the transformation \\(A\\) scales space. For example, if \\(|\\det(A)| = 2\\), the transformation doubles the volume.\nTo make this idea concrete, consider the illustration below. The left figure shows a uniform distribution over the unit square \\([0, 1]^2\\). When we apply the linear transformation \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\), each point in the square is mapped to a new location, stretching the square into a parallelogram. The area of this parallelogram — and hence the volume scaling — is given by the absolute value of the determinant \\(|\\det(A)| = |ad - bc|\\).\n\n\n\n A linear transformation maps a unit square to a parallelogram. \n\n\nThis geometric intuition becomes essential when we apply the same logic to probability densities. The area of the parallelogram equals the absolute value of the determinant, |det(A)|, indicating how the transformation scales area.\n\n\n2.3 Determinants and Probability Density\nPreviously, we saw how a linear transformation scales volume. Now we apply the same idea to probability densities — since density is defined per unit volume, scaling the volume also affects the density.\nTo transform the density from \\(\\mathbf{Z}\\) to \\(\\mathbf{X}\\), we use the change-of-variable formula. Since \\(\\mathbf{X} = A\\mathbf{Z}\\), the inverse transformation is \\(\\mathbf{Z} = A^{-1} \\mathbf{X}\\). This tells us how to evaluate the density at \\(\\mathbf{x}\\) by “pulling it back” through the inverse mapping. Applying the multivariate change-of-variable rule:\n\\[\np_X(\\mathbf{x}) = p_Z(W \\mathbf{x}) \\cdot \\left| \\det(W) \\right| \\quad \\text{where } W = A^{-1}\n\\]\nThis is directly analogous to the 1D change-of-variable rule:\n\\[\np_X(x) = p_Z(h(x)) \\cdot |h'(x)|\n\\]\nbut now in multiple dimensions using the determinant of the inverse transformation.\nTo make this more concrete, here’s a simple 2D example demonstrating how linear transformations affect probability density.\nLet \\(\\mathbf{Z}\\) be a random vector uniformly distributed over the unit square \\([0, 1]^2\\). Suppose we apply the transformation \\(\\mathbf{X} = A\\mathbf{Z}\\), where\n\\[\nA = \\begin{bmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\quad \\text{so that} \\quad\nW = A^{-1} =\n\\begin{bmatrix}\n\\frac{1}{2} & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\nThis transformation stretches the square horizontally, doubling its width while keeping the height unchanged. As a result, the area is doubled:\n\\[\n|\\det(A)| = 2 \\quad \\text{and} \\quad |\\det(W)| = \\frac{1}{2}\n\\] Since the same total probability must be spread over a larger area, the density decreases, meaning the probability per unit area is reduced due to the increased area over which the same total probability is distributed.\nNow, let’s say \\(p_Z(z) = 1\\) inside the unit square (a uniform distribution). To compute \\(p_X(\\mathbf{x})\\) at a point \\(\\mathbf{x}\\) in the transformed space, we use:\n\\[\np_X(\\mathbf{x}) = p_Z(W\\mathbf{x}) \\cdot |\\det(W)| = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}\n\\]\nSo, the transformed density is halved — the same total probability (which must remain 1) is now spread over an area that is twice as large.\n\n\n2.4 Generalizing to Nonlinear Transformations\nFor nonlinear transformations \\(\\mathbf{x} = f(\\mathbf{z})\\), the idea is similar. But instead of a constant matrix \\(A\\), we now consider the Jacobian matrix of the function \\(f\\):\n\\[\nJ_f(\\mathbf{z}) = \\frac{\\partial f}{\\partial \\mathbf{z}}\n\\]\nThe Jacobian matrix generalizes derivatives to multivariable functions, capturing how a transformation scales and rotates space locally through all partial derivatives. Its determinant tells us how much the transformation stretches or compresses space — acting as a local volume scaling factor.\n\n\n2.5 Multivariate Change-of-Variable\nGiven an invertible transformation \\(\\mathbf{x} = f(\\mathbf{z})\\), the probability density transforms as:\n\\[\np_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\cdot \\left| \\det \\left( \\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nAlternatively, in the forward form (often used during training):\n\\[\np_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\cdot \\left| \\det \\left( \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}} \\right) \\right|^{-1}\n\\]\nThis generalizes the 1D rule and enables us to compute exact likelihoods for complex distributions as long as the transformation is invertible and differentiable. This formula is pivotal in machine learning, where transformations of probability distributions are common — such as in the implementation of normalizing flows for generative modeling."
  },
  {
    "objectID": "flows.html#flow-model",
    "href": "flows.html#flow-model",
    "title": "Normalizing Flow Models",
    "section": "3 Flow Model",
    "text": "3 Flow Model\nA normalizing flow model defines a one-to-one and reversible transformation between observed variables \\(\\mathbf{x}\\) and latent variables \\(\\mathbf{z}\\). This transformation is given by an invertible, differentiable function \\(f_\\theta\\), parameterized by \\(\\theta\\):\n\\[\n\\mathbf{x} = f_\\theta(\\mathbf{z}) \\quad \\text{and} \\quad \\mathbf{z} = f_\\theta^{-1}(\\mathbf{x})\n\\]\n\n\n\nFlow model showing forward and inverse transformations\n\n\nFigure: A flow-based model uses a forward transformation \\(f_\\theta\\) to map from latent variables (\\(\\mathbf{z}\\)) to data (\\(\\mathbf{x}\\)), and an inverse transformation \\(f_\\theta^{-1}\\) to compute likelihoods. Adapted from class notes (XCS236, Stanford).\nBecause the transformation is invertible, we can apply the change-of-variable formula to compute the exact probability of \\(\\mathbf{x}\\):\n\\[\np_X(\\mathbf{x}; \\theta) = p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nThis makes it possible to evaluate exact likelihoods and learn the model via maximum likelihood estimation (MLE).\n\nNote: Both \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) must be continuous and have the same dimensionality since the transformation must be invertible.\n\n\n3.1 Model Architecture: A Sequence of Invertible Transformations\nThe term flow refers to the fact that we can compose multiple invertible functions to form a more expressive transformation:\n\\[\n\\mathbf{z}_m = f_\\theta^{(m)} \\circ f_\\theta^{(m-1)} \\circ \\cdots \\circ f_\\theta^{(1)}(\\mathbf{z}_0)\n\\]\nIn this setup:\n\n\\(\\mathbf{z}_0 \\sim p_Z\\) is sampled from a simple base distribution (e.g., standard Gaussian)\n\\(\\mathbf{x} = \\mathbf{z}_M\\) is the final transformed variable\nThe full transformation \\(f_\\theta\\) is the composition of \\(M\\) sequential invertible functions. Each function slightly reshapes the distribution, and together they produce a highly expressive mapping from a simple base distribution to a complex one.\n\nThe visuals below illustrate this idea from two angles. The first diagram illustrates the structure of a normalizing flow as a composition of invertible steps, while the second shows how this architecture reshapes simple distributions into complex ones through repeated transformations.\n\n\n\n Adapted from Wikipedia: Mapping simple distributions to complex ones via invertible transformations. \n\n\n\n\n\n Adapted from class notes (XCS236, Stanford), originally based on Rezende & Mohamed, 2016. \n\n\nThe density of \\(\\mathbf{x}\\) is given by the change-of-variable formula:\n\\[\np_X(\\mathbf{x}; \\theta) = p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot \\prod_{m=1}^M \\left| \\det \\left( \\frac{\\partial (f_\\theta^{(m)})^{-1}(\\mathbf{z}_m)}{\\partial \\mathbf{z}_m} \\right) \\right|\n\\]\nThis approach allows the model to approximate highly complex distributions using simple building blocks."
  },
  {
    "objectID": "flows.html#learning-and-inference",
    "href": "flows.html#learning-and-inference",
    "title": "Normalizing Flow Models",
    "section": "4 Learning and Inference",
    "text": "4 Learning and Inference\nTraining a flow-based model is done by maximizing the log-likelihood over the dataset \\(\\mathcal{D}\\):\n\\[\n\\max_\\theta \\log p_X(\\mathcal{D}; \\theta) = \\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p_Z(f_\\theta^{-1}(\\mathbf{x})) + \\log \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nKey advantages of normalizing flows:\n\nExact likelihoods: No approximation needed — just apply the change-of-variable rule\nEfficient sampling: Generate new data by drawing \\(\\mathbf{z} \\sim p_Z\\) and computing \\(\\mathbf{x} = f_\\theta(\\mathbf{z})\\)\nLatent inference: Invert \\(f_\\theta\\) to compute latent codes \\(\\mathbf{z} = f_\\theta^{-1}(\\mathbf{x})\\), without needing a separate encoder\n\n\n4.1 Computational Considerations\nOne challenge in training normalizing flow models is that computing the exact likelihood requires evaluating the determinant of the Jacobian matrix of the transformation:\n\nFor a transformation \\(f : \\mathbb{R}^n \\to \\mathbb{R}^n\\), the Jacobian is an \\(n \\times n\\) matrix.\nComputing its determinant has a cost of \\(\\mathcal{O}(n^3)\\), which is computationally expensive during training — especially in high dimensions.\n\n\n4.1.1 Key Insight\nTo make normalizing flows scalable, we design transformations where the Jacobian has a special structure that makes the determinant easy to compute.\nFor example: - If the Jacobian is a triangular matrix, the determinant is just the product of the diagonal entries, which can be computed in \\(\\mathcal{O}(n)\\) time. - This works because in a triangular matrix, all the off-diagonal elements are zero — so the determinant simplifies significantly.\nIn practice, flow models like RealNVP and MAF are designed so that each output dimension \\(x_i\\) depends only on some subset of the input dimensions \\(z_{\\leq i}\\) (for lower triangular structure) or \\(z_{\\geq i}\\) (for upper triangular structure). This results in a Jacobian of the form:\n\\[\nJ = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial z_1} & 0 & \\cdots & 0 \\\\\n\\ast & \\frac{\\partial f_2}{\\partial z_2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\ast & \\ast & \\cdots & \\frac{\\partial f_n}{\\partial z_n}\n\\end{pmatrix}\n\\]\nBecause of this triangular structure, computing the determinant becomes as simple as multiplying the diagonal terms:\n\\[\n\\det(J) = \\prod_{i=1}^{n} \\frac{\\partial f_i}{\\partial z_i}\n\\]\nThis is why many modern flow models rely on coupling layers or autoregressive masking: they preserve invertibility and enable efficient, exact likelihood computation."
  },
  {
    "objectID": "flows.html#types-of-flow-architectures",
    "href": "flows.html#types-of-flow-architectures",
    "title": "Normalizing Flow Models",
    "section": "5 Types of Flow Architectures",
    "text": "5 Types of Flow Architectures\nThis section introduces common architectural families used in normalizing flows, highlighting their core ideas, strengths, and limitations.\n\n5.1 Elementwise Flows\n\nIdea: Apply a simple invertible function to each variable independently.\nExamples: Leaky ReLU, Softplus, ELU.\nStrengths: Extremely fast; easy to implement; analytically tractable.\nLimitations: Cannot model interactions or dependencies between variables.\n\n\n\n5.2 Linear Flows\n\nIdea: Apply a linear transformation using an invertible matrix (e.g., permutation, rotation, LU decomposition).\nExamples: Glow’s 1x1 Convolution, LU flows.\nStrengths: Efficiently models global dependencies; can be used to permute variables.\nLimitations: Limited expressiveness when used alone.\n\n\n\n5.3 Coupling Flows\n\nIdea: Split the input into two parts. One half remains unchanged while the other is transformed based on it.\nExamples: NICE (additive), RealNVP (affine).\nStrengths: Easy to invert and compute Jacobians; scalable to high dimensions.\nLimitations: Requires stacking multiple layers to mix information across all dimensions.\n\n\n\n5.4 Autoregressive Flows\n\nIdea: Model the transformation of each variable conditioned on the previous ones in a fixed order.\nExamples: Masked Autoregressive Flow (MAF), Inverse Autoregressive Flow (IAF).\nStrengths: Highly expressive; models arbitrary dependencies.\nLimitations: Slower sampling or density evaluation depending on flow direction.\n\n\n\n5.5 Residual Flows\n\nIdea: Add residual connections while enforcing invertibility (e.g., using constraints on Jacobian eigenvalues).\nExamples: Planar flows, Radial flows, Residual Flows (Behrmann et al.).\nStrengths: Flexible and capable of complex transformations.\nLimitations: May require care to ensure invertibility; harder to train.\n\n\n\n5.6 Continuous Flows\n\nIdea: Model the transformation as the solution to a differential equation parameterized by a neural network.\nExamples: Neural ODEs, FFJORD.\nStrengths: Highly flexible; enables continuous-time modeling.\nLimitations: Computationally expensive; uses ODE solvers during training and inference.\n\n\nThese architectures can be mixed and matched in real-world models to balance expressiveness, efficiency, and tractability. Each comes with trade-offs, and their selection often depends on the task and data at hand.\nIn the rest of this article, we focus on Coupling Flows, briefly introducing NICE and then diving deeper into the structure, intuition, and implementation details of RealNVP.\n\n\n5.7 NICE: Nonlinear Independent Components Estimation\nThe NICE (Nonlinear Independent Components Estimation) model, introduced by Laurent Dinh, David Krueger, and Yoshua Bengio in 2014, is a foundational work in the development of normalizing flows.\nIt provides a framework for transforming complex high-dimensional data into a simpler latent space (often a standard Gaussian), enabling both exact likelihood estimation and sampling — two fundamental goals in generative modeling.\n\n5.7.1 Core Concepts\n\nInvertible Transformations:\nNICE constructs a chain of invertible functions to map inputs to latent variables. This ensures that both the forward and inverse transformations are tractable.\nAdditive Coupling Layers:\nThe model partitions the input into two parts and applies an additive transformation to one part using a function of the other. This design yields a triangular Jacobian with determinant 1, making log-likelihood computation efficient.\nVolume-Preserving Mapping:\nBecause additive coupling layers do not scale the space, NICE preserves volume — i.e., the Jacobian determinant is exactly 1. While this limits expressiveness, it simplifies training and inference.\nScaling Layer (Optional):\nThe original NICE paper includes an optional scaling layer at the end to allow some volume change per dimension.\nExact Log-Likelihood:\nUnlike VAEs or GANs, which rely on approximations, NICE enables exact evaluation of the log-likelihood, making it a fully probabilistic, likelihood-based model.\n\n\n\n5.7.2 Additive Coupling Layer\nTo make the transformation invertible and computationally efficient, NICE splits the input vector into two parts. One part is kept unchanged, while the other part is modified using a function of the unchanged part. This way, we can easily reverse the process because we always know what was kept intact.\nLet’s partition the input \\(\\mathbf{z} \\in \\mathbb{R}^n\\) into two subsets: \\(\\mathbf{z}_{1:d}\\) and \\(\\mathbf{z}_{d+1:n}\\) for some \\(1 \\leq d &lt; n\\).\n\nForward Mapping \\(\\mathbf{z} \\mapsto \\mathbf{x}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)} \\\\\n\\mathbf{x}_{d+1:n} &= \\mathbf{z}_{d+1:n} + m_\\theta(\\mathbf{z}_{1:d})\n\\end{aligned}\n\\]\nwhere \\(m_\\theta(\\cdot)\\) is a neural network with parameters \\(\\theta\\), \\(d\\) input units, and \\(n - d\\) output units.\n\nInverse Mapping \\(\\mathbf{x} \\mapsto \\mathbf{z}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)} \\\\\n\\mathbf{z}_{d+1:n} &= \\mathbf{x}_{d+1:n} - m_\\theta(\\mathbf{x}_{1:d})\n\\end{aligned}\n\\]\n\nJacobian of the forward mapping:\n\nThe Jacobian matrix captures how much the transformation stretches or compresses space. Because the unchanged subset passes through as-is and the transformation is purely additive (no scaling), the Jacobian is triangular with 1s on the diagonal — so its determinant is 1 — meaning the transformation preserves volume.\n\\[\nJ = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\n\\frac{\\partial m_\\theta}{\\partial \\mathbf{z}_{1:d}} & I_{n-d}\n\\end{pmatrix}\n\\]\n\\[\n\\det(J) = 1\n\\]\nHence, additive coupling is a volume-preserving transformation.\n\n\n5.7.3 Rescaling Layer\nTo overcome the limitation of fixed volume, NICE adds a diagonal scaling transformation at the end, allowing the model to contract or expand space. This is not part of the coupling layers, but is crucial to increase flexibility.\n\nForward Mapping:\n\n\\[\nx_i = s_i z_i \\quad \\text{with} \\quad s_i &gt; 0\n\\]\n\nInverse Mapping:\n\n\\[\nz_i = \\frac{x_i}{s_i}\n\\]\n\nJacobian:\n\n\\[\nJ = \\text{diag}(\\mathbf{s})\n\\quad \\Rightarrow \\quad\n\\det(J) = \\prod_{i=1}^n s_i\n\\]\nHowever, the volume-preserving property of NICE limits its expressiveness. RealNVP extends this idea by introducing affine coupling layers that enable volume changes during transformation.\n\n\n\n5.8 Real-NVP: Non-Volume Preserving Extension of NICE\nReal-NVP (Dinh et al., 2017) extends NICE by introducing a scaling function that allows the model to change volume, enabling more expressive transformations. This is achieved using affine coupling layers that apply learned scaling and translation functions to part of the input while keeping the rest unchanged.\n\n\n\n Visualization of a single affine coupling layer in RealNVP. The identity path and affine transform structure allow exact inversion and efficient computation. \n\n\nWe partition the input \\(\\mathbf{z} \\in \\mathbb{R}^n\\) into two subsets: \\(\\mathbf{z}_{1:d}\\) and \\(\\mathbf{z}_{d+1:n}\\).\n\nForward Mapping \\(\\mathbf{z} \\mapsto \\mathbf{x}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)} \\\\\\\\\n\\mathbf{x}_{d+1:n} &= \\mathbf{z}_{d+1:n} \\odot \\exp(\\alpha_\\theta(\\mathbf{z}_{1:d})) + \\mu_\\theta(\\mathbf{z}_{1:d})\n\\end{aligned}\n\\]\nHere, \\(\\boldsymbol{\\alpha}_\\theta(\\cdot)\\) and \\(\\boldsymbol{\\mu}_\\theta(\\cdot)\\) are neural networks with parameters \\(\\theta\\) that take the unchanged subset \\(\\mathbf{z}_{1:d}\\) as input and produce scale and shift parameters, respectively, for the transformed subset \\(\\mathbf{z}_{d+1:n}\\). These functions enable flexible, learnable affine transformations while preserving invertibility.\n\nInverse Mapping \\(\\mathbf{x} \\mapsto \\mathbf{z}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)} \\\\\\\\\n\\mathbf{z}_{d+1:n} &= \\left( \\mathbf{x}_{d+1:n} - \\mu_\\theta(\\mathbf{x}_{1:d}) \\right) \\odot \\exp(-\\alpha_\\theta(\\mathbf{x}_{1:d}))\n\\end{aligned}\n\\]\nThe inverse mapping recovers the latent variable \\(\\mathbf{z}\\) from the data \\(\\mathbf{x}\\). The first subset \\(\\mathbf{x}_{1:d}\\) remains unchanged and directly becomes \\(\\mathbf{z}_{1:d}\\). To reconstruct \\(\\mathbf{z}_{d+1:n}\\), we first subtract the shift \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_{1:d})\\) from \\(\\mathbf{x}_{d+1:n}\\), and then apply an elementwise rescaling using \\(\\exp(-\\boldsymbol{\\alpha}_\\theta(\\mathbf{x}_{1:d}))\\). This inversion relies on the same neural networks used in the forward pass and ensures that the transformation is exactly reversible.\n\nJacobian of Forward Mapping:\n\n\\[\nJ = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\\\\\n\\frac{\\partial \\mathbf{x}_{d+1:n}}{\\partial \\mathbf{z}_{1:d}} & \\operatorname{diag}\\left(\\exp(\\alpha_\\theta(\\mathbf{z}_{1:d}))\\right)\n\\end{pmatrix}\n\\]\nThe Jacobian matrix of the RealNVP forward transformation has a special block structure due to the design of the affine coupling layer:\n\nUpper left block: \\(\\mathbf{I}_d\\)\nThis corresponds to the partial derivatives of \\(\\mathbf{x}_{1:d}\\) with respect to \\(\\mathbf{z}_{1:d}\\). Since the first \\(d\\) variables are passed through unchanged (\\(\\mathbf{x}_{1:d} = \\mathbf{z}_{1:d}\\)), their derivatives form an identity matrix.\nUpper right block: \\(0\\)\nThese derivatives are zero because \\(\\mathbf{x}_{1:d}\\) does not depend on \\(\\mathbf{z}_{d+1:n}\\) at all — they’re completely decoupled.\nLower right block: (diagonal)\nEach element of \\(\\mathbf{x}_{d+1:n}\\) is scaled elementwise by \\(\\exp\\left(\\left[\\alpha_\\theta(\\mathbf{z}_{1:d})\\right]_i\\right)\\). This means the Jacobian of this part is a diagonal matrix, where each diagonal entry is the corresponding scale factor.\nLower left block:\nThis part can contain non-zero values because \\(\\mathbf{x}_{d+1:n}\\) depends on \\(\\mathbf{z}_{1:d}\\) via the neural networks. But thanks to the triangular structure of the Jacobian, we don’t need this block to compute the determinant.\n\n\n\n\n Jacobian of the RealNVP forward transformation. Upper triangular structure arises because the first subset is unchanged, while the second is scaled and shifted based on the first. \n\n\n\n5.8.1 Why This Structure Matters\nBecause the Jacobian is triangular, its determinant is simply the product of the diagonal entries.\n\\[\n\\det(J) = \\prod_{i=d+1}^{n} \\exp\\left( \\alpha_\\theta(\\mathbf{z}_{1:d})_i \\right)\n= \\exp\\left( \\sum_{i=d+1}^{n} \\alpha_\\theta(\\mathbf{z}_{1:d})_i \\right)\n\\]\nIn log-space, this becomes a sum:\n\\[\n\\log \\det(J) = \\sum_{i=d+1}^{n} \\alpha_\\theta(\\mathbf{z}_{1:d})_i\n\\]\nThis makes the computation of log-likelihoods fast and tractable.\nTaking the product of the diagonal entries gives us a measure of how much the transformation expands or contracts local volume. If the determinant is greater than 1, the transformation expands space; if it’s less than 1, it contracts space. Since the determinant is not fixed, RealNVP performs a non-volume preserving transformation — allowing it to model more complex distributions than NICE, which preserves volume by design.\n\n\n5.8.2 Stacking Coupling Layers\nEach coupling layer only transforms part of the input. To ensure that every dimension is eventually updated, RealNVP stacks multiple coupling layers and alternates the masking pattern between them.\n\nIn one layer, the first half is fixed, and the second half is transformed.\nIn the next layer, the roles are reversed.\n\nThis alternating structure ensures: - All input dimensions are updated across layers - The full transformation remains invertible - The total log-determinant is the sum of the log-determinants of each layer\n\n\n5.8.3 RealNVP in Action (Two Moons)\nThe following plots illustrate how RealNVP transforms data in practice:\n\n\n\n Top-left: Original two-moons data (X)\nTop-right: Encoded latent space (Z) Bottom-left: Latent samples from base distribution\nBottom-right: Generated samples mapped back to (X) space\n\n\n\n\n\n5.8.4 Summary\nTo recap the key distinctions between NICE and RealNVP, here’s a side-by-side comparison:\n\n\n\n\n\n\n\n\nAspect\nNICE\nRealNVP\n\n\n\n\nType of coupling\nAdditive\nAffine (scaling + shift)\n\n\nVolume change\nOnly possible with rescaling layer\nBuilt into each coupling layer\n\n\nJacobian determinant\n1 (in coupling layers)\nVaries (depends on learned scale)\n\n\nExpressiveness\nLimited (volume-preserving layers)\nHigher (learns scale & shift)\n\n\nLog-likelihood\nExact\nExact"
  },
  {
    "objectID": "flows.html#try-it-yourself-flow-model-in-pytorch",
    "href": "flows.html#try-it-yourself-flow-model-in-pytorch",
    "title": "Normalizing Flow Models",
    "section": "6 🧪 Try It Yourself: Flow Model in Pytorch",
    "text": "6 🧪 Try It Yourself: Flow Model in Pytorch\nYou can explore a minimal PyTorch implementation of a normalizing flow model:\n\n📘 View Notebook on GitHub\n🚀 Run in Google Colab"
  },
  {
    "objectID": "flows.html#references",
    "href": "flows.html#references",
    "title": "Normalizing Flow Models",
    "section": "7 References",
    "text": "7 References\n[1] Stanford CS236 Notes. “Normalizing Flows”\n[2] UT Austin Calculus Notes. “Jacobian and Change of Variables”\n[3] Danilo Jimenez Rezende, and Shakir Mohamed. “Variational Inference with Normalizing Flows”\n[4] Kobyzev, Prince, and Brubaker. “Normalizing Flows: An Introduction and Review of Current Methods”\n[5] Wikipedia. “Normalizing Flow”"
  },
  {
    "objectID": "flows.html#further-reading",
    "href": "flows.html#further-reading",
    "title": "Normalizing Flow Models",
    "section": "8 Further Reading",
    "text": "8 Further Reading\n[1] George Papamakarios et al. “Normalizing Flows for Probabilistic Modeling and Inference”\n[2] Lilian Weng. “Flow-based Models”\n[3] Eric Jang. “Normalizing Flows Tutorial – Part 1”\n[4] Eric Jang. “Normalizing Flows Tutorial – Part 2”"
  },
  {
    "objectID": "diffusion.html",
    "href": "diffusion.html",
    "title": "Diffusion Models",
    "section": "",
    "text": "Diffusion models are a powerful class of generative models that learn to create data—such as images—by reversing a gradual noising process. During training, real data is progressively corrupted by adding small amounts of Gaussian noise over many steps until it becomes nearly indistinguishable from pure noise. A neural network is then trained to learn the reverse process: transforming noise back into realistic samples, one step at a time.\nThis approach has enabled state-of-the-art results in image generation, powering tools like DALL·E 2, Imagen, and Stable Diffusion. One of the key advantages of diffusion models lies in their training stability and output quality, especially when compared to earlier generative approaches:\n\nGANs generate sharp images but rely on adversarial training, which can be unstable and prone to mode collapse.\nVAEs are more stable but often produce blurry outputs due to their reliance on Gaussian assumptions and variational approximations.\nNormalizing Flows provide exact log-likelihoods and stable training but require invertible architectures, which limit model expressiveness.\nDiffusion models avoid adversarial dynamics and use a simple denoising objective. This makes them easier to train and capable of producing highly detailed and diverse samples.\n\nThis combination of theoretical simplicity, training robustness, and high-quality outputs has made diffusion models one of the most effective generative modeling techniques in use today."
  },
  {
    "objectID": "diffusion.html#introduction",
    "href": "diffusion.html#introduction",
    "title": "Diffusion Models",
    "section": "",
    "text": "Diffusion models are a powerful class of generative models that learn to create data—such as images—by reversing a gradual noising process. During training, real data is progressively corrupted by adding small amounts of Gaussian noise over many steps until it becomes nearly indistinguishable from pure noise. A neural network is then trained to learn the reverse process: transforming noise back into realistic samples, one step at a time.\nThis approach has enabled state-of-the-art results in image generation, powering tools like DALL·E 2, Imagen, and Stable Diffusion. One of the key advantages of diffusion models lies in their training stability and output quality, especially when compared to earlier generative approaches:\n\nGANs generate sharp images but rely on adversarial training, which can be unstable and prone to mode collapse.\nVAEs are more stable but often produce blurry outputs due to their reliance on Gaussian assumptions and variational approximations.\nNormalizing Flows provide exact log-likelihoods and stable training but require invertible architectures, which limit model expressiveness.\nDiffusion models avoid adversarial dynamics and use a simple denoising objective. This makes them easier to train and capable of producing highly detailed and diverse samples.\n\nThis combination of theoretical simplicity, training robustness, and high-quality outputs has made diffusion models one of the most effective generative modeling techniques in use today."
  },
  {
    "objectID": "diffusion.html#math-review",
    "href": "diffusion.html#math-review",
    "title": "Diffusion Models",
    "section": "2 Math Review",
    "text": "2 Math Review\n\n2.1 Forward Diffusion Process\nThe forward diffusion process gradually turns a data sample (such as an image) into pure noise by adding a little bit of random noise at each step. This process is a Markov chain, meaning each step depends only on the previous one.\n\n2.1.1 Start with a Data Sample\nBegin with a data point \\(x_0\\), sampled from dataset (such as a real image). The goal is to slowly corrupt \\(x_0\\) by adding noise over many steps, until it becomes indistinguishable from random Gaussian noise.\nWe’ll later see that it’s also possible to sample \\(x_t\\) directly from \\(x_0\\), without simulating every step.\n\n\n2.1.2 Add Noise Recursively\nAt each time step \\(t\\), the process is defined as: \\[\nq(x_t \\mid x_{t-1}) = \\mathcal{N}\\left(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) I\\right)\n\\]\nWhere:\n\n\\(\\alpha_t = 1 - \\beta_t\\), where \\(\\beta_t\\) a small positive number controlling the noise level at step \\(t\\), while \\(\\alpha_t\\) emphasizes the amount of original signal retained.\n\\(I\\) is the identity matrix, so noise is added independently to each component.\n\n\n\n\n\n\n\nIntuition: At each step, we shrink the signal and add new Gaussian noise. Over many steps, the image becomes blurrier and more like random noise.\n\n\n\n\nWhy keep \\(\\beta_t\\) small?\nKeeping \\(\\beta_t\\) small ensures that noise is added gradually. This allows the model to retain structure across steps and converge slowly to pure noise. Large values of \\(\\beta_t\\) would destroy the signal too quickly, making it harder for the reverse model to reconstruct the data. The design of the forward process balances signal decay (via \\(\\sqrt{\\alpha_t}\\)) and noise growth (via \\(\\sqrt{1 - \\alpha_t}\\)) to ensure a smooth, learnable transition.\n\n\n\n2.1.3 The Markov Chain\nThe full sequence is:\n\\[\nx_0 \\rightarrow x_1 \\rightarrow x_2 \\rightarrow \\ldots \\rightarrow x_T\n\\]\nThe joint probability of the sequence is:\n\\[\nq(x_{1:T} \\mid x_0) = \\prod_{t=1}^{T} q(x_t \\mid x_{t-1})\n\\]\nThis means we can sample the whole chain by repeatedly applying the noise step.\n\n\n\n\n\n\nInsight: While the forward process defines a full Markov chain from \\(x_0\\) to \\(x_T\\), we’ll soon see that it’s also possible to sample any \\(x_t\\) directly from \\(x_0\\) using a closed-form Gaussian — without simulating each intermediate step.\n\n\n\n\n\n2.1.4 Deriving the Marginal Distribution \\(q(x_t \\mid x_0)\\)\n How do we get the formula that lets us sample \\(x_t\\) directly from \\(x_0\\) (without simulating all the intermediate steps)?\n\nLet’s see how \\(x_t\\) is built up from \\(x_0\\):\nFor \\(t = 1\\): \\[\nx_1 = \\sqrt{\\alpha_1} x_0 + \\sqrt{1 - \\alpha_1} \\epsilon_1, \\qquad \\epsilon_1 \\sim \\mathcal{N}(0, I)\n\\]\nFor \\(t = 2\\): \\[\nx_2 = \\sqrt{\\alpha_2} x_1 + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\] Substitute \\(x_1\\): \\[\nx_2 = \\sqrt{\\alpha_2} \\left( \\sqrt{\\alpha_1} x_0 + \\sqrt{1 - \\alpha_1} \\epsilon_1 \\right) + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\] \\[\n= \\sqrt{\\alpha_2 \\alpha_1} x_0 + \\sqrt{\\alpha_2 (1 - \\alpha_1)} \\epsilon_1 + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\]\nFor general \\(t\\), recursively expanding gives: \\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sum_{i=1}^t \\left( \\sqrt{ \\left( \\prod_{j=i+1}^t \\alpha_j \\right) (1 - \\alpha_i) } \\, \\epsilon_i \\right)\n\\] where \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\).\nEach \\(\\epsilon_i\\) is independent Gaussian noise. The sum of independent Gaussians (each scaled by a constant) is still a Gaussian, with variance equal to the sum of the variances: \\[\n\\text{Total variance} = \\sum_{i=1}^t \\left( \\prod_{j=i+1}^t \\alpha_j \\right) (1 - \\alpha_i)\n\\] This sum simplifies to: \\[\n1 - \\bar{\\alpha}_t\n\\]\nThis can be proved by induction or by telescoping the sum.\nAll the little bits of noise added at each step combine into one big Gaussian noise term, with variance \\(1 - \\bar{\\alpha}_t\\).\n\n\n2.1.5 The Final Marginal Distribution\nSo, we can sample \\(x_t\\) directly from \\(x_0\\) using: \\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, I)\n\\]\nThis lets us sample \\(x_t\\) directly from \\(x_0\\), without recursively computing all previous steps \\(x_1, x_2, \\dots, x_{t-1}\\).\nThis means: \\[\nq(x_t \\mid x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I\\right)\n\\]\nAs \\(t\\) increases, \\(\\bar{\\alpha}_t\\) shrinks toward zero. Eventually, \\(x_t\\) becomes pure noise:\n\\[\nx_T \\sim \\mathcal{N}(0, I)\n\\]\n\n\n2.1.6 Recap: Forward Diffusion Steps\n\n\n\n\n\n\n\n\nStep\nFormula\nExplanation\n\n\n\n\n1\n\\(x_0\\)\nOriginal data sample\n\n\n2\n\\(q(x_t \\mid x_{t-1}) = \\mathcal{N}(\\sqrt{\\alpha_t} x_{t-1}, (1-\\alpha_t) I)\\)\nAdd noise at each step\n\n\n3\n\\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\\)\nDirectly sample \\(x_t\\) from \\(x_0\\) using noise \\(\\epsilon\\)\n\n\n4\n\\(q(x_t \\mid x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I)\\)\nMarginal distribution at step \\(t\\)\n\n\n5\n\\(x_T \\sim \\mathcal{N}(0, I)\\)\nAfter many steps, pure noise\n\n\n\n\n\n2.1.7 Key Takeaways\n\nThe forward diffusion process is just repeatedly adding noise to your data.\nThanks to properties of Gaussian noise, you can describe the result as the original data scaled down plus one cumulative chunk of Gaussian noise.\n\nAfter enough steps, the data becomes indistinguishable from random noise.\n\n\n\n\n\n2.2 Reverse Diffusion Process\nLet’s break down the reverse diffusion process step by step. This is the generative phase of diffusion models, where we learn to turn pure noise back into data. For clarity, we’ll use the same notation as in the forward process:\n\nForward process: Gradually adds noise to data via \\(q(x_t \\mid x_{t-1})\\)\nReverse process: Gradually removes noise via \\(p_\\theta(x_{t-1} \\mid x_t)\\), learned by a neural network\n\nThe Goal of the Reverse Process\nObjective: Given a noisy sample \\(x_t\\), we want to estimate the conditional distribution \\(q(x_{t-1} \\mid x_t)\\). However, this is intractable because it would require knowing the true data distribution.\nInstead, we train a neural network to approximate it: \\[\np_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n\\]\nHere, \\(\\mu_\\theta(x_t, t)\\) is the predicted mean and \\(\\Sigma_\\theta(x_t, t)\\) is the predicted covariance (often diagonal) of the reverse Gaussian distribution.\nIn practice, many diffusion models do not directly predict \\(\\mu_\\theta\\) or \\(x_0\\), but instead predict the noise \\(\\epsilon\\) added in the forward process. This makes the objective simpler and more effective, as we’ll see in the next section.\nKey Insight from the Forward Process\nIf the noise added in the forward process is small (i.e., \\(\\beta_t \\ll 1\\)), then the reverse conditional \\(q(x_{t-1} \\mid x_t)\\) is also Gaussian: \\[\nq(x_{t-1} \\mid x_t) \\approx \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t), \\tilde{\\beta}_t I)\n\\]\nThis approximation works because the forward process adds Gaussian noise in small increments at each step. The Markov chain formed by these small Gaussian transitions ensures that local conditionals (like \\(q(x_{t-1} \\mid x_t)\\)) remain Gaussian under mild assumptions.\n\n\n\n\n\n\nGlossary of Symbols\n\n\n\n\n\\(\\alpha_t\\): Variance-preserving noise coefficient at step \\(t\\)\n\\(\\bar{\\alpha}_t\\): Cumulative product of \\(\\alpha_t\\), i.e., \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\)\n\\(\\beta_t\\): Variance of the noise added at step \\(t\\), typically \\(\\beta_t = 1 - \\alpha_t\\)\n\\(x_0\\): Original clean data sample (e.g., image)\n\\(x_t\\): Noisy version of \\(x_0\\) at timestep \\(t\\)\n\\(\\epsilon\\): Standard Gaussian noise sampled from \\(\\mathcal{N}(0, I)\\)\n\\(\\tilde{\\mu}_t\\): Mean of the reverse process distribution at time \\(t\\)\n\\(\\tilde{\\beta}_t\\): Variance of the reverse process distribution at time \\(t\\)\n\n\n\n\n2.2.1 Deriving \\(q(x_{t-1} \\mid x_t, x_0)\\) Using Bayes’ Rule\nWe can’t directly evaluate \\(q(x_{t-1} \\mid x_t)\\), but we can derive the posterior \\(q(x_{t-1} \\mid x_t, x_0)\\) using Bayes’ rule:\n\\[\nq(x_{t-1} \\mid x_t, x_0) = \\frac{q(x_t \\mid x_{t-1}, x_0) \\cdot q(x_{t-1} \\mid x_0)}{q(x_t \\mid x_0)}\n\\]\nFrom the forward process, we know:\n\n\\(q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1},\\, \\beta_t I)\\)\n\n\\(q(x_{t-1} \\mid x_0) = \\mathcal{N}(x_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} x_0,\\, (1 - \\bar{\\alpha}_{t-1}) I)\\)\n\n\\(q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0,\\, (1 - \\bar{\\alpha}_t) I)\\)\n\nTo derive a usable form of the posterior, we substitute the Gaussian densities into Bayes’ rule. The multivariate normal density is:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\Sigma) \\propto \\exp\\left( -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\n\\]\nSince all covariances here are multiples of the identity matrix, \\(\\Sigma = \\sigma^2 I\\), the formula simplifies to:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 \\right)\n\\]\n\n\n\n\n\n\nUnderstanding the squared norm\n\n\n\nThe expression \\(\\|x - \\mu\\|^2\\) is the squared distance between two vectors. In 1D, it’s just \\((x - \\mu)^2\\), but in higher dimensions, it becomes:\n\\[\n\\|x - \\mu\\|^2 = \\sum_{i=1}^d (x_i - \\mu_i)^2\n\\]\nThis term appears in the exponent of the Gaussian and represents how far the sample is from the center (mean), scaled by the variance.\n\n\nApplying this to the forward process terms:\n\n\\(q(x_t \\mid x_{t-1}) \\propto \\exp\\left( -\\frac{1}{2\\beta_t} \\| x_t - \\sqrt{\\alpha_t} x_{t-1} \\|^2 \\right)\\)\n\n\\(q(x_{t-1} \\mid x_0) \\propto \\exp\\left( -\\frac{1}{2(1 - \\bar{\\alpha}_{t-1})} \\| x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} x_0 \\|^2 \\right)\\)\n\nWe can ignore \\(q(x_t \\mid x_0)\\) in the denominator, since it is independent of \\(x_{t-1}\\) and will be absorbed into a proportionality constant.\nPutting these together:\n\\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n-\\frac{1}{2} \\left[\n\\frac{ \\|x_t - \\sqrt{\\alpha_t} x_{t-1} \\|^2 }{\\beta_t} +\n\\frac{ \\| x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} x_0 \\|^2 }{1 - \\bar{\\alpha}_{t-1}}\n\\right]\n\\right)\n\\]\n\n\n\n\n\n\nWhy does the product of Gaussians give another Gaussian?\n\n\n\nWhen we multiply two Gaussian distributions over the same variable, the result is also a Gaussian.\nHere, we are multiplying two Gaussians in \\(x_{t-1}\\):\n- One centered at \\(\\sqrt{\\alpha_t} x_t\\)\n- One centered at \\(\\sqrt{\\bar{\\alpha}_{t-1}} x_0\\)\nThe product is another Gaussian in \\(x_{t-1}\\), with a new mean that is a weighted average of both.\nWe’ll derive this explicitly by completing the square in the exponent.\n\n\n\nAlthough we won’t use this posterior directly during sampling, this closed-form expression is essential for defining the ELBO used in training. It gives us a precise target that the reverse model attempts to approximate.\n\nWe now complete the square to put the expression into standard Gaussian form.\n\n\n\n2.2.2 Complete the square\nTo express the exponent in Gaussian form, we’ll complete the square using the identity:\n\\[\na x^2 - 2 b x = a \\left( x - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a}\n\\]\nFrom earlier, we arrived at this expression for the exponent of the posterior:\n\\[\n-\\frac{1}{2} \\left[\n\\frac{(x_t - \\sqrt{\\alpha_t} \\, x_{t-1})^2}{\\beta_t} +\n\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0)^2}{1 - \\bar{\\alpha}_{t-1}}\n\\right]\n\\]\nWe expand both terms:\nFirst term:\n\\[\n\\frac{(x_t - \\sqrt{\\alpha_t} \\, x_{t-1})^2}{\\beta_t}\n= \\frac{x_t^2 - 2 \\sqrt{\\alpha_t} \\, x_t x_{t-1} + \\alpha_t x_{t-1}^2}{\\beta_t}\n\\]\nSecond term:\n\\[\n\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0)^2}{1 - \\bar{\\alpha}_{t-1}}\n= \\frac{x_{t-1}^2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_{t-1} x_0 + \\bar{\\alpha}_{t-1} x_0^2}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nGroup like terms\nNow we collect all the terms involving \\(x_{t-1}\\):\nCoefficient of \\(x_{t-1}^2\\):\n\\[\na = \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nCoefficient of \\(x_{t-1}\\) (the full linear term):\n\\[\n-2 \\left(\n\\frac{ \\sqrt{\\alpha_t} \\, x_t }{ \\beta_t } + \\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0 }{ 1 - \\bar{\\alpha}_{t-1} }\n\\right)\n\\]\nSo we define:\n\\[\nb = \\frac{ \\sqrt{\\alpha_t} \\, x_t }{ \\beta_t } + \\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0 }{ 1 - \\bar{\\alpha}_{t-1} }\n\\]\nRemaining terms (like \\(x_t^2\\) and \\(x_0^2\\)) are independent of \\(x_{t-1}\\) and can be absorbed into a constant.\nWe are modeling the conditional distribution \\(q(x_{t-1} \\mid x_t, x_0)\\), which means both \\(x_t\\) and \\(x_0\\) are known and fixed. So any expression involving only \\(x_t\\) or \\(x_0\\) behaves like a constant and does not influence the shape of the Gaussian over \\(x_{t-1}\\).\nThe exponent now has the form:\n\\[\n-\\frac{1}{2} \\left( a x_{t-1}^2 - 2 b x_{t-1} \\right) + \\text{(constants)}\n\\]\nApply the identity\nUsing the identity: \\[\na x^2 - 2 b x = a \\left( x - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a}\n\\]\nwe rewrite the exponent: \\[\n-\\frac{1}{2} \\left( a x_{t-1}^2 - 2 b x_{t-1} \\right)\n= -\\frac{1}{2} \\left[ a \\left( x_{t-1} - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a} \\right]\n\\]\nWe drop the constant term \\(\\frac{b^2}{a}\\) under proportionality. This transforms the exponent into the Gaussian form: \\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n- \\frac{1}{2 \\tilde{\\beta}_t} \\| x_{t-1} - \\tilde{\\mu}_t \\|^2\n\\right)\n\\]\n\n\n\n\n\n\nNote: This matches the standard Gaussian\n\n\n\nThe standard Gaussian is written as: \\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) \\propto \\exp\\left(\n- \\frac{1}{2\\sigma^2} \\| x - \\mu \\|^2\n\\right)\n\\]\nSo in our case:\n\n\\(\\tilde{\\mu}_t = \\frac{b}{a}\\) is the mean\n\\(\\tilde{\\beta}_t = \\frac{1}{a}\\) is the variance\n\nWe keep the notation \\(\\tilde{\\beta}_t\\) instead of \\(\\sigma^2\\) because it connects directly to the noise schedule (\\(\\beta_t\\), \\(\\bar{\\alpha}_t\\)) used in the diffusion model. This helps tie everything back to how the forward and reverse processes relate.\n\n\nFinal expressions\nNow we can directly read off the expressions for the mean and variance from the completed square.\nWe had: \\[\na = \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}, \\quad\nb = \\frac{\\sqrt{\\alpha_t} \\, x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nFrom the identity: \\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n- \\frac{1}{2 \\tilde{\\beta}_t} \\| x_{t-1} - \\tilde{\\mu}_t \\|^2\n\\right)\n\\]\nwe identify: - \\(\\tilde{\\mu}_t = \\frac{b}{a}\\), - \\(\\tilde{\\beta}_t = \\frac{1}{a}\\)\nLet’s compute these explicitly:\nMean: \\[\n\\tilde{\\mu}_t = \\frac{b}{a} =\n\\frac{\n\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) x_t +\n\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t x_0\n}{\n1 - \\bar{\\alpha}_t\n}\n\\]\nVariance: \\[\n\\tilde{\\beta}_t = \\frac{1}{a}\n= \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t\n\\]\nSo the final expression for the posterior becomes: \\[\nq(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1};\\, \\tilde{\\mu}_t,\\, \\tilde{\\beta}_t I)\n\\]\n\n\n2.2.3 Parameterizing the Reverse Process\nDuring training, we can compute the posterior exactly because \\(x_0\\) is known. But at sampling time, we don’t have access to \\(x_0\\), so we must express everything in terms of the current noisy sample \\(x_t\\) and the model’s prediction of noise \\(\\epsilon\\).\nWe start from the forward noising equation:\n\\[\nx_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\n\\]\nThis expresses how noise is added to the clean image \\(x_0\\) to produce the noisy observation \\(x_t\\).\nWe rearrange this to solve for \\(x_0\\) in terms of \\(x_t\\) and \\(\\epsilon\\):\n\\[\nx_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\n\\]\nNow we substitute this into the posterior mean expression \\(\\tilde{\\mu}_t\\), which originally depended on \\(x_0\\):\n\\[\n\\tilde{\\mu}_t =\n\\frac{\n\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) x_t +\n\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t x_0\n}{\n1 - \\bar{\\alpha}_t\n}\n\\]\nSubstituting \\(x_0\\) into this gives:\n\\[\n\\tilde{\\mu}_t =\n\\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon \\right)\n\\]\nThis allows us to compute the mean of the reverse process using only \\(x_t\\), \\(\\epsilon\\), and known scalars from the noise schedule.\n\n\\(\\epsilon\\) is the noise that was added to \\(x_0\\) to get \\(x_t\\)\nAt test time, we use the model’s prediction \\(\\epsilon_\\theta(x_t, t)\\) in its place\n\n\n\n\n2.3 Recap: Reverse Diffusion Steps\n\n\n\n\n\n\n\n\nStep\nFormula\nExplanation\n\n\n\n\n1\n\\(q(x_{t-1} \\mid x_t, x_0)\\)\nTrue posterior used during training (when \\(x_0\\) is known)\n\n\n2\n\\(\\tilde{\\mu}_t = \\dfrac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\dfrac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon \\right)\\)\nPosterior mean rewritten using \\(x_t\\) and noise\n\n\n3\n\\(\\epsilon \\approx \\epsilon_\\theta(x_t, t)\\)\nAt test time, model predicts the noise\n\n\n4\n\\(p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t I)\\)\nReverse step sampled from model’s predicted mean and fixed variance"
  },
  {
    "objectID": "diffusion.html#forward-diffusion-process",
    "href": "diffusion.html#forward-diffusion-process",
    "title": "Diffusion Models",
    "section": "2 Forward Diffusion Process",
    "text": "2 Forward Diffusion Process\nThe forward diffusion process gradually turns a data sample (such as an image) into pure noise by adding a little bit of random noise at each step. This process is a Markov chain, meaning each step depends only on the previous one.\n\n2.1 Start with a Data Sample\nBegin with a data point \\(x_0\\), sampled from dataset (such as a real image). The goal is to slowly corrupt \\(x_0\\) by adding noise over many steps, until it becomes indistinguishable from random Gaussian noise.\nWe’ll later see that it’s also possible to sample \\(x_t\\) directly from \\(x_0\\), without simulating every step.\n\n\n2.2 Add Noise Recursively\nAt each time step \\(t\\), the process is defined as: \\[\nq(x_t \\mid x_{t-1}) = \\mathcal{N}\\left(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1 - \\alpha_t) I\\right)\n\\]\nWhere:\n\n\\(\\alpha_t = 1 - \\beta_t\\), where \\(\\beta_t\\) a small positive number controlling the noise level at step \\(t\\), while \\(\\alpha_t\\) emphasizes the amount of original signal retained.\n\\(I\\) is the identity matrix, so noise is added independently to each component.\n\n\n\n\n\n\n\nIntuition: At each step, we shrink the signal and add new Gaussian noise. Over many steps, the image becomes blurrier and more like random noise.\n\n\n\n\nWhy keep \\(\\beta_t\\) small?\nKeeping \\(\\beta_t\\) small ensures that noise is added gradually. This allows the model to retain structure across steps and converge slowly to pure noise. Large values of \\(\\beta_t\\) would destroy the signal too quickly, making it harder for the reverse model to reconstruct the data. The design of the forward process balances signal decay (via \\(\\sqrt{\\alpha_t}\\)) and noise growth (via \\(\\sqrt{1 - \\alpha_t}\\)) to ensure a smooth, learnable transition.\n\n\n\n2.3 The Markov Chain\nThe full sequence is:\n\\[\nx_0 \\rightarrow x_1 \\rightarrow x_2 \\rightarrow \\ldots \\rightarrow x_T\n\\]\nThe joint probability of the sequence is:\n\\[\nq(x_{1:T} \\mid x_0) = \\prod_{t=1}^{T} q(x_t \\mid x_{t-1})\n\\]\nThis means we can sample the whole chain by repeatedly applying the noise step.\n\n\n\n\n\n\nInsight: While the forward process defines a full Markov chain from \\(x_0\\) to \\(x_T\\), we’ll soon see that it’s also possible to sample any \\(x_t\\) directly from \\(x_0\\) using a closed-form Gaussian — without simulating each intermediate step.\n\n\n\n\n\n2.4 Deriving the Marginal Distribution \\(q(x_t \\mid x_0)\\)\n How do we get the formula that lets us sample \\(x_t\\) directly from \\(x_0\\) (without simulating all the intermediate steps)?\n\nLet’s see how \\(x_t\\) is built up from \\(x_0\\):\nFor \\(t = 1\\): \\[\nx_1 = \\sqrt{\\alpha_1} x_0 + \\sqrt{1 - \\alpha_1} \\epsilon_1, \\qquad \\epsilon_1 \\sim \\mathcal{N}(0, I)\n\\]\nFor \\(t = 2\\): \\[\nx_2 = \\sqrt{\\alpha_2} x_1 + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\] Substitute \\(x_1\\): \\[\nx_2 = \\sqrt{\\alpha_2} \\left( \\sqrt{\\alpha_1} x_0 + \\sqrt{1 - \\alpha_1} \\epsilon_1 \\right) + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\] \\[\n= \\sqrt{\\alpha_2 \\alpha_1} x_0 + \\sqrt{\\alpha_2 (1 - \\alpha_1)} \\epsilon_1 + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\]\nFor general \\(t\\), recursively expanding gives: \\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sum_{i=1}^t \\left( \\sqrt{ \\left( \\prod_{j=i+1}^t \\alpha_j \\right) (1 - \\alpha_i) } \\, \\epsilon_i \\right)\n\\] where \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\).\nEach \\(\\epsilon_i\\) is independent Gaussian noise. The sum of independent Gaussians (each scaled by a constant) is still a Gaussian, with variance equal to the sum of the variances: \\[\n\\text{Total variance} = \\sum_{i=1}^t \\left( \\prod_{j=i+1}^t \\alpha_j \\right) (1 - \\alpha_i)\n\\] This sum simplifies to: \\[\n1 - \\bar{\\alpha}_t\n\\]\nThis can be proved by induction or by telescoping the sum.\nAll the little bits of noise added at each step combine into one big Gaussian noise term, with variance \\(1 - \\bar{\\alpha}_t\\).\n\n\n2.5 The Final Marginal Distribution\nSo, we can sample \\(x_t\\) directly from \\(x_0\\) using: \\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, I)\n\\]\nThis lets us sample \\(x_t\\) directly from \\(x_0\\), without recursively computing all previous steps \\(x_1, x_2, \\dots, x_{t-1}\\).\nThis means: \\[\nq(x_t \\mid x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I\\right)\n\\]\nAs \\(t\\) increases, \\(\\bar{\\alpha}_t\\) shrinks toward zero. Eventually, \\(x_t\\) becomes pure noise:\n\\[\nx_T \\sim \\mathcal{N}(0, I)\n\\]\n\n\n2.6 Recap: Forward Diffusion Steps\n\n\n\n\n\n\n\n\nStep\nFormula\nExplanation\n\n\n\n\n1\n\\(x_0\\)\nOriginal data sample\n\n\n2\n\\(q(x_t \\mid x_{t-1}) = \\mathcal{N}(\\sqrt{\\alpha_t} x_{t-1}, (1-\\alpha_t) I)\\)\nAdd noise at each step\n\n\n3\n\\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\\)\nDirectly sample \\(x_t\\) from \\(x_0\\) using noise \\(\\epsilon\\)\n\n\n4\n\\(q(x_t \\mid x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I)\\)\nMarginal distribution at step \\(t\\)\n\n\n5\n\\(x_T \\sim \\mathcal{N}(0, I)\\)\nAfter many steps, pure noise\n\n\n\n\n\n2.7 Key Takeaways\n\nThe forward diffusion process is just repeatedly adding noise to your data.\nThanks to properties of Gaussian noise, you can describe the result as the original data scaled down plus one cumulative chunk of Gaussian noise.\n\nAfter enough steps, the data becomes indistinguishable from random noise."
  },
  {
    "objectID": "diffusion.html#reverse-diffusion-process",
    "href": "diffusion.html#reverse-diffusion-process",
    "title": "Diffusion Models",
    "section": "3 Reverse Diffusion Process",
    "text": "3 Reverse Diffusion Process\nLet’s break down the reverse diffusion process step by step. This is the generative phase of diffusion models, where we learn to turn pure noise back into data. For clarity, we’ll use the same notation as in the forward process:\n\nForward process: Gradually adds noise to data via \\(q(x_t \\mid x_{t-1})\\)\nReverse process: Gradually removes noise via \\(p_\\theta(x_{t-1} \\mid x_t)\\), learned by a neural network\n\nThe Goal of the Reverse Process\nObjective: Given a noisy sample \\(x_t\\), we want to estimate the conditional distribution \\(q(x_{t-1} \\mid x_t)\\). However, this is intractable because it would require knowing the true data distribution.\nInstead, we train a neural network to approximate it: \\[\np_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n\\]\nHere, \\(\\mu_\\theta(x_t, t)\\) is the predicted mean and \\(\\Sigma_\\theta(x_t, t)\\) is the predicted covariance (often diagonal) of the reverse Gaussian distribution.\nIn practice, many diffusion models do not directly predict \\(\\mu_\\theta\\) or \\(x_0\\), but instead predict the noise \\(\\epsilon\\) added in the forward process. This makes the objective simpler and more effective, as we’ll see in the next section.\nKey Insight from the Forward Process\nIf the noise added in the forward process is small (i.e., \\(\\beta_t \\ll 1\\)), then the reverse conditional \\(q(x_{t-1} \\mid x_t)\\) is also Gaussian: \\[\nq(x_{t-1} \\mid x_t) \\approx \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t), \\tilde{\\beta}_t I)\n\\]\nThis approximation works because the forward process adds Gaussian noise in small increments at each step. The Markov chain formed by these small Gaussian transitions ensures that local conditionals (like \\(q(x_{t-1} \\mid x_t)\\)) remain Gaussian under mild assumptions.\n\n\n\n\n\n\nGlossary of Symbols\n\n\n\n\n\\(\\alpha_t\\): Variance-preserving noise coefficient at step \\(t\\)\n\\(\\bar{\\alpha}_t\\): Cumulative product of \\(\\alpha_t\\), i.e., \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\)\n\\(\\beta_t\\): Variance of the noise added at step \\(t\\), typically \\(\\beta_t = 1 - \\alpha_t\\)\n\\(x_0\\): Original clean data sample (e.g., image)\n\\(x_t\\): Noisy version of \\(x_0\\) at timestep \\(t\\)\n\\(\\epsilon\\): Standard Gaussian noise sampled from \\(\\mathcal{N}(0, I)\\)\n\\(\\tilde{\\mu}_t\\): Mean of the reverse process distribution at time \\(t\\)\n\\(\\tilde{\\beta}_t\\): Variance of the reverse process distribution at time \\(t\\)\n\n\n\n\n3.1 Deriving \\(q(x_{t-1} \\mid x_t, x_0)\\) Using Bayes’ Rule\nWe can’t directly evaluate \\(q(x_{t-1} \\mid x_t)\\), but we can derive the posterior \\(q(x_{t-1} \\mid x_t, x_0)\\) using Bayes’ rule:\n\\[\nq(x_{t-1} \\mid x_t, x_0) = \\frac{q(x_t \\mid x_{t-1}, x_0) \\cdot q(x_{t-1} \\mid x_0)}{q(x_t \\mid x_0)}\n\\]\nFrom the forward process, we know:\n\n\\(q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1},\\, \\beta_t I)\\)\n\n\\(q(x_{t-1} \\mid x_0) = \\mathcal{N}(x_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} x_0,\\, (1 - \\bar{\\alpha}_{t-1}) I)\\)\n\n\\(q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0,\\, (1 - \\bar{\\alpha}_t) I)\\)\n\nTo derive a usable form of the posterior, we substitute the Gaussian densities into Bayes’ rule. The multivariate normal density is:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\Sigma) \\propto \\exp\\left( -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\n\\]\nSince all covariances here are multiples of the identity matrix, \\(\\Sigma = \\sigma^2 I\\), the formula simplifies to:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 \\right)\n\\]\n\n\n\n\n\n\nUnderstanding the squared norm\n\n\n\nThe expression \\(\\|x - \\mu\\|^2\\) is the squared distance between two vectors. In 1D, it’s just \\((x - \\mu)^2\\), but in higher dimensions, it becomes:\n\\[\n\\|x - \\mu\\|^2 = \\sum_{i=1}^d (x_i - \\mu_i)^2\n\\]\nThis term appears in the exponent of the Gaussian and represents how far the sample is from the center (mean), scaled by the variance.\n\n\nApplying this to the forward process terms:\n\n\\(q(x_t \\mid x_{t-1}) \\propto \\exp\\left( -\\frac{1}{2\\beta_t} \\| x_t - \\sqrt{\\alpha_t} x_{t-1} \\|^2 \\right)\\)\n\n\\(q(x_{t-1} \\mid x_0) \\propto \\exp\\left( -\\frac{1}{2(1 - \\bar{\\alpha}_{t-1})} \\| x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} x_0 \\|^2 \\right)\\)\n\nWe can ignore \\(q(x_t \\mid x_0)\\) in the denominator, since it is independent of \\(x_{t-1}\\) and will be absorbed into a proportionality constant.\nPutting these together:\n\\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n-\\frac{1}{2} \\left[\n\\frac{ \\|x_t - \\sqrt{\\alpha_t} x_{t-1} \\|^2 }{\\beta_t} +\n\\frac{ \\| x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} x_0 \\|^2 }{1 - \\bar{\\alpha}_{t-1}}\n\\right]\n\\right)\n\\]\n\n\n\n\n\n\nWhy does the product of Gaussians give another Gaussian?\n\n\n\nWhen we multiply two Gaussian distributions over the same variable, the result is also a Gaussian.\nHere, we are multiplying two Gaussians in \\(x_{t-1}\\):\n- One centered at \\(\\sqrt{\\alpha_t} x_t\\)\n- One centered at \\(\\sqrt{\\bar{\\alpha}_{t-1}} x_0\\)\nThe product is another Gaussian in \\(x_{t-1}\\), with a new mean that is a weighted average of both.\nWe’ll derive this explicitly by completing the square in the exponent.\n\n\n\nAlthough we won’t use this posterior directly during sampling, this closed-form expression is essential for defining the ELBO used in training. It gives us a precise target that the reverse model attempts to approximate.\n\nWe now complete the square to put the expression into standard Gaussian form.\n\n\n\n3.2 Complete the square\nTo express the exponent in Gaussian form, we’ll complete the square using the identity:\n\\[\na x^2 - 2 b x = a \\left( x - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a}\n\\]\nFrom earlier, we arrived at this expression for the exponent of the posterior:\n\\[\n-\\frac{1}{2} \\left[\n\\frac{(x_t - \\sqrt{\\alpha_t} \\, x_{t-1})^2}{\\beta_t} +\n\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0)^2}{1 - \\bar{\\alpha}_{t-1}}\n\\right]\n\\]\nWe expand both terms:\nFirst term:\n\\[\n\\frac{(x_t - \\sqrt{\\alpha_t} \\, x_{t-1})^2}{\\beta_t}\n= \\frac{x_t^2 - 2 \\sqrt{\\alpha_t} \\, x_t x_{t-1} + \\alpha_t x_{t-1}^2}{\\beta_t}\n\\]\nSecond term:\n\\[\n\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0)^2}{1 - \\bar{\\alpha}_{t-1}}\n= \\frac{x_{t-1}^2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_{t-1} x_0 + \\bar{\\alpha}_{t-1} x_0^2}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nGroup like terms\nNow we collect all the terms involving \\(x_{t-1}\\):\nCoefficient of \\(x_{t-1}^2\\):\n\\[\na = \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nCoefficient of \\(x_{t-1}\\) (the full linear term):\n\\[\n-2 \\left(\n\\frac{ \\sqrt{\\alpha_t} \\, x_t }{ \\beta_t } + \\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0 }{ 1 - \\bar{\\alpha}_{t-1} }\n\\right)\n\\]\nSo we define:\n\\[\nb = \\frac{ \\sqrt{\\alpha_t} \\, x_t }{ \\beta_t } + \\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0 }{ 1 - \\bar{\\alpha}_{t-1} }\n\\]\nRemaining terms (like \\(x_t^2\\) and \\(x_0^2\\)) are independent of \\(x_{t-1}\\) and can be absorbed into a constant.\nWe are modeling the conditional distribution \\(q(x_{t-1} \\mid x_t, x_0)\\), which means both \\(x_t\\) and \\(x_0\\) are known and fixed. So any expression involving only \\(x_t\\) or \\(x_0\\) behaves like a constant and does not influence the shape of the Gaussian over \\(x_{t-1}\\).\nThe exponent now has the form:\n\\[\n-\\frac{1}{2} \\left( a x_{t-1}^2 - 2 b x_{t-1} \\right) + \\text{(constants)}\n\\]\nApply the identity\nUsing the identity: \\[\na x^2 - 2 b x = a \\left( x - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a}\n\\]\nwe rewrite the exponent: \\[\n-\\frac{1}{2} \\left( a x_{t-1}^2 - 2 b x_{t-1} \\right)\n= -\\frac{1}{2} \\left[ a \\left( x_{t-1} - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a} \\right]\n\\]\nWe drop the constant term \\(\\frac{b^2}{a}\\) under proportionality. This transforms the exponent into the Gaussian form: \\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n- \\frac{1}{2 \\tilde{\\beta}_t} \\| x_{t-1} - \\tilde{\\mu}_t \\|^2\n\\right)\n\\]\n\n\n\n\n\n\nNote: This matches the standard Gaussian\n\n\n\nThe standard Gaussian is written as: \\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) \\propto \\exp\\left(\n- \\frac{1}{2\\sigma^2} \\| x - \\mu \\|^2\n\\right)\n\\]\nSo in our case:\n\n\\(\\tilde{\\mu}_t = \\frac{b}{a}\\) is the mean\n\\(\\tilde{\\beta}_t = \\frac{1}{a}\\) is the variance\n\nWe keep the notation \\(\\tilde{\\beta}_t\\) instead of \\(\\sigma^2\\) because it connects directly to the noise schedule (\\(\\beta_t\\), \\(\\bar{\\alpha}_t\\)) used in the diffusion model. This helps tie everything back to how the forward and reverse processes relate.\n\n\nFinal expressions\nNow we can directly read off the expressions for the mean and variance from the completed square.\nWe had: \\[\na = \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}, \\quad\nb = \\frac{\\sqrt{\\alpha_t} \\, x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nFrom the identity: \\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n- \\frac{1}{2 \\tilde{\\beta}_t} \\| x_{t-1} - \\tilde{\\mu}_t \\|^2\n\\right)\n\\]\nwe identify: - \\(\\tilde{\\mu}_t = \\frac{b}{a}\\), - \\(\\tilde{\\beta}_t = \\frac{1}{a}\\)\nLet’s compute these explicitly:\nMean: \\[\n\\tilde{\\mu}_t = \\frac{b}{a} =\n\\frac{\n\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) x_t +\n\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t x_0\n}{\n1 - \\bar{\\alpha}_t\n}\n\\]\nVariance: \\[\n\\tilde{\\beta}_t = \\frac{1}{a}\n= \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t\n\\]\nSo the final expression for the posterior becomes: \\[\nq(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1};\\, \\tilde{\\mu}_t,\\, \\tilde{\\beta}_t I)\n\\]\n\n\n3.3 Parameterizing the Reverse Process\nDuring training, we can compute the posterior exactly because \\(x_0\\) is known. But at sampling time, we don’t have access to \\(x_0\\), so we must express everything in terms of the current noisy sample \\(x_t\\) and the model’s prediction of noise \\(\\epsilon\\).\nWe start from the forward noising equation:\n\\[\nx_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\n\\]\nThis expresses how noise is added to the clean image \\(x_0\\) to produce the noisy observation \\(x_t\\).\nWe rearrange this to solve for \\(x_0\\) in terms of \\(x_t\\) and \\(\\epsilon\\):\n\\[\nx_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\n\\]\nNow we substitute this into the posterior mean expression \\(\\tilde{\\mu}_t\\), which originally depended on \\(x_0\\):\n\\[\n\\tilde{\\mu}_t =\n\\frac{\n\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) x_t +\n\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t x_0\n}{\n1 - \\bar{\\alpha}_t\n}\n\\]\nSubstituting \\(x_0\\) into this gives:\n\\[\n\\tilde{\\mu}_t =\n\\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon \\right)\n\\]\nThis allows us to compute the mean of the reverse process using only \\(x_t\\), \\(\\epsilon\\), and known scalars from the noise schedule.\n\n\\(\\epsilon\\) is the noise that was added to \\(x_0\\) to get \\(x_t\\)\nAt test time, we use the model’s prediction \\(\\epsilon_\\theta(x_t, t)\\) in its place\n\n\n\n3.4 Recap: Reverse Diffusion Steps\n\n\n\n\n\n\n\n\nStep\nFormula\nExplanation\n\n\n\n\n1\n\\(q(x_{t-1} \\mid x_t, x_0)\\)\nTrue posterior used during training (when \\(x_0\\) is known)\n\n\n2\n\\(\\tilde{\\mu}_t = \\dfrac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\dfrac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon \\right)\\)\nPosterior mean rewritten using \\(x_t\\) and noise\n\n\n3\n\\(\\epsilon \\approx \\epsilon_\\theta(x_t, t)\\)\nAt test time, model predicts the noise\n\n\n4\n\\(p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t I)\\)\nReverse step sampled from model’s predicted mean and fixed variance"
  },
  {
    "objectID": "diffusion.html#training-understanding-the-elbo",
    "href": "diffusion.html#training-understanding-the-elbo",
    "title": "Diffusion Models",
    "section": "4 Training: Understanding the ELBO",
    "text": "4 Training: Understanding the ELBO\nWhat is the Goal? The ultimate goal in diffusion models is to train the neural network so that it can reverse the noising process. In other words, we want the network to learn how to turn random noise back into realistic data (like images). But how do we actually train the network? We need a loss function—a way to measure how good or bad the network’s predictions are, so we can improve it.\n\n4.1 What is the ELBO?\nThe ELBO is a lower bound on the log-likelihood of the data. Maximizing the ELBO is equivalent to maximizing the likelihood that the model can generate the training data. For diffusion models, the ELBO ensures that the reverse process (denoising) aligns with the forward process (noising).\n\n\n4.2 Deriving the ELBO for Diffusion Models\nGoal:\nWe want to maximize the log-likelihood of the data:\n\\[\n\\log p_\\theta(x_0)\n\\]\nwhere \\(x_0\\) is a clean data sample (e.g., an image).\nProblem:\nComputing \\(\\log p_\\theta(x_0)\\) directly is intractable because it involves integrating over all possible noisy intermediate states \\(x_{1:T}\\).\nSolution:\nUse Jensen’s Inequality to derive a lower bound (the ELBO) that we can optimize instead.\n\n\n4.3 Full Derivation (Step-by-Step)\nStep 1: Start with the log-likelihood\n\\[\n\\log p_\\theta(x_0) = \\log \\int p_\\theta(x_{0:T}) \\, dx_{1:T}\n\\]\nStep 2: Introduce the forward process \\(q(x_{1:T} \\mid x_0)\\)\nMultiply and divide by the fixed forward process:\n\\[\n\\log p_\\theta(x_0) = \\log \\int \\frac{p_\\theta(x_{0:T})}{q(x_{1:T} \\mid x_0)} q(x_{1:T} \\mid x_0) \\, dx_{1:T}\n\\]\nStep 3: Rewrite as an expectation\n\\[\n\\log p_\\theta(x_0) = \\log \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[ \\frac{p_\\theta(x_{0:T})}{q(x_{1:T} \\mid x_0)} \\right]\n\\]\nStep 4: Apply Jensen’s Inequality\n\\[\n\\log p_\\theta(x_0) \\geq \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[ \\log \\frac{p_\\theta(x_{0:T})}{q(x_{1:T} \\mid x_0)} \\right] = \\text{ELBO}\n\\]\nStep 5: Expand \\(p_\\theta(x_{0:T})\\) and \\(q(x_{1:T} \\mid x_0)\\)\nForward process: \\[\nq(x_{1:T} \\mid x_0) = \\prod_{t=1}^T q(x_t \\mid x_{t-1})\n\\]\nReverse process: \\[\np_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} \\mid x_t)\n\\]\nSubstitute into the ELBO:\n\\[\n\\text{ELBO} = \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[ \\log p(x_T) + \\sum_{t=1}^T \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_t \\mid x_{t-1})} \\right]\n\\]\nStep 6: Decompose the ELBO\n\\[\n\\text{ELBO} = \\mathbb{E}_{q} \\left[ \\log p(x_T) \\right] + \\sum_{t=2}^T \\mathbb{E}_q \\left[ \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_t \\mid x_{t-1})} \\right] + \\mathbb{E}_q \\left[ \\log p_\\theta(x_0 \\mid x_1) \\right]\n\\]\n\n\n4.4 Interpreting Each Term\nReconstruction Term:\n- \\(\\mathbb{E}_q[\\log p_\\theta(x_0 \\mid x_1)]\\) measures how well the model can reconstruct \\(x_0\\) from the first noisy sample \\(x_1\\).\nPrior Matching Term:\n- \\(\\mathbb{E}_q[\\log p(x_T)]\\) encourages the final state \\(x_T\\) to match the prior \\(\\mathcal{N}(0, I)\\).\nConsistency Terms:\n- \\(\\sum \\mathbb{E}_q[\\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_t \\mid x_{t-1})}]\\) ensures each denoising step approximates the forward noising step.\n\n\n4.5 Practical Training Simplification (DDPM)\nIn DDPM:\n\n\\(p(x_T) = \\mathcal{N}(0, I)\\) is fixed and known.\n\nThe reconstruction term is small and often ignored.\n\nThe KL terms are approximated via noise prediction:\n\n\\[\n\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{x_0, t, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]\n\\]\nWhere: - \\(\\epsilon\\) is the actual noise\n- \\(\\epsilon_\\theta\\) is the predicted noise by the neural network\nWhy? If the network can predict the noise \\(\\epsilon\\), it can denoise \\(x_t\\) and reverse the diffusion.\n\n\n4.6 The Noise Prediction Network\nSo far, we’ve seen that the training objective can be simplified to:\n\\[\n\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{x_0, t, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]\n\\]\nBut what exactly is this \\(\\epsilon_\\theta(x_t, t)\\)?\n\n4.6.1 What the Network Learns\n\nDuring training, we know the true noise \\(\\epsilon\\) used to generate the noisy sample \\(x_t\\) from \\(x_0\\).\nThe network \\(\\epsilon_\\theta(x_t, t)\\) is trained to predict this noise.\nOnce trained, it can “undo” the noise and help reconstruct \\(x_0\\) at test time.\n\n\n\n4.6.2 Why Predicting Noise Works\nRecall the forward process:\n\\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\n\\]\nRearranging this gives us an estimate of the original (clean) image:\n\\[\nx_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon \\right)\n\\]\nSo if the model can accurately predict the noise \\(\\epsilon\\), we can plug it in to estimate \\(x_0\\).\nThis denoised estimate can then be used to compute the mean of the posterior distribution \\(q(x_{t-1} \\mid x_t, x_0)\\) — which is the key to reversing the diffusion process step by step.\n\n\n4.6.3 Architecture Note\nThe network \\(\\epsilon_\\theta(x_t, t)\\) is typically implemented as a U-Net, which takes: - A noisy image \\(x_t\\) - A timestep \\(t\\) (encoded using sinusoidal or learned embeddings)\nIt outputs the predicted noise \\(\\epsilon\\).\n\nThis lets the model gradually learn how to denoise.\n\n\n\n4.7 Connection to VAEs\n\n\n\n\n\n\n\n\nAspect\nVAEs\nDiffusion Models\n\n\n\n\nForward process\nLearned encoder \\(q_\\phi(z \\mid x)\\)\nFixed noising process \\(q(x_t \\mid x_{t-1})\\)\n\n\nReverse process\nLearned decoder \\(p_\\theta(x \\mid z)\\)\nLearned denoising network \\(p_\\theta(x_{t-1} \\mid x_t)\\)\n\n\nTraining objective\nOptimize ELBO over latent variables\nOptimize ELBO via noise prediction loss\n\n\n\n\n\n\n4.8 Takeaways\n\nThe ELBO is a tractable lower bound on \\(\\log p_\\theta(x_0)\\).\n\nIt aligns the reverse (learned) process with the forward (fixed) noising process.\n\nIn practice, training reduces to minimizing the difference between predicted and true noise."
  },
  {
    "objectID": "diffusion.html#sampling-from-the-reverse-process",
    "href": "diffusion.html#sampling-from-the-reverse-process",
    "title": "Diffusion Models",
    "section": "5 Sampling from the Reverse Process",
    "text": "5 Sampling from the Reverse Process\nNow that we’ve learned how to train the model using the ELBO, let’s understand how it generates new data at test time.\n\n5.1 Recap: What Are We Trying to Do?\nIn the forward diffusion process, we gradually add noise to a data sample (like an image) over \\(T\\) steps, eventually turning it into nearly pure Gaussian noise.\nIn the reverse process, we start from that noise and apply a learned denoising step at each time step to recover a realistic data sample. This is the generation phase.\n\n\n5.2 The Reverse Process as a Markov Chain\nAt sampling time, we generate a new sample using the reverse process defined by:\n\\[\np_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1} \\mid x_t)\n\\]\n\n\\(p(x_T)\\) is a standard Gaussian \\(\\mathcal{N}(0, I)\\).\nEach \\(p_\\theta(x_{t-1} \\mid x_t)\\) is modeled as a Gaussian whose mean \\(\\mu_\\theta(x_t, t)\\) is predicted by the neural network.\nThe variance may be fixed or learned (e.g., \\(\\beta_t\\), \\(\\Sigma_\\theta\\)).\n\nEach reverse step is:\n\\[\nx_{t-1} = \\mu_\\theta(x_t, t) + \\sigma_t z, \\quad z \\sim \\mathcal{N}(0, I)\n\\]\nThis recursive process transforms noise into a structured sample like an image.\n\n\n5.3 What Does the Neural Network Predict?\nDuring training, we derived the true posterior mean \\(\\tilde{\\mu}_t\\) as:\n\\[\n\\tilde{\\mu}_t = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon \\right)\n\\]\nAt test time, since we don’t have access to the true noise \\(\\epsilon\\), the network predicts it as \\(\\epsilon_\\theta(x_t, t)\\) and substitutes it into the expression:\n\\[\n\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon_\\theta(x_t, t) \\right)\n\\]\nThis lets the model estimate the mean of the reverse Gaussian using only \\(x_t\\), \\(t\\), and the predicted noise.\n\n\n5.4 Sampling Procedure\nTo generate a sample, we do the following:\n\nSample initial noise:\n\\(x_T \\sim \\mathcal{N}(0, I)\\)\nFor \\(t = T, T-1, \\dots, 1\\):\n\nPredict \\(\\mu_\\theta(x_t, t)\\)\n\nOptionally use fixed or learned variance \\(\\sigma_t^2\\)\n\nSample from the reverse step:\n\\(x_{t-1} = \\mu_\\theta(x_t, t) + \\sigma_t z\\), where \\(z \\sim \\mathcal{N}(0, I)\\)\n\nReturn \\(x_0\\) as the final generated sample.\n\nThis is how diffusion models synthesize data — by gradually denoising random noise using the learned reverse process."
  },
  {
    "objectID": "diffusion.html#the-noise-prediction-network",
    "href": "diffusion.html#the-noise-prediction-network",
    "title": "Diffusion Models",
    "section": "5 The Noise Prediction Network",
    "text": "5 The Noise Prediction Network\nSo far, we’ve seen that the training objective can be simplified to:\n\\[\n\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{x_0, t, \\epsilon} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]\n\\]\nBut what exactly is this \\(\\epsilon_\\theta(x_t, t)\\)?\n\n5.1 What the Network Learns\n\nDuring training, we know the true noise \\(\\epsilon\\) used to generate the noisy sample \\(x_t\\) from \\(x_0\\).\nThe network \\(\\epsilon_\\theta(x_t, t)\\) is trained to predict this noise.\nOnce trained, it can “undo” the noise and help reconstruct \\(x_0\\) at test time.\n\n\n\n5.2 Why Predicting Noise Works\nRecall the forward process:\n\\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\n\\]\nRearranging this gives us an estimate of the original (clean) image:\n\\[\nx_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon \\right)\n\\]\nSo if the model can accurately predict the noise \\(\\epsilon\\), we can plug it in to estimate \\(x_0\\).\nThis denoised estimate can then be used to compute the mean of the posterior distribution \\(q(x_{t-1} \\mid x_t, x_0)\\) — which is the key to reversing the diffusion process step by step.\n\n\n5.3 Architecture Note\nThe network \\(\\epsilon_\\theta(x_t, t)\\) is typically implemented as a U-Net, which takes: - A noisy image \\(x_t\\) - A timestep \\(t\\) (encoded using sinusoidal or learned embeddings)\nIt outputs the predicted noise \\(\\epsilon\\).\n\nThis lets the model gradually learn how to denoise.\n\n\n5.4 Connection to VAEs\n\n\n\n\n\n\n\n\nAspect\nVAEs\nDiffusion Models\n\n\n\n\nForward process\nLearned encoder \\(q_\\phi(z \\mid x)\\)\nFixed noising process \\(q(x_t \\mid x_{t-1})\\)\n\n\nReverse process\nLearned decoder \\(p_\\theta(x \\mid z)\\)\nLearned denoising network \\(p_\\theta(x_{t-1} \\mid x_t)\\)\n\n\nTraining objective\nOptimize ELBO over latent variables\nOptimize ELBO via noise prediction loss\n\n\n\n\n\n\n5.5 Takeaways\n\nThe ELBO is a tractable lower bound on \\(\\log p_\\theta(x_0)\\).\n\nIt aligns the reverse (learned) process with the forward (fixed) noising process.\n\nIn practice, training reduces to minimizing the difference between predicted and true noise."
  }
]