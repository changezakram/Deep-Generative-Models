[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.\n\n\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\n\n\n\n\n\\(\\mathbf{z}\\) is a hidden variable, unobserved during training\nDefines a mixture of infinitely many Gaussians\nThe marginal likelihood requires integration: \\[\np(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n\\]\n\n\n\n\nThis integral requires integrating over: - All possible values of \\(\\mathbf{z}\\) (often high-dimensional) - Non-linear transformations through neural networks\nResult: Exact computation is intractable, motivating variational inference techniques like ELBO (developed next)."
  },
  {
    "objectID": "index.html#autoencoders-vs-variational-autoencoders",
    "href": "index.html#autoencoders-vs-variational-autoencoders",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.\n\n\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\n\n\n\n\n\\(\\mathbf{z}\\) is a hidden variable, unobserved during training\nDefines a mixture of infinitely many Gaussians\nThe marginal likelihood requires integration: \\[\np(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n\\]\n\n\n\n\nThis integral requires integrating over: - All possible values of \\(\\mathbf{z}\\) (often high-dimensional) - Non-linear transformations through neural networks\nResult: Exact computation is intractable, motivating variational inference techniques like ELBO (developed next)."
  },
  {
    "objectID": "index.html#estimation-techniques",
    "href": "index.html#estimation-techniques",
    "title": "Deep Generative Models",
    "section": "Estimation Techniques",
    "text": "Estimation Techniques\n\nNaive Monte Carlo Estimation\nOne natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nHowever, this fails in practice. For most values of \\(z\\), the joint probability \\(p_\\theta(x, z)\\) is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has:\n\nHigh variance (sparse meaningful samples)\nLow hit rate (rarely samples likely \\(z\\) values)\n\n\n\nImportance Sampling\nTo address this, we use importance sampling, introducing a proposal distribution \\(q(z)\\):\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nKey Properties: - Unbiased estimator when \\(\\text{supp}(q) \\supseteq \\text{supp}(p_\\theta)\\) - Optimal \\(q(z) \\approx p_\\theta(z|x)\\) (the true posterior) - More efficient sampling in high-probability regions\n\nThe Log-Likelihood Challenge\nFor optimization, we need the log-likelihood, but:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\neq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nJensen’s Inequality reveals the relationship:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\geq \\underbrace{\\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]}_{\\text{ELBO}}\n\\]\nThis gives us the Evidence Lower Bound (ELBO), which is lower bounds the true log-likelihood."
  }
]