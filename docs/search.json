[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\n\nThe latent space is unstructured, offering little control or interpretation\n\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.\n\n\n\nVAEs assume data is generated by a two-step process:\n\nSample latent variable:\n\\[\n\\mathbf{z} \\sim \\mathcal{N}(0, I)\n\\]\nGenerate observation from:\n\\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\]\nwhere \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\nHere, \\(\\mathbf{z}\\) is unobserved during training. The model defines a mixture of infinitely many Gaussians — one for each \\(\\mathbf{z}\\).\nTo compute the likelihood of a data point \\(\\mathbf{x}\\), we marginalize over \\(\\mathbf{z}\\):\n\\[\np(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n\\]\nThis integral is high-dimensional and non-linear (through neural nets), making it intractable. So we turn to variational inference."
  },
  {
    "objectID": "vae.html#autoencoders-vs-variational-autoencoders",
    "href": "vae.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\n\nThe latent space is unstructured, offering little control or interpretation\n\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.\n\n\n\nVAEs assume data is generated by a two-step process:\n\nSample latent variable:\n\\[\n\\mathbf{z} \\sim \\mathcal{N}(0, I)\n\\]\nGenerate observation from:\n\\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\]\nwhere \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\nHere, \\(\\mathbf{z}\\) is unobserved during training. The model defines a mixture of infinitely many Gaussians — one for each \\(\\mathbf{z}\\).\nTo compute the likelihood of a data point \\(\\mathbf{x}\\), we marginalize over \\(\\mathbf{z}\\):\n\\[\np(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n\\]\nThis integral is high-dimensional and non-linear (through neural nets), making it intractable. So we turn to variational inference."
  },
  {
    "objectID": "vae.html#estimating-the-marginal-likelihood",
    "href": "vae.html#estimating-the-marginal-likelihood",
    "title": "Variational Autoencoders",
    "section": "Estimating the Marginal Likelihood",
    "text": "Estimating the Marginal Likelihood\n\nNaive Monte Carlo Estimation\nApproximate using random samples from a uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nBut this fails: most \\(z_j\\) have low \\(p(x, z_j)\\), leading to high variance and poor estimates.\n\n\n\nImportance Sampling\nUse a proposal distribution \\(q(z)\\) to sample more effectively:\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p(x, z)}{q(z)} \\right]\n\\]\nThis gives an unbiased estimator of \\(p(x)\\), but:\n\\[\n\\log p(x) = \\log \\mathbb{E}_{q(z)} \\left[ \\frac{p(x, z)}{q(z)} \\right]\n\\]\nis still hard to optimize due to the log-outside-the-expectation.\n\n\n\nFrom Log-Likelihood to ELBO\nWe apply Jensen’s Inequality:\n\\[\n\\log p(x) \\geq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p(x, z)}{q(z)} \\right]\n\\]\nThis lower bound is called the Evidence Lower Bound (ELBO):\n\\[\n\\text{ELBO} = \\mathbb{E}_{q_\\phi(z|x)} \\left[ \\log p_\\theta(x|z) \\right] - \\text{KL}(q_\\phi(z|x) \\| p(z))\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#key-model-families",
    "href": "index.html#key-model-families",
    "title": "Deep Generative Models",
    "section": "Key Model Families",
    "text": "Key Model Families\n\n1. Variational Autoencoders (VAEs)\n\nProbabilistic encoder-decoder architecture\nLearn latent spaces with Gaussian distributions\nLearn more about VAEs\n\n\n\n2. Autoregressive Models\n\nGenerate sequences element-by-element\nExamples: PixelCNN, WaveNet\n\n\n\n3. Generative Adversarial Networks (GANs)\n\nAdversarial training with generator/discriminator\nHigh-quality image generation\n\n\n\n4. Normalizing Flows\n\nInvertible transformations for exact likelihood\nFlexible density estimation\n\n\n\n5. Energy-Based Models (EBMs)\n\nLearn energy functions for data distribution\nFlexible for discrete/continuous data\n\n\n\n6. Diffusion Models\n\nIterative denoising process\nState-of-the-art image generation"
  }
]