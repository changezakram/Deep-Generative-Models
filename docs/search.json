[
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html",
    "href": "gen-ai-use-cases/banking-use-cases.html",
    "title": "Generative AI Use Cases in Banking",
    "section": "",
    "text": "Generative AI is redefining how banks operate ‚Äî from automating customer service to streamlining underwriting. Below are practical examples showing how leading financial institutions are applying GenAI for real business value."
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#summary-of-use-cases",
    "href": "gen-ai-use-cases/banking-use-cases.html#summary-of-use-cases",
    "title": "Generative AI Use Cases in Banking",
    "section": "1 üìã Summary of Use Cases",
    "text": "1 üìã Summary of Use Cases\n\n\n\n\n\n\n\n\n\nUse Case Area\nExamples\nBenefits\nUnderlying Models\n\n\n\n\nFraud Detection\nMastercard Decision Intelligence Pro\nReal-time anomaly detection, improved fraud response\nEBM, Graph Models, VAE\n\n\nCredit Approval and Underwriting\nJPM COiN, Zest AI LuLu Stragey\nFaster decisions, consistent underwriting\nLLM, Tabular GANs\n\n\nCustomer Engagement\nErica (BoA), Cora+ (NatWest), Fargo, Eliza (BNYM)\nAutomated support, 24/7 service\nLLM\n\n\nPersonalized Insights\nBBVA ChatGPT, Morgan Stanley Debrief\nTailored financial advice, better CX\nLLM (GPT-4), RAG\n\n\nGenerative BI\nMicrosoft Copilot, ThoughtSpot Sage\nInstant dashboards, conversational BI\nLLM, NLQ, NLG\n\n\nDocument Generation\nSARs, compliance memos, offer letters\nAuto-drafted docs, reduced manual effort\nLLM, IDP, Diffusion\n\n\nDocument Summarization\nJPM AI Suite, Morgan Stanley AskResearchGPT\nFaster reading, internal reporting, decision support\nLLM\n\n\nSynthetic Data Generation\nInternal testing, privacy-safe modeling\nAugmented training data, preserves confidentiality\nVAE, Diffusion, GAN"
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#detailed-use-cases",
    "href": "gen-ai-use-cases/banking-use-cases.html#detailed-use-cases",
    "title": "Generative AI Use Cases in Banking",
    "section": "2 üîç Detailed Use Cases",
    "text": "2 üîç Detailed Use Cases\n\n2.1 Fraud Detection\n\nProblem: Detecting rare, evolving fraud patterns is difficult with traditional rule-based systems.\nSolution: Energy-Based Models (EBMs) and VAE-based anomaly detection are used to identify outlier transactions in real time.[1]\nImpact: +20% detection accuracy and ‚Äì85% false positives (Mastercard).[2]\n\n\n\n2.2 Credit Approval and Loan Underwriting\n\nProblem: Credit underwriting requires reviewing financial statements, legal documents, and industry data under time pressure ‚Äî a process that is slow, manual, and error-prone.\n\nSolution: Generative AI accelerates underwriting by analyzing credit memos, extracting key risk metrics, and summarizing legal clauses, enabling faster and more consistent loan decisions.\n\nImpact: JPMorgan COiN saved over 360,000 hours by automating legal reviews; Zest AI reduced underwriting time from days to minutes and increased approvals by up to 25% with no added risk.[3][4]\n\n\n\n2.3 Customer Engagement\n\nProblem: Call centers and support channels face high volumes of routine queries, long wait times, and rising operational costs.\n\nSolution: Generative AI powers conversational agents that provide real-time, personalized support across multiple languages and channels, improving CX while reducing human workload.\n\nImpact: Erica has handled 2B+ interactions with an average 44s resolution time; Cora+ reduced escalations by 50% and improved customer satisfaction by 150%. [5][6]\n\n\n\n2.4 Personalized Insights\n\nProblem: Financial advisors often struggle to provide timely, personalized advice due to information overload and manual follow-up tasks.\nSolution: Generative AI automates note-taking, analyzes client profiles and market trends, and generates tailored investment recommendations.\nImpact: Morgan Stanley‚Äôs AI @ Morgan Stanley Debrief saves ~30‚ÄØminutes per meeting, freeing up 10‚Äì15‚ÄØhours/week per advisor for higher-value interactions. [7][8]\n\n\n\n2.5 Generative BI\n\nProblem: Business leaders often struggle to extract timely insights from complex, multi-source data environments, relying on static dashboards or analysts.\n\nSolution: Generative AI combines LLMs with backend analytical pipelines to deliver real-time, conversational insights tailored to executive needs.\n\nImpact: Forrester reports a 379% ROI and 20% analyst efficiency gain from Power BI with Copilot; users save up to 2.4 hours/week through automated tasks. [9][10]\n\n\n\n2.6 Document Generation\n\nProblem: Writing compliance memos, suspicious activity reports (SARs), and internal summaries requires significant manual effort and is prone to inconsistencies.\n\nSolution: Generative AI models‚Äîparticularly LLMs and intelligent document processing (IDP) systems‚Äîautomate document classification, data extraction, summarization, and drafting, while supporting compliance standards.\n\nImpact: AI-powered IDP platforms have reduced review time by up to 90%, improved processing accuracy, and enabled faster turnaround for compliance documents. [11][12][13]\n\n\n\n2.7 Document Summarization\n\nProblem: Manual summarization of research and compliance documents is time-consuming and error-prone.\n\nSolution: GenAI tools powered by LLMs extract, synthesize, and summarize large volumes of internal content at scale.\n\nImpact: Morgan Stanley‚Äôs AskResearchGPT summarizes insights from 70,000+ proprietary reports; EY‚Äôs DI Platform saves 90% review time, cuts costs by 80%, and improves accuracy by 25%.[14][15][16]\n\n\n\n2.8 Synthetic Data Generation\n\nProblem: Real data can‚Äôt be shared freely due to privacy, regulatory, and operational constraints.\n\nSolution: Use VAE, Diffusion, or GAN-based models to generate synthetic datasets that preserve statistical properties while protecting sensitive information.\n\nImpact: JPMorgan uses synthetic data to develop and test AI models in secure environments; industry surveys highlight growing adoption of GANs and Diffusion for financial data simulation.[17][18]"
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#references",
    "href": "gen-ai-use-cases/banking-use-cases.html#references",
    "title": "Generative AI Use Cases in Banking",
    "section": "3 References",
    "text": "3 References\n[1] NVIDIA - How Is AI Used in Fraud Detection? https://blogs.nvidia.com/blog/ai-fraud-detection-rapids-triton-tensorrt-nemo/\n[2] Mastercard supercharges consumer protection with Gen AI. https://www.mastercard.com/news/press/2024/february/mastercard-supercharges-consumer-protection-with-gen-ai/\n[3] Zest AI Launches LuLu Strategy Module to Expand Generative AI to Financial Institutions https://www.zest.ai/company/announcements/zest-ai-launches-lulu-strategy-module-to-expand-generative-ai-to-financial-institutions/\n[4] Bloomberg. (2017). JPMorgan Software Does in Seconds What Took Lawyers 360,000 Hours. https://www.bloomberg.com/news/articles/2017-02-28/jpmorgan-marshals-an-army-of-developers-to-automate-high-finance?embedded-checkout=true\n[5] Erica at Bank of America https://info.bankofamerica.com/en/digital-banking/erica\n[6] NatWest Cora+ https://www.natwestgroup.com/news-and-insights/news-room/press-releases/data-and-technology/2024/jun/natwest-launches-cora-plus-the-latest-generative-ai-upgrade-to-t.html\n[7] Morgan Stanley. (2024). AI @ Morgan Stanley Debrief Launch. https://www.morganstanley.com/press-releases/ai-at-morgan-stanley-debrief-launch\n[8] The Wall Street Journal. (2024). Inside OpenAI‚Äôs Deal With BBVA. https://www.wsj.com/articles/six-months-thousands-of-gpts-and-some-big-unknowns-inside-openais-deal-with-bbva-5d6f1c03\n[9] Microsoft. (2024). Forrester Total Economic Impact‚Ñ¢ study: Microsoft Fabric delivers 379% ROI over three years. https://www.microsoft.com/en-us/microsoft-fabric/blog/2024/06/03/forrester-total-economic-impact-study-microsoft-fabric-delivers-379-roi-over-three-years/\n[10] Forrester. (2024). New Technology: The Projected Total Economic Impact‚Ñ¢ Of Microsoft Copilot For Microsoft 365. https://tei.forrester.com/go/Microsoft/365Copilot/?lang=en-us\n[11] U.S. Department of the Treasury. (2024). Artificial Intelligence in Financial Services. https://home.treasury.gov/system/files/136/Artificial-Intelligence-in-Financial-Services.pdf\n[12] EY. (2024). Document Intelligence Platform. https://www.ey.com/en_gl/alliances/microsoft/document-intelligence-platform\n[13] McKinsey & Company. (2024). How Generative AI Can Help Banks Manage Risk and Compliance. https://www.mckinsey.com/capabilities/risk-and-resilience/our-insights/how-generative-ai-can-help-banks-manage-risk-and-compliance\n[14] Morgan Stanley. (2024). AskResearchGPT helps advisors access 70,000+ research reports. https://www.morganstanley.com/press-releases/morgan-stanley-research-announces-askresearchgpt\n[15] JPMorgan Chase. (2024). LLM Suite for operations and reporting automation. https://www.cnbc.com/2024/08/09/jpmorgan-chase-ai-artificial-intelligence-assistant-chatgpt-openai.html\n[16] EY. (2024). Document Intelligence Platform: Transforming document processing in financial services. https://www.ey.com/en_gl/alliances/microsoft/document-intelligence-platform\n[17] J.P. Morgan. (2024). Synthetic data enables privacy-preserving model development. https://www.jpmorgan.com/technology/artificial-intelligence/initiatives/synthetic-data\n[18] X. Wang et al.¬†(2024). A Survey on Financial Synthetic Data Generation with GANs and Diffusion. https://arxiv.org/pdf/2410.18897v1"
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#further-reading",
    "href": "gen-ai-use-cases/banking-use-cases.html#further-reading",
    "title": "Generative AI Use Cases in Banking",
    "section": "4 Further Reading",
    "text": "4 Further Reading\n[1] Boston Consulting Group (BCG). (2023). A Generative AI Roadmap for Financial Institutions. https://www.bcg.com/publications/2023/a-genai-roadmap-for-fis\n[2] McKinsey & Company. (2023). How generative AI can help banks manage risk and compliance. https://www.mckinsey.com/capabilities/risk-and-resilience/our-insights/how-generative-ai-can-help-banks-manage-risk-and-compliance\n[3] McKinsey & Company. (2023). Capturing the full value of generative AI in banking. https://www.mckinsey.com/industries/financial-services/our-insights/capturing-the-full-value-of-generative-ai-in-banking\n[4] Gartner. (2023). Emerging Tech Impact Radar: Artificial Intelligence in Banking. https://www.gartner.com/en/documents/4558699\n[5] CTO Magazine. (2023). JPMorgan Chase Accelerates AI Adoption. https://ctomagazine.com/jp-morgan-chase-accelerates-ai-adoption/\n[6] Bain & Company. (2023). Generative AI in Banking ‚Äì Interactive. https://www.bain.com/insights/generative-ai-banking-interactive/\n[7] PwC Middle East. (2023). Leveraging Generative AI in Banking. https://www.pwc.com/m1/en/publications/leveraging-generative-ai-in-banking.html\n[8] KPMG. (2023). Unleashing Potential: Exploring Generative AI‚Äôs Role in Banking. https://kpmg.com/xx/en/our-insights/ai-and-technology/unleashing-potential-exploring-generative-ai-role-in-banking.html\n[9] Mastercard. (2023). Signals: Generative AI is Transforming Banking. https://innovationinsights.mastercard.com/signals-generative-ai-transforming-banking/p/1\n[10] Mastercard. (2023). Generative Banking: How Financial Institutions Are Embracing the New AI. https://newsroom.mastercard.com/news/perspectives/2023/generative-banking-how-financial-institutions-are-embracing-the-new-ai/\n[11] Accenture. (2023). Generative AI in Banking. https://www.accenture.com/us-en/insights/banking/generative-ai-banking\n[12] Accenture. (2023). 3 Ways Generative AI Will Transform Banking. https://bankingblog.accenture.com/3-ways-generative-ai-will-transform-banking\n[13] Deloitte. (2023). Generative AI in Financial Services: Google & Deloitte Alliance. https://www.deloitte.com/global/en/alliances/google/blogs/generative-ai-in-financial-services.html\n[14] KPMG US. (2024). How Generative AI Can Help Banks Accelerate Digital Transformation. https://kpmg.com/kpmg-us/content/dam/kpmg/pdf/2024/generative-ai-help-bank-accelerate-digital-transformation.pdf\n[15] Bain & Company. (2023). How Bank CIOs Can Build a Solid Foundation for Generative AI. https://www.bain.com/insights/how-bank-cios-can-build-a-solid-foundation-for-generative-ai/\n[16] PwC US. (2023). Generative AI in Financial Services ‚Äì Salesforce Partnership. https://www.pwc.com/us/en/technology/alliances/library/salesforce-generative-ai-banking-financial-services.html\n[17] IBM. (2023). Generative AI in Banking. https://www.ibm.com/think/topics/generative-ai-banking"
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "",
    "text": "Banks are not short on AI ambition.\nEvery executive agenda mentions AI. Vendor demos are polished. Pilots are running across chatbots, copilots, and automation tools. Most banks can point to at least a few ‚ÄúAI initiatives‚Äù already in motion.\nAnd yet, very few banks have successfully scaled AI into core, trusted, enterprise-wide capabilities.\nIndustry surveys consistently show that while a majority of banks are experimenting with AI, most struggle to move even half of their initiatives into production ‚Äî largely due to data reliability, governance, and operating model gaps.\nThe gap between AI experimentation and AI in production remains wide.\nThis is not because banks lack models, vendors, or funding. It is because AI readiness is widely misunderstood."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#introduction",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#introduction",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "",
    "text": "Banks are not short on AI ambition.\nEvery executive agenda mentions AI. Vendor demos are polished. Pilots are running across chatbots, copilots, and automation tools. Most banks can point to at least a few ‚ÄúAI initiatives‚Äù already in motion.\nAnd yet, very few banks have successfully scaled AI into core, trusted, enterprise-wide capabilities.\nIndustry surveys consistently show that while a majority of banks are experimenting with AI, most struggle to move even half of their initiatives into production ‚Äî largely due to data reliability, governance, and operating model gaps.\nThe gap between AI experimentation and AI in production remains wide.\nThis is not because banks lack models, vendors, or funding. It is because AI readiness is widely misunderstood."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#the-readiness-illusion",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#the-readiness-illusion",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "2 The Readiness Illusion",
    "text": "2 The Readiness Illusion\nIn many organizations, AI readiness is treated as a technology milestone.\nDo we have cloud infrastructure?\nDo we have a data lake?\nDo we have access to large language models?\nIf the answer is ‚Äúyes,‚Äù the organization is assumed to be AI-ready.\nIn banking, that assumption breaks down quickly.\nScaling AI is not primarily a modeling problem. It is a data, governance, and operating model problem ‚Äî made harder by regulation, risk accountability, and the realities of how banks actually function."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#what-ai-readiness-frameworks-get-right",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#what-ai-readiness-frameworks-get-right",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "3 What AI Readiness Frameworks Get Right",
    "text": "3 What AI Readiness Frameworks Get Right\nMost AI readiness frameworks are directionally correct.\nWhether they come from technology vendors, consultancies, or academia, they tend to emphasize the same core dimensions:\n\nStrategy and leadership alignment\n\nData quality and availability\n\nGovernance and risk management\n\nInfrastructure and tooling\n\nTalent and organizational readiness\n\nThese elements are all necessary. Banks that ignore any one of them will struggle.\nBut in regulated financial institutions, these dimensions are not abstract concepts. They have very specific meanings ‚Äî and that is where most frameworks begin to fall short."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#where-ai-readiness-breaks-down-in-banking",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#where-ai-readiness-breaks-down-in-banking",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "4 Where AI Readiness Breaks Down in Banking",
    "text": "4 Where AI Readiness Breaks Down in Banking\n\n4.1 Data Readiness Is Not Just ‚ÄúClean Data‚Äù\nIn banking, ‚Äúdata-ready‚Äù does not simply mean centralized or well-modeled data.\nIt means traceable, explainable, auditable, and defensible.\nIf an AI system influences credit decisions, pricing, customer communication, or regulatory responses, the bank must be able to answer:\n\nWhere did this data come from?\n\nWho owns it?\n\nWho approved its use?\n\nWhat transformations were applied?\n\nCan we reproduce the output six months later?\n\nMany AI pilots quietly bypass these questions. That may work in experimentation ‚Äî but it is exactly why they never reach production.\nThis gap is not theoretical. Multiple banking studies point to data maturity ‚Äî not model capability ‚Äî as the primary constraint on scaling AI beyond pilots.\n\n\n\n4.2 Governance Is Not a Checkbox\nOutside of banking, governance is often framed as something that slows innovation.\nInside a bank, governance is what makes innovation possible.\nWithout data classification, role-based access, clear usage policies, and human accountability, AI quickly becomes a risk liability rather than a strategic asset.\nThis is especially true for generative AI. The assumption that ‚Äúall internal data is fair game‚Äù immediately collides with privacy rules, confidentiality obligations, and regulatory expectations.\nBanks that scale AI successfully design governance into the architecture so that AI systems only access data they are explicitly allowed to use.\nAdvisory firms working closely with banks consistently report the same pattern ‚Äî AI initiatives stall not because governance exists, but because it arrives too late. When classification, access controls, and usage policies are embedded upfront, risk teams become enablers rather than blockers.\n\n\n\n4.3 Pilots Fail Because Ownership Is Fuzzy\nA common pattern in banks:\n\nBusiness sponsors the use case\n\nIT enables the platform\n\nData teams provide access\n\nRisk reviews late ‚Äî or not at all\n\nWhen the pilot works, no one is sure who owns it.\nWhen something goes wrong, everyone is suddenly involved.\nAI systems that influence regulated decisions cannot exist in organizational gray zones. Successful banks assign clear accountability ‚Äî not just for building AI, but for how it behaves over time."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#unstructured-data-the-missed-opportunity",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#unstructured-data-the-missed-opportunity",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "5 Unstructured Data: The Missed Opportunity",
    "text": "5 Unstructured Data: The Missed Opportunity\nBanks hold enormous volumes of unstructured data:\n\nEmails\n\nCall transcripts\n\nComplaints\n\nCredit memos\n\nPolicies\n\nExaminer feedback\n\nThis is where generative AI can deliver real value.\nIndustry surveys repeatedly highlight that banks expect the greatest AI impact in areas like customer communication, compliance review, and document-intensive processes ‚Äî all of which are dominated by unstructured data.\nYet most banks either avoid this data entirely due to governance concerns or use it informally without enterprise controls.\nAI readiness does not mean opening the floodgates. It means applying the same discipline to unstructured data that banks already apply to transactional systems.\nWhen done correctly, this enables:\n\nFaster complaint resolution\n\nBetter relationship manager productivity\n\nMore consistent credit documentation\n\nImproved regulatory responsiveness"
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#humans-are-still-accountable-by-design",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#humans-are-still-accountable-by-design",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "6 Humans Are Still Accountable ‚Äî By Design",
    "text": "6 Humans Are Still Accountable ‚Äî By Design\nOne misconception about AI is that scaling requires removing humans from the loop.\nIn banking, the opposite is true.\nAI scales when humans understand what the system is doing, responsibilities are explicit, and accountability remains with people ‚Äî not models.\nThe most effective AI implementations in banks augment judgment rather than replace it, particularly in credit, compliance, and customer-facing decisions where trust and explainability matter more than speed.\nBoth academic research and industry experience consistently emphasize that explainability and human accountability are prerequisites for trust in regulated AI systems."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#the-board-level-disconnect",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#the-board-level-disconnect",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "7 The Board-Level Disconnect",
    "text": "7 The Board-Level Disconnect\nBoards often hear two conflicting narratives:\n\nAI will transform everything\n\nAI will deliver rapid ROI\n\nBoth are incomplete.\nSeveral industry studies point to a growing disconnect between executive expectations for rapid AI returns and the reality that data, governance, and operating maturity take years to build ‚Äî not quarters.\nAI readiness is not a one-time investment. It is an ongoing capability, similar to risk management or cybersecurity.\nBanks that succeed frame AI investments around productivity gains, consistency, risk reduction, and better use of existing expertise ‚Äî not immediate headcount reduction."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#a-bank-centric-view-of-ai-readiness",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#a-bank-centric-view-of-ai-readiness",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "8 A Bank-Centric View of AI Readiness",
    "text": "8 A Bank-Centric View of AI Readiness\nDrawing from industry frameworks, academic research, and practical experience in banking environments, AI readiness ultimately comes down to a small set of non-negotiables:\n\nTrusted and traceable data\n\nGoverned access aligned to roles\n\nHuman-in-the-loop decisioning\n\nAI fluency across business users, not just technologists\n\nClear ownership and accountability\n\nWithout these, AI remains a collection of demos.\nWith them, AI becomes a scalable, defensible capability."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#conclusion",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#conclusion",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nBanks are not behind on AI because they lack technology.\nThey struggle because AI forces uncomfortable questions about data ownership, governance, accountability, and operating models ‚Äî questions that cannot be solved by tools alone.\nAI readiness is a leadership decision.\nThe banks that acknowledge this reality will scale AI safely and sustainably. The rest will continue to experiment ‚Äî impressed by the hype, but blocked by their foundations."
  },
  {
    "objectID": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#references-further-reading",
    "href": "gen-ai-use-cases/Why-Banks-Cant-Scale-AI.html#references-further-reading",
    "title": "Why Banks Can‚Äôt Scale AI, Despite All the Hype",
    "section": "10 References & Further Reading",
    "text": "10 References & Further Reading\n\nMcKinsey ‚Äî Extracting Value from AI in Banking\n\nOpenText ‚Äî State of AI in Banking\n\nKlariVis / Cornerstone Advisors ‚Äî The AI Readiness Gap in Banking\n\nBaker Tilly ‚Äî AI in Banking: Readiness and Risk\n\nNature ‚Äî AI Integration in Financial Services"
  },
  {
    "objectID": "gen-ai-use-cases/ai-roadmap-for-banking.html",
    "href": "gen-ai-use-cases/ai-roadmap-for-banking.html",
    "title": "An AI Roadmap for Banking: From Predictive Intelligence to Generative Scale",
    "section": "",
    "text": "Banks are not new to artificial intelligence. For more than a decade, predictive models have quietly powered core decisions across credit risk, fraud detection, customer attrition, pricing, and marketing. What has changed with the rise of Generative AI is not the relevance of these models, but the scope of what AI can now support. Instead of only predicting outcomes, AI systems can increasingly understand unstructured information, synthesize context, and communicate insights in human language.\nBCG‚Äôs Generative AI Roadmap for Financial Institutions highlights an important distinction that is often lost in the excitement around large language models: generative AI does not replace predictive AI. It complements it. Banks that frame their strategy as ‚ÄúGenAI versus traditional analytics‚Äù risk discarding proven capabilities rather than building on them. The real opportunity lies in combining both forms of intelligence within end-to-end banking workflows."
  },
  {
    "objectID": "gen-ai-use-cases/ai-roadmap-for-banking.html#introduction",
    "href": "gen-ai-use-cases/ai-roadmap-for-banking.html#introduction",
    "title": "An AI Roadmap for Banking: From Predictive Intelligence to Generative Scale",
    "section": "",
    "text": "Banks are not new to artificial intelligence. For more than a decade, predictive models have quietly powered core decisions across credit risk, fraud detection, customer attrition, pricing, and marketing. What has changed with the rise of Generative AI is not the relevance of these models, but the scope of what AI can now support. Instead of only predicting outcomes, AI systems can increasingly understand unstructured information, synthesize context, and communicate insights in human language.\nBCG‚Äôs Generative AI Roadmap for Financial Institutions highlights an important distinction that is often lost in the excitement around large language models: generative AI does not replace predictive AI. It complements it. Banks that frame their strategy as ‚ÄúGenAI versus traditional analytics‚Äù risk discarding proven capabilities rather than building on them. The real opportunity lies in combining both forms of intelligence within end-to-end banking workflows."
  },
  {
    "objectID": "gen-ai-use-cases/ai-roadmap-for-banking.html#predictive-ai-and-generative-ai-distinct-roles-shared-value",
    "href": "gen-ai-use-cases/ai-roadmap-for-banking.html#predictive-ai-and-generative-ai-distinct-roles-shared-value",
    "title": "An AI Roadmap for Banking: From Predictive Intelligence to Generative Scale",
    "section": "2 Predictive AI and Generative AI: Distinct Roles, Shared Value",
    "text": "2 Predictive AI and Generative AI: Distinct Roles, Shared Value\nPredictive AI and generative AI solve fundamentally different problems. Predictive models estimate probabilities‚Äîthe likelihood of default, fraud, or customer churn‚Äîand remain indispensable for risk-driven decisions. Generative models, by contrast, specialize in language, synthesis, and content creation. They can read documents, summarize large volumes of information, draft narratives, and support decision-makers by reducing cognitive and operational load.\nA mature banking AI strategy recognizes these differences and uses each capability where it is strongest. A credit decision, for example, may still rely on a predictive risk score, while generative AI reviews financial statements, highlights unusual contractual terms, and drafts a credit memo for analyst review. The value comes not from choosing one paradigm, but from combining them within the same workflow."
  },
  {
    "objectID": "gen-ai-use-cases/ai-roadmap-for-banking.html#predictive-ai-use-cases-in-banking",
    "href": "gen-ai-use-cases/ai-roadmap-for-banking.html#predictive-ai-use-cases-in-banking",
    "title": "An AI Roadmap for Banking: From Predictive Intelligence to Generative Scale",
    "section": "3 Predictive AI Use Cases in Banking",
    "text": "3 Predictive AI Use Cases in Banking\nPredictive AI remains the analytical backbone of modern banking. It underpins some of the most critical and regulated decisions banks make, and its strengths are well understood. In credit risk, predictive models estimate probability of default, loss severity, and exposure across retail and commercial portfolios, informing underwriting, pricing, capital planning, and stress testing. While techniques have evolved, the role of predictive models in enforcing consistency and discipline has remained constant.\nFraud detection and financial crime monitoring represent another core domain. Predictive models analyze transaction patterns and behavioral signals to surface anomalies in real time, adapting to evolving fraud tactics more effectively than static rules. When deployed correctly, these models reduce false positives while improving detection accuracy, directly impacting customer experience and operational efficiency.\nPredictive AI also plays a central role in customer analytics. Propensity and churn models help banks anticipate customer behavior, personalize offers, and prioritize retention efforts. In many institutions, these models quietly drive material revenue impact without ever being visible to customers.\nAcross these use cases, predictive AI excels where outcomes are measurable, historical data is available, and decisions require probabilistic reasoning. Its limitations are equally important to recognize. Predictive models do not naturally interpret unstructured information or explain outcomes in human terms. This is where generative AI begins to add meaningful value."
  },
  {
    "objectID": "gen-ai-use-cases/ai-roadmap-for-banking.html#generative-ai-use-cases-in-banking",
    "href": "gen-ai-use-cases/ai-roadmap-for-banking.html#generative-ai-use-cases-in-banking",
    "title": "An AI Roadmap for Banking: From Predictive Intelligence to Generative Scale",
    "section": "4 Generative AI Use Cases in Banking",
    "text": "4 Generative AI Use Cases in Banking\nGenerative AI introduces a new class of capabilities by enabling systems to understand, summarize, and generate language. In banking, this unlocks high-value use cases that were historically difficult to automate because they depended heavily on human interpretation of documents, policies, and free-form text.\nOne of the most immediate areas of impact is document intelligence. Banks process vast volumes of contracts, financial statements, regulatory guidance, and internal reports. Generative AI can summarize lengthy documents, extract key themes, highlight risks, and draft standardized outputs such as credit memos or compliance summaries. Human oversight remains essential, but the time required to move from raw information to informed decision-making is dramatically reduced.\nCustomer- and employee-facing assistants represent another major application. Modern generative systems go beyond simple chatbots, handling more complex, contextual interactions. Externally, they improve self-service for routine inquiries. Internally, they help employees navigate policies, retrieve institutional knowledge, and synthesize information across systems. The value lies not in novelty, but in reduced friction and faster access to trusted information.\nGenerative AI also enables Generative BI, allowing business users to interact with enterprise data using natural language rather than predefined dashboards. When grounded in curated semantic models and governed appropriately, this capability lowers the barrier to insight without undermining analytical rigor.\nFinally, generative models enable synthetic data generation. Privacy-preserving synthetic datasets support model development, testing, and scenario analysis in environments where real data cannot be freely shared. This allows banks to innovate while respecting regulatory and confidentiality constraints.\nAcross these use cases, generative AI does not replace predictive decision-making. Instead, it surrounds it‚Äîpreparing inputs, explaining outputs, and accelerating downstream actions. Predictive models determine what decision should be made; generative models help humans understand, contextualize, and act on those decisions."
  },
  {
    "objectID": "gen-ai-use-cases/ai-roadmap-for-banking.html#architecture-to-enable-predictive-and-generative-ai-in-banking",
    "href": "gen-ai-use-cases/ai-roadmap-for-banking.html#architecture-to-enable-predictive-and-generative-ai-in-banking",
    "title": "An AI Roadmap for Banking: From Predictive Intelligence to Generative Scale",
    "section": "5 Architecture to Enable Predictive and Generative AI in Banking",
    "text": "5 Architecture to Enable Predictive and Generative AI in Banking\nPredictive and generative AI place different demands on technology, but banks cannot scale them through separate, siloed stacks. A credible AI roadmap requires a shared architectural foundation that supports both probabilistic modeling and language-based capabilities, while meeting the security, reliability, and governance standards of a regulated institution.\nEverything starts with data readiness. Predictive models depend on clean, well-labeled historical data. Generative models rely on trusted, well-curated knowledge sources. In both cases, the constraint is rarely model capability; it is data quality, lineage, and accessibility. Banks that scale AI invest in centralized, governed data platforms integrating transactional systems, risk data, customer data, and unstructured content. A well-defined semantic layer is critical to ensure consistency and trust.\nAbove the data foundation sits the model and analytics layer, where predictive and generative workloads coexist. Predictive AI runs on mature machine learning platforms with established practices for training, validation, explainability, and lifecycle management. Generative AI introduces additional components‚Äîlarge language models, embeddings, and retrieval mechanisms‚Äîto ground outputs in enterprise data. Despite tooling differences, both must be deployed through controlled, auditable pipelines rather than treated as ad hoc experiments.\nIntegration is where architecture either enables scale or becomes a bottleneck. Predictive models often operate invisibly, scoring transactions or customers in real time. Generative models sit closer to users, embedded in dashboards and workflow tools. A scalable architecture exposes both through well-defined services and APIs, allowing them to be composed into end-to-end workflows without tight coupling.\nSecurity and governance are design constraints, not afterthoughts. Generative AI introduces new risks such as data leakage and opaque outputs, while predictive models remain subject to rigorous model risk management. Strong access controls, auditability, and monitoring must be embedded into existing risk and compliance frameworks to enable responsible scaling.\nFinally, architecture must be built for change. Models will evolve, regulations will shift, and new capabilities will emerge. A modular, layered architecture allows banks to adopt new techniques without destabilizing existing systems, turning AI from a series of pilots into a durable enterprise capability."
  },
  {
    "objectID": "gen-ai-use-cases/ai-roadmap-for-banking.html#executive-conclusion",
    "href": "gen-ai-use-cases/ai-roadmap-for-banking.html#executive-conclusion",
    "title": "An AI Roadmap for Banking: From Predictive Intelligence to Generative Scale",
    "section": "6 Executive Conclusion",
    "text": "6 Executive Conclusion\nAI in banking is no longer a question of experimentation; it is a question of execution. Predictive AI has already proven its value across risk, fraud, and customer analytics. Generative AI extends these capabilities by interpreting unstructured information, accelerating decisions, and reducing operational friction. The opportunity lies not in choosing between them, but in integrating both within the same business workflows.\nBanks that succeed will avoid the trap of isolated pilots and instead focus on end-to-end transformation. That requires clarity on where predictive models drive decisions, where generative models augment human judgment, and how both are governed within a regulated environment. Without this clarity, AI investments remain fragmented and difficult to scale.\nArchitecture is the decisive factor. A shared data foundation, disciplined model lifecycle management, secure integration patterns, and embedded governance turn AI from a collection of tools into an enterprise capability. Institutions that treat architecture as a first-class element of their AI roadmap position themselves to scale responsibly, adapt as technology evolves, and deliver durable business value.\nIn the end, competitive advantage will not come from adopting the latest model first, but from building the foundations that allow AI‚Äîpredictive and generative alike‚Äîto be applied consistently, safely, and at scale across the bank."
  },
  {
    "objectID": "normalizing_flow_pytorch.html",
    "href": "normalizing_flow_pytorch.html",
    "title": "RealNVP Model in PyTorch",
    "section": "",
    "text": "This notebook provides a simple implementation of a RealNVP flow model.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nfrom torch.distributions import MultivariateNormal\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nUsing device: cuda\n\n\n\n1 Create a Normalize the Two Moons Dataset\n\n# Generate the two moons dataset\nn_samples = 30000\ndata, _ = make_moons(n_samples=n_samples, noise=0.05)\ndata = data.astype(np.float32)\n\n# Normalize the dataset\nscaler = StandardScaler()\nnormalized_data = scaler.fit_transform(data)\n\n# Convert to PyTorch tensor and move to device\nnormalized_data = torch.tensor(normalized_data, dtype=torch.float32).to(device)\n\n# Visualize the data\nplt.figure(figsize=(6, 6))\nplt.scatter(normalized_data[:, 0].cpu(), normalized_data[:, 1].cpu(), s=1, c='green')\nplt.title(\"Normalized Two Moons Dataset\")\nplt.axis(\"equal\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nclass CouplingMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n\n        # Scale output: initialized near zero with tanh for stability\n        self.scale = nn.Linear(hidden_dim, input_dim)\n        self.translate = nn.Linear(hidden_dim, input_dim)\n\n        # Apply Xavier initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        h = self.net(x)\n        s = torch.tanh(self.scale(h)) * 2.0  # bounded scaling\n        t = self.translate(h)\n        return s, t\n\n\nclass RealNVP(nn.Module):\n    def __init__(self, input_dim=2, num_coupling_layers=6):\n        super().__init__()\n        self.input_dim = input_dim\n        self.num_coupling_layers = num_coupling_layers\n\n        # Prior distribution in latent space\n        self.prior = torch.distributions.MultivariateNormal(\n            torch.zeros(input_dim).to(device),\n            torch.eye(input_dim).to(device)\n        )\n\n        # Alternating binary masks, e.g., [0,1], [1,0], ...\n        self.masks = torch.tensor(\n            [[0, 1], [1, 0]] * (num_coupling_layers // 2),\n            dtype=torch.float32\n        ).to(device)\n\n        # Create coupling layers\n        self.coupling_layers = nn.ModuleList([\n            CouplingMLP(input_dim=input_dim) for _ in range(num_coupling_layers)\n        ])\n\n    def forward(self, x, reverse=False):\n        \"\"\"\n        If reverse=False: maps data ‚Üí latent (x ‚Üí z)\n        If reverse=True:  maps latent ‚Üí data (z ‚Üí x)\n        \"\"\"\n        log_det = torch.zeros(x.shape[0], device=x.device)\n        direction = -1 if not reverse else 1\n        layers = range(self.num_coupling_layers)[::direction]\n\n        for i in layers:\n            mask = self.masks[i]\n            x_masked = x * mask\n            s, t = self.coupling_layers[i](x_masked)\n\n            s = s * (1 - mask)\n            t = t * (1 - mask)\n\n            if reverse:\n                x = (x - t) * torch.exp(-s) * (1 - mask) + x_masked\n            else:\n                x = (x * torch.exp(s) + t) * (1 - mask) + x_masked\n                log_det += torch.sum(s, dim=1)\n\n        return x, log_det\n\n    def log_prob(self, x):\n        \"\"\"\n        Log probability of input x under the model.\n        \"\"\"\n        z, log_det = self.forward(x, reverse=False)\n        log_prob_z = self.prior.log_prob(z)\n        return log_prob_z + log_det\n\n    def loss(self, x):\n        \"\"\"\n        Negative log-likelihood loss.\n        \"\"\"\n        return -self.log_prob(x).mean()\n\n\nbatch_size = 256\nepochs = 100\nlearning_rate = 1e-4\n\n\ndataset = TensorDataset(normalized_data)\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n\nmodel = RealNVP(input_dim=2, num_coupling_layers=6).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n\nlosses = []\n\nfor epoch in range(epochs):\n    epoch_loss = 0.0\n    model.train()\n\n    for batch in train_loader:\n        x = batch[0].to(device)\n\n        optimizer.zero_grad()\n        loss = model.loss(x)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * x.size(0)\n\n    avg_loss = epoch_loss / len(train_loader.dataset)\n    losses.append(avg_loss)\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n\nEpoch [10/100], Loss: 1.3349\nEpoch [20/100], Loss: 1.2672\nEpoch [30/100], Loss: 1.2408\nEpoch [40/100], Loss: 1.2296\nEpoch [50/100], Loss: 1.2191\nEpoch [60/100], Loss: 1.2145\nEpoch [70/100], Loss: 1.1950\nEpoch [80/100], Loss: 1.1911\nEpoch [90/100], Loss: 1.1978\nEpoch [100/100], Loss: 1.1867\n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(losses, label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Negative Log-Likelihood\")\nplt.title(\"RealNVP Training Loss\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    z, _ = model(normalized_data, reverse=False)\n\n\nwith torch.no_grad():\n    z_samples = model.prior.sample((3000,))\n    x_samples, _ = model(z_samples, reverse=True)\n\n\nf, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Original data\naxes[0, 0].scatter(normalized_data[:, 0].cpu(), normalized_data[:, 1].cpu(), s=1, c='green')\naxes[0, 0].set_title(\"Original Data (X)\")\naxes[0, 0].axis(\"equal\")\n\n# Latent representation of data\naxes[0, 1].scatter(z[:, 0].cpu(), z[:, 1].cpu(), s=1, c='blue')\naxes[0, 1].set_title(\"Encoded Latent Space (Z)\")\naxes[0, 1].axis(\"equal\")\n\n# Random latent samples\naxes[1, 0].scatter(z_samples[:, 0].cpu(), z_samples[:, 1].cpu(), s=1, c='orange')\naxes[1, 0].set_title(\"Sampled Latent Z\")\naxes[1, 0].axis(\"equal\")\n\n# Transformed back to data\naxes[1, 1].scatter(x_samples[:, 0].cpu(), x_samples[:, 1].cpu(), s=1, c='red')\naxes[1, 1].set_title(\"Generated Samples (X)\")\naxes[1, 1].axis(\"equal\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "vae_mnist.html",
    "href": "vae_mnist.html",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "",
    "text": "This notebook explains a Variational Autoencoder (VAE) trained on the MNIST dataset using PyTorch.\nEach step is annotated with detailed comments to help beginners understand what‚Äôs happening.\n!pip install torch torchvision"
  },
  {
    "objectID": "vae_mnist.html#import-required-libraries",
    "href": "vae_mnist.html#import-required-libraries",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "1 1. Import Required Libraries",
    "text": "1 1. Import Required Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "href": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "2 2. Load and Prepare the MNIST Dataset",
    "text": "2 2. Load and Prepare the MNIST Dataset\n\n# We transform MNIST images into tensors.\ntransform = transforms.ToTensor()\n\n# Download and load the training data\ntrain_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n\n# DataLoader for batching and shuffling\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
  },
  {
    "objectID": "vae_mnist.html#define-the-vae-model",
    "href": "vae_mnist.html#define-the-vae-model",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "3 3. Define the VAE Model",
    "text": "3 3. Define the VAE Model\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):\n        super(VAE, self).__init__()\n        # Encoder layers: input -&gt; hidden -&gt; (mu, logvar)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # outputs mean of q(z|x)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # outputs log-variance of q(z|x)\n\n        # Decoder layers: latent -&gt; hidden -&gt; reconstruction\n        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, input_dim)\n\n    def encode(self, x):\n        # Apply a hidden layer then split into mean and logvar\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        # Apply the reparameterization trick\n        std = torch.exp(0.5 * logvar)      # standard deviation\n        eps = torch.randn_like(std)        # random normal noise\n        return mu + eps * std              # sample z\n\n    def decode(self, z):\n        # Reconstruct input from latent representation\n        h = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h))  # Output in [0, 1] range for binary MNIST\n\n    def forward(self, x):\n        # Full VAE forward pass\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        return recon_x, mu, logvar"
  },
  {
    "objectID": "vae_mnist.html#define-the-elbo-loss",
    "href": "vae_mnist.html#define-the-elbo-loss",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "4 4. Define the ELBO Loss",
    "text": "4 4. Define the ELBO Loss\n\ndef elbo_loss(recon_x, x, mu, logvar):\n    # Binary cross-entropy for reconstruction\n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n\n    # KL divergence term to regularize q(z|x) against standard normal p(z)\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    # Total loss is negative ELBO\n    return BCE + KLD"
  },
  {
    "objectID": "vae_mnist.html#train-the-vae",
    "href": "vae_mnist.html#train-the-vae",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "5 5. Train the VAE",
    "text": "5 5. Train the VAE\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = VAE().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for x, _ in train_loader:\n        x = x.view(-1, 784).to(device)               # Flatten 28x28 images into 784 vectors\n        recon_x, mu, logvar = model(x)               # Forward pass\n        loss = elbo_loss(recon_x, x, mu, logvar)     # Compute loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader.dataset):.2f}\")"
  },
  {
    "objectID": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "href": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "6 6. Visualize Original and Reconstructed Digits",
    "text": "6 6. Visualize Original and Reconstructed Digits\n\nmodel.eval()\nwith torch.no_grad():\n    x, _ = next(iter(train_loader))\n    x = x.view(-1, 784).to(device)\n    recon_x, _, _ = model(x)\n\n    # Convert back to image format\n    x = x.view(-1, 1, 28, 28).cpu()\n    recon_x = recon_x.view(-1, 1, 28, 28).cpu()\n\n    fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n    for i in range(10):\n        axs[0, i].imshow(x[i][0], cmap='gray')\n        axs[0, i].axis('off')\n        axs[1, i].imshow(recon_x[i][0], cmap='gray')\n        axs[1, i].axis('off')\n    axs[0, 0].set_ylabel(\"Original\", fontsize=12)\n    axs[1, 0].set_ylabel(\"Reconstruction\", fontsize=12)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "vae_mnist.html#visualize-latent-space",
    "href": "vae_mnist.html#visualize-latent-space",
    "title": "Variational Autoencoder (VAE) on MNIST ‚Äî Beginner Friendly Walkthrough",
    "section": "7 7. Visualize Latent Space",
    "text": "7 7. Visualize Latent Space\n\nimport seaborn as sns\n\nmodel.eval()\nall_z = []\nall_labels = []\n\n# Go through a few batches and collect latent representations\nwith torch.no_grad():\n    for x, y in train_loader:\n        x = x.view(-1, 784).to(device)\n        mu, _ = model.encode(x)  # use the mean as representation\n        all_z.append(mu.cpu())\n        all_labels.append(y)\n\n# Concatenate all batches\nz = torch.cat(all_z, dim=0).numpy()\nlabels = torch.cat(all_labels, dim=0).numpy()\n\n# Plot with seaborn\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=z[:, 0], y=z[:, 1], hue=labels, palette=\"tab10\", s=15)\nplt.title(\"Latent Space Visualization (using Œº)\")\nplt.xlabel(\"z[0]\")\nplt.ylabel(\"z[1]\")\nplt.legend(title=\"Digit\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generative Models Overview",
    "section": "",
    "text": "Generative AI could add $4.4 trillion in annual economic value globally, according to McKinsey. That‚Äôs not a distant projection‚Äîorganizations are already deploying these systems in production. Capturing that value while managing the risks requires understanding where generative AI fits in the broader evolution of AI capabilities.\nAI has moved through three distinct phases, each expanding what machines can do while introducing new challenges. Traditional AI operates on fixed rules‚Äîreliable for repetitive tasks like fraud detection or email filtering, but requiring constant human direction. Generative AI learns patterns from data to create new content: text, images, code, synthetic data for testing. Agentic AI takes this further, planning multi-step workflows and pursuing goals with minimal human oversight.\nHere‚Äôs how they compare:\n\n\n\n\n\n\n\n\n\n\nCapability\nTraditional AIRule-driven tools\nGenerative AISmart assistants\nAgentic AIAutonomous actors\n\n\n\n\nWhat it does\nFollows hard-coded logic and rules\nUnderstands prompts and generates output\nPlans, reasons, and acts across steps\n\n\nHow it behaves\nRigid, predictable\nCreative but guided\nGoal-seeking and adaptive\n\n\nHuman involvement\nFully manual setup and supervision\nNeeds context and oversight\nCan operate independently (with guardrails)\n\n\nStrengths\nReliable on structured tasks\nGreat at summarizing, drafting, generating\nHandles multistep workflows\n\n\nBest used for\nRepetitive decisions and automation\nInsight generation and copiloting\nFull process orchestration\n\n\n\n\nEach phase unlocked new capabilities but also introduced new failure modes. Traditional AI breaks when edge cases appear. Generative AI can produce convincing but completely incorrect outputs. Agentic AI raises questions about control and accountability when systems operate autonomously. Understanding this progression matters because implementation decisions depend on which type of AI you‚Äôre deploying‚Äîand in regulated industries like banking and healthcare, the governance requirements differ substantially."
  },
  {
    "objectID": "index.html#evolution-of-ai-capabilities",
    "href": "index.html#evolution-of-ai-capabilities",
    "title": "Generative Models Overview",
    "section": "",
    "text": "Generative AI could add $4.4 trillion in annual economic value globally, according to McKinsey. That‚Äôs not a distant projection‚Äîorganizations are already deploying these systems in production. Capturing that value while managing the risks requires understanding where generative AI fits in the broader evolution of AI capabilities.\nAI has moved through three distinct phases, each expanding what machines can do while introducing new challenges. Traditional AI operates on fixed rules‚Äîreliable for repetitive tasks like fraud detection or email filtering, but requiring constant human direction. Generative AI learns patterns from data to create new content: text, images, code, synthetic data for testing. Agentic AI takes this further, planning multi-step workflows and pursuing goals with minimal human oversight.\nHere‚Äôs how they compare:\n\n\n\n\n\n\n\n\n\n\nCapability\nTraditional AIRule-driven tools\nGenerative AISmart assistants\nAgentic AIAutonomous actors\n\n\n\n\nWhat it does\nFollows hard-coded logic and rules\nUnderstands prompts and generates output\nPlans, reasons, and acts across steps\n\n\nHow it behaves\nRigid, predictable\nCreative but guided\nGoal-seeking and adaptive\n\n\nHuman involvement\nFully manual setup and supervision\nNeeds context and oversight\nCan operate independently (with guardrails)\n\n\nStrengths\nReliable on structured tasks\nGreat at summarizing, drafting, generating\nHandles multistep workflows\n\n\nBest used for\nRepetitive decisions and automation\nInsight generation and copiloting\nFull process orchestration\n\n\n\n\nEach phase unlocked new capabilities but also introduced new failure modes. Traditional AI breaks when edge cases appear. Generative AI can produce convincing but completely incorrect outputs. Agentic AI raises questions about control and accountability when systems operate autonomously. Understanding this progression matters because implementation decisions depend on which type of AI you‚Äôre deploying‚Äîand in regulated industries like banking and healthcare, the governance requirements differ substantially."
  },
  {
    "objectID": "index.html#why-generative-ai-matters",
    "href": "index.html#why-generative-ai-matters",
    "title": "Generative Models Overview",
    "section": "2 Why Generative AI Matters",
    "text": "2 Why Generative AI Matters\nGenerative AI creates novel content‚Äîtext, images, code, synthetic data‚Äîby learning patterns from existing examples. Since 2022, adoption has accelerated rapidly. Models like ChatGPT, Claude, and Gemini are deployed across use cases ranging from content generation and fraud detection to software development and clinical documentation.\n\nMcKinsey estimates that generative AI could contribute up to $4.4 trillion in annual global economic value, with wide-ranging implications for productivity, personalization, and decision support. However, moving from promise to reality remains challenging. While 88% of organizations now use AI regularly, most are still in the pilot phase. Only about one-third have successfully scaled AI across their enterprises, and just 39% report any measurable impact on earnings. The gap between individual use case success and enterprise-wide transformation is the defining challenge of 2025.\n\nThe technology introduces significant risks alongside its capabilities. GenAI can generate outputs that appear authoritative but are incorrect, biased, or harmful. In banking, healthcare, and legal contexts where errors carry serious consequences, deployment requires strong governance frameworks, human oversight, and careful risk management."
  },
  {
    "objectID": "index.html#how-generative-models-work",
    "href": "index.html#how-generative-models-work",
    "title": "Generative Models Overview",
    "section": "3 How Generative Models Work",
    "text": "3 How Generative Models Work\nNow that we‚Äôve seen why GenAI matters, let‚Äôs look under the hood. How do these models actually learn to create realistic content? The basic idea is pretty simple. These models look at tons of real examples‚Äîdog photos, text, whatever‚Äîand figure out the patterns. Then they use those patterns to make new stuff that looks real.\n\n\n\nFigure 1: Generative models learn to approximate the true data distribution \\(P_{\\text{data}}\\) by optimizing a parameterized model \\(P_\\theta\\). The goal is to minimize divergence \\(d(P_{\\text{data}}, P_\\theta)\\). Source: Adapted from Stanford CS236.\n\n\nHere‚Äôs what‚Äôs going on under the hood:\nReal data has patterns. Dog photos usually have four legs, fur, a tail. We call this the data distribution \\(P_{\\text{data}}\\). The model tries to learn this pattern by building its own version, called \\(P_\\theta\\) (where \\(\\theta\\) represents the model‚Äôs internal settings).\nTraining means adjusting those settings until the model‚Äôs distribution \\(P_\\theta\\) matches the data distribution \\(P_{\\text{data}}\\). The model keeps tweaking itself to minimize the gap between what it generates and what actually exists. Once trained, you can generate new content by sampling from what the model learned.\nThe basics:\n\nThe model learns a pattern \\(P_\\theta\\) that captures what makes data look real\nNew content comes from drawing examples from this learned pattern\nTraining means making \\(P_\\theta\\) match the real data pattern as closely as possible\n\nDifferent models, different approaches:\nDifferent types of models use different techniques to learn these patterns:\n\nAutoregressive models (like GPT) and Normalizing Flows directly maximize how likely the real data is under their model\nVAEs use a workaround called ELBO (Evidence Lower Bound) to approximate this\nGANs pit two networks against each other‚Äîone creates fakes, the other spots them‚Äîuntil the fakes become incredibly convincing\nDiffusion models and EBMs use techniques like score matching to learn the patterns\n\nAll these models are trying to learn ‚Äúwhat real data looks like‚Äù so they can create convincing new examples. They just take different paths to get there."
  },
  {
    "objectID": "index.html#how-we-got-here",
    "href": "index.html#how-we-got-here",
    "title": "Generative Models Overview",
    "section": "4 How We Got Here",
    "text": "4 How We Got Here\nTo understand today‚Äôs AI, it helps to look back at the key breakthroughs that got us here. The timeline below shows how Generative AI evolved over five distinct periods:\n\n\n\nFigure: Generative AI: Key Milestones. Tracks key advances from early neural nets and probabilistic models to modern transformers, diffusion models, and real-world deployment.\n\n\nEach era built on what came before, introducing new techniques and expanding what AI could do. Let‚Äôs walk through each one.\n1980‚Äì1990s: Early Foundations\nIn the late 1980s, researchers introduced Recurrent Neural Networks (RNNs), which could process sequences of data like sentences or time series. LSTMs came in 1997, making it easier for models to remember context over longer sequences. Around the same time, early generative models started appearing‚Äîa shift from AI that just classified things to AI that could actually create new content. Probabilistic models like Bayesian networks and Hidden Markov Models gave us ways to model uncertainty and complex patterns. These models laid the groundwork for everything that followed.\n2000s: Learning Better Representations\nThe 2000s saw neural networks make a comeback. Researchers figured out how to train them to learn meaningful features from raw data automatically‚Äîno hand-crafted rules needed. The big breakthrough was Deep Belief Networks in 2006. They used a clever training trick called layer-wise pretraining that made deep learning practical again after years of disappointing results. This proved that neural networks could learn hierarchical patterns on their own, which opened the door to modern deep learning.\n2010s: The Big Leap\nThis decade brought the models that most people think of as ‚Äúmodern AI.‚Äù VAEs showed up in 2013 and GANs in 2014-both could generate realistic images and learn useful representations. Then came Transformers in 2017, which changed everything for language models. Their self-attention mechanism made it possible to build the large language models we use today. Diffusion models appeared in 2015 too, though they didn‚Äôt work well until around 2020-2022. Together, these architectures created the foundation for the AI boom we‚Äôre seeing now.\n2020‚Äì2022: Going Big\nGPT-3 launched in 2020 and DALL-E 2 in 2022, showing what massive models trained on huge datasets could do‚Äîwrite coherent text, generate photorealistic images. Models got better at zero-shot and few-shot learning, meaning they could handle new tasks without needing specific training. We also saw multimodal models that could work with text, images, and audio all at once. Diffusion models matured during this time too, often producing better images than GANs. This was when GenAI went from a research topic to something people could actually use.\n2023‚Äì2025: Real-World Deployment\nOpen-source models like LLaMA and Mistral made powerful AI accessible to more people, not just big tech companies with massive budgets. This competition sped up innovation. Tools like LangChain and RAG emerged to help AI systems handle complex, multi-step tasks and base their answers on real-world data. Companies started deploying GenAI in production, which meant they had to get serious about governance, safety, and explainability.\n\nThe latest frontier is Agentic AI that can plan and execute multi-step workflows autonomously. Recent McKinsey research shows that by 2025, nearly two-thirds of organizations are at least experimenting with AI agents, with early adoption concentrated in IT service management and knowledge operations. This shift from AI-as-tool to AI-as-agent marks another evolution in how businesses deploy these technologies.\n\nNow that we‚Äôve seen how GenAI evolved, let‚Äôs dive deeper into the main types of models. Each family has its own way of learning patterns and generating new content."
  },
  {
    "objectID": "index.html#key-gen-ai-model-families",
    "href": "index.html#key-gen-ai-model-families",
    "title": "Generative Models Overview",
    "section": "5 Key Gen AI Model Families",
    "text": "5 Key Gen AI Model Families\nThe generative AI landscape includes several distinct model families, each with its own approach to learning and creating content. Understanding these architectures helps you choose the right tool for specific applications.\n\n5.1 Variational Autoencoders (VAEs)\nVAEs compress data into compact representations (latent spaces), then reconstruct it. What makes them useful is how they organize this latent space‚Äîsimilar inputs map to nearby points, which allows smooth interpolation between outputs.\nHow they learn: VAEs optimize the Evidence Lower Bound (ELBO), which approximates the true data distribution through a balance of reconstruction accuracy and regularization in the latent space.\nApplications & Implementations: In drug discovery, VAEs generate new molecular structures by learning the latent space of known compounds. Researchers explore this space to find candidates that share properties with existing drugs but have novel structures. Notable implementations include Œ≤-VAE for disentangled representations and Conditional VAE for controlled generation.\nLearn more about VAE Models ‚Üí\n\n\n5.2 Autoregressive Models\nAutoregressive models generate sequences one element at a time, where each new element depends on all previous ones. GPT models follow this pattern‚Äîthey predict each token based on the preceding context.\nHow they learn: The training objective is straightforward: maximize the probability of the next token given all previous tokens. This captures sequential dependencies and long-range patterns in the data.\nApplications & Implementations: GPT-4 and Claude generate text by predicting each subsequent token based on context, maintaining coherence across thousands of tokens. GitHub Copilot applies this principle to code generation, suggesting completions based on surrounding code and natural language comments. PixelRNN extends the autoregressive approach to image generation by predicting pixels sequentially.\n\n\n5.3 Normalizing Flows\nNormalizing Flows transform simple distributions (typically Gaussian noise) into complex data distributions through invertible mappings. The reversibility is what sets them apart‚Äîyou can move from noise to data and back with equal ease.\nHow they learn: By chaining invertible transformations, flows can compute exact log-likelihoods‚Äîa unique property among generative models. This makes them particularly valuable when precise probability estimates are required.\nApplications & Implementations: WaveGlow uses normalizing flows for natural-sounding speech synthesis. The reversibility also enables density estimation for anomaly detection‚Äîdetermining how likely a particular data point is under the learned distribution. RealNVP and Glow demonstrate the architecture‚Äôs flexibility across image and audio domains.\nLearn more about Flow Models ‚Üí\n\n\n5.4 Energy-Based Models (EBMs)\nEnergy-Based Models assign scalar energy values to inputs‚Äîlower energy corresponds to more probable data. Rather than modeling probabilities directly, EBMs learn what distinguishes realistic data from noise.\nHow they learn: Training involves learning an energy function through techniques like contrastive divergence or score matching. Generation requires iterative refinement‚Äîstarting from random initialization and following the energy gradient toward low-energy regions.\nApplications & Implementations: Score-based diffusion models (a type of EBM) power modern image generators like Midjourney. They learn the score (gradient of the energy) and use it to denoise images step by step, starting from pure noise and arriving at photorealistic results. Training approaches include contrastive divergence and denoising score matching.\nLearn more about EBMs ‚Üí\n\n\n5.5 Generative Adversarial Networks (GANs)\nGANs train two networks simultaneously: a generator that creates synthetic samples and a discriminator that distinguishes real from generated data. The adversarial training process pushes the generator to produce increasingly realistic outputs.\nHow they learn: The training objective is a minimax game where the discriminator maximizes classification accuracy while the generator minimizes it. At equilibrium, the generator‚Äôs distribution matches the data distribution.\nApplications & Implementations: StyleGAN generates photorealistic synthetic faces with fine-grained control over attributes, widely adopted for data augmentation and creative tools. CycleGAN enables unpaired image-to-image translation, such as converting photographs to paintings without requiring matched training pairs. The original DALL-E also used a GAN-based architecture before later versions shifted to diffusion.\n\n\n5.6 Diffusion Models\nDiffusion models generate data through iterative denoising. Starting from pure noise, they progressively remove noise over multiple steps until reaching a clean sample. The model learns the reverse of a gradual noising process applied during training.\nHow they learn: Training involves two Markov chains: a forward diffusion process that adds Gaussian noise over T steps, and a learned reverse process that denoises. The model learns to predict the noise component at each step, enabling gradual reconstruction from random noise.\nApplications & Implementations: Stable Diffusion and DALL-E 3 generate images from text prompts using this approach. Given a description like ‚Äúsunset over mountains in Van Gogh style,‚Äù the model begins with Gaussian noise and iteratively denoises while conditioning on the encoded text. Midjourney and Imagen represent other leading implementations. Diffusion models have largely supplanted GANs for image synthesis due to superior training stability and sample diversity.\nLearn more about Diffusion Models ‚Üí\n\n\n\n\n\n\nExplore Related Topics\n\n\n\nExpand your understanding of Generative AI with these supporting deep dives:\n\nTransformers ‚Üí\nUnderstand the self-attention architecture behind modern LLMs and Gen AI models.\nPost-Training Techniques ‚Üí\nLearn how fine-tuning, RLHF, and instruction tuning make base models usable in the real world.\nEvaluation Strategies ‚Üí\nDiscover how we evaluate GenAI output quality ‚Äî from traditional metrics to modern LLM-based approaches."
  },
  {
    "objectID": "index.html#use-case-framing-prioritization",
    "href": "index.html#use-case-framing-prioritization",
    "title": "Generative Models Overview",
    "section": "6 Use-Case Framing & Prioritization",
    "text": "6 Use-Case Framing & Prioritization\nBefore diving into specific industry use cases, it‚Äôs critical to assess where and how generative AI can deliver real value. Not every idea is equally feasible, impactful, or low-risk. A structured framing process helps ensure that AI initiatives align with business priorities and responsible innovation.\nWe recommend evaluating GenAI use cases across three key dimensions:\n\nBusiness Value: Consider how the use case impacts revenue generation, cost reduction, operational efficiency, risk mitigation, or customer experience. This ensures alignment with strategic business outcomes.\nFeasibility: Evaluate the availability and quality of data, the readiness of models, the complexity of integration, and infrastructure or compute requirements. This grounds ideas in technical and operational reality.\nGovernance Sensitivity: Assess how much oversight is needed to meet regulatory, ethical, or reputational expectations‚Äîespecially in domains like banking and healthcare. This includes explainability, auditability, and the potential for misuse.\n\n\nMcKinsey research on high-performing AI implementations reveals that workflow redesign is the single strongest predictor of success. Organizations achieving significant business impact don‚Äôt just add AI to existing processes‚Äîthey fundamentally rethink how work gets done. They‚Äôre also three times more likely to have senior leadership actively championing AI initiatives and to set growth and innovation goals alongside efficiency targets.\n\nThis framing helps prioritize AI use cases that are not only promising, but also implementable and sustainable‚Äîespecially in highly regulated environments."
  },
  {
    "objectID": "index.html#industry-use-cases",
    "href": "index.html#industry-use-cases",
    "title": "Generative Models Overview",
    "section": "7 Industry Use Cases",
    "text": "7 Industry Use Cases\nGenAI is being deployed across industries in two ways: horizontal use cases that work everywhere, and industry-specific applications tailored to particular sectors. Let‚Äôs look at both.\n\n7.1 Horizontal Use Cases (Apply Across Industries)\nThese use cases work in nearly any industry‚Äîthey‚Äôre about common business functions like content creation, customer service, and compliance.\n\n\n\n\n\n\n\n\nUse Case\nWhat It Does\nBusiness Impact\n\n\n\n\nContent Generation\nCreates reports, emails, social media posts\nScale content marketing efforts without proportional headcount increases\n\n\nPersonalized Marketing\nGenerates customized emails, landing pages, and social posts\nReach target audiences more effectively and increase conversion rates\n\n\nCustomer Service\nPowers chatbots that answer questions and resolve problems\nFree up human agents to focus on complex issues\n\n\nRisk Management\nIdentifies and predicts fraud, cyberattacks, supply chain disruptions\nMitigate risks and protect assets before problems occur\n\n\nCompliance\nGenerates compliant documents like contracts, reports, disclosures\nSave time and money while reducing noncompliance risk\n\n\nSoftware Development\nGenerates code, provides snippets, documents and refactors code\nSpeed up development, reduce errors, generate test cases\n\n\nData Augmentation\nCreates synthetic data when real data is insufficient\nEnable model training when privacy or scarcity limits real data availability\n\n\nContract Management\nDrafts legal documents and understands regulatory requirements\nReduce human mistakes and make informed decisions faster\n\n\n\nRecent McKinsey data shows cost benefits concentrate in software engineering, manufacturing, and IT operations, while revenue gains primarily come from marketing and sales, strategy functions, and product development. Organizations using AI across multiple functions see better results than those with isolated pilots.\n\n\n7.2 Banking & Financial Services\nFinancial institutions use GenAI for scenario modeling, risk assessment, and customer operations‚Äîall while navigating strict regulatory requirements.\n\n\n\n\n\n\n\nUse Case\nExample Application\n\n\n\n\nCustomer Service\nVirtual agents for handling account queries and FAQs. Bank of America‚Äôs Erica has handled 2B+ interactions with 44-second average resolution time.\n\n\nFraud Detection\nReal-time anomaly detection in transaction behavior. Mastercard Decision Intelligence Pro achieves 20% higher fraud detection with 85% fewer false positives.\n\n\nRisk Modeling\nScenario analysis for credit risk and market stress testing.\n\n\nOperational Efficiency\nAuto-summarizing calls and processing back-office documents. JPMorgan‚Äôs COiN reviews 12,000 loan agreements annually, saving 360,000 hours.\n\n\nPersonalized Insights\nAutomated note-taking and tailored investment recommendations. Morgan Stanley‚Äôs Debrief saves 30 minutes per meeting, freeing 10‚Äì15 hours/week per advisor.\n\n\nDocument Summarization\nExtracting insights from internal research. Morgan Stanley‚Äôs AskResearchGPT summarizes insights from 70,000+ proprietary reports, saving 90% review time, cutting costs by 80%, and improving accuracy by 25%.\n\n\nBusiness Intelligence\nSpotting unusual patterns in product or branch-level KPIs.\n\n\nMarketing & Personalization\nCreating personalized offers based on customer behavior and transaction history.\n\n\nProduct Development\nSimulating scenarios to develop new financial products and services.\n\n\n\nWhat makes financial services different:\nDecision-making requires scenario simulation, risk model assessment, and customer personalization based on transaction history. Everything must be explainable and auditable for regulators.\n\nReal-world examples from JPMorgan, Mastercard, Wells Fargo, and Morgan Stanley.\nView complete banking implementation guide ‚Üí\n\n\n\n\n7.3 Healthcare & Life Sciences\nHealthcare organizations deploy GenAI for drug discovery, clinical documentation, and personalized treatment‚Äîall while maintaining patient privacy and safety standards.\n\n\n\n\n\n\n\nUse Case\nExample Application\n\n\n\n\nClinical Documentation\nAuto-generating visit summaries and physician notes. Nuance DAX reduces documentation time by 50%, saving 7 minutes per encounter and enabling 3‚Äì5 additional appointments daily.\n\n\nDrug Discovery\nAccelerating compound generation and molecular simulation. Insilico Medicine advanced a drug candidate to Phase II trials in under 30 months (&lt;18 months from target to candidate).\n\n\nMedical Device Design\nCreating and optimizing new medical devices.\n\n\nTreatment Plans\nGenerating personalized patient treatment plans.\n\n\nMedical Imaging\nImproving and reconstructing radiological images. MIT CSAIL‚Äôs AI reduced false positives by 37.3%, biopsy rates by 27.8%, and unnecessary surgeries by over 30%.\n\n\nDiagnostics & Triage\nSupporting diagnosis with uncertainty-aware modeling (confidence scores).\n\n\nPatient Education\nExplaining test results, medication instructions, and drug interactions in plain language.\n\n\n\nWhat makes healthcare different:\nGenAI develops new drugs and treatments, designs medical devices, creates personalized treatment plans, and generates patient documentation on instructions, risks, and drug interactions. Patient safety and privacy (HIPAA) are non-negotiable.\n\nExamples from Nuance (Microsoft), Mayo Clinic, DeepMind, and Insilico Medicine.\nView complete healthcare implementation guide ‚Üí"
  },
  {
    "objectID": "index.html#governance-risk-warnings",
    "href": "index.html#governance-risk-warnings",
    "title": "Generative Models Overview",
    "section": "8 Governance & Risk Warnings",
    "text": "8 Governance & Risk Warnings\nGenerative AI introduces exciting new capabilities‚Äîbut also carries unique risks that traditional analytics and rules-based systems did not. Without strong governance, these risks can quickly undermine trust, compliance, and effectiveness.\nWhat Happens When Governance Fails\nWe‚Äôre already seeing what goes wrong when organizations skip governance.\nBank of America found out in 2020 during the COVID unemployment surge. They deployed automated fraud detection‚Äîbasically a simple set of flags that would freeze suspicious accounts. It caught fraud, sure, but it also locked tens of thousands of legitimate customers out of their unemployment benefits. And the bank had long hold times and limited staff to help people unfreeze accounts, which made a bad situation worse. The bill came to $225 million in regulatory fines, plus hundreds of millions more in customer refunds. Regulators said it violated the Electronic Fund Transfer Act. The real problem wasn‚Äôt the automation‚Äîit was taking humans out of the loop without building in ways to catch and fix mistakes.\nIn 2024, the SEC went after two investment advisers for what they called ‚ÄúAI-washing‚Äù‚Äîbasically lying about AI capabilities to attract investors. Delphia claimed it used machine learning to ‚Äúpredict which companies and trends are about to make it big‚Äù using client data. They‚Äôd been saying this since 2019. Global Predictions went even further, calling itself the ‚Äúfirst regulated AI financial advisor‚Äù with ‚Äúexpert AI-driven forecasts.‚Äù When SEC examiners showed up and asked for documentation? There wasn‚Äôt any. Delphia had actually admitted back in 2021 they didn‚Äôt have the algorithm they‚Äôd been marketing, then kept making the same claims for two more years. The penalties hit $400,000 combined, plus censures. SEC Chair Gary Gensler didn‚Äôt sugarcoat it: ‚ÄúInvestment advisers should not mislead the public by saying they are using an AI model when they are not.‚Äù\nKey Risks to Watch:\n\nInaccuracy & Hallucination: GenAI can confidently generate responses that sound right‚Äîbut are completely wrong or misleading.\nBias & Fairness: Models can unintentionally reinforce historical bias found in training data. These outcomes could affect customers or patients.\nSecurity & Privacy: Prompts or training data may inadvertently expose sensitive, private, or proprietary information.\nOvertrust: Users may take outputs at face value without critical thinking, especially in high-volume environments like BI dashboards or chatbots.\nRegulatory Exposure: Lack of transparency, explainability, or auditability may put the organization at odds with standards such as OCC guidelines, HIPAA, GDPR, or emerging AI laws.\n\n\nThese risks are not hypothetical. McKinsey surveys show that 51% of organizations using AI have already experienced at least one negative consequence, with inaccuracy being the most commonly reported issue. High-performing organizations‚Äîthose deploying more AI use cases‚Äîexperience more problems but also invest more heavily in mitigation. The average organization now actively addresses four AI-related risks, double the number from just three years ago.\n\nGovernance Practices to Put in Place:\n\nHuman-in-the-Loop (HITL): Require review and validation for GenAI-generated content in regulated or customer-facing contexts.\nExplainability & Traceability: Where possible, use interpretable model frameworks, or add metadata like model version, confidence score, or decision path.\nPrompt & Output Logging: Maintain logs of GenAI usage for auditing, debugging, and continuous refinement.\nAccess Control & Masking: Limit who can access GenAI systems and ensure sensitive data is redacted before prompt injection or model training.\nAlignment with Ethical and Regulatory Frameworks: Embed enterprise values and industry-specific compliance into your AI lifecycle‚Äîfrom design to deployment.\n\n\nBottom line: In highly regulated industries like banking and healthcare, governance isn‚Äôt just a best practice‚Äîit‚Äôs a business requirement. The goal is to innovate responsibly and scale safely."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Generative Models Overview",
    "section": "9 Conclusion",
    "text": "9 Conclusion\nGenerative AI is changing how we create and use content‚Äînot just incrementally, but fundamentally. Many organizations are past the demo phase now. The real work is figuring out which use cases actually deliver value, building governance that works in practice (not just on paper), and getting this integrated into workflows people actually use. The next few years won‚Äôt be about who has the flashiest model. It‚Äôll be about execution‚Äîwho can deploy at scale without compliance blowups, team burnout, or budget overruns on projects that go nowhere. We‚Äôll see more intelligent copilots that understand business context, agents that handle complex workflows, and hopefully AI systems with governance baked in from the start instead of patched on later when regulators come knocking.\nIn financial services especially, the pattern is pretty clear. Organizations getting this right treat AI as business transformation, not a technology project. They ask the hard questions early: Does this solve a real problem? How do we know it‚Äôs working? What‚Äôs our plan when it fails? Can we actually explain this to regulators? Skip those questions and you end up like the examples we covered‚ÄîBank of America paying $225 million in fines, investment advisers getting hit for making false AI claims. The penalties are real, the reputational damage sticks around, and trying to retrofit governance after the fact is brutal.\nThe competitive advantage goes to companies that figure out the balance‚Äîmoving fast while staying compliant, automating intelligently while keeping humans in the right places. Everyone else gets left behind."
  },
  {
    "objectID": "diffusion.html",
    "href": "diffusion.html",
    "title": "Diffusion Models",
    "section": "",
    "text": "Diffusion models are a powerful class of generative models that learn to create data‚Äîsuch as images‚Äîby reversing a gradual noising process. During training, real data is progressively corrupted by adding small amounts of Gaussian noise over many steps until it becomes nearly indistinguishable from pure noise. A neural network is then trained to learn the reverse process: transforming noise back into realistic samples, one step at a time.\n\n\n\nDiffusion models start by gradually adding noise to real data, then learn to reverse this process step by step to generate realistic samples.\n\n\nAdapted from the CVPR 2023 Tutorial on Diffusion Models by Arash Vahdat.\nThis approach has enabled state-of-the-art results in image generation, powering tools like DALL¬∑E 2, Imagen, and Stable Diffusion. One of the key advantages of diffusion models lies in their training stability and output quality, especially when compared to earlier generative approaches:\n\nGANs generate sharp images but rely on adversarial training, which can be unstable and prone to mode collapse.\nVAEs are more stable but often produce blurry outputs due to their reliance on Gaussian assumptions and variational approximations.\nNormalizing Flows provide exact log-likelihoods and stable training but require invertible architectures, which limit model expressiveness.\nDiffusion models avoid adversarial dynamics and use a simple denoising objective. This makes them easier to train and capable of producing highly detailed and diverse samples.\n\nThis combination of theoretical simplicity, training robustness, and high-quality outputs has made diffusion models one of the most effective generative modeling techniques in use today.\n\n\nDiffusion models may seem very different from VAEs at first glance, but they share a surprising number of structural similarities. Both involve a forward process that adds noise and a reverse process that reconstructs data. And both optimize a form of the ELBO ‚Äî though with very different interpretations.\n\n\n\nVisual comparison of latent structure in VAEs vs.¬†Diffusion Models.\n\n\nThe table below highlights key conceptual parallels:\n\n\n\n\n\n\n\n\nAspect\nVAEs\nDiffusion Models\n\n\n\n\nForward process\nLearned encoder \\(q_\\phi(z \\mid x)\\)\nFixed noising process \\(q(x_t \\mid x_{t-1})\\)\n\n\nReverse process\nLearned decoder \\(p_\\theta(x \\mid z)\\)\nLearned denoising model \\(p_\\theta(x_{t-1} \\mid x_t)\\)\n\n\nLatent space\nExplicit latent variable \\(z\\)\nNo explicit latent; \\(x_t\\) acts as noisy latent\n\n\nTraining objective\nMaximize ELBO over \\(z\\)\nMaximize ELBO via simplified KL terms and noise prediction\n\n\n\n\n\n\nDiffusion models have rapidly moved from research labs to real-world applications. Today, they power many state-of-the-art generative tools:\n\n\n\nDALL¬∑E 2 (OpenAI): Generates realistic images from text prompts.\nImagen (Google): Leverages powerful language encoders for high-fidelity image synthesis.\n\n\n\n\n\nMake-A-Video (Meta): Extends image models to video generation using textual input.\nImagen Video (Google): Builds on Imagen to generate coherent video sequences.\n\n\n\n\n\nDreamFusion (Google): Produces 3D scenes from text by combining diffusion with 3D rendering techniques.\n\nThese use cases highlight why diffusion models are considered one of the most promising families of generative models today. To understand how these systems operate, we‚Äôll first explore the mathematical core of diffusion models."
  },
  {
    "objectID": "diffusion.html#introduction",
    "href": "diffusion.html#introduction",
    "title": "Diffusion Models",
    "section": "",
    "text": "Diffusion models are a powerful class of generative models that learn to create data‚Äîsuch as images‚Äîby reversing a gradual noising process. During training, real data is progressively corrupted by adding small amounts of Gaussian noise over many steps until it becomes nearly indistinguishable from pure noise. A neural network is then trained to learn the reverse process: transforming noise back into realistic samples, one step at a time.\n\n\n\nDiffusion models start by gradually adding noise to real data, then learn to reverse this process step by step to generate realistic samples.\n\n\nAdapted from the CVPR 2023 Tutorial on Diffusion Models by Arash Vahdat.\nThis approach has enabled state-of-the-art results in image generation, powering tools like DALL¬∑E 2, Imagen, and Stable Diffusion. One of the key advantages of diffusion models lies in their training stability and output quality, especially when compared to earlier generative approaches:\n\nGANs generate sharp images but rely on adversarial training, which can be unstable and prone to mode collapse.\nVAEs are more stable but often produce blurry outputs due to their reliance on Gaussian assumptions and variational approximations.\nNormalizing Flows provide exact log-likelihoods and stable training but require invertible architectures, which limit model expressiveness.\nDiffusion models avoid adversarial dynamics and use a simple denoising objective. This makes them easier to train and capable of producing highly detailed and diverse samples.\n\nThis combination of theoretical simplicity, training robustness, and high-quality outputs has made diffusion models one of the most effective generative modeling techniques in use today.\n\n\nDiffusion models may seem very different from VAEs at first glance, but they share a surprising number of structural similarities. Both involve a forward process that adds noise and a reverse process that reconstructs data. And both optimize a form of the ELBO ‚Äî though with very different interpretations.\n\n\n\nVisual comparison of latent structure in VAEs vs.¬†Diffusion Models.\n\n\nThe table below highlights key conceptual parallels:\n\n\n\n\n\n\n\n\nAspect\nVAEs\nDiffusion Models\n\n\n\n\nForward process\nLearned encoder \\(q_\\phi(z \\mid x)\\)\nFixed noising process \\(q(x_t \\mid x_{t-1})\\)\n\n\nReverse process\nLearned decoder \\(p_\\theta(x \\mid z)\\)\nLearned denoising model \\(p_\\theta(x_{t-1} \\mid x_t)\\)\n\n\nLatent space\nExplicit latent variable \\(z\\)\nNo explicit latent; \\(x_t\\) acts as noisy latent\n\n\nTraining objective\nMaximize ELBO over \\(z\\)\nMaximize ELBO via simplified KL terms and noise prediction\n\n\n\n\n\n\nDiffusion models have rapidly moved from research labs to real-world applications. Today, they power many state-of-the-art generative tools:\n\n\n\nDALL¬∑E 2 (OpenAI): Generates realistic images from text prompts.\nImagen (Google): Leverages powerful language encoders for high-fidelity image synthesis.\n\n\n\n\n\nMake-A-Video (Meta): Extends image models to video generation using textual input.\nImagen Video (Google): Builds on Imagen to generate coherent video sequences.\n\n\n\n\n\nDreamFusion (Google): Produces 3D scenes from text by combining diffusion with 3D rendering techniques.\n\nThese use cases highlight why diffusion models are considered one of the most promising families of generative models today. To understand how these systems operate, we‚Äôll first explore the mathematical core of diffusion models."
  },
  {
    "objectID": "diffusion.html#forward-diffusion-process",
    "href": "diffusion.html#forward-diffusion-process",
    "title": "Diffusion Models",
    "section": "2 Forward Diffusion Process",
    "text": "2 Forward Diffusion Process\nThe forward diffusion process gradually turns a data sample (such as an image) into pure noise by adding a little bit of random noise at each step. This process is a Markov chain, meaning each step depends only on the previous one.\n\n2.1 Start with a Data Sample\nBegin with a data point \\(x_0\\), sampled from dataset (such as a real image). The goal is to slowly corrupt \\(x_0\\) by adding noise over many steps, until it becomes indistinguishable from random Gaussian noise.\nWe‚Äôll later see that it‚Äôs also possible to sample \\(x_t\\) directly from \\(x_0\\), without simulating every step.\n\n\n2.2 Add Noise Recursively\nTo model the forward diffusion process, we recursively add Gaussian noise at each step. Each step \\(q(x_t \\mid x_{t-1})\\) is defined as a Gaussian distribution, where the mean retains a fraction of the signal from the previous step and the variance controls the amount of noise added.\nWe define the noise schedule as a sequence of small positive values \\(\\{ \\beta_1, \\beta_2, \\dots, \\beta_T \\}\\), where \\(\\beta_t \\in (0, 1)\\) controls how much noise is injected at step \\(t\\).\nTo visualize the effect of the forward process on the entire data distribution (not just individual samples), consider the following:\n\n\n\nForward diffusion process in distribution space: data distribution gradually transitions to a standard Gaussian\n\n\nAdapted from Stanford CS236: Deep Generative Models (Winter 2023).\n\\[\nq(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} \\, x_{t-1}, \\beta_t \\, \\mathbf{I})\n\\]\nFor convenience, we define:\n\n\\(\\alpha_t = 1 - \\beta_t\\) (signal retention at step \\(t\\))\n\\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\) (cumulative signal retention up to step \\(t\\))\n\nThis lets us express the forward process directly in terms of the original data \\(x_0\\):\n\\[\nq(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} \\, x_0, (1 - \\bar{\\alpha}_t) \\, \\mathbf{I})\n\\]\nAt each step, the signal is scaled down by a factor of \\(\\sqrt{\\alpha_t}\\), and new Gaussian noise is added with variance \\((1 - \\alpha_t)\\). After many steps, the sample becomes increasingly noisy and approaches an isotropic Gaussian distribution.\n\nIntuition: The signal shrinks gradually while fresh noise is added. Over many steps, the data becomes indistinguishable from pure noise.\n\n\n\n\nForward diffusion process: gradually adds Gaussian noise to the input image across \\(T\\) steps.\n\n\nAdapted from the CVPR 2023 Tutorial on Diffusion Models by Arash Vahdat.\n\n\n\n\n\n\nWhy keep \\(\\beta_t\\) small?\n\n\n\nChoosing small values of \\(\\beta_t\\) ensures that noise is added gradually at each step, allowing the model to retain structure over time. This makes the reverse process easier to learn, as the signal decays smoothly via \\(\\sqrt{\\bar{\\alpha}_t}\\) rather than being overwhelmed by noise. Large \\(\\beta_t\\) values, on the other hand, can destroy signal too quickly, leading to poor reconstructions.\n\n\nThe choice of \\(\\beta_t\\) at each step‚Äîknown as the noise schedule‚Äîplays a critical role in how effectively the model can learn the reverse denoising process. A well-designed schedule balances signal retention and noise addition, ensuring gradual degradation of structure without overwhelming the data too early. In the next section, we compare common scheduling strategies and their effect on training and image quality.\n\n\n2.3 Noise Schedules: Linear vs Cosine\nThe noise schedule determines how much Gaussian noise is added at each step via the sequence \\(\\{\\beta_1, \\beta_2, \\dots, \\beta_T\\}\\). A well-designed schedule ensures that noise increases gradually‚Äîpreserving structure early on and enabling the model to learn the reverse process more effectively.\nTwo commonly used schedules are:\nLinear Schedule\nIn a linear schedule, \\(\\beta_t\\) increases linearly from a small to a larger value. This causes the retained signal \\(\\bar{\\alpha}_t\\) to decay rapidly, injecting noise aggressively in early steps. As a result, fine details are lost early, making denoising more difficult.\nCosine Schedule\nCosine schedules are designed to make \\(\\bar{\\alpha}_t\\) follow a smooth cosine decay curve. This retains more signal over a longer period, allowing the model to degrade structure more gradually and learn a more stable denoising path.\n\n2.3.1 Visualizing Signal and Noise Tradeoffs\nThe following plot compares signal preservation and noise growth across the diffusion process for linear and cosine schedules:\n\n\n\nComparison of signal and noise schedules\n\n\nThe cosine schedule maintains more signal during early and middle steps, whereas the linear schedule injects noise quickly, causing structure to be lost early in the process.\n\n\n2.3.2 Visualizing Effects on Images\nThis plot shows how an image is progressively corrupted under each schedule:\n\n\n\nLatent image degradation under linear vs cosine noise schedules\n\n\nAdapted from Nichol & Dhariwal (2021), ‚ÄúImproved Denoising Diffusion Probabilistic Models‚Äù, arXiv:2102.09672.\nCosine schedules preserve details longer, while linear schedules degrade the image more aggressively in fewer steps‚Äîhighlighting why noise scheduling is critical for downstream generation quality.\n\n\n2.3.3 Takeaways\n\nCosine schedules produce better sample quality by preserving image structure longer.\nLinear schedules are simpler but may hinder training due to early signal loss.\nMost modern diffusion models (e.g., Stable Diffusion, Imagen) prefer cosine schedules for their superior denoising behavior.\n\n\n\n\n\n\n\nPractical Guidance: Choosing a Noise Schedule\n\n\n\n\nUse a cosine schedule if your goal is to generate high-quality samples or train a stable diffusion model. It preserves structure longer and results in smoother denoising.\nA linear schedule may be acceptable for simple tasks or quick experimentation but can inject noise too aggressively in early steps.\nAlways visualize your \\(\\bar{\\alpha}_t\\) or signal-to-noise ratio across timesteps to understand how your schedule behaves.\nMany libraries (e.g., Hugging Face diffusers, OpenAI‚Äôs improved DDPM repo) implement cosine schedules by default‚Äîuse these as reliable starting points.\n\n\n\n\n\n\n2.4 The Markov Chain\nThe forward diffusion process forms a Markov chain: each state \\(x_t\\) depends only on the previous state \\(x_{t-1}\\). This property enables efficient sampling and simplifies the math behind both training and generation.\nThe full sequence can be written as:\n\\[\nx_0 \\rightarrow x_1 \\rightarrow x_2 \\rightarrow \\cdots \\rightarrow x_T\n\\]\nThe joint probability over all noisy samples given the original data \\(x_0\\) is:\n\\[\nq(x_{1:T} \\mid x_0) = \\prod_{t=1}^{T} q(x_t \\mid x_{t-1})\n\\]\nThis formulation means we can generate the full noise trajectory by repeatedly applying the noise step defined earlier.\n\n\n\n\n\n\nWhy it‚Äôs still useful to model the full chain\n\n\n\nInsight: Even though we can sample any \\(x_t\\) directly from \\(x_0\\) using a closed-form Gaussian, modeling the full Markov chain is still essential. It forms the foundation for defining the reverse (denoising) process, which we‚Äôll learn next.\n\n\n\n\n2.5 Deriving the Marginal Distribution \\(q(x_t \\mid x_0)\\)\nSo far, we‚Äôve modeled the full Markov chain step-by-step. But how do we compute \\(x_t\\) directly from \\(x_0\\), without simulating each intermediate step?\nLet‚Äôs see how \\(x_t\\) accumulates noise from \\(x_0\\).\nFor \\(t = 1\\): \\[\nx_1 = \\sqrt{\\alpha_1} x_0 + \\sqrt{1 - \\alpha_1} \\epsilon_1, \\qquad \\epsilon_1 \\sim \\mathcal{N}(0, I)\n\\]\nFor \\(t = 2\\): \\[\nx_2 = \\sqrt{\\alpha_2} x_1 + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\] Substitute \\(x_1\\): \\[\nx_2 = \\sqrt{\\alpha_2} \\left( \\sqrt{\\alpha_1} x_0 + \\sqrt{1 - \\alpha_1} \\epsilon_1 \\right) + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\] \\[\n= \\sqrt{\\alpha_2 \\alpha_1} x_0 + \\sqrt{\\alpha_2 (1 - \\alpha_1)} \\epsilon_1 + \\sqrt{1 - \\alpha_2} \\epsilon_2\n\\]\nFor general \\(t\\), recursively expanding gives: \\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sum_{i=1}^t \\left( \\sqrt{ \\left( \\prod_{j=i+1}^t \\alpha_j \\right) (1 - \\alpha_i) } \\, \\epsilon_i \\right)\n\\] where \\(\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i\\).\nEach \\(\\epsilon_i\\) is independent Gaussian noise. The sum of independent Gaussians (each scaled by a constant) is still a Gaussian, with variance equal to the sum of the variances: \\[\n\\text{Total variance} = \\sum_{i=1}^t \\left( \\prod_{j=i+1}^t \\alpha_j \\right) (1 - \\alpha_i)\n\\] This sum simplifies to: \\[\n1 - \\bar{\\alpha}_t\n\\]\nThis can be proved by induction or by telescoping the sum.\nAll the little bits of noise added at each step combine into one big Gaussian noise term, with variance \\(1 - \\bar{\\alpha}_t\\).\n\n\n2.6 The Final Marginal Distribution\nSo, we can sample \\(x_t\\) directly from \\(x_0\\) using: \\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, I)\n\\]\nThis lets us sample \\(x_t\\) directly from \\(x_0\\), without recursively computing all previous steps \\(x_1, x_2, \\dots, x_{t-1}\\).\nThis means: \\[\nq(x_t \\mid x_0) = \\mathcal{N}\\left(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I\\right)\n\\]\nAs \\(t\\) increases, \\(\\bar{\\alpha}_t\\) shrinks toward zero. Eventually, \\(x_t\\) becomes pure noise:\n\\[\nx_T \\sim \\mathcal{N}(0, I)\n\\]\n\n\n\nInstead of simulating each step recursively, we can directly compute \\(x_t\\) from \\(x_0\\) using the closed-form expression.\n\n\nAdapted from the CVPR 2023 Tutorial on Diffusion Models by Arash Vahdat.\n\n\n2.7 Recap: Forward Diffusion Steps\n\n\n\n\n\n\n\n\nStep\nFormula\nExplanation\n\n\n\n\n1\n\\(x_0\\)\nOriginal data sample\n\n\n2\n\\(q(x_t \\mid x_{t-1}) = \\mathcal{N}(\\sqrt{\\alpha_t} x_{t-1}, (1-\\alpha_t) I)\\)\nAdd noise at each step\n\n\n3\n\\(x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\\)\nDirectly sample \\(x_t\\) from \\(x_0\\) using noise \\(\\epsilon\\)\n\n\n4\n\\(q(x_t \\mid x_0) = \\mathcal{N}(\\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I)\\)\nMarginal distribution at step \\(t\\)\n\n\n5\n\\(x_T \\sim \\mathcal{N}(0, I)\\)\nAfter many steps, pure noise\n\n\n\n\n\n2.8 Key Takeaways\n\nThe forward diffusion process is just repeatedly adding noise to your data.\nThanks to properties of Gaussian noise, you can describe the result as the original data scaled down plus one cumulative chunk of Gaussian noise.\n\nAfter enough steps, the data becomes indistinguishable from random noise."
  },
  {
    "objectID": "diffusion.html#reverse-diffusion-process",
    "href": "diffusion.html#reverse-diffusion-process",
    "title": "Diffusion Models",
    "section": "3 Reverse Diffusion Process",
    "text": "3 Reverse Diffusion Process\nLet‚Äôs break down the reverse diffusion process step by step. This is the generative phase of diffusion models, where we learn to turn pure noise back into data. For clarity, we‚Äôll use the same notation as in the forward process:\n\nForward process: Gradually adds noise to data via \\(q(x_t \\mid x_{t-1})\\)\nReverse process: Gradually removes noise via \\(p_\\theta(x_{t-1} \\mid x_t)\\), learned by a neural network\n\n\n\n\nReverse process: a neural network learns to iteratively denoise samples from \\(x_T\\) back to \\(x_0\\).\n\n\nAdapted from the CVPR 2023 Tutorial on Diffusion Models by Arash Vahdat.\nThe Goal of the Reverse Process\nObjective: Given a noisy sample \\(x_t\\), we want to estimate the conditional distribution \\(q(x_{t-1} \\mid x_t)\\). However, this is intractable because it would require knowing the true data distribution.\nInstead, we train a neural network to approximate it: \\[\np_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))\n\\]\nHere, \\(\\mu_\\theta(x_t, t)\\) is the predicted mean and \\(\\Sigma_\\theta(x_t, t)\\) is the predicted covariance (often diagonal) of the reverse Gaussian distribution. In many implementations, the variance \\(\\Sigma_\\theta(x_t, t)\\) is either fixed or parameterized separately, so the model focuses on learning the mean \\(\\mu_\\theta(x_t, t)\\) during training.\nIn practice, many diffusion models do not directly predict \\(\\mu_\\theta\\) or \\(x_0\\), but instead predict the noise \\(\\epsilon\\) added in the forward process. This makes the objective simpler and more effective, as we‚Äôll see in the next section.\nKey Insight from the Forward Process\nIf the noise added in the forward process is small (i.e., \\(\\beta_t \\ll 1\\)), then the reverse conditional \\(q(x_{t-1} \\mid x_t)\\) is also Gaussian: \\[\nq(x_{t-1} \\mid x_t) \\approx \\mathcal{N}(x_{t-1}; \\tilde{\\mu}_t(x_t), \\tilde{\\beta}_t I)\n\\]\nThis approximation works because the forward process adds Gaussian noise in small increments at each step. The Markov chain formed by these small Gaussian transitions ensures that local conditionals (like \\(q(x_{t-1} \\mid x_t)\\)) remain Gaussian under mild assumptions.\n\n\n\n\n\n\nGlossary of Symbols\n\n\n\n\n\\(\\alpha_t\\): Variance-preserving noise coefficient at step \\(t\\)\n\\(\\bar{\\alpha}_t\\): Cumulative product of \\(\\alpha_t\\), i.e., \\(\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s\\)\n\\(\\beta_t\\): Variance of the noise added at step \\(t\\), typically \\(\\beta_t = 1 - \\alpha_t\\)\n\\(x_0\\): Original clean data sample (e.g., image)\n\\(x_t\\): Noisy version of \\(x_0\\) at timestep \\(t\\)\n\\(\\epsilon\\): Standard Gaussian noise sampled from \\(\\mathcal{N}(0, I)\\)\n\\(\\tilde{\\mu}_t\\): Mean of the reverse process distribution at time \\(t\\)\n\\(\\tilde{\\beta}_t\\): Variance of the reverse process distribution at time \\(t\\)\n\n\n\n\n3.1 Deriving \\(q(x_{t-1} \\mid x_t, x_0)\\) Using Bayes‚Äô Rule\nWe can‚Äôt directly evaluate \\(q(x_{t-1} \\mid x_t)\\), but we can derive the posterior \\(q(x_{t-1} \\mid x_t, x_0)\\) using Bayes‚Äô rule:\n\\[\nq(x_{t-1} \\mid x_t, x_0) = \\frac{q(x_t \\mid x_{t-1}, x_0) \\cdot q(x_{t-1} \\mid x_0)}{q(x_t \\mid x_0)}\n\\]\nFrom the forward process, we know:\n\n\\(q(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1},\\, \\beta_t I)\\)\n\n\\(q(x_{t-1} \\mid x_0) = \\mathcal{N}(x_{t-1}; \\sqrt{\\bar{\\alpha}_{t-1}} x_0,\\, (1 - \\bar{\\alpha}_{t-1}) I)\\)\n\n\\(q(x_t \\mid x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0,\\, (1 - \\bar{\\alpha}_t) I)\\)\n\nTo derive a usable form of the posterior, we substitute the Gaussian densities into Bayes‚Äô rule. The multivariate normal density is:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\Sigma) \\propto \\exp\\left( -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\n\\]\nSince all covariances here are multiples of the identity matrix, \\(\\Sigma = \\sigma^2 I\\), the formula simplifies to:\n\\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\|x - \\mu\\|^2 \\right)\n\\]\n\n\n\n\n\n\nUnderstanding the squared norm\n\n\n\nThe expression \\(\\|x - \\mu\\|^2\\) is the squared distance between two vectors. In 1D, it‚Äôs just \\((x - \\mu)^2\\), but in higher dimensions, it becomes:\n\\[\n\\|x - \\mu\\|^2 = \\sum_{i=1}^d (x_i - \\mu_i)^2\n\\]\nThis term appears in the exponent of the Gaussian and represents how far the sample is from the center (mean), scaled by the variance.\n\n\nApplying this to the forward process terms:\n\n\\(q(x_t \\mid x_{t-1}) \\propto \\exp\\left( -\\frac{1}{2\\beta_t} \\| x_t - \\sqrt{\\alpha_t} x_{t-1} \\|^2 \\right)\\)\n\n\\(q(x_{t-1} \\mid x_0) \\propto \\exp\\left( -\\frac{1}{2(1 - \\bar{\\alpha}_{t-1})} \\| x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} x_0 \\|^2 \\right)\\)\n\nWe can ignore \\(q(x_t \\mid x_0)\\) in the denominator, since it is independent of \\(x_{t-1}\\) and will be absorbed into a proportionality constant.\nPutting these together:\n\\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n-\\frac{1}{2} \\left[\n\\frac{ \\|x_t - \\sqrt{\\alpha_t} x_{t-1} \\|^2 }{\\beta_t} +\n\\frac{ \\| x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} x_0 \\|^2 }{1 - \\bar{\\alpha}_{t-1}}\n\\right]\n\\right)\n\\]\n\n\n\n\n\n\nWhy does the product of Gaussians give another Gaussian?\n\n\n\nWhen we multiply two Gaussian distributions over the same variable, the result is also a Gaussian.\nHere, we are multiplying two Gaussians in \\(x_{t-1}\\):\n- One centered at \\(\\sqrt{\\alpha_t} x_t\\)\n- One centered at \\(\\sqrt{\\bar{\\alpha}_{t-1}} x_0\\)\nThe product is another Gaussian in \\(x_{t-1}\\), with a new mean that is a weighted average of both.\nWe‚Äôll derive this explicitly by completing the square in the exponent.\n\n\n\nAlthough we won‚Äôt use this posterior directly during sampling, this closed-form expression is essential for defining the ELBO used in training. It gives us a precise target that the reverse model attempts to approximate.\n\nWe now complete the square to put the expression into standard Gaussian form.\n\n\n3.2 Complete the square\nWe complete the square by rewriting the quadratic expression in a way that matches the standard form of a Gaussian. This lets us rewrite the posterior \\(q(x_{t-1} \\mid x_t, x_0)\\) in standard Gaussian form by identifying its mean and variance.\n\\[\na x^2 - 2 b x = a \\left( x - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a}\n\\]\nFrom earlier, we arrived at this expression for the exponent of the posterior:\n\\[\n-\\frac{1}{2} \\left[\n\\frac{(x_t - \\sqrt{\\alpha_t} \\, x_{t-1})^2}{\\beta_t} +\n\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0)^2}{1 - \\bar{\\alpha}_{t-1}}\n\\right]\n\\]\nWe expand both terms:\nFirst term:\n\\[\n\\frac{(x_t - \\sqrt{\\alpha_t} \\, x_{t-1})^2}{\\beta_t}\n= \\frac{x_t^2 - 2 \\sqrt{\\alpha_t} \\, x_t x_{t-1} + \\alpha_t x_{t-1}^2}{\\beta_t}\n\\]\nSecond term:\n\\[\n\\frac{(x_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0)^2}{1 - \\bar{\\alpha}_{t-1}}\n= \\frac{x_{t-1}^2 - 2 \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_{t-1} x_0 + \\bar{\\alpha}_{t-1} x_0^2}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nGroup like terms\nNow we collect all the terms involving \\(x_{t-1}\\):\nCoefficient of \\(x_{t-1}^2\\):\n\\[\na = \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nCoefficient of \\(x_{t-1}\\) (the full linear term):\n\\[\n-2 \\left(\n\\frac{ \\sqrt{\\alpha_t} \\, x_t }{ \\beta_t } + \\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0 }{ 1 - \\bar{\\alpha}_{t-1} }\n\\right)\n\\]\nSo we define:\n\\[\nb = \\frac{ \\sqrt{\\alpha_t} \\, x_t }{ \\beta_t } + \\frac{ \\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0 }{ 1 - \\bar{\\alpha}_{t-1} }\n\\]\nRemaining terms (like \\(x_t^2\\) and \\(x_0^2\\)) are independent of \\(x_{t-1}\\) and can be absorbed into a constant.\nWe are modeling the conditional distribution \\(q(x_{t-1} \\mid x_t, x_0)\\), which means both \\(x_t\\) and \\(x_0\\) are known and fixed. So any expression involving only \\(x_t\\) or \\(x_0\\) behaves like a constant and does not influence the shape of the Gaussian over \\(x_{t-1}\\).\nThe exponent now has the form:\n\\[\n-\\frac{1}{2} \\left( a x_{t-1}^2 - 2 b x_{t-1} \\right) + \\text{(constants)}\n\\]\nApply the identity\nUsing the identity: \\[\na x^2 - 2 b x = a \\left( x - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a}\n\\]\nwe rewrite the exponent: \\[\n-\\frac{1}{2} \\left( a x_{t-1}^2 - 2 b x_{t-1} \\right)\n= -\\frac{1}{2} \\left[ a \\left( x_{t-1} - \\frac{b}{a} \\right)^2 - \\frac{b^2}{a} \\right]\n\\]\nWe drop the constant term \\(\\frac{b^2}{a}\\) under proportionality. This transforms the exponent into the Gaussian form: \\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n- \\frac{1}{2 \\tilde{\\beta}_t} \\| x_{t-1} - \\tilde{\\mu}_t \\|^2\n\\right)\n\\]\n\n\n\n\n\n\nNote: This matches the standard Gaussian\n\n\n\nThe standard Gaussian is written as: \\[\n\\mathcal{N}(x \\mid \\mu, \\sigma^2 I) \\propto \\exp\\left(\n- \\frac{1}{2\\sigma^2} \\| x - \\mu \\|^2\n\\right)\n\\]\nSo in our case:\n\n\\(\\tilde{\\mu}_t = \\frac{b}{a}\\) is the mean\n\\(\\tilde{\\beta}_t = \\frac{1}{a}\\) is the variance\n\nWe keep the notation \\(\\tilde{\\beta}_t\\) instead of \\(\\sigma^2\\) because it connects directly to the noise schedule (\\(\\beta_t\\), \\(\\bar{\\alpha}_t\\)) used in the diffusion model. This helps tie everything back to how the forward and reverse processes relate.\n\n\nFinal expressions\nNow we can directly read off the expressions for the mean and variance from the completed square.\nWe had: \\[\na = \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}, \\quad\nb = \\frac{\\sqrt{\\alpha_t} \\, x_t}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\, x_0}{1 - \\bar{\\alpha}_{t-1}}\n\\]\nFrom the identity: \\[\nq(x_{t-1} \\mid x_t, x_0) \\propto \\exp\\left(\n- \\frac{1}{2 \\tilde{\\beta}_t} \\| x_{t-1} - \\tilde{\\mu}_t \\|^2\n\\right)\n\\]\nwe identify: - \\(\\tilde{\\mu}_t = \\frac{b}{a}\\), - \\(\\tilde{\\beta}_t = \\frac{1}{a}\\)\nLet‚Äôs compute these explicitly:\nMean: \\[\n\\tilde{\\mu}_t = \\frac{b}{a} =\n\\frac{\n\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) x_t +\n\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t x_0\n}{\n1 - \\bar{\\alpha}_t\n}\n\\]\nVariance: \\[\n\\tilde{\\beta}_t = \\frac{1}{a}\n= \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t\n\\]\nSo the final expression for the posterior becomes: \\[\nq(x_{t-1} \\mid x_t, x_0) = \\mathcal{N}(x_{t-1};\\, \\tilde{\\mu}_t,\\, \\tilde{\\beta}_t I)\n\\]\n\n\n3.3 Parameterizing the Reverse Process\nDuring training, we can compute the posterior exactly because \\(x_0\\) is known. But at sampling time, we don‚Äôt have access to \\(x_0\\), so we must express everything in terms of the current noisy sample \\(x_t\\) and the model‚Äôs prediction of noise \\(\\epsilon\\).\nWe start from the forward noising equation:\n\\[\nx_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\n\\]\nThis expresses how noise is added to the clean image \\(x_0\\) to produce the noisy observation \\(x_t\\).\nWe rearrange this to solve for \\(x_0\\) in terms of \\(x_t\\) and \\(\\epsilon\\):\n\\[\nx_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon}{\\sqrt{\\bar{\\alpha}_t}}\n\\]\nNow we substitute this into the posterior mean expression \\(\\tilde{\\mu}_t\\), which originally depended on \\(x_0\\):\n\\[\n\\tilde{\\mu}_t =\n\\frac{\n\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) x_t +\n\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t x_0\n}{\n1 - \\bar{\\alpha}_t\n}\n\\]\nSubstituting \\(x_0\\) into this gives:\n\\[\n\\tilde{\\mu}_t =\n\\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon \\right)\n\\]\nThis allows us to compute the mean of the reverse process using only \\(x_t\\), \\(\\epsilon\\), and known scalars from the noise schedule.\n\n\\(\\epsilon\\) is the noise that was added to \\(x_0\\) to get \\(x_t\\)\nAt test time, we use the model‚Äôs prediction \\(\\epsilon_\\theta(x_t, t)\\) in its place\n\n\n\n3.4 Recap: Reverse Diffusion Steps\n\n\n\n\n\n\n\n\nStep\nFormula\nExplanation\n\n\n\n\n1\n\\(q(x_{t-1} \\mid x_t, x_0)\\)\nTrue posterior used during training (when \\(x_0\\) is known)\n\n\n2\n\\(\\tilde{\\mu}_t = \\dfrac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\dfrac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon \\right)\\)\nPosterior mean rewritten using \\(x_t\\) and noise\n\n\n3\n\\(\\epsilon \\approx \\epsilon_\\theta(x_t, t)\\)\nAt test time, model predicts the noise\n\n\n4\n\\(p_\\theta(x_{t-1} \\mid x_t) = \\mathcal{N}(\\tilde{\\mu}_t, \\tilde{\\beta}_t I)\\)\nReverse step sampled from model‚Äôs predicted mean and fixed variance\n\n\n\n\n\n3.5 Key Takeaways\n\nThe reverse diffusion process defines a learned Markov chain that gradually removes noise from the input.\nAlthough we can‚Äôt compute the true reverse distribution \\(q(x_{t-1} \\mid x_t)\\), we derive a tractable Gaussian approximation using Bayes‚Äô rule.\nThe reverse mean depends on both \\(x_t\\) and the original data \\(x_0\\), but at test time we use the model‚Äôs prediction of noise \\(\\epsilon_\\theta(x_t, t)\\) to estimate \\(x_0\\).\nThe reverse steps preserve a Gaussian structure, enabling efficient sampling using the predicted mean \\(\\mu_\\theta(x_t, t)\\) and fixed variance \\(\\Sigma_\\theta(x_t, t)\\)."
  },
  {
    "objectID": "diffusion.html#training-understanding-the-elbo",
    "href": "diffusion.html#training-understanding-the-elbo",
    "title": "Diffusion Models",
    "section": "4 Training: Understanding the ELBO",
    "text": "4 Training: Understanding the ELBO\nWhat is the Goal? The ultimate goal in diffusion models is to train the neural network so that it can reverse the noising process. In other words, we want the network to learn how to turn random noise back into realistic data (like images). But how do we actually train the network? We need a loss function‚Äîa way to measure how good or bad the network‚Äôs predictions are, so we can improve it.\n\n4.1 What is the ELBO?\nThe ELBO is a lower bound on the log-likelihood of the data. Maximizing the ELBO is equivalent to maximizing the likelihood that the model can generate the training data. For diffusion models, the ELBO ensures that the reverse process (denoising) aligns with the forward process (noising).\nNote: For a foundational introduction to the ELBO and its role in generative models, see my Variational Autoencoder (VAE) post at changezakram.github.io/Deep-Generative-Models/vae.html.\n\n\n4.2 Deriving the ELBO for Diffusion Models\nGoal:\nWe want to maximize the log-likelihood of the data:\n\\[\n\\log p_\\theta(x_0)\n\\]\nwhere \\(x_0\\) is a clean data sample (e.g., an image).\nProblem:\nComputing \\(\\log p_\\theta(x_0)\\) directly is intractable because it involves integrating over all possible noisy intermediate states \\(x_{1:T}\\).\nSolution:\nUse Jensen‚Äôs Inequality to derive a lower bound (the ELBO) that we can optimize instead.\n\n\n4.3 Full Derivation (Step-by-Step)\nStep 1: Start with the log-likelihood\n\\[\n\\log p_\\theta(x_0) = \\log \\int p_\\theta(x_{0:T}) \\, dx_{1:T}\n\\]\nStep 2: Introduce the forward process \\(q(x_{1:T} \\mid x_0)\\)\nMultiply and divide by the fixed forward process:\n\\[\n\\log p_\\theta(x_0) = \\log \\int \\frac{p_\\theta(x_{0:T})}{q(x_{1:T} \\mid x_0)} q(x_{1:T} \\mid x_0) \\, dx_{1:T}\n\\]\nStep 3: Rewrite as an expectation\n\\[\n\\log p_\\theta(x_0) = \\log \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[ \\frac{p_\\theta(x_{0:T})}{q(x_{1:T} \\mid x_0)} \\right]\n\\]\nStep 4: Apply Jensen‚Äôs Inequality\n\\[\n\\log p_\\theta(x_0) \\geq \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[ \\log \\frac{p_\\theta(x_{0:T})}{q(x_{1:T} \\mid x_0)} \\right]\n\\]\nStep 5: Expand \\(p_\\theta(x_{0:T})\\) and \\(q(x_{1:T} \\mid x_0)\\)\nThe reverse (generative) process is:\n\\[\np_\\theta(x_{0:T}) = p(x_T) \\cdot \\prod_{t=1}^T p_\\theta(x_{t-1} \\mid x_t)\n\\]\nThe forward (noising) process is:\n\\[\nq(x_{1:T} \\mid x_0) = \\prod_{t=1}^T q(x_t \\mid x_{t-1})\n\\]\nSubstitute both into the ELBO:\n\\[\n\\text{ELBO} = \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[\n\\log \\left(\n\\frac{p(x_T) \\cdot \\prod_{t=1}^T p_\\theta(x_{t-1} \\mid x_t)}\n     {\\prod_{t=1}^T q(x_t \\mid x_{t-1})}\n\\right)\n\\right]\n\\]\nSplit the logarithm:\n\\[\n\\text{ELBO} = \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[\n\\log p(x_T)\n+ \\sum_{t=1}^T \\log p_\\theta(x_{t-1} \\mid x_t)\n- \\sum_{t=1}^T \\log q(x_t \\mid x_{t-1})\n\\right]\n\\]\nGroup the terms:\n\\[\n\\text{ELBO} = \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[\n\\log p(x_T)\n+ \\sum_{t=1}^T \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_t \\mid x_{t-1})}\n\\right]\n\\]\nStep 6: Decompose the ELBO\nWe now break down the Evidence Lower Bound (ELBO) into three interpretable components:\n\nThe prior loss ‚Äî how well the final noisy sample matches the prior\n\nThe denoising KL terms ‚Äî how well the model learns to denoise at each timestep\n\nThe reconstruction loss ‚Äî how well the model recovers the original input\n\nELBO Expression from Previous Step\n\\[\n= \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[\n\\log p(x_T) + \\sum_{t=1}^T \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_t \\mid x_{t-1})}\n\\right]\n\\]\nIsolating the Reconstruction Term\nThe case for \\(t = 1\\) is special: it‚Äôs the step where the model tries to reconstruct \\(x_0\\) from \\(x_1\\). So we isolate it from the rest of the trajectory-based KL terms.\n\\[\n= \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[\n\\log p(x_T)\n+ \\sum_{t=2}^T \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_t \\mid x_{t-1})}\n+ \\log \\frac{p_\\theta(x_0 \\mid x_1)}{q(x_1 \\mid x_0)}\n\\right]\n\\]\nRewriting Using the Known Forward Process\nThe forward process gives us a complete description of how noise is added to data. Because of this, we can calculate the exact probability of earlier steps given later ones. In particular, since both \\(x_t\\) and \\(x_0\\) are known during training, we can compute the true backward distribution \\(q(x_{t-1} \\mid x_t, x_0)\\). This lets us directly compare it to the model‚Äôs learned reverse process \\(p_\\theta(x_{t-1} \\mid x_t)\\).\nThis gives:\n\\[\n= \\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[\n\\log p(x_T)\n+ \\sum_{t=2}^T \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_{t-1} \\mid x_t, x_0)}\n+ \\log p_\\theta(x_0 \\mid x_1)\n- \\log q(x_1 \\mid x_0)\n\\right]\n\\]\nThe last term, \\(\\log q(x_1 \\mid x_0)\\), comes from the known forward process and does not depend on the model parameters. Since it stays constant during training, we drop it from the objective and retain the remaining three terms.\nThe first two log-ratios can now be rewritten as KL divergences, and the third term becomes a standard reconstruction loss.\nRewriting the First Term as a KL Divergence\nWe begin with the first term from the ELBO expression:\n\\[\n\\mathbb{E}_{q(x_{1:T} \\mid x_0)} \\left[ \\log p(x_T) \\right]\n\\]\nSince this expectation only involves \\(x_T\\), we can simplify it as:\n\\[\n\\mathbb{E}_{q(x_T \\mid x_0)} \\left[ \\log p(x_T) \\right]\n\\]\nNow recall the definition of KL divergence between two distributions \\(q(x)\\) and \\(p(x)\\):\n\\[\nD_{\\text{KL}}(q(x) \\,\\|\\, p(x)) = \\mathbb{E}_{q(x)} \\left[ \\log \\frac{q(x)}{p(x)} \\right]\n= \\mathbb{E}_{q(x)} [\\log q(x)] - \\mathbb{E}_{q(x)} [\\log p(x)]\n\\]\nRearranging this gives:\n\\[\n\\mathbb{E}_{q(x)} [\\log p(x)] = -D_{\\text{KL}}(q(x) \\,\\|\\, p(x)) + \\mathbb{E}_{q(x)} [\\log q(x)]\n= -D_{\\text{KL}}(q(x) \\,\\|\\, p(x)) + \\mathbb{H}[q(x)]\n\\]\nApplying this identity to \\(q(x_T \\mid x_0)\\) ‚Äî which is analytically tractable due to the known forward process ‚Äî and the prior \\(p(x_T)\\):\n\\[\n\\mathbb{E}_{q(x_T \\mid x_0)} [\\log p(x_T)] = -D_{\\text{KL}}(q(x_T \\mid x_0) \\,\\|\\, p(x_T)) + \\mathbb{H}[q(x_T \\mid x_0)]\n\\]\nSince \\(q(x_T \\mid x_0)\\) is part of the fixed forward process, its entropy \\(\\mathbb{H}[q(x_T \\mid x_0)]\\) is independent of model parameters and can be ignored during training. So we drop it:\n\\[\n\\mathbb{E}_{q(x_T \\mid x_0)} [\\log p(x_T)]\n\\approx -D_{\\text{KL}}(q(x_T \\mid x_0) \\parallel p(x_T))\n\\quad \\text{(ignoring constant entropy term)}\n\\]\nThis shows that the first term in the ELBO corresponds to \\(D_{\\text{KL}}(q(x_T \\mid x_0) \\,\\|\\, p(x_T))\\), comparing the forward process at time \\(T\\) to the model‚Äôs prior.\nRewriting the Second Terms as KL Divergences\nNext, we consider the sum of log-ratio terms from the ELBO expression:\n\\[\n\\sum_{t=2}^T \\mathbb{E}_q \\left[ \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_{t-1} \\mid x_t, x_0)} \\right]\n\\]\nThis expression compares two distributions:\n\n\\(p_\\theta(x_{t-1} \\mid x_t)\\): the model‚Äôs learned reverse (denoising) process\n\n\\(q(x_{t-1} \\mid x_t, x_0)\\): the true posterior over \\(x_{t-1}\\) given \\(x_t\\) and the original data \\(x_0\\)\n(this is computable in closed-form since the forward process is known)\n\nNow recall the definition of KL divergence:\n\\[\nD_{\\text{KL}}(q(x) \\,\\|\\, p(x)) = \\mathbb{E}_{q(x)} \\left[ \\log \\frac{q(x)}{p(x)} \\right]\n\\]\nIf we flip the log-ratio, we get:\n\\[\n\\mathbb{E}_{q(x)} \\left[ \\log \\frac{p(x)}{q(x)} \\right] = - D_{\\text{KL}}(q(x) \\,\\|\\, p(x))\n\\]\nSo each log term becomes the negative KL divergence:\n\\[\n\\mathbb{E}_q \\left[ \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_{t-1} \\mid x_t, x_0)} \\right]\n= - D_{\\text{KL}}(q(x_{t-1} \\mid x_t, x_0) \\,\\|\\, p_\\theta(x_{t-1} \\mid x_t))\n\\]\nApplying this for every timestep from \\(t = 2\\) to \\(T\\), we get:\n\\[\n\\sum_{t=2}^T \\mathbb{E}_q \\left[ \\log \\frac{p_\\theta(x_{t-1} \\mid x_t)}{q(x_{t-1} \\mid x_t, x_0)} \\right]\n= - \\sum_{t=2}^T D_{\\text{KL}}(q(x_{t-1} \\mid x_t, x_0) \\,\\|\\, p_\\theta(x_{t-1} \\mid x_t))\n\\]\nThis shows that the middle terms in the ELBO can be rewritten as a sum of KL divergences between the true posterior and the model‚Äôs learned reverse process at each timestep.\nRewriting the Third Term as a Reconstruction Loss\nThe last part of the ELBO expression is:\n\\[\n\\mathbb{E}_q \\left[ \\log p_\\theta(x_0 \\mid x_1) \\right]\n\\]\nThis term does not involve any KL divergence ‚Äî it directly corresponds to the model‚Äôs attempt to reconstruct the original input \\(x_0\\) from \\(x_1\\).\n\n\\(x_1\\) is a slightly noisy version of \\(x_0\\) (after one step of the forward process).\n\\(p_\\theta(x_0 \\mid x_1)\\) is the model‚Äôs decoder ‚Äî it tries to map the noisy input \\(x_1\\) back to the clean data \\(x_0\\).\n\nDuring training, this term is treated as a standard log-likelihood loss. Since we want to maximize the ELBO, we want to maximize this log-probability ‚Äî which is equivalent to minimizing the negative log-likelihood:\n\\[\n- \\log p_\\theta(x_0 \\mid x_1)\n\\]\nThis is why the reconstruction term appears with a positive sign in the loss (as a value we minimize), but a negative sign inside the ELBO.\nThis is referred to as the reconstruction loss in diffusion models.\nIf \\(p_\\theta(x_0 \\mid x_1)\\) is modeled as a Gaussian, this term becomes a mean squared error between the predicted and true \\(x_0\\) values.\nELBO vs.¬†Loss\nWe write the ELBO as:\n\\[\n\\text{ELBO} =\n\\underbrace{- D_{\\text{KL}}(q(x_T \\mid x_0) \\parallel p(x_T))}_{\\mathcal{L}_T}\n\\quad\n\\underbrace{- \\sum_{t=2}^T D_{\\text{KL}}(q(x_{t-1} \\mid x_t, x_0) \\parallel p_\\theta(x_{t-1} \\mid x_t))}_{\\mathcal{L}_{1:T-1}}\n\\quad\n\\underbrace{+ \\mathbb{E}_q \\left[ \\log p_\\theta(x_0 \\mid x_1) \\right]}_{\\mathcal{L}_0}\n\\]\nSince we minimize loss instead of maximizing ELBO, we flip the sign.\nWe write the loss as:\n\\[\n\\text{Loss} =\n\\underbrace{+ D_{\\text{KL}}(q(x_T \\mid x_0) \\parallel p(x_T))}_{\\mathcal{L}_T}\n\\quad\n\\underbrace{+ \\sum_{t=2}^T D_{\\text{KL}}(q(x_{t-1} \\mid x_t, x_0) \\parallel p_\\theta(x_{t-1} \\mid x_t))}_{\\mathcal{L}_{1:T-1}}\n\\quad\n\\underbrace{- \\mathbb{E}_q \\left[ \\log p_\\theta(x_0 \\mid x_1) \\right]}_{\\mathcal{L}_0}\n\\]\n\n\n4.4 Interpreting Each Term in the ELBO\nReconstruction Loss (\\(\\mathcal{L}_0\\))\n- Encourages the model to reconstruct \\(x_0\\) from the first noisy sample \\(x_1\\)\n- Comes from the log-probability term \\(\\log p_\\theta(x_0 \\mid x_1)\\)\n- Treated as a negative log-likelihood (e.g., MSE if modeled as Gaussian)\nPrior Matching Loss (\\(\\mathcal{L}_T\\))\n- Penalizes mismatch between the final noisy sample \\(x_T\\) and the prior \\(p(x_T)\\)\n- Comes from the KL divergence \\(D_{\\text{KL}}(q(x_T \\mid x_0) \\parallel p(x_T))\\)\n- Ensures the generative process starts from pure noise\nDenoising KL Terms (\\(\\mathcal{L}_{1:T-1}\\))\n- Encourage the model to learn the correct reverse step at each \\(t = 2\\) to \\(T\\)\n- Each term compares \\(q(x_{t-1} \\mid x_t, x_0)\\) to the learned \\(p_\\theta(x_{t-1} \\mid x_t)\\)\n- Drives step-by-step denoising behavior\n\n\n\n\n\n\nBridging ELBO and the DDPM Training Loss\n\n\n\nThe ELBO gives us a principled objective that encourages the learned reverse process to match the true posterior. In practice, however, the KL terms can be simplified when we assume Gaussian forward and reverse processes.\nThis lets us rewrite the training objective in terms of the model‚Äôs ability to predict the noise \\(\\epsilon\\) that was added during the forward process.\n\n\nThis lets us rewrite the training objective in terms of the model‚Äôs ability to predict the noise \\(\\epsilon\\) that was added during the forward process.\n\n\n4.5 The core loss used in DDPM training\nWhen all forward and reverse steps in the diffusion process are modeled as Gaussians, the KL terms in the ELBO can be simplified significantly.\nFor example, the KL divergence between the true posterior \\(q(x_{t-1} \\mid x_t, x_0)\\) and the model‚Äôs reverse process \\(p_\\theta(x_{t-1} \\mid x_t)\\) becomes:\n\\[\nD_{\\text{KL}}\\left( q(x_{t-1} \\mid x_t, x_0) \\;\\|\\; p_\\theta(x_{t-1} \\mid x_t) \\right)\n\\]\nSince both distributions are Gaussian, this KL has a closed-form expression. If we fix the variances and only match the means, minimizing the KL reduces to a squared error loss between the true noise \\(\\epsilon\\) and the model‚Äôs prediction \\(\\epsilon_\\theta\\).\nTo approximate the ELBO in practice, DDPM uses a simplified MSE loss between the predicted and true noise:\n\\[\n\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{x_0, t, \\epsilon} \\left[\n\\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|^2\n\\right]\n\\]\nIn the forward process, the noisy sample \\(x_t\\) is generated from \\(x_0\\) by adding scaled Gaussian noise \\(\\epsilon\\):\n\\[\nx_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\n\\]\nThe network input originally is \\(x_t\\), but now we explicitly substitute its definition:\n\\[\n\\epsilon_\\theta(x_t, t)\n\\quad \\Rightarrow \\quad\n\\epsilon_\\theta\\left( \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\, t \\right)\n\\]\nWe now substitute this new expression into the full loss:\n\\[\n\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{x_0, t, \\epsilon} \\left[\n\\left\\| \\epsilon -\n\\epsilon_\\theta\\left( \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\, t \\right)\n\\right\\|^2\n\\right]\n\\]\nThis is the fully expanded form of the DDPM loss used during training.\n\nThe model receives a noisy input \\(x_t\\) that is synthesized from \\(x_0\\) and \\(\\epsilon\\)\nIt tries to predict the exact noise \\(\\epsilon\\) that was added\nThe loss compares the true noise to the predicted noise, and minimizes their difference using squared error\n\n\n\n\n\n\n\nHow the ELBO Helps Us Understand the MSE Loss\n\n\n\nEven though we end up training with a simplified MSE loss in DDPM, deriving the full ELBO helps us understand what the model is truly learning ‚Äî and why the simplification works.\n\nThe ELBO gives us a principled, variational objective\nIt tells us that training involves matching noise distributions and reconstructing data step-by-step\nThe simplified DDPM loss is an approximation of the denoising KL terms (\\(\\mathcal{L}_{1:T-1}\\))\nOther terms (like \\(\\mathcal{L}_T\\) and \\(\\mathcal{L}_0\\)) are often treated as constants or dropped for efficiency\nMany advanced diffusion models return to the ELBO to add back or rethink these terms\n\nSo the ELBO is like the blueprint ‚Äî and the DDPM loss is an optimized shortcut that works because we understand the full path.\n\n\n\n\n4.6 The Noise Prediction Network\nNow that we‚Äôve arrived at the simplified training loss used in DDPM, let‚Äôs understand what the model is actually learning.\n\n4.6.1 What the Network Learns\n\nDuring training, we know the true noise \\(\\epsilon\\) used to generate the noisy sample \\(x_t\\) from \\(x_0\\)\n\nThe network \\(\\epsilon_\\theta(x_t, t)\\) is trained to predict this noise\n\nOnce trained, the model can use this prediction to ‚Äúundo‚Äù the noise and reconstruct \\(x_0\\) at test time\n\n\n\n4.6.2 Why Predicting Noise Works\nRecall the forward process:\n\\[\nx_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon\n\\]\nRearranging this gives an estimate of the original (clean) image:\n\\[\nx_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon \\right)\n\\]\nThis is exactly why the DDPM training objective minimizes\n\\[\n\\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|^2\n\\]\n‚Äî because if we can predict the noise, we can recover the clean image \\(x_0\\).\nThis denoised estimate is also used during sampling to compute the mean of the reverse distribution \\(q(x_{t-1} \\mid x_t, x_0)\\) ‚Äî which is key to reversing the diffusion process one step at a time."
  },
  {
    "objectID": "diffusion.html#u-net-architecture-for-denoising",
    "href": "diffusion.html#u-net-architecture-for-denoising",
    "title": "Diffusion Models",
    "section": "5 U-Net Architecture for Denoising",
    "text": "5 U-Net Architecture for Denoising\nThe network \\(\\epsilon_\\theta(x_t, t)\\) is typically implemented as a U-Net, which takes:\n\nA noisy image \\(x_t\\)\nA timestep \\(t\\), encoded using sinusoidal or learned embeddings\n\nIt outputs the predicted noise \\(\\epsilon\\), allowing the model to reverse the diffusion process.\nThe U-Net architecture is particularly effective in diffusion models because it preserves the spatial resolution of the input throughout the network. Unlike VAEs, which compress the input into a lower-dimensional latent space, the U-Net uses skip connections to pass fine-grained information directly from downsampling to upsampling layers. This helps preserve image detail and texture throughout the denoising process.\n\n5.1 Key components of the U-Net used in diffusion models:\n\nThe input is a noisy image (e.g., shape \\(64 \\times 64 \\times 3\\)) that we wish to denoise.\nA second input provides the noise variance (or timestep), passed through a sinusoidal embedding function.\nThe time embedding is upsampled and concatenated with the noisy image along the channel dimension.\nThe combined input is passed through:\n\nA downsampling path composed of DownBlocks, which increase the number of channels while reducing spatial dimensions.\nResidual blocks at the bottleneck that help learn deeper representations.\nAn upsampling path composed of UpBlocks, which mirror the downsampling layers.\nSkip connections between corresponding down and up blocks to preserve spatial detail.\n\nThe output is produced by a final \\(1 \\times 1\\) convolutional layer initialized to zeros.\n\nThis structure allows the model to iteratively refine and denoise its estimate of the clean image across reverse steps.\n\n\n\nThe denoising model receives the noisy input \\(x_t\\) and a time embedding \\(t\\), and predicts the noise \\(\\epsilon_\\theta(x_t, t)\\) that was added at step \\(t\\).\n\n\nAdapted from Stanford CS236: Deep Generative Models (Winter 2023).\n\n\n5.2 Takeaways\n\nThe ELBO provides a tractable lower bound on the data likelihood \\(\\log p_\\theta(x_0)\\), and serves as the theoretical training objective.\nIt decomposes into loss terms that align the learned reverse process with the fixed forward noising process, step by step.\nIn practice (as in DDPM), training is simplified to minimizing the mean squared error between the true noise \\(\\epsilon\\) and the predicted noise \\(\\epsilon_\\theta(x_t, t)\\)."
  },
  {
    "objectID": "diffusion.html#training-and-sampling-algorithms-ddpm",
    "href": "diffusion.html#training-and-sampling-algorithms-ddpm",
    "title": "Diffusion Models",
    "section": "6 Training and Sampling Algorithms (DDPM)",
    "text": "6 Training and Sampling Algorithms (DDPM)\n\n6.1 Algorithm: Training (DDPM)\nSteps: - Sample a data point:‚ÄÉ\\(x_0 \\sim q(x_0)\\)\n- Choose a timestep:‚ÄÉ\\(t \\sim \\text{Uniform}(\\{1, \\dots, T\\})\\)\n- Sample noise:‚ÄÉ\\(\\epsilon \\sim \\mathcal{N}(0, I)\\)\n- Take a gradient step to minimize loss\nThe training loss is:\n\\[\n\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{x_0, t, \\epsilon} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta\\left( \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, t \\right) \\right\\|^2 \\right]\n\\]\nExplanation\n- Sample a real data point \\(x_0\\)\n- Corrupt it to get \\(x_t\\) using the forward noising process\n- Train \\(\\epsilon_\\theta\\) to predict the added noise \\(\\epsilon\\)\n- Optimize using mean squared error between predicted and true noise\n\n\n6.2 Algorithm: Sampling (DDPM)\nSteps: - Start with noise:‚ÄÉ\\(x_T \\sim \\mathcal{N}(0, I)\\)\n- For \\(t = T, \\dots, 1\\):\n‚ÄÉ‚ÄÉ- \\(z \\sim \\mathcal{N}(0, I)\\) if \\(t &gt; 1\\), else \\(z = 0\\)\n‚ÄÉ‚ÄÉ- Use predicted noise to update \\(x_{t-1}\\)\n- Return \\(x_0\\)\nThe update equation is:\n\\[\nx_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t z\n\\]\nExplanation\n- Start from pure Gaussian noise \\(x_T\\)\n- At each timestep \\(t\\), use \\(\\epsilon_\\theta\\) to estimate and remove noise\n- Add back a small amount of Gaussian noise \\(z\\)\n- Continue until you obtain a clean sample \\(x_0\\)"
  },
  {
    "objectID": "diffusion.html#tricks-for-improving-generation",
    "href": "diffusion.html#tricks-for-improving-generation",
    "title": "Diffusion Models",
    "section": "7 Tricks for Improving Generation",
    "text": "7 Tricks for Improving Generation\nWhile the original DDPM formulation already achieves impressive results, several refinements have been proposed to improve sample quality and likelihood estimates. Below are three effective categories of improvements.\n\n7.1 Better Noise Schedules\nIn DDPM, the noise schedule defines how much noise is added at each timestep during the forward process. A common default is a linear schedule, but this can overly distort early inputs:\n\nA linear noise schedule adds noise too aggressively in early steps, quickly erasing structure from \\(x_0\\).\nTo address this, researchers proposed a cosine schedule, where noise increases more gradually near the ends.\n\n\nKey Insight: A cosine schedule preserves structure for longer, giving the reverse process a better chance at learning to denoise effectively ‚Äî even though the choice of cosine was somewhat arbitrary, it empirically works better.\n\n\n\n7.2 Learning the Variance (Covariance Matrix)\nIn the basic DDPM, the reverse process uses a fixed variance (i.e., isotropic Gaussian). However, variance also influences generation:\n\nDDPM authors initially used fixed values:\n\\(\\Sigma_\\theta(x_t, t) = \\sigma_t^2 I\\)\nwhere \\(\\sigma_t^2\\) is either \\(\\beta_t\\) or a smoothed estimate \\(\\tilde{\\beta}_t\\).\nNichol and Dhariwal proposed a learned variance model: \\[\n\\Sigma_\\theta(x_t, t) = \\exp \\left( v \\log \\beta_t + (1 - v) \\log \\tilde{\\beta}_t \\right)\n\\]\n\n\nWhy it helps: While the mean dominates sample quality, a better variance estimate improves likelihood estimation without hurting visual fidelity.\n\n\n\n7.3 Architectural Enhancements\nIn addition to schedule and variance tweaks, the architecture of the denoising network matters:\n\nDepth vs.¬†Width: Wider U-Nets improve sample quality more efficiently than deeper ones.\nMulti-Resolution Attention: Adding attention layers at multiple resolutions enhances the model‚Äôs ability to capture global structure.\nBigGAN Residual Blocks: Using residual blocks from BigGAN for upsampling/downsampling improves learning and gradient flow.\nAdaptive Group Normalization: A normalization technique that conditions on timestep (and class label if available) to help the model adapt better at each denoising step.\n\nThese tricks are often combined in modern diffusion models and form the backbone of high-quality sample generation pipelines like improved DDPMs and diffusion-based image generators (e.g., ADM, Imagen, and Stable Diffusion).\n\n\n7.4 Classifier-Free Guidance\nUsed by: Stable Diffusion, Imagen, DALLE 2, and more.\n\nAllows the model to control generation (e.g., with text prompts) without requiring a separate classifier.\nDuring training, the model sees both conditional and unconditional inputs. During sampling, we guide generation by interpolating the two outputs:\n\n\\[\n\\epsilon_\\theta^{\\text{guided}} = (1 + w) \\cdot \\epsilon_\\theta(x_t, t, y) - w \\cdot \\epsilon_\\theta(x_t, t)\n\\]\nWhere: - \\(w\\) is the guidance scale (typically 1‚Äì5). - \\(y\\) is the conditioning input (e.g., a caption). - \\(\\epsilon_\\theta(x_t, t, y)\\) is the predicted noise with conditioning. - \\(\\epsilon_\\theta(x_t, t)\\) is the predicted noise without conditioning.\nHigher \\(w\\) increases adherence to the prompt ‚Äî but can also reduce diversity or realism.\n\n\n7.5 Improved Samplers (DDIM, DPM-Solver)\nThe original DDPM sampling takes ~1000 steps. These improved samplers accelerate generation dramatically.\n\nDDIM (Denoising Diffusion Implicit Models):\n\nUses a non-Markovian deterministic process.\nCan sample in 10‚Äì50 steps while maintaining high fidelity.\n\nDPM-Solver:\n\nTreats the reverse process as an ODE and solves it directly using numerical methods.\nEven faster than DDIM with excellent sample quality.\n\n\nThese methods let us trade off between speed and quality, enabling practical use in real-time applications."
  },
  {
    "objectID": "diffusion.html#references",
    "href": "diffusion.html#references",
    "title": "Diffusion Models",
    "section": "8 üìö References",
    "text": "8 üìö References\n[1] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. arXiv. https://arxiv.org/pdf/2006.11239\n[2] Stanford University. (2023). CS236: Deep Generative Models ‚Äì Winter 2023 Course Materials. https://deepgenerativemodels.github.io/\n[3] Vahdat, A. (2023). CVPR 2023 Tutorial on Diffusion Models. https://cvpr2023-tutorial-diffusion-models.github.io/\n[4] Nichol, A., & Dhariwal, P. (2021). Improved Denoising Diffusion Probabilistic Models. arXiv:2102.09672.\n[5] Akram, C. (2024). Variational Autoencoders (VAEs). changezakram.github.io/Deep-Generative-Models/vae.html"
  },
  {
    "objectID": "diffusion.html#further-reading",
    "href": "diffusion.html#further-reading",
    "title": "Diffusion Models",
    "section": "9 üìò Further Reading",
    "text": "9 üìò Further Reading\n[1] Song, Y. (2021). Score-Based Generative Modeling: A Primer. https://yang-song.net/blog/2021/score/\n[2] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. https://arxiv.org/pdf/2006.11239\n[3] Vincent, P. (2011). A Connection Between Score Matching and Denoising Autoencoders. https://arxiv.org/pdf/2010.02502\n[4] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. https://arxiv.org/pdf/1503.03585\n[5] Hoogeboom, E., Satorras, V. G., Berg, R. v. d., & Welling, M. (2022). Equivariant Diffusion for Molecule Generation in 3D. https://arxiv.org/pdf/2209.00796\n[6] Harvard University. (n.d.). Foundation of Diffusion Generative Models ‚Äì Harvard ML Course Notes. https://scholar.harvard.edu/binxuw/classes/machine-learning-scratch/materials/foundation-diffusion-generative-models"
  },
  {
    "objectID": "nlp-eval.html",
    "href": "nlp-eval.html",
    "title": "Evaluating Language Model Outputs: Metrics, Models, and Benchmarks",
    "section": "",
    "text": "NLP evaluation measures how well language models actually work. It answers basic questions: Is this model good enough? Can we trust it with real users? But here‚Äôs the challenge‚Äîtesting language models isn‚Äôt like testing regular software. When code breaks, it‚Äôs obvious. When a language model fails, it might write something that sounds perfect but is completely wrong. A model might be 92% accurate overall but fail every time it sees sarcasm. This is what makes NLP evaluation so tricky: we‚Äôre measuring how well computers understand the messy, complex world of human language.\nWhy It Matters Now More Than Ever\nIn 2023 alone, we saw an AI chatbot fail to recognize suicide warning signs (leading to a death), lawyers submit fake AI-generated cases to courts, and Air Canada forced to honor a refund policy its bot invented. As Chip Huyen warns: ‚ÄúThe more AI is used, the more opportunity there is for catastrophic failure.‚Äù The smarter our models get, the harder they become to evaluate. It‚Äôs easy to check a kid‚Äôs math homework, but verifying if an AI‚Äôs medical advice is accurate requires medical expertise. We need good evaluation to build better models, but we need expertise to do good evaluation.\nTwo Types of Tasks, Two Different Challenges\n\nClosed-ended tasks have clear right answers (Is this email spam? What‚Äôs the sentiment?). We can use traditional metrics like accuracy and precision, but even these ‚Äúsimple‚Äù tasks suffer from shortcuts, dataset problems, and human labeling errors.\nOpen-ended tasks have no single right answer (Write a summary, translate this text, answer this question). Traditional metrics completely fail here. Word-matching might say ‚ÄúHeck no!‚Äù is similar to ‚ÄúHeck yes!‚Äù because they share words.\n\nWhat‚Äôs Ahead\nThis overview covers how to evaluate both closed and open-ended tasks, why current methods fail and what‚Äôs replacing them, major problems like contamination and bias, and practical solutions for real-world applications."
  },
  {
    "objectID": "nlp-eval.html#introduction",
    "href": "nlp-eval.html#introduction",
    "title": "Evaluating Language Model Outputs: Metrics, Models, and Benchmarks",
    "section": "",
    "text": "NLP evaluation measures how well language models actually work. It answers basic questions: Is this model good enough? Can we trust it with real users? But here‚Äôs the challenge‚Äîtesting language models isn‚Äôt like testing regular software. When code breaks, it‚Äôs obvious. When a language model fails, it might write something that sounds perfect but is completely wrong. A model might be 92% accurate overall but fail every time it sees sarcasm. This is what makes NLP evaluation so tricky: we‚Äôre measuring how well computers understand the messy, complex world of human language.\nWhy It Matters Now More Than Ever\nIn 2023 alone, we saw an AI chatbot fail to recognize suicide warning signs (leading to a death), lawyers submit fake AI-generated cases to courts, and Air Canada forced to honor a refund policy its bot invented. As Chip Huyen warns: ‚ÄúThe more AI is used, the more opportunity there is for catastrophic failure.‚Äù The smarter our models get, the harder they become to evaluate. It‚Äôs easy to check a kid‚Äôs math homework, but verifying if an AI‚Äôs medical advice is accurate requires medical expertise. We need good evaluation to build better models, but we need expertise to do good evaluation.\nTwo Types of Tasks, Two Different Challenges\n\nClosed-ended tasks have clear right answers (Is this email spam? What‚Äôs the sentiment?). We can use traditional metrics like accuracy and precision, but even these ‚Äúsimple‚Äù tasks suffer from shortcuts, dataset problems, and human labeling errors.\nOpen-ended tasks have no single right answer (Write a summary, translate this text, answer this question). Traditional metrics completely fail here. Word-matching might say ‚ÄúHeck no!‚Äù is similar to ‚ÄúHeck yes!‚Äù because they share words.\n\nWhat‚Äôs Ahead\nThis overview covers how to evaluate both closed and open-ended tasks, why current methods fail and what‚Äôs replacing them, major problems like contamination and bias, and practical solutions for real-world applications."
  },
  {
    "objectID": "nlp-eval.html#closed-ended-tasks",
    "href": "nlp-eval.html#closed-ended-tasks",
    "title": "Evaluating Language Model Outputs: Metrics, Models, and Benchmarks",
    "section": "2 Closed-Ended Tasks",
    "text": "2 Closed-Ended Tasks\nClosed-ended tasks are widely used in NLP evaluation because they provide clear right or wrong answers. Since the model‚Äôs output is limited to a small set of predefined choices‚Äîoften fewer than ten‚Äîthey make it easier to compare models using standard metrics like accuracy, precision, recall, and F1-score. This structure allows for objective evaluation, consistent benchmarking, and easier tracking of progress over time.\nBounded Output Space: Unlike text generation where models can produce any sequence of tokens, closed-ended tasks constrain outputs to predefined categories or structured formats.\nObjective Evaluation: Success can be measured automatically using established metrics from classical machine learning‚Äîaccuracy, precision, recall, and F1-score.\nReproducible Benchmarks: Standard datasets enable fair comparison across models and over time.\nSystematic Progress Tracking: Clear metrics allow the field to monitor advancement and identify when models have genuinely improved.\nWhile this constraint makes evaluation more straightforward than open-ended generation, closed-ended tasks still present significant challenges that can mislead researchers and practitioners. Understanding these problems is important because closed-ended evaluation is often the first test of model quality, affecting research focus and decisions about which models to deploy.\n\n2.1 Evaluation Metrics for Closed-Ended Tasks\nUnderstanding how to measure performance is fundamental to closed-ended evaluation. Different metrics serve different purposes and can lead to very different conclusions about model quality. Following metrics evaluate performance on individual labeled tasks:\n\nAccuracy: Percentage of correct predictions (simple but can be misleading with imbalanced data).\n\nPrecision: Of all positive predictions, how many were correct? (important when false alarms are costly).\n\nRecall: Of all actual positive cases, how many were found? (important when missing cases is costly).\n\nF1-Score: Harmonic mean of precision and recall (balances both concerns).\n\nThese metrics are typically applied to standardized datasets called benchmarks, which we‚Äôll explore in the next section. Like standardized tests for AI, benchmarks enable fair comparison between models from different research groups and track progress over time.\n\n\n2.2 Types of Closed-Ended Tasks\n\n\n\n\n\n\n\n\n\n\nTask Type\nDescription\nExample\nPopular Benchmarks\nCommon Pitfalls\n\n\n\n\nSentiment Analysis\nClassify emotional tone (positive/negative/neutral)\n‚ÄúRead the book, forget the movie!‚Äù ‚Üí Negative\nSST, IMDB, Yelp\nFails on sarcasm, irony, cultural context\n\n\nTextual Entailment\nDoes sentence B logically follow from sentence A?\nPremise: ‚ÄúA soccer game with multiple males playing‚ÄùHypothesis: ‚ÄúSome men are playing sport‚Äù ‚Üí Entailment\nSNLI, MultiNLI, RTE\nShortcut learning (e.g., keyword overlap); see SNLI issues\n\n\nNamed Entity Recognition (NER)\nIdentify and classify proper nouns\n‚ÄúApple released the iPhone.‚Äù ‚Üí [Apple‚ÄìORG], [iPhone‚ÄìPRODUCT]\nCoNLL-2003\nAmbiguity: Apple (fruit vs company)\n\n\nPart-of-Speech Tagging\nAssign grammatical categories to words\n‚ÄúThe quick brown fox‚Äù ‚Üí [The‚ÄìDET], [quick‚ÄìADJ], [brown‚ÄìADJ], [fox‚ÄìNOUN]\nPenn Treebank (PTB)\nOften used as foundation for parsing & other tasks\n\n\nCoreference Resolution\nDetermine pronoun references\n‚ÄúMark told Pete lies about himself. He should have been more truthful.‚Äù ‚Üí ‚ÄúHe‚Äù = Mark\nWSC, OntoNotes\nRequires deep context or world knowledge\n\n\nQuestion Answering\nExtract answers from passage\nContext: ‚ÄúThe ESA was passed in 1973.‚ÄùQ: ‚ÄúWhen was it passed?‚Äù ‚Üí 1973\nSQuAD, SQuAD 2.0\nModels memorize patterns or position, not true reasoning\n\n\n\nThese individual tasks can also be grouped into multi-task benchmarks that evaluate general language understanding across a range of closed-ended challenges. SuperGLUE is one of the most prominent such benchmarks.\n\n\n2.3 Multi-Task Benchmark: SuperGLUE\nWhile the previous section outlined individual closed-ended tasks, real-world evaluation often demands a unified benchmark that spans multiple task types. SuperGLUE is a widely adopted closed-ended multi-task benchmark created to evaluate broad general language understanding. It builds on its predecessor (GLUE) with harder tasks, more robust metrics, and an emphasis on reasoning.\nSuperGLUE combines a diverse set of tasks‚Äîranging from entailment and coreference resolution to causal reasoning and word sense disambiguation‚Äîdesigned to holistically assess a model‚Äôs linguistic and reasoning capabilities across multiple dimensions.\nTasks in SuperGLUE:\n\n\n\n\n\n\n\n\nTask\nDescription\nType\n\n\n\n\nBoolQ, MultiRC\nReading comprehension\nQA / Inference\n\n\nCB, RTE\nNatural language inference\nEntailment\n\n\nCOPA\nCausal reasoning (cause/effect)\nReasoning\n\n\nReCoRD\nReading comprehension with commonsense reasoning\nQA / Commonsense\n\n\nWiC\nWord meaning in context\nWord Sense Disambiguation\n\n\nWSC\nCoreference resolution\nCoreference\n\n\n\nTogether, these tasks go beyond surface-level prediction‚Äîtesting abilities like logical reasoning, commonsense application, coreference tracking, and contextual understanding.\nLeaderboard Highlights (v2.0):\nTo measure and compare model performance on SuperGLUE, an official leaderboard tracks results across all tasks using standardized metrics. The v2.0 leaderboard showcases most advanced models‚Äîranging from parameter-efficient Mixture of Experts (MoE) systems to massive transformer-based architectures‚Äîoffering a clear snapshot of the state of the art in general language understanding.\n\nTop models: Vega v2, ST-MoE-32B, ERNIE, PaLM 540B, T5\n\nMetrics used: Accuracy, F1 score, Exact Match, Gender Parity, etc.\n\nThe leaderboard emphasizes balanced generalization, rewarding models that perform consistently well across diverse task types‚Äînot just a few. This makes it a reliable benchmark for tracking progress toward broadly capable language models.\n\n\n2.4 Domain-Rich Multi-Task Benchmark: MMLU\nWhile SuperGLUE focuses on general linguistic reasoning, Massive Multitask Language Understanding (MMLU) shifts the spotlight to domain knowledge. It has rapidly become the de facto benchmark for evaluating a model‚Äôs grasp of academic and professional subjects‚Äîeffectively acting as a proxy for general intelligence in many LLM leaderboards.\nWhat is MMLU?\n\n57 subjects spanning elementary math, anatomy, law, philosophy, computer science, US history, and more\n\nMultiple-choice format (A, B, C, D), with ~100 questions per subject\n\nBalanced question design that mimics real academic tests and professional licensing exams\n\nClosed-book evaluation testing what the model has internalized from pretraining\n\nMMLU has emerged as a standardized benchmark for evaluating foundational knowledge across domains. It allows for direct accuracy-based comparisons between models of different sizes and architectures. Performance gains have been dramatic‚Äîrising from ~25% (random guessing) to over 90% accuracy in just four years.\n\nNote: MMLU is often treated as a shortcut for measuring ‚Äúgeneral intelligence,‚Äù but that can be misleading. What it really tests is how well a model can recall facts and recognize patterns‚Äînot necessarily how well it can reason or think abstractly. We‚Äôll explore these limitations later.\n\nMMLU in Modern LLM Leaderboards\nModern LLMs‚Äîincluding GPT-4, Claude 3, Gemini, LLaMA 3, and PaLM‚Äîroutinely report MMLU scores as a primary metric. As with SuperGLUE, MMLU supports multi-subject generalization, but with a stronger emphasis on world knowledge rather than linguistic nuance.\nSuperGLUE vs MMLU: A Comparison\n\n\n\n\n\n\n\n\nAspect\nSuperGLUE\nMMLU\n\n\n\n\nFocus\nLanguage reasoning\nFactual subject knowledge\n\n\nFormat\nVaried NLP tasks\nMultiple choice\n\n\nTasks / Subjects\n8 tasks\n57 subjects\n\n\nPrimary Skill Tested\nInference, disambiguation, coreference\nRetained domain knowledge\n\n\nMetric\nAccuracy, F1, etc.\nAccuracy only\n\n\n\n\nWhile both are multi-task benchmarks, they evaluate very different capabilities‚ÄîSuperGLUE emphasizes reasoning and understanding, whereas MMLU stresses factual recall across disciplines.\n\n\n\n2.5 Challenges in Closed-Ended Evaluation\n\nMetric Selection: Different metrics highlight different aspects of model performance. For example: Accuracy can be misleading on imbalanced datasets‚Äîit may just reflect majority-class predictions. Precision measures correctness (fewer false positives), while Recall measures completeness (fewer false negatives). Using a single metric in isolation‚Äîespecially on skewed tasks‚Äîcan hide a model‚Äôs weaknesses.\nMetric Aggregation: Benchmarks like SuperGLUE combine many tasks, each with its own metric (e.g., F1, accuracy, loss). Simply averaging scores can give an incomplete picture. Some tasks are easier than others, and their metrics scale differently. Without proper weighting or normalization, overall scores may not reflect true performance.\nLabel Quality: Poorly defined or inconsistent labels can introduce noise into both training and evaluation. This reduces reliability and makes it hard to tell if performance differences are meaningful or just due to annotation issues.\nSpurious Correlations: Models may rely on patterns or keywords rather than real understanding.\n\nExample (SNLI):\n\nPremise: ‚ÄúThe economy could be still better.‚Äù\n\nHypothesis: ‚ÄúThe economy has never been better.‚Äù\n\nModel might infer contradiction simply due to the word never rather than actual reasoning\n\n\nAnnotation Artifacts and Dataset Bias: Some datasets contain structural biases‚Äîfor example, QA answers often appear at the start of a passage. Models may exploit these shortcuts without real comprehension. Stylistic cues (e.g., sentence length or formality) may also correlate with certain labels, inflating scores without reflecting true understanding.\nAdversarial Robustness: Small tweaks‚Äîlike changing a word or rephrasing a sentence‚Äîcan confuse models even if the meaning stays the same. This shows a lack of generalization. Robust evaluation should test whether models maintain performance under such paraphrased or adversarial inputs.\n\nThis highlights that even well-defined tasks require evaluation beyond raw accuracy to capture true model behavior and ensure robustness."
  },
  {
    "objectID": "nlp-eval.html#open-ended-text-generation",
    "href": "nlp-eval.html#open-ended-text-generation",
    "title": "Evaluating Language Model Outputs: Metrics, Models, and Benchmarks",
    "section": "3 Open-Ended Text Generation",
    "text": "3 Open-Ended Text Generation\nOpen-ended generation models, such as large language models (LLMs), can produce diverse free-form outputs like summaries, translations, stories, or answers to instructions. Unlike classification models that generate a fixed label or number, open-ended models return entire sequences of text‚Äîoften with many valid responses for the same input.\nThis flexibility makes them powerful but difficult to evaluate. There‚Äôs rarely a single ‚Äúcorrect‚Äù output, and even seemingly unrelated completions may still be valid. For example, the prompt ‚ÄúTell me something interesting about the moon‚Äù could yield many accurate and fluent yet different answers.\nEvaluating such responses goes beyond simple reference matching‚Äîit requires assessing coherence, fluency, relevance, factual accuracy, style, and semantic alignment. As a result, evaluation of open-ended generation must adopt a more nuanced, multi-faceted approach.\nThe following sections outline key evaluation methods and challenges.\n\n3.1 Content Overlap Metrics\nThese methods compute similarity based on surface-level word overlap between the generated text and a human-written reference. They are fast, interpretable, and have long been used in machine translation, summarization, and captioning. However, they often fail to recognize valid paraphrases or semantic equivalence.\nPopular metrics:\n\n3.1.1 BLEU (Bilingual Evaluation Understudy)\n\nFocus: Precision-oriented n-gram overlap\nCalculation: Geometric mean of 1-gram through 4-gram precision, with brevity penalty\nUse Case: Machine Translation ‚Äî how much of the generated text appears in the reference\nFormula: \\[\n\\text{BLEU} = \\text{BP} \\cdot \\exp\\left( \\sum_{n=1}^{N} \\frac{1}{N} \\cdot \\log(p_n) \\right)\n\\] where \\(p_n\\) is n-gram precision and BP is brevity penalty.\n\n\n\n3.1.2 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n\nFocus: Recall-oriented n-gram overlap\nVariants: ROUGE-1: Unigrams, ROUGE-2: Bigrams, ROUGE-L: Longest common subsequence\nUse Case: Summarization ‚Äî how much of the reference is captured in the generated text\nFormula (ROUGE-L F1 score):\n\\[\n\\text{ROUGE-L} = \\frac{(1 + \\beta^2) \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Recall} + \\beta^2 \\cdot \\text{Precision}}\n\\] where:\n\nPrecision = \\(\\frac{LCS}{\\text{candidate length}}\\)\nRecall = \\(\\frac{LCS}{\\text{reference length}}\\)\nLCS = Longest Common Subsequence\n\\(\\beta\\) balances recall and precision (often \\(\\beta = 1\\))\n\n\n\nLimitation: N-gram metrics have no concept of meaning. They fail when different words express the same idea.\n\nExample 1:\nReference: \"Heck yes!\"\n\n\n\n\n\n\n\n\nGenerated Output\nBLEU Score\nSemantic Meaning\n\n\n\n\n\"Yes!\"\n67%\nCorrect\n\n\n\"You know it!\"\nLow\nCorrect\n\n\n\"Yup\"\n0%\nCorrect\n\n\n\"Heck no!\"\n67%\nWrong (opposite meaning)\n\n\n\n\nThese metrics reward lexical matches even when the meaning is incorrect.\n\nExample 2:\nReference: \"The innovative startup secured substantial funding\"\n\nGenerated A:\n\"The creative company obtained significant investment\"\n‚Üí 0% BLEU overlap, perfect semantic match\nGenerated B:\n\"The innovative startup funding substantial secured\"\n‚Üí 83% BLEU overlap, grammatically broken and semantically wrong\n\n\nBLEU would incorrectly score Generated B higher than A due to word overlap.\n\n\n\n\n3.2 Model-Based Metrics\nAs NLP systems generate increasingly fluent and diverse outputs, traditional word-overlap metrics like BLEU and ROUGE often fail to capture deeper aspects of quality such as semantic fidelity, naturalness, and paraphrasing. To overcome these limitations, researchers have developed model-based evaluation metrics that harness the semantic understanding of pretrained language models.\nTwo widely adopted examples are BERTScore, which uses contextual embeddings for token-level semantic similarity, and BLEURT, which is trained to predict human quality judgments directly.\n\n3.2.1 BERTScore: Semantic Matching with Contextual Embeddings\nBERTScore measures how semantically close a generated text is to a reference by comparing their contextualized token embeddings from BERT. It operates in three main steps:\n\nToken Embedding: Each token is mapped to a contextual vector using a pre-trained BERT model.\n\nSimilarity Matrix: Cosine similarity is computed between each candidate and reference token.\n\nGreedy Matching:\n\nPrecision: Max similarity for each candidate token to reference.\n\nRecall: Max similarity for each reference token to candidate.\n\nF1 Score: Harmonic mean of the two.\n\n\nOptional Enhancements\n\nIDF Weighting: Emphasizes rare and informative words.\n\nBaseline Rescaling: Normalizes scores for consistency.\n\n\n\n\nFigure: BERTScore computes contextual similarity between candidate and reference tokens using BERT embeddings and cosine similarity, followed by precision, recall, and F1 aggregation. (Source: Zhang et al.¬†(2020))\n\n\nExample:\n- Reference: ‚ÄúThe weather is cold today‚Äù\n- Candidate: ‚ÄúIt is freezing today‚Äù\nBLEU would assign a low score due to low word overlap. BERTScore correctly identifies that ‚Äúcold‚Äù and ‚Äúfreezing‚Äù are semantically similar and aligns ‚Äúit‚Äù with ‚Äúweather,‚Äù producing a high score.\n\n\n3.2.2 BLEURT: Learned Quality Estimation\nBLEURT offers a learning-based alternative to similarity-based metrics like BERTScore. Instead of relying on heuristic rules, BLEURT is trained to predict human quality judgments directly using fine-tuned BERT models.\n\nPre-training: Initialized with BERT and trained on synthetically modified sentence pairs. For example, original sentences and their noisy or paraphrased versions. This helps the model learn how edits, errors, or rewordings affect meaning.\nFine-tuning: BLEURT is fine-tuned using real sentence pairs labeled by humans in shared tasks. This allows it to learn what humans consider high- or low-quality responses.\n(Optional) Application-Specific Fine-tuning: For custom use cases (e.g., legal, medical, or customer support), BLEURT can be further fine-tuned using domain-specific human feedback, improving alignment with task-specific quality standards.\n\nGiven a candidate and reference sentence, BLEURT returns a scalar score, typically between -1 and 1, reflecting how closely the candidate aligns with human expectations in meaning, fluency, and grammatical correctness.\n\n\n\nFigure: BLEURT learns to predict human ratings by pre-training on synthetic sentence pairs and fine-tuning on labeled examples, optionally adapting to task-specific human feedback. (Source: Sellam et al.¬†(2020))\n\n\nExample:\n- Reference: ‚ÄúThe weather is cold today‚Äù\n- Candidate: ‚ÄúIt is freezing today‚Äù\nDespite limited word overlap, BLEURT assigns a high score, recognizing semantic equivalence and fluent expression ‚Äî something traditional metrics might miss.\n\n\n3.2.3 Summary: When to Use Which?\n\n\n\n\n\n\n\n\nMetric\nStrengths\nLimitations\n\n\n\n\nBERTScore\nFast, interpretable; captures token-level semantics\nRelies on heuristic alignment; less fluency-aware\n\n\nBLEURT\nTrained on human ratings; captures fluency and variation\nMore computationally intensive; less transparent\n\n\n\nBoth metrics represent a shift toward evaluation methods that better reflect semantic correctness, paraphrasing, and generation quality. BERTScore excels in semantic alignment, while BLEURT better captures fluency and natural language variation.\n\n\n\n3.3 The Reference Quality Crisis\nReference-based evaluation metrics‚Äîsuch as BLEU, ROUGE, BERTScore, and BLEURT‚Äîare only as reliable as the references themselves. If the reference is flawed‚Äîdue to poor quality, weak alignment, or narrow scope‚Äîit can mislead the metric and penalize otherwise valid model outputs.\n\n3.3.1 Case Study: CNN/Daily Mail Summarization\nA widely used benchmark for summarization, the CNN/Daily Mail dataset, illustrates the pitfalls of poor reference quality in real-world evaluation. The dataset was constructed by collecting CNN news articles along with their accompanying bullet-point highlights. These highlights, written by journalists under deadline pressure, were used as ‚Äúgold standard‚Äù summaries to train and evaluate models.\nHowever, closer scrutiny revealed major issues. The highlights were often incomplete, loosely structured, or inconsistent in tone. They sometimes emphasized trivial surface-level details over summary-worthy content, and rarely reflected what real users actually wanted from a summary. This raised serious concerns about their reliability as evaluation references.\nSeveral studies have examined this issue. For example, Fabbri et al.¬†(2021) found that common metrics like ROUGE showed weak or near-zero correlation with human quality judgments under such conditions. However, when expert-written or carefully revised references were used, the same metrics exhibited significantly stronger correlation (r ‚âà 0.6‚Äì0.7), suggesting that the evaluation metric itself wasn‚Äôt broken‚Äîthe references were. Similar conclusions were drawn by Akter et al.¬†(2022), who proposed a semantic-aware alternative and highlighted how poor references in CNN/Daily Mail limit metric reliability.\n\n\n3.3.2 Broader Challenges with Reference-Based Evaluation\nBeyond CNN/Daily Mail, reference quality issues are widespread across many NLP benchmarks:\n\nSparse or Limited References: Many benchmarks provide only a single reference output, even though multiple diverse and valid responses may exist.\nStylistic or Tonal Inconsistency: Reference texts often vary in formality, phrasing, or verbosity, which can skew similarity-based metrics.\nTask Misalignment: Some references emphasize different aspects than what users or tasks actually prioritize.\nNoisy or Imperfect Texts: Crowdsourced or time-constrained references may include grammatical mistakes, ambiguity, or shallow reasoning.\n\nThese limitations are not just theoretical. They directly influence metric scores and evaluation outcomes in ways that can misrepresent model performance.\n\n\n3.3.3 Consequences\nThese reference quality problems have several downstream consequences:\n\nMisleading comparisons between models\n\nUnderestimation of strong models that diverge stylistically from the reference\n\nOptimization toward flawed targets (‚Äúgaming the metric‚Äù)\n\nMotivation to explore Reference-Free Evaluation, which avoids dependency on flawed gold standards\n\n\n\n\n3.4 Reference-Free Evaluation\nInstead of comparing model outputs to human-written references, reference-free evaluation methods attempt to judge quality directly‚Äîwithout requiring a gold standard. This is especially useful in open-ended generation tasks where multiple outputs may be valid, and reference quality is often poor (as discussed in Section 3.3).\nReference-based metrics can penalize valid outputs that differ in wording or structure from the reference. As an alternative, reference-free methods evaluate the generation directly‚Äîby learning quality predictors or leveraging language models as judges.\n\n3.4.1 Traditional Reference-Free Approaches\nBefore the rise of large language models (LLMs), researchers explored simpler reference-free methods by training regression models to predict human quality ratings.\n\nInput: Model-generated output only (no reference)\nOutput: Predicted quality score, usually trained on large annotated datasets\nLimitation: Required costly human-labeled examples and struggled to generalize across domains or tasks\n\nThese approaches demonstrated the possibility of reference-free evaluation, but lacked robustness and flexibility.\n\n\n3.4.2 LLM-as-Judge: A Paradigm Shift\nRecent advances have shown that LLMs like GPT-4 can act as sophisticated, multi-criteria evaluators‚Äîscoring generated outputs with surprisingly high alignment to human judgments.\nMethodology\nA typical prompt might look like this:\n\nOriginal Article: [article text]\nGenerated Summary: [summary text]\nEvaluate on a scale of 1 to 5 based on:\n- Accuracy (factual correctness)\n- Completeness (coverage of key points)\n- Clarity (writing quality and coherence)\n- Conciseness (appropriate length and focus)\nScore: ___\nExplanation: ___\n\nThis format enables LLMs to perform nuanced evaluation‚Äîwithout needing a gold reference‚Äîand produce both numeric ratings and reasoning.\nAdvantages of LLM-Based Evaluation\nResearch has shown that LLM-as-judge methods offer several compelling benefits:\n\nMulti-dimensional: Can evaluate factuality, fluency, style, etc.\nScalable and Fast: ~100√ó faster and cheaper than human annotators\nConsistent: Inter-rater agreement higher than among humans (85% vs 67%)\nCorrelated with Human Judgments: Pearson correlation of r = 0.8‚Äì0.9 in many tasks\n\n\nGPT-4 often shows higher agreement with humans than humans show with each other.\n\nPopular Benchmarks\nSeveral modern benchmarks now use LLMs as evaluation agents:\n\nAlpacaEval: GPT-4 judges instruction-following responses\n\nMT-Bench: Evaluates multi-turn dialogues with GPT-based raters\n\nChatbot Arena: Combines LLM judgments with human preferences at scale\n\nThese benchmarks have helped validate the use of LLMs for practical, scalable evaluation.\nLimitations and Biases of LLM-as-Judge Methods\nDespite their promise, LLM-based evaluations are not without flaws. Known issues include:\n\nSelf-Preference Bias\n\nLLMs (e.g., GPT-4) tend to prefer outputs generated by themselves\n\nMitigation: Use diverse evaluator models and blind prompts\nLevel of Effort: Moderate ‚Äî requires model access and prompt engineering adjustments\n\nLength Bias\n\nTendency to favor longer outputs (~70% of the time), regardless of quality\n\nMitigation: Normalize for length or include brevity constraints\nLevel of Effort: Low ‚Äî can be addressed with prompt tuning and post-processing rules\n\nStyle Bias\n\nPreference for structured or bullet-style outputs over more creative formats\n\nMitigation: Introduce style calibration or reference multiple formats during scoring\n\nLevel of Effort: Moderate to High ‚Äî requires developing style-sensitive evaluation logic\n\nCultural and Demographic Bias\n\nReflect evaluator training data (e.g., Western academic tone)\n\nMitigation (long-term goal): Encourage more inclusive evaluator design and culturally diverse data sources for training and evaluation\n\nLevel of Effort: High ‚Äî requires curating diverse datasets and broader systemic changes to training pipelines\n\n\nThese limitations suggest that while LLM-based evaluation is powerful, careful prompt design, evaluator diversity, and bias monitoring are essential.\n\n\n\n3.5 Human Evaluations\nDespite rapid advances in automatic metrics, human judgment remains the gold standard for evaluating open-ended tasks. Automated scoring methods often miss subtle details, personal opinions, and user preferences‚Äîespecially in tasks like summarization, dialogue, or creative generation. Human evaluation remains essential for validating whether model outputs are helpful, coherent, accurate, and aligned with human expectations.\nWhat Human Evaluation Measures\nEvaluators typically assess a model‚Äôs output along key quality dimensions:\n\nFluency ‚Äì Is the text grammatically correct and natural?\n\nCoherence ‚Äì Does the output flow logically and make sense throughout?\n\nRelevance ‚Äì Is the response appropriate to the input and task?\n\nFactual Accuracy ‚Äì Are the claims factually correct?\n\nCompleteness ‚Äì Does the output cover all necessary points?\n\nOriginality ‚Äì Is the output non-redundant and creative?\n\nThese dimensions align closely with human expectations in real-world applications like summarization, dialogue, and question answering.\nEvaluation Methodologies\nThere are two primary approaches to scoring: absolute scoring and comparative judgment.\nAbsolute scoring methods use Likert scales, where annotators rate outputs numerically across different dimensions‚Äîtypically on a 1-to-5 or 1-to-7 scale. While this provides granular data, it can introduce subjectivity due to differences in annotators‚Äô interpretations of the scale.\nComparative methods, such as pairwise ranking or best‚Äìworst scaling, tend to be more robust. In pairwise ranking, annotators compare two or more outputs and choose the better one. Best‚Äìworst scaling takes this further by having annotators identify both the best and worst examples from a set. These approaches reduce variability and are commonly used in benchmark leaderboards and model comparisons.\n\n3.5.1 The Human Evaluation Crisis\nWhile human evaluations are critical, they are often plagued by reproducibility, consistency, and cost challenges.\nAn analysis of NLP papers from 2015‚Äì2020 found that only 5% of human evaluation studies were fully reproducible. The primary culprits were lack of methodological detail and unavailability of materials, which made it difficult for others to verify or replicate the results. Common issues included missing annotator guidelines, inconsistent scoring criteria, and the absence of publicly shared data. This undermines scientific rigor and prevents meaningful comparisons across models and studies.\nEven when guidelines are clear, annotator disagreement remains high. Studies have shown inter-annotator agreement rates hovering around 60‚Äì70%, and for more subjective tasks‚Äîlike helpfulness or summarization‚Äîagreement can drop to as low as 40‚Äì50%. For example, in one study on summarization quality, even expert annotators disagreed 30‚Äì40% of the time.\nHuman evaluations are also expensive and time-consuming. Annotators typically charge $15‚Äì$30 per hour, and evaluating a single output can take 5‚Äì15 minutes. Large-scale evaluations, which require hundreds or thousands of samples, quickly become cost-prohibitive.\nMoreover, quality control is non-trivial. Annotators need training, ongoing calibration, and clear rubrics. Without rigorous oversight, annotators may interpret vague instructions differently or apply inconsistent standards. There‚Äôs also a natural incentive misalignment: annotators may prioritize speed over quality, while researchers seek careful, nuanced judgments. This tension can lead to superficial or biased evaluations.\n\n\n3.5.2 Best Practices for Human Evaluation\nTo mitigate these issues, researchers have proposed several best practices. First, providing clear instructions‚Äîincluding detailed rubrics and examples‚Äîcan significantly reduce ambiguity. Conducting pilot studies before full-scale evaluation helps refine the task and catch misunderstandings early.\nUsing multiple annotators (typically 3‚Äì5 per example) improves reliability, especially when combined with inter-annotator agreement tracking. Regular calibration sessions and feedback loops ensure that annotators stay aligned over time and surface discrepancies before they affect results.\nWhen applied carefully, these practices make human evaluation more consistent, interpretable, and fair.\n\nNote: Human scores should never be compared across studies, as prompts, instructions, and annotators vary widely.\n\n\n\n\n3.6 Evaluation Pitfalls and Biases\nAutomatic metrics are only as good as the references they rely on. If reference texts are low-quality, misaligned with the task, or poorly written, evaluation results can be misleading. For example, studies have shown that ROUGE is poorly correlated with human preferences unless expert-written references are used.\nBeyond reference quality, evaluation metrics can suffer from spurious correlations‚Äîpatterns that have nothing to do with true output quality. For instance, length bias leads to a ~70% preference for longer outputs, regardless of informativeness. Similarly, models often benefit from list bias (bullet-style responses), position bias (left-vs-right placement in comparisons), and self-bias, where models like GPT-4 subtly favor outputs they generated themselves.\nThese pitfalls suggest that even widely used metrics can be unintentionally skewed. Careful design and manual oversight are crucial to avoid false confidence in metric-based scores.\n\n\n3.7 Broader Challenges\n\nConsistency Issues: Results can vary significantly based on small changes in prompt style or decoding strategy. MMLU scores can fluctuate by up to 15% due to formatting alone‚Äîchanging ‚ÄúAnswer: A‚Äù to ‚ÄúA)‚Äù or adding ‚ÄúPlease‚Äù can shift model rankings. This makes benchmark comparisons unreliable when papers don‚Äôt specify exact evaluation setups.\nContamination: Models sometimes perform well on benchmarks because they were trained on test data, not because they generalize. GPT-4‚Äôs performance on Codeforces pre-2021 vs post-2021 illustrates this concern. Recent analysis suggests 10-30% of benchmark improvements may reflect contamination rather than genuine capability gains‚Äîmodels scoring 85% on original MMLU might only achieve 65% on contamination-free variants.\nOverfitting: Benchmarks saturate quickly, reducing their long-term usefulness. The pattern is predictable: introduction (40-60% scores) ‚Üí optimization (rapid progress) ‚Üí saturation (85-95%) ‚Üí gaming (exploiting artifacts). GLUE reached human performance within 2 years and required replacement by SuperGLUE.\nMonoculture: A lack of linguistic and cultural diversity‚Äî70% of ACL 2021 papers were English-only‚Äîlimits the generalizability of results. With 85% of NLP benchmarks focusing on English despite 6B non-English speakers globally, models may fail catastrophically in non-Western contexts.\nSingle Metric Fallacy: Over-relying on one metric (e.g., accuracy) oversimplifies model performance and can obscure fairness, bias, and usability trade-offs. A customer service chatbot optimized for accuracy might achieve 95% correct answers but sound robotic, leading to poor user satisfaction despite high benchmark scores.\nEvaluation Infrastructure Gaps: 78% of papers don‚Äôt specify exact prompts, 65% don‚Äôt report model versions or temperature settings, and only 23% provide reproducible evaluation code. This makes meta-analyses impossible and progress measurement unreliable.\n\nThese challenges collectively suggest that evaluation metrics provide useful signals but should be interpreted cautiously and combined with multiple assessment approaches.\n\n\n3.8 Evaluation Tools and Frameworks\nAutomated Metric Libraries\n\nHugging Face Evaluate: 60+ metrics with standardized interface. Best for rapid prototyping and consistent implementation.\nNLTK: Traditional metrics (BLEU, ROUGE, METEOR). Mature and well-tested but slower than modern alternatives.\n\nModel-Based Evaluation\n\nBERTScore/BLEURT: Semantic similarity using pretrained models. Use for paraphrase-heavy tasks when reference quality varies.\nComet: Neural MT evaluation framework with high human correlation (r=0.87).\n\nLLM-as-Judge Frameworks\n\nG-Eval: GPT-4-based evaluation with chain-of-thought reasoning. Cost: ~$0.01-0.05 per evaluation.\nAlpacaEval: Standardized instruction-following evaluation with 85% human agreement.\n\nComprehensive Suites\n\nHELM (Stanford): Holistic evaluation across 16 scenarios, 7 metrics. Best for research organizations.\nEleutherAI LM Evaluation Harness: 200+ tasks, open-source. Ideal for reproducible benchmarking.\n\nSelection Guide:\nChoosing the right evaluation method depends on your specific task requirements, resource constraints, and quality needs. The following framework provides a systematic approach to selecting appropriate metrics based on these factors. While this guide offers clear starting points, remember that production systems often benefit from combining multiple evaluation approaches‚Äîusing fast automated metrics for development iteration and higher-quality methods for final validation.\n\n\n\nFigure: Choosing evaluation metrics based on task type and requirements. Classification tasks are best evaluated using traditional metrics like accuracy or F1. For text generation tasks, the choice of metric depends on context‚ÄîBLEU/ROUGE for speed, BERTScore/BLEURT for semantic alignment, human evaluation for high-stakes scenarios, and LLM-as-judge for scalable, high-quality assessment.\n\n\n\n\n3.9 Key Takeaways\nEvaluating open-ended tasks is inherently complex. No single approach‚Äîautomatic, human, or LLM-based‚Äîworks universally well across all scenarios.\nTraditional metrics like BLEU and ROUGE still serve a role, especially for structured tasks like translation. But for generation tasks involving reasoning, creativity, or nuance, they increasingly fall short. Human evaluations offer deeper insights but remain expensive, slow, and often difficult to reproduce. LLM-based evaluations (like GPT-4 as a judge) are fast, scalable, and correlate better with human preferences than n-gram metrics‚Äîyet they can hallucinate scores and inherit training biases.\nPractical Recommendations:\n\nStart with automated metrics for rapid iteration, then validate with human evaluation\nUse multiple metrics rather than relying on a single score\nFor high-stakes applications, always include domain expert review\nConsider the evaluation cost vs.¬†application risk trade-off when choosing methods\n\nIn practice, the most effective strategy is hybrid: combine multiple metrics, validate with human feedback, and always review outputs manually. Quantitative scores alone can be misleading.\nAs generative systems grow more powerful and widely deployed, evaluation becomes a critical bottleneck. The future lies in developing evaluation methods that are both scalable and semantically meaningful, while addressing emerging challenges like benchmark contamination and safety alignment. The quality of our evaluation methods will ultimately determine how safely and effectively we can deploy these powerful systems."
  },
  {
    "objectID": "nlp-eval.html#references-further-reading",
    "href": "nlp-eval.html#references-further-reading",
    "title": "Evaluating Language Model Outputs: Metrics, Models, and Benchmarks",
    "section": "4 References & Further Reading",
    "text": "4 References & Further Reading\n[1] Wang, A., et al.¬†(2019). SuperGLUE: A Stickier Benchmark for General‚ÄëPurpose Language Understanding Systems. https://arxiv.org/pdf/1905.00537\n[2] Hendrycks, D., et al.¬†(2020). Measuring Massive Multitask Language Understanding (MMLU). https://arxiv.org/pdf/2009.03300\n[3] Papineni, K., et al.¬†(2002). BLEU: A Method for Automatic Evaluation of Machine Translation. https://aclanthology.org/P02-1040\n[4] Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. https://aclanthology.org/W04-1013\n[5] Chang, T., et al.¬†(2020). BERTSCORE: Evaluating Text Generation With BERT. https://arxiv.org/pdf/1904.09675\n[6] Sellam, T., et al.¬†(2020). BLEURT: Learning Robust Metrics for Text Generation. https://arxiv.org/pdf/2004.04696\n[7] Durmus, E., et al.¬†(2022). Spurious Correlations in Reference‚ÄëFree Evaluation of Text Generation. https://arxiv.org/abs/2204.09890\n[8] Fabbri, A., et al.¬†(2021). SummEval: Re-evaluating Summarization Evaluation. https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation\n[9] Akter, M., et al.¬†(2022). Can We Do Better than ROUGE? Sem-nCG for Evaluating Summarization. https://aclanthology.org/2022.findings-acl.122.pdf\n[10] Specia, L., et al.¬†(2010). Quality Estimation for Machine Translation Without Human-Labeled Data. https://www.aclweb.org/anthology/W10-1753.pdf\n[11] Su, Y., et al.¬†(2023). Reference-Free Evaluation Metrics for Text Generation: A Survey. https://arxiv.org/pdf/2305.04648.pdf\n[12] Rei, M., et al.¬†(2020). Comet: A neural framework for MT evaluation. https://arxiv.org/pdf/2009.09025\n[13] Zheng, S., et al.¬†(2023). Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. https://arxiv.org/pdf/2306.05685\n[14] Fu, Y., et al.¬†(2023). G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. https://arxiv.org/pdf/2303.16634\n[15] Tuan, Y., et al.¬†(2021). Quality Estimation without Human-labeled Data. https://arxiv.org/pdf/2102.04020\n[16] Celikyilmaz, A., et al.¬†(2020). Evaluation of Text Generation: A Survey. https://arxiv.org/abs/2006.14799\n[17] Krishna, K., et al.¬†(2023). LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization. https://aclanthology.org/2023.eacl-main.121.pdf\n[18] Liang, P. et al.¬†(2023). Holistic Evaluation of Language Models. https://arxiv.org/pdf/2211.09110\n[19] Zheng, L. et al.¬†(2024). Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. https://arxiv.org/pdf/2306.05685\n[20] Ouyang, L. et al.¬†(2022). Training language models to follow instructions with human feedback. https://arxiv.org/pdf/2203.02155\n[21] Anthropic. (2022). Constitutional AI: Harmlessness from AI Feedback. https://arxiv.org/pdf/2212.08073\n[22] Wei, J. et al.¬†(2023). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. https://arxiv.org/pdf/2201.11903\n[23] Huyen, C. (2024). AI Engineering: Building Applications with Foundation Models. O‚ÄôReilly Media.\n[24] Sebastian R. (2025) Understanding the 4 Main Approaches to LLM Evaluation (From Scratch) https://magazine.sebastianraschka.com/p/llm-evaluation-4-approaches"
  },
  {
    "objectID": "vae_faces.html",
    "href": "vae_faces.html",
    "title": "Face Generation with Convolutional VAE",
    "section": "",
    "text": "This notebook implements a convolutional variational autoencoder (VAE) trained on the CelebA face dataset using PyTorch.\nIt uses convolutional layers to encode and decode 64x64 face images, and demonstrates generation by sampling from the latent space.\n!pip install torch torchvision matplotlib\n# Enable autoreloading in Jupyter (if applicable)\n%load_ext autoreload\n%autoreload 2\n\n# Core libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import norm  # Used for visualizing latent distributions\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms, utils\n# Hyperparameters\n\nIMAGE_SIZE = 32        # Input image resolution (32x32)\nCHANNELS = 3           # Number of color channels (RGB)\nBATCH_SIZE = 128       # Batch size for training\nNUM_FEATURES = 128     # Hidden layer size or intermediate feature size\nZ_DIM = 200            # Dimensionality of the latent space\nLEARNING_RATE = 5e-4   # Learning rate for optimizer\nEPOCHS = 1             # Number of training epochs\nBETA = 2000            # Weight for KL divergence term in loss (used in beta-VAE)"
  },
  {
    "objectID": "vae_faces.html#loading-and-preprocessing-the-celeba-dataset",
    "href": "vae_faces.html#loading-and-preprocessing-the-celeba-dataset",
    "title": "Face Generation with Convolutional VAE",
    "section": "0.1 Loading and Preprocessing the CelebA Dataset",
    "text": "0.1 Loading and Preprocessing the CelebA Dataset\n\nfrom torchvision.datasets import CelebA\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Transform pipeline: resize and convert to tensor\ntransform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor()\n])\n\n# Load CelebA dataset\ntrain_data = CelebA(\n    root=\"./data\",                      # Where to download/save the data\n    split=\"train\",                      # Options: \"train\", \"valid\", \"test\", or \"all\"\n    download=True,                      # Download if not already there\n    transform=transform\n)\n\n# DataLoader\ntrain_loader = DataLoader(\n    train_data,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\n\nval_data = CelebA(\n    root=\"./data\",\n    split=\"valid\",\n    download=True,\n    transform=transform\n)\n\nval_loader = DataLoader(\n    val_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)"
  },
  {
    "objectID": "vae_faces.html#visualizing-training-data",
    "href": "vae_faces.html#visualizing-training-data",
    "title": "Face Generation with Convolutional VAE",
    "section": "0.2 Visualizing Training Data",
    "text": "0.2 Visualizing Training Data\n\ndef sample_batch(dataloader, num_samples=16):\n    \"\"\"\n    Sample one batch from the dataloader and return the first `num_samples` images.\n    \"\"\"\n    for images, _ in dataloader:\n        return images[:num_samples]\n\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\ndef display(images, cmap=None, nrow=8):\n    \"\"\"\n    Display a grid of images using matplotlib.\n    \"\"\"\n    grid = make_grid(images, nrow=nrow, padding=2, normalize=True)\n    np_img = grid.permute(1, 2, 0).cpu().numpy()\n\n    plt.figure(figsize=(10, 5))\n    plt.imshow(np_img, cmap=cmap)\n    plt.axis('off')\n    plt.show()\n\n\ntrain_sample = sample_batch(train_loader, num_samples=16)\ndisplay(train_sample)"
  },
  {
    "objectID": "vae_faces.html#reparameterization-trick",
    "href": "vae_faces.html#reparameterization-trick",
    "title": "Face Generation with Convolutional VAE",
    "section": "0.3 Reparameterization Trick",
    "text": "0.3 Reparameterization Trick\n\ndef reparameterize(z_mean, z_log_var):\n    \"\"\"\n    Reparameterization trick: z = mu + sigma * epsilon\n    where epsilon ~ N(0, I)\n    \"\"\"\n    std = torch.exp(0.5 * z_log_var)\n    eps = torch.randn_like(std)\n    return z_mean + eps * std"
  },
  {
    "objectID": "vae_faces.html#encoder",
    "href": "vae_faces.html#encoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "0.4 Encoder",
    "text": "0.4 Encoder\n\nclass Encoder(nn.Module):\n    def __init__(self, z_dim):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(CHANNELS, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 32 ‚Üí 16\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 16 ‚Üí 8\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 8 ‚Üí 4\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 4 ‚Üí 2\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n        )\n\n        self.flatten = nn.Flatten()\n        self.feature_shape = (NUM_FEATURES, IMAGE_SIZE // 16, IMAGE_SIZE // 16)\n        self.flat_dim = NUM_FEATURES * (IMAGE_SIZE // 16) ** 2\n\n        self.fc_mu = nn.Linear(self.flat_dim, z_dim)\n        self.fc_logvar = nn.Linear(self.flat_dim, z_dim)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_flat = self.flatten(x)\n        mu = self.fc_mu(x_flat)\n        logvar = self.fc_logvar(x_flat)\n        return mu, logvar"
  },
  {
    "objectID": "vae_faces.html#vae-model-wrapper-encoder-reparameterization-and-decoder",
    "href": "vae_faces.html#vae-model-wrapper-encoder-reparameterization-and-decoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.1 VAE Model Wrapper: Encoder, Reparameterization, and Decoder",
    "text": "1.1 VAE Model Wrapper: Encoder, Reparameterization, and Decoder\n\nclass VAE(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"\n        z = mu + sigma * epsilon\n        where epsilon ~ N(0, I)\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass:\n        - Encode input to get mu and logvar\n        - Sample z using reparameterization trick\n        - Decode z to reconstruct image\n        \"\"\"\n        mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        x_recon = self.decoder(z)\n        return x_recon, mu, logvar"
  },
  {
    "objectID": "vae_faces.html#vae-loss",
    "href": "vae_faces.html#vae-loss",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.2 VAE Loss",
    "text": "1.2 VAE Loss\n\ndef vae_loss(x, x_recon, mu, logvar, beta=BETA):\n    \"\"\"\n    VAE loss = beta * reconstruction loss (MSE) + KL divergence\n    \"\"\"\n    # Reconstruction loss (MSE)\n    recon_loss = F.mse_loss(x_recon, x, reduction='mean') * beta\n\n    # KL divergence between q(z|x) and N(0, I)\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    kl_loss = kl_loss / x.size(0)  # average over batch\n\n    return recon_loss + kl_loss, recon_loss, kl_loss"
  },
  {
    "objectID": "vae_faces.html#generating-and-saving-sample-images-from-the-latent-space",
    "href": "vae_faces.html#generating-and-saving-sample-images-from-the-latent-space",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.3 Generating and Saving Sample Images from the Latent Space",
    "text": "1.3 Generating and Saving Sample Images from the Latent Space\n\nfrom torchvision.utils import save_image\nimport os\n\ndef generate_images(model, epoch, z_dim, num_img=8, path=\"./output\"):\n    \"\"\"\n    Generate and save images from random latent vectors after each epoch.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    model.eval()\n    z = torch.randn(num_img, z_dim).to(next(model.parameters()).device)\n    with torch.no_grad():\n        generated = model.decoder(z)\n    for i in range(num_img):\n        save_image(generated[i], f\"{path}/generated_img_{epoch:03d}_{i}.png\")"
  },
  {
    "objectID": "vae_faces.html#save-checkpoint",
    "href": "vae_faces.html#save-checkpoint",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.4 Save Checkpoint",
    "text": "1.4 Save Checkpoint\n\ndef save_checkpoint(model, epoch, loss, best_loss, path=\"./checkpoint.pt\"):\n    if loss &lt; best_loss:\n        print(f\"Saving new best model at epoch {epoch} with loss {loss:.4f}\")\n        torch.save(model.state_dict(), path)\n        return loss\n    return best_loss"
  },
  {
    "objectID": "vae_faces.html#training-step",
    "href": "vae_faces.html#training-step",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.5 Training Step",
    "text": "1.5 Training Step\n\ndef train_step(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n\n    for batch in dataloader:\n        x, _ = batch if isinstance(batch, (tuple, list)) else (batch, None)\n        x = x.to(device)\n\n        optimizer.zero_grad()\n\n        x_recon, mu, logvar = model(x)\n        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_recon += recon_loss.item()\n        total_kl += kl_loss.item()\n\n    num_batches = len(dataloader)\n    return {\n        \"loss\": total_loss / num_batches,\n        \"reconstruction_loss\": total_recon / num_batches,\n        \"kl_loss\": total_kl / num_batches\n    }"
  },
  {
    "objectID": "vae_faces.html#validation-step",
    "href": "vae_faces.html#validation-step",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.6 Validation Step",
    "text": "1.6 Validation Step\n\n@torch.no_grad()\ndef validate_epoch(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n\n    for batch in dataloader:\n        x, _ = batch if isinstance(batch, (tuple, list)) else (batch, None)\n        x = x.to(device)\n        x_recon, mu, logvar = model(x)\n        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar)\n        total_loss += loss.item()\n        total_recon += recon_loss.item()\n        total_kl += kl_loss.item()\n\n    num_batches = len(dataloader)\n    return {\n        \"loss\": total_loss / num_batches,\n        \"reconstruction_loss\": total_recon / num_batches,\n        \"kl_loss\": total_kl / num_batches\n    }"
  },
  {
    "objectID": "vae_faces.html#instantiate-model-optimizer-writer-device",
    "href": "vae_faces.html#instantiate-model-optimizer-writer-device",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.7 Instantiate model, optimizer, writer, device",
    "text": "1.7 Instantiate model, optimizer, writer, device\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nencoder = Encoder(z_dim=Z_DIM)\ndecoder = Decoder(z_dim=Z_DIM)\nvae = VAE(encoder, decoder).to(device)\n\noptimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n\nwriter = SummaryWriter(log_dir=\"./logs\")"
  },
  {
    "objectID": "vae_faces.html#training-the-vae-model",
    "href": "vae_faces.html#training-the-vae-model",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.8 Training the VAE Model",
    "text": "1.8 Training the VAE Model\n\nbest_loss = float(\"inf\")\n\nfor epoch in range(EPOCHS):\n    train_logs = train_step(vae, train_loader, optimizer, device)\n    val_logs = validate_epoch(vae, val_loader, device)\n\n    print(f\"Epoch {epoch+1:02d} | \"\n          f\"Train Loss: {train_logs['loss']:.4f} | \"\n          f\"Val Loss: {val_logs['loss']:.4f}\")\n\n    # Save best model\n    best_loss = save_checkpoint(vae, epoch, val_logs[\"loss\"], best_loss)\n\n    # Generate and save sample images\n    generate_images(vae, epoch, Z_DIM)\n\n    # Log to TensorBoard\n    writer.add_scalar(\"Loss/train\", train_logs[\"loss\"], epoch)\n    writer.add_scalar(\"Loss/val\", val_logs[\"loss\"], epoch)\n    writer.add_scalar(\"KL/train\", train_logs[\"kl_loss\"], epoch)\n    writer.add_scalar(\"Recon/train\", train_logs[\"reconstruction_loss\"], epoch)\n\nEpoch 01 | Train Loss: 72.4671 | Val Loss: 58.4050\nSaving new best model at epoch 0 with loss 58.4050"
  },
  {
    "objectID": "vae_faces.html#reconstruct-using-the-variational-autoencoder",
    "href": "vae_faces.html#reconstruct-using-the-variational-autoencoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.9 Reconstruct using the variational autoencoder",
    "text": "1.9 Reconstruct using the variational autoencoder\n\n# Select a batch of images from the training set\nexample_images, _ = next(iter(train_loader))  # Ignore labels\nexample_images = example_images[:8].to(device)  # Select first 8 images\n\n# Set model to evaluation mode\nvae.eval()\n\n# Forward pass through the VAE to get reconstructions\nwith torch.no_grad():\n    reconstructions, _, _ = vae(example_images)\n\n# Move tensors to CPU for display\nexample_images = example_images.cpu()\nreconstructions = reconstructions.cpu()\n\n# Display original images\nprint(\"Example real faces\")\ndisplay(example_images)\n\n# Display reconstructed images\nprint(\"Reconstructions\")\ndisplay(reconstructions)\n\nExample real faces\n\n\n\n\n\n\n\n\n\nReconstructions"
  },
  {
    "objectID": "vae_faces.html#generate-new-faces",
    "href": "vae_faces.html#generate-new-faces",
    "title": "Face Generation with Convolutional VAE",
    "section": "1.10 Generate New Faces",
    "text": "1.10 Generate New Faces\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample latent vectors from standard normal distribution\ngrid_width, grid_height = 10, 3\nnum_samples = grid_width * grid_height\nz_sample = torch.randn(num_samples, Z_DIM).to(device)\n\n# Decode to generate new faces\nvae.eval()\nwith torch.no_grad():\n    generated_faces = vae.decoder(z_sample).cpu()  # Shape: [N, C, H, W]\n\n# Plot grid of generated images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(num_samples):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    img = generated_faces[i].permute(1, 2, 0).numpy()  # CHW ‚Üí HWC\n    ax.imshow(img)\n\nplt.suptitle(\"Generated Faces from Sampled Latent Vectors\", fontsize=16)\nplt.show()"
  },
  {
    "objectID": "post-training.html",
    "href": "post-training.html",
    "title": "Post-Training LLMs",
    "section": "",
    "text": "Foundation models like GPT-4, Claude, and Gemini are large neural networks trained on massive text corpora to predict the next word in a sequence. This stage of training ‚Äî often called pre-training ‚Äî gives models broad generality. They learn patterns of language, knowledge, and relationships in data. This flexibility allows them to adapt across many tasks.\nBut in their raw form, these models are still optimized for next-token prediction rather than following instructions or aligning with human expectations. As a result, they can generate inconsistent reasoning, misleading statements, or unhelpful answers. This gap between raw capability and user needs is why post-training is essential‚Äî adapting a foundation model into a safe, reliable, instruction-following AI assistant, much like learning how to apply the knowledge from a vast library in useful, trustworthy ways.\nPost-training typically involves two key stages:\n\nSupervised Fine-Tuning (SFT): teaching the model to follow instructions and carry out tasks\nPreference Optimization: aligning outputs with human feedback using methods like RLHF, DPO, or RLAIF\n\nAlongside these methods, prompting techniques also play a key role. Approaches such as zero-shot, few-shot, and chain-of-thought prompting don‚Äôt change the model‚Äôs weights but can steer its behavior at inference time. Prompting often delivers strong results without additional training, making it an essential part of real-world LLM deployment.\nIn the sections that follow, we‚Äôll explore post-training techniques alongside prompting strategies, and how they work together to transform general-purpose LLMs into dependable, aligned assistants. This transformation explains why ChatGPT felt so different from GPT-3, why Claude can engage in dialogue that feels helpful rather than predictive, and why modern AI systems reliably follow complex instructions while staying aligned with user intent.\n\n\n\nFigure: High-level taxonomy of large language model (LLM) post-training strategies, showing where scaling methods (bottom right) fit alongside reinforcement, tuning, and alignment approaches. (Source: Kumar, K., Ashraf, T., Thawakar, O., et al., 2025)"
  },
  {
    "objectID": "post-training.html#introduction-why-post-training-matters-for-reasoning",
    "href": "post-training.html#introduction-why-post-training-matters-for-reasoning",
    "title": "Post-Training LLMs",
    "section": "",
    "text": "Foundation models like GPT-4, Claude, and Gemini are large neural networks trained on massive text corpora to predict the next word in a sequence. This stage of training ‚Äî often called pre-training ‚Äî gives models broad generality. They learn patterns of language, knowledge, and relationships in data. This flexibility allows them to adapt across many tasks.\nBut in their raw form, these models are still optimized for next-token prediction rather than following instructions or aligning with human expectations. As a result, they can generate inconsistent reasoning, misleading statements, or unhelpful answers. This gap between raw capability and user needs is why post-training is essential‚Äî adapting a foundation model into a safe, reliable, instruction-following AI assistant, much like learning how to apply the knowledge from a vast library in useful, trustworthy ways.\nPost-training typically involves two key stages:\n\nSupervised Fine-Tuning (SFT): teaching the model to follow instructions and carry out tasks\nPreference Optimization: aligning outputs with human feedback using methods like RLHF, DPO, or RLAIF\n\nAlongside these methods, prompting techniques also play a key role. Approaches such as zero-shot, few-shot, and chain-of-thought prompting don‚Äôt change the model‚Äôs weights but can steer its behavior at inference time. Prompting often delivers strong results without additional training, making it an essential part of real-world LLM deployment.\nIn the sections that follow, we‚Äôll explore post-training techniques alongside prompting strategies, and how they work together to transform general-purpose LLMs into dependable, aligned assistants. This transformation explains why ChatGPT felt so different from GPT-3, why Claude can engage in dialogue that feels helpful rather than predictive, and why modern AI systems reliably follow complex instructions while staying aligned with user intent.\n\n\n\nFigure: High-level taxonomy of large language model (LLM) post-training strategies, showing where scaling methods (bottom right) fit alongside reinforcement, tuning, and alignment approaches. (Source: Kumar, K., Ashraf, T., Thawakar, O., et al., 2025)"
  },
  {
    "objectID": "post-training.html#prompting",
    "href": "post-training.html#prompting",
    "title": "Post-Training LLMs",
    "section": "2 Prompting",
    "text": "2 Prompting\n\n2.1 In-Context Learning: Zero-Shot and Few-Shot\nConcept:\nIn-Context Learning (ICL) lets models learn new tasks by using examples directly in the prompt‚Äîno parameter updates required. Zero-shot uses just an instruction, while few-shot provides labeled examples. In both cases, the model infers patterns from the prompt and applies them to new inputs.\nKey Variants\n1. Zero-Shot Prompting: Ask the model to perform a task using only an instruction or question ‚Äî without providing any explicit examples of how the task should be done.\n\nThe model relies solely on its pre-trained knowledge and its understanding of the instruction to produce the output.\n\nEven without examples, the instruction itself acts as a minimal form of context that the model conditions on.\n\nExample: Translate the following sentence into French: The weather is beautiful today.\nNo translations are shown beforehand ‚Äî the model generates the French sentence using patterns it learned during training.\n2. Few-Shot Prompting: Provide a small number of (input ‚Üí output) examples before asking for a new prediction.\n\nOne example = one ‚Äúshot‚Äù, 5 examples = 5-shot.\nMore examples generally improve performance, but are limited by the model‚Äôs maximum context length.\n\nExample: Showing labeled sentiment examples before asking for classification of a new sentence.\nThe diagram below visualizes zero-shot, one-shot, and few-shot prompting, and contrasts them with fine-tuning for clarity.\n\n\n\nFigure: Illustration of three In-Context Learning settings ‚Äî Zero-Shot, One-Shot, and Few-Shot ‚Äî compared with traditional fine-tuning. In ICL, the model adapts based on examples in the prompt without weight updates, while fine-tuning updates model parameters through repeated training. (Source: Brown et al., 2020)\n\n\nThese prompting strategies differ in how much context the model receives before making a prediction. The charts below reveal how both model size and the number of in-context examples impact benchmark accuracy.\n\n\n\nFigure: SuperGLUE performance comparison between zero-shot, one-shot, and few-shot prompting, showing the effect of model size (left) and number of in-context examples (right). (Source: Brown et al., 2020)\n\n\nLeft chart: Larger models consistently improve SuperGLUE scores across all prompting types, with few-shot prompting delivering the largest gains ‚Äî the jump from the smallest (~0.1B) to the largest (175B) model is over 20 points in few-shot mode. Right chart: For GPT-3 (175B parameters), zero-shot starts at 68.0, one-shot improves to 69.8, and 8-shot reaches 72.5, matching Fine-Tuned BERT++. At 32-shot, performance climbs to 75.5, surpassing Fine-Tuned BERT++ without any parameter updates.\n\n\n\n\n\n\nPerformance Summary\n\n\n\nAcross all prompting types, performance consistently improves as model size increases. Few-shot prompting delivers the largest gains, with GPT-3 (175B) achieving scores close to fine-tuned models without any parameter updates.\n\n\nAdvantages\n\nFlexibility: The model can incorporate new information at inference time, enabling it to respond to queries beyond its original training cut-off date.\nNo Retraining Needed: Eliminates the need to re-train for minor domain or task changes.\nEmergent Performance: Works surprisingly well in very large models (e.g., GPT-3 with 175B parameters) for diverse tasks like translation, reading comprehension, arithmetic, SAT questions, and commonsense reasoning.\n\nLimitations\n\nContext Limits: Constrained by max input size; too many examples can‚Äôt fit.\nVariable Performance: Output quality depends on prompt wording, ordering, and examples.\nNo True Learning: The model‚Äôs weights aren‚Äôt updated‚Äîadaptation disappears once context is gone.\nReasoning Weaknesses: Struggles with multi-step reasoning and provides little interpretability for debugging.\n\n\n\n\n\n\n\nKey Insight\n\n\n\nICL effectively turns prompting into a form of temporary fine-tuning, where the ‚Äútraining data‚Äù exists only within the prompt window. This makes it a powerful method for continual adaptation without persistent retraining.\n\n\nSome of these reasoning limitations ‚Äî particularly weak multi-step reasoning ‚Äî can be addressed with Chain-of-Thought prompting (Wei et al., 2022), which guides the model to generate intermediate steps before producing the final answer.\n\n\n2.2 Addressing ICL‚Äôs Reasoning Limitations: Chain-of-Thought Prompting\nConcept:\nChain-of-Thought (CoT) prompting augments in-context learning by encouraging models to generate intermediate reasoning steps before the final answer. Instead of jumping directly from input ‚Üí output, the prompt shows worked-out examples of step-by-step reasoning, which the model then imitates. This produces more transparent, accurate results, especially on arithmetic or multi-step problems.\n\n\n\nFigure: Comparison of standard prompting and Chain-of-Thought prompting on arithmetic reasoning tasks. CoT provides explicit step-by-step reasoning, enabling the model to correctly solve problems that standard prompting fails. (Source: Wei et al., 2022)\n\n\nWhy It Helps:\nCoT prompting addresses several ICL weaknesses:\n\nImproved Multi-Step Reasoning: By breaking down problems into smaller sub-problems, the model performs better on tasks requiring arithmetic computation, symbolic reasoning, and commonsense logic.\nInterpretability: Produces intermediate steps that allow humans to verify and debug reasoning.\nError Reduction: Reduces the likelihood of ‚Äúshortcut‚Äù or guess-based answers by forcing the model to articulate the full solution path.\n\nEmpirical Evidence (Wei et al., 2022):\nThe chart below is arranged by benchmark in rows (Row 1 = GSM8K, Row 2 = SVAMP, Row 3 = MAWPS) and by model family in columns (Column 1 = LaMDA, Column 2 = GPT, Column 3 = PaLM).\n\nOn the GSM8K math word problem benchmark (Row 1), CoT prompting with GPT-3 175B improved solve rate from 17.7% (standard prompting) to 57.1%, a gain of nearly 40 percentage points.\n\nOn the SVAMP benchmark (Row 2), CoT prompting with GPT-3 175B increased performance from about 44% to 72%, surpassing the prior supervised best.\n\nOn the MAWPS benchmark (Row 3), CoT prompting with PaLM 540B boosted accuracy from roughly 78% to 92%, matching or exceeding the prior supervised best.\n\nAcross all benchmarks and model families (LaMDA, GPT, PaLM), larger models consistently show greater gains from CoT prompting, with the blue curves sitting above the black curves, especially at the largest scales.\n\n\n\nFigure: Performance comparison between standard prompting and Chain-of-Thought prompting across GSM8K, SVAMP, and MAWPS benchmarks for different model families (LaMDA, GPT, PaLM). CoT delivers large gains, particularly for larger models. (Source: Wei et al., 2022)\n\n\nWhen It Works Best:\n\nTasks with compositional structure where intermediate reasoning steps are natural.\n\nModels with sufficient scale (CoT‚Äôs benefits appear more strongly in models &gt;100B parameters).\n\nPrompts that clearly format and separate reasoning from final answers.\n\nAdvantages\n\nEnhanced Multi-Step Reasoning: Significant gains on benchmarks like GSM8K and MultiArith.\nInterpretability: Step-by-step outputs help in auditing and debugging.\nPrompt-Only Method: No fine-tuning required; works within ICL framework.\nEmergent Capability: Reasoning skills become apparent in large-scale models.\n\nLimitations\n\nModel Size Dependency: Large performance gains appear mainly in models &gt;100B parameters; smaller models often produce incoherent chains.\nIncreased Output Length: More tokens may affect latency and cost.\nLimited Benefit for Simple Tasks: Minimal improvement for factual recall or single-step reasoning.\nError Propagation: Early mistakes in reasoning can cascade to incorrect final answers.\nHallucinated Logic: Reasoning may be coherent but factually wrong.\n\nBeyond prompt engineering, another way to boost model performance without changing its weights is through Test-Time Scaling (TTS).\nWhile both prompting and TTS work without retraining the model, they target different levers for improvement:\n\n\n\n\n\n\n\n\nFeature\nPrompting\nTest-Time Scaling (TTS)\n\n\n\n\nWhen applied\nBefore inference\nDuring inference\n\n\nWhat changes\nThe input prompt to guide the model‚Äôs output\nThe inference process ‚Äî computation allocation, search, sampling\n\n\nCore mechanism\nBetter instructions, examples, or formatting\nIterative refinement, search-based decoding, adaptive sampling\n\n\nGoal\nSteer the model toward a better answer\nImprove accuracy/reasoning by giving the model more ‚Äúthinking‚Äù time or attempts\n\n\nAnalogy\nAsking a student a well-phrased question\nGiving the student more scratch paper and extra tries\n\n\n\nThis distinction is important: prompting focuses on how you ask, while TTS focuses on how the model thinks and searches for answers once asked."
  },
  {
    "objectID": "post-training.html#testtime-scaling-methods",
    "href": "post-training.html#testtime-scaling-methods",
    "title": "Post-Training LLMs",
    "section": "3 Test‚ÄëTime Scaling Methods",
    "text": "3 Test‚ÄëTime Scaling Methods\nConcept:\nTest-time scaling (TTS) boosts a model‚Äôs reasoning and performance during inference‚Äîwithout changing its parameters‚Äîby allocating more ‚Äúthinking time.‚Äù It does this by running extra compute, exploring multiple reasoning paths, and blending the best results. Empirical results show strong efficiency: TTS can match the performance of models up to 14√ó larger while using 4√ó less compute[1]. Techniques like Self-Consistency‚Äîsampling multiple reasoning paths and taking a majority vote‚Äîfurther improve reasoning accuracy, with gains of +17.9% on GSM8K, +11.0% on SVAMP, and +12.2% on AQuA[9].\nTTS is especially valuable when inference budgets are limited or the base model already has strong core competence. However, for tasks that require fundamentally new capabilities, pre-training remains essential, since larger models inherently encode deeper reasoning ability.\nAdvantages:\n\nImproves performance without retraining or altering model parameters\n\nCan match or exceed much larger models on certain tasks at a fraction of the compute cost\n\nFlexible allocation of compute based on input complexity\n\nEnables on‚Äëdemand scaling in resource‚Äëconstrained settings\n\nWorks well as part of a hybrid strategy with pre-training\n\nLimitations:\n\nGains are often smaller on tasks requiring novel skills not present in the base model\n\nAdditional inference compute can increase latency and operational costs\n\nRequires effective task‚Äëcomplexity estimation to allocate resources efficiently\n\nMay yield diminishing returns if scaling is excessive for a given task\n\nThe diagram below organizes TTS techniques into categories, showing typical integration flows between methods.\n\n\n\nFigure: An overview of Test-Time Scaling methods, organized into categories such as parallel scaling, sequential scaling, and search-based methods. Arrows indicate common integration flows‚Äîe.g., Chain-of-Thought Prompting feeding into Tree-of-Thoughts for deeper reasoning. This structure helps practitioners select the right combination of strategies based on task complexity. (Source: Kumar et al., 2023)\n\n\nCategory Overview:\n\nScaling Strategies ‚Äì Methods that expand search breadth or depth, exploring multiple reasoning paths to increase accuracy (e.g., Beam Search, Monte Carlo Tree Search, Best-of-N Search).\n\nAdvanced Sampling ‚Äì Techniques that selectively sample outputs based on confidence or external verification to improve efficiency and quality (e.g., Confidence-Based Sampling, Search Against Verifiers).\n\nImproved Reasoning ‚Äì Approaches that structure thinking into multi-step processes for complex problem-solving (e.g., Chain-of-Thought Prompting, Tree-of-Thoughts, Self-Consistency Decoding).\n\nSequential Revision ‚Äì Iterative refinement strategies that repeatedly improve answers until they meet quality criteria (e.g., Self-Improvement via Refinements).\n\nWhile many approaches exist for test‚Äëtime scaling, a few have emerged as especially influential in practical LLM applications. The sections below explore some of these methods in greater detail, including their mechanics, strengths, and trade‚Äëoffs.\n\n3.1 Best-of-N Search (Rejection Sampling)\nConcept:\nBest-of-N (BoN) search enhances model performance at inference by generating N candidate outputs (often via sampling) and selecting the one that scores highest according to a chosen criterion ‚Äî such as a reward model, likelihood score, or rule-based evaluator. This approach systematically explores multiple solution paths and prunes all but the top-rated result. Compared to Beam Search, BoN treats each candidate independently, which can increase diversity but may also be more computationally expensive.\nAdvantages:\n\nCan significantly boost task performance, especially with a robust reward model\nFlexible ‚Äî works with both rule-based and learned scoring functions\nSimple to implement; requires only control over N and the selection criterion\nCompetitive with post-training methods like RLHF and DPO when paired with strong evaluators\n\nLimitations:\n\nHigher computational cost as N increases\n\nPerformance depends heavily on the quality of the scoring function or reward model\n\nMay select lower-probability solutions if scoring is imperfect (reward hacking risk)\n\nInstability can occur if the N parameter is too large or too small for the task\n\n\n\n3.2 Self-Consistency Decoding\nConcept:\nSelf-Consistency is a decoding strategy designed to improve reasoning by aggregating answers from multiple reasoning paths. Instead of following a single chain of thought, the model samples diverse reasoning chains (using techniques like prompt engineering to encourage diversity, temperature sampling, or stochastic decoding) and then outputs the final answer that is most consistent across them.\nThe underlying intuition is that if a complex question has a unique correct answer, different valid reasoning paths should converge on the same result. This majority vote or highest probability after marginalization approach reduces the likelihood of errors from flawed single reasoning chains and is especially effective in reasoning tasks.\nAdvantages:\n\nImproves correctness in complex reasoning scenarios\n\nWorks well for arithmetic, commonsense reasoning, and multi-step problem-solving\n\nReduces reliance on any single, potentially flawed reasoning path\n\nCan be combined with Chain-of-Thought prompting for greater gains\n\nLimitations:\n\nHigher inference cost due to multiple reasoning path generations\n\nGains diminish for simpler tasks where a single reasoning path is sufficient\n\nEffectiveness depends on diversity and quality of sampled reasoning paths\n\n\n\n\n\n\n\nKey Difference: Best-of-N vs.¬†Self-Consistency\n\n\n\n\nBest-of-N Search (Rejection Sampling): Goal is to find the highest-quality single output.\nThe model is run multiple times with the same prompt, and each output is scored or filtered using a heuristic, probability, or verifier. The output with the best score is selected.\nSelf-Consistency Decoding: Goal is to find the answer most reasoning paths agree on.\nThe model is run multiple times, encouraging different step-by-step reasoning chains. The final answer from each chain is collected, and the one appearing most often (majority vote) is chosen.\n\n\n\nThe following table summarizes key test‚Äëtime scaling methods described in LLM Post‚ÄëTraining: A Deep Dive into Reasoning by Komal Kumar, Tajamul Ashraf et‚ÄØal[1].\n\n\n\n\n\n\n\n\n\nMethod\nGoal & Common Use Cases\nBenefits\nLimitations\n\n\n\n\nBeam Search\nMaintain top‚ÄëN highest‚Äëprobability reasoning paths at each step; used in structured reasoning, planning, and Tree‚Äëof‚ÄëThought search.\nImproves coherence and accuracy by systematically exploring multiple reasoning chains.\nComputationally expensive; beam width must be tuned; may still miss rare but correct paths.\n\n\nBest‚Äëof‚ÄëN (Rejection Sampling)\nGenerate N candidate outputs (via sampling) and select the best using a criterion (e.g., reward model, model likelihood).\nImproves answer quality for easier tasks; straightforward to implement; flexible budget.\nHigh cost if N is large; requires effective selection criteria; may miss diversity benefits.\n\n\nSelf‚ÄëConsistency Decoding\nSample multiple reasoning chains, then select the most common final answer.\nImproves multi‚Äëstep reasoning accuracy; simple and model‚Äëagnostic.\nHigher inference cost; relies on majority vote, which may fail if most outputs are wrong.\n\n\nTree of Thoughts (ToT)\nExpand multiple reasoning paths as a search tree, evaluating and pruning branches.\nEnhances complex problem solving and planning; allows backtracking.\nHigh computational cost; requires good heuristics to prune effectively.\n\n\nSearch‚ÄëAugmented Verification\nUse external verifiers to evaluate and rank candidate answers or reasoning steps.\nIncreases correctness in binary decision tasks; modular and flexible.\nDependent on verifier quality; additional inference steps add latency.\n\n\nSelf‚ÄëImprovement via Refinements\nModel iteratively critiques and revises its own answers until acceptable.\nCan improve accuracy across varied tasks; useful for open‚Äëended reasoning.\nRisk of over‚Äëediting or drifting from initial intent; higher inference cost.\n\n\n\nWhile prompting and test-time scaling work well, they have limits‚Äîsometimes you need to actually change how the model thinks, which requires updating its weights through supervised fine-tuning."
  },
  {
    "objectID": "post-training.html#supervised-fine-tuning-sft",
    "href": "post-training.html#supervised-fine-tuning-sft",
    "title": "Post-Training LLMs",
    "section": "4 Supervised Fine-Tuning (SFT)",
    "text": "4 Supervised Fine-Tuning (SFT)\nConcept:\nSupervised Fine-Tuning (SFT) is a post-training method where a pre-trained foundation model is further trained on labeled examples (input‚Äìoutput pairs) to better align it with desired behaviors or domain needs. While pre-training learns broad language patterns, SFT narrows the focus‚Äîadapting the model for specific goals by adjusting its internal weights on high-quality datasets. In contrast to prompting or test-time scaling, which only influence outputs at inference time, SFT changes the model‚Äôs parameters directly, so the improvements stay in the model even after the training. It also serves as the foundation for later optimization steps such as preference alignment (e.g., RLHF or DPO).\nSFT can be applied in two main ways:\n\nGeneral-purpose instruction tuning ‚Äì teaching the model to follow diverse natural language instructions across many tasks.\n\nDomain-specific tuning ‚Äì adapting the model for specialized fields such as legal, medical, or financial applications.\n\nWhile many approaches exist for supervised fine‚Äëtuning, Instruction Fine‚ÄëTuning and Domain‚ÄëSpecific Fine‚ÄëTuning are among the most prevalent in modern LLM post‚Äëtraining pipelines. The sections below explore these methods in greater detail.\n\n4.1 Instruction Fine-Tuning\nConcept:\nInstruction Fine-Tuning is the most common form of SFT, focusing on training a pre-trained model with a broad range of instruction‚Äìresponse examples across tasks like summarization, question answering, classification, and creative writing. The aim is to help the model reliably follow natural language instructions ‚Äî even for tasks it hasn‚Äôt explicitly seen ‚Äî by aligning outputs with the user‚Äôs intent rather than simply predicting the next word.\nAdvantages:\n\nStrong generalization to unseen tasks in zero-shot and few-shot settings.\nProduces more helpful, consistent, and structured outputs.\nImproves controllability across varied prompt styles.\nProvides a solid baseline for later optimization.\n\nLimitations:\n\nNeeds large, diverse, and high-quality instruction datasets.\nOpen-ended tasks can be hard to evaluate objectively.\nHuman-generated data may bring bias, inconsistency, or noise.\nRisk of overfitting to the style of the fine-tuning dataset if it lacks diversity.\n\nThe table below reports results from the Flan (Wei et al., 2022) study, which evaluated the impact of supervised fine-tuning (SFT) ‚Äî specifically instruction fine-tuning ‚Äî on multiple pre-trained language models of different sizes (ranging from 80M to 540B parameters).\nPerformance is measured across four benchmarks:\n\nMMLU ‚Äì a multi-task test covering 57 diverse subjects.\n\nBBH ‚Äì the Big-Bench Hard set of reasoning tasks.\n\nTyDiQA ‚Äì a multilingual question-answering benchmark.\n\nMGSM ‚Äì multilingual grade school math problems.\n\nThe ‚ÄúNorm. avg.‚Äù column is the unweighted normalized average score across all benchmarks, while ‚ÄúDirect‚Äù and ‚ÄúCoT‚Äù show performance under direct prompting and chain-of-thought prompting. Gains in parentheses represent improvement in normalized average from adding SFT.\n\n\n\nFigure: Performance of instruction-fine-tuned models (FLAN, FLAN-PaLM) compared to their base models on a mix of unseen task categories. Results show consistent improvements across model sizes, with gains of up to 9‚Äì10 points in average accuracy after instruction fine-tuning. (Source: Wei et al., 2022)\n\n\nInstruction fine-tuning consistently improves performance across model sizes. In the Flan-T5 family, gains tend to grow with model size ‚Äî from +6.1 for the 80M model to +26.6 for the 11B model. For PaLM, the 62B version‚Äôs normalized average jumps from 28.4 to 38.8 (+10.4), with notable gains on MMLU (55.1 ‚Üí 59.6) and MGSM (18.2 ‚Üí 28.5). The largest 540B model also improves from 49.1 to 58.4 (+9.3) and boosts TyDiQA accuracy from 52.9 to 67.8. While the improvement is smaller in absolute terms for the 540B model, this is because it starts from a much higher baseline, leaving less headroom for improvement[5].\n\n\n4.2 Domain-Specific Fine-Tuning\nConcept:\nDomain-Specific Fine-Tuning narrows the model‚Äôs focus to excel in a specialized area ‚Äî such as finance, healthcare, law, climate science, or software engineering ‚Äî by training it on carefully selected domain-relevant datasets. This targeted approach strengthens the model‚Äôs command of the terminology, style, and knowledge specific to the field, enabling more precise, trustworthy, and context-aware outputs for professional or high-stakes use.\nAdvantages:\n\nBoosts accuracy, factual grounding, and relevance in the chosen domain.\nBuilds user trust in sensitive or regulated applications.\nSupports compliance with industry standards.\nCan reduce hallucinations by anchoring responses in vetted domain content.\n\nLimitations:\n\nRequires high-quality domain datasets, which may be expensive or hard to source.\nMay lose versatility on out-of-domain tasks.\nInherits biases or blind spots from domain data.\nRisk of over-specialization if coverage is too narrow.\n\nThe chart below reports results from the BioMedLM study (Singhal et al., 2022), which applied domain-specific fine-tuning to a large pre-trained model (Flan-PaLM 540B) using biomedical and clinical datasets from the MultiMedQA benchmark. Performance is compared against the best previously published models across three key datasets:\n\nMedMCQA ‚Äì general medical knowledge in Indian medical entrance exams.\n\nMedQA (USMLE) ‚Äì general medical knowledge in US medical licensing exams.\n\nPubMedQA ‚Äì biomedical literature question answering.\n\nAccuracy (%) is shown for the previous state-of-the-art (SOTA) and for the domain-tuned Flan-PaLM 540B model.\n\n\n\nFigure: Accuracy of Flan-PaLM 540B after domain-specific fine-tuning compared to previous state-of-the-art models on three biomedical QA benchmarks. (Source: Singhal et al., 2022)\n\n\nDomain-specific fine-tuning delivers substantial accuracy gains across all three benchmarks. On MedQA (USMLE), Flan-PaLM 540B achieves 67.6%, exceeding the previous best (PubMedGPT) by over 17 points. On MedMCQA, performance rises to 57.6% from the prior best of 52.9%, while on PubMedQA it reaches 79.0%, slightly surpassing the earlier record of 78.2%. These results demonstrate that aligning a large language model with specialized biomedical knowledge can yield meaningful improvements, especially for complex, domain-specific reasoning tasks, even when starting from an already strong general-purpose model[6].\nThe following table summarizes supervised fine-tuning methods described in LLM Post-Training: A Deep Dive into Reasoning by Komal Kumar, Tajamul Ashraf‚ÄØet‚ÄØal[1].\n\n\n\n\n\n\n\n\n\nFine-tuning Type\nGoal & Common Use Cases\nBenefits\nLimitations\n\n\n\n\nInstruction Fine-Tuning\nTrain LLMs to follow diverse instructions (e.g., summarization, classification, QA, creative writing). Enables zero-/few-shot generalization across tasks.\nImproves generalization and alignment; makes outputs more helpful and controllable.\nRequires large, curated datasets; open-ended tasks are harder to evaluate; may reflect human bias.\n\n\nDialogue (Multi-turn) Fine-Tuning\nEnable coherent, context-aware multi-turn conversations for chatbots and digital assistants.\nImproves coherence, context tracking, and conversational experience.\nCan overfit to chattiness; needs large, high-quality multi-turn dialogue datasets.\n\n\nChain-of-Thought (CoT) Reasoning Fine-Tuning\nEncourage step-by-step reasoning in math, logic puzzles, multi-hop QA.\nImproves reasoning interpretability and multi-step accuracy.\nRequires structured reasoning traces; limited to reasoning-style tasks.\n\n\nDomain-Specific Fine-Tuning\nAdapt models for specialized fields (e.g., biomedicine, finance, legal, climate, code).\nImproves accuracy and relevance in domain-specific applications.\nNeeds high-quality, domain-specific corpora; risk of reduced generality.\n\n\nDistillation-Based Fine-Tuning\nTransfer capabilities from a large ‚Äúteacher‚Äù model to a smaller ‚Äústudent‚Äù model.\nProduces smaller, faster models with high performance; reduces compute cost.\nMay lose nuance or performance compared to teacher; quality depends on teacher data.\n\n\nPreference/Alignment SFT\nTrain models on labeled or ranked preference data before RLHF or DPO stages.\nImproves alignment with human values; reduces harmful or irrelevant outputs.\nLimited by scope and quality of preference data; definitions of ‚Äúdesirable‚Äù can vary.\n\n\nParameter-Efficient Fine-Tuning (PEFT)\nEfficiently adapt models without updating all weights (e.g., LoRA, adapters, prefix tuning).\nResource-efficient; enables adaptation on limited hardware.\nMay underperform full fine-tuning; sensitive to hyperparameter choices.\n\n\n\nWhile SFT equips models to follow a wide range of tasks, it has inherent limitations ‚Äî from the cost of collecting high-quality datasets, to challenges with subjective or open-ended tasks that have no single ‚Äúright‚Äù answer, to mismatches between language modeling objectives and human expectations. For example, language modeling penalizes all token-level mistakes equally, even though some errors are more serious than others. These issues can lead SFT-trained models to produce factually correct but unsatisfying outputs, or to mirror suboptimal human-generated answers. To overcome these gaps, the next step is often preference optimization ‚Äî aligning the model more directly with human values, judgments, and desired behaviors."
  },
  {
    "objectID": "post-training.html#preference-optimization",
    "href": "post-training.html#preference-optimization",
    "title": "Post-Training LLMs",
    "section": "5 Preference Optimization",
    "text": "5 Preference Optimization\nConcept:\nPreference optimization is the process of training a language model so that its outputs align with human preferences ‚Äî not just to complete a task correctly, but to respond in ways that are safe, contextually relevant, and consistent with user expectations. This is achieved by providing the model with feedback signals, often referred to as rewards, that indicate which responses are better. Unlike standard task accuracy metrics, these rewards are based on human or AI-generated judgments. Feedback can come from humans (e.g., ranking or rating responses) or from AI-generated comparisons, and can evaluate factual correctness, reasoning, and coherence.\nHowever, large language models make preference optimization more complex than traditional reinforcement learning. They operate in a vast vocabulary action space, where there are millions of possible token sequences. Rewards are often delayed until an entire output is produced, and models must balance multiple ‚Äî sometimes conflicting ‚Äî objectives. Unlike small, well-defined RL environments, there are no universal ‚Äúright‚Äù answers, and perceptions of what is ‚Äúdesirable‚Äù vary across cultures, topics, and personal beliefs. The ultimate goal is to guide the model‚Äôs behavior toward human values and expectations, producing responses that are not only correct but also helpful, safe, and aligned with the intended use.\nAdvantages:\n\nAligns outputs with expectations ‚Äî Matches user needs and ethical guidelines.\nImproves perceived helpfulness and safety in real-world applications.\nReduces harmful, offensive, or irrelevant outputs.\nOptimizes for nuanced objectives such as tone, reasoning quality, and factuality.\nSupports personalization to match specific user or organizational needs.\n\nLimitations:\n\nSubjectivity of preferences ‚Äî What is ‚Äúhelpful‚Äù or ‚Äúappropriate‚Äù can vary across individuals, contexts, and cultures.\nControversial topics ‚Äî Risk of alienating some users; overly cautious models may appear bland or evasive.\nTechnical complexity ‚Äî Large action spaces, delayed rewards, and balancing multiple objectives make optimization challenging.\nBias and fairness risks ‚Äî Preferences used in training may embed societal biases.\nDeployment trade-offs ‚Äî Overly strict filtering can reduce engagement; insufficient filtering can create reputational and safety risks.\n\nWhile many approaches exist for preference optimization, Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are two of the most widely used in modern LLM alignment pipelines. The next sections explore these methods in greater detail.\n\n5.1 Reinforcement Learning from Human Feedback (RLHF)\nConcept:\nRLHF is a widely used method for aligning large language models (LLMs) with human preferences by collecting human feedback and using it to guide reinforcement learning. Instead of simply optimizing for accuracy on a fixed dataset, RLHF incorporates judgments about which outputs are more helpful, safe, or aligned with user expectations.\nThe process involves three main stages:\n\nSupervised Fine-Tuning (SFT): Train a baseline model on high-quality instruction‚Äìresponse pairs so it learns the basics of following human instructions.\n\nPreference Data Collection: Gather human feedback by having annotators rank multiple model responses for the same prompt from best to worst. For example, if a model gives a technically correct but rude response, human annotators would rank a polite version higher, teaching the reward model to value helpfulness.\n\nReinforcement Learning Optimization: Fine-tune the model with reinforcement learning (often using Proximal Policy Optimization, PPO) so that it produces outputs that maximize a learned reward model based on collected preferences.\n\nThe figure below illustrates this process from start to finish, showing how RLHF moves from human demonstrations to ranking comparisons and finally to policy optimization.\n\n\n\nFigure 1: RLHF pipeline showing three stages ‚Äî (1) supervised fine-tuning from demonstrations, (2) preference ranking to train a reward model, and (3) policy optimization using reinforcement learning. (Source: Ouyang et al., 2022)\n\n\nRLHF Performance Gains:\nRLHF has been shown to produce higher-quality outputs than models trained only with pre-training or supervised fine-tuning. In human evaluations on the TL;DR summarization dataset, RLHF-trained models not only outperform models trained without RLHF but also exceed the quality of the human-written reference summaries‚Äîbaseline summaries created by people, shown as the dotted black line in the figure below. This performance advantage is consistent across model sizes and increases steadily as model capacity grows.\n\n\n\nFigure: Human preference ratings for model-generated summaries on the TL;DR dataset. RLHF significantly outperforms both pre-training and supervised fine-tuning across all model sizes. (Source: Stiennon et al., 2020)\n\n\nAdvantages:\n\nProduces highly aligned and safe responses.\n\nCan optimize for complex, nuanced objectives beyond raw accuracy.\n\nAllows fine-grained control through reward model design.\n\nLimitations:\n\nVery expensive and labor-intensive to gather high-quality preference data.\n\nQuality depends heavily on annotator skill and consistency.\n\nReward models can be exploited (reward hacking).\n\nSensitive to bias in collected preferences.\n\nWhile RLHF achieves strong alignment, it comes with heavy complexity and cost. Training requires fitting a value function, online sampling, and careful hyperparameter tuning ‚Äî processes that are expensive, time-consuming, and fragile. These challenges motivated simpler alternatives. Direct Preference Optimization (DPO) offers one: a streamlined approach that skips reinforcement learning while still optimizing for human preferences.\n\n\n5.2 Direct Preference Optimization (DPO)\nConcept:\nDirect Preference Optimization (DPO) is a simpler, more efficient alternative to RLHF that eliminates the need for a separate reward model and reinforcement learning loop. Instead of predicting absolute reward scores, DPO learns directly from preference pairs (chosen vs.¬†rejected outputs) by optimizing the log-likelihood ratio to favor preferred responses. For example, DPO directly learns that ‚ÄúHere‚Äôs a step-by-step solution‚Ä¶‚Äù is better than ‚ÄúI think the answer is‚Ä¶‚Äù for math problems, without needing a separate reward model. This ‚Äúbakes in‚Äù human preferences directly into the model parameters, avoiding costly online sampling, PPO training, and hyperparameter sensitive RL steps.\nThe main difference between RLHF and DPO lies in how they use preference data. The figure below shows how DPO streamlines the process by removing the reward model and RL loop.\n\n\n\nFigure: High-level comparison of RLHF and DPO pipelines. RLHF trains a separate reward model and uses reinforcement learning to optimize the policy, while DPO bypasses the reward model and RL loop, directly fine-tuning the model on preference pairs. (Source: Rafailov, 2023)\n\n\nAs shown in the figure, RLHF involves three stages: (1) supervised fine-tuning, (2) training a separate reward model, and (3) applying reinforcement learning (often PPO) to update the policy. In contrast, DPO eliminates the reward model and the reinforcement learning step entirely. Instead, it directly fine-tunes the model on preference pairs, using a mathematically derived objective that encourages the preferred response to be more likely than the rejected one. This simplification reduces complexity and resource requirements while still leveraging human preference data for alignment.\nBeyond its architectural simplicity, DPO has demonstrated strong empirical performance across summarization and dialogue tasks. Figure below reports GPT-4-evaluated ‚Äúhelpfulness‚Äù win rates against ground truth for multiple models and baselines. DPO consistently outperforms PPO, SFT, and other methods, achieving results on par with or better than the ‚ÄúBest of 128‚Äù baseline across both tasks.\n\n\n\nFigure: Summarization and dialogue helpfulness win rates vs.¬†ground truth, showing DPO outperforming other methods. (Source: Stanford CS224N, 2024, based on Rafailov et al., 2023)\n\n\nAdvantages:\n\nMuch simpler and more stable than RLHF.\n\nNo need for online sampling or PPO training.\n\nEasily scales to large datasets.\n\nWell-suited for popular open-source models like LLaMA and OpenChat.\n\nLimitations:\n\nNo per-step credit assignment ‚Äî scores full outputs instead of incremental steps.\nMay underperform RLHF on multi-step reasoning or complex, long-horizon tasks.\n\nDependent on high-quality preference pairs; poor data reduces effectiveness.\n\nOverall, DPO offers a practical trade-off ‚Äî delivering much of RLHF‚Äôs alignment benefits with significantly lower complexity and cost, making it a compelling choice for many modern LLM training pipelines, especially when rapid iteration is important.\n\n\n\n\n\n\nRLHF vs DPO: Key Differences\n\n\n\nReinforcement Learning from Human Feedback (RLHF)\n\nTrains an explicit reward model on comparison data to predict a score for a given completion.\nOptimizes the LM to maximize the predicted score under a KL-constraint.\nVery effective when tuned well, but computationally expensive and tricky to get right.\nRequires multiple steps: supervised fine-tuning ‚Üí reward model training ‚Üí policy optimization (e.g., PPO).\n\nDirect Preference Optimization (DPO)\n\nOptimizes LM parameters directly on preference data by solving a binary classification problem.\nAvoids reward model training and reinforcement learning loops entirely.\nSimpler and more efficient than RLHF while maintaining similar alignment benefits.\nDoes not leverage online data; operates purely on static preference pairs.\n\n\n\nThe following table summarizes supervised Preference Optimization methods described in LLM Post-Training: A Deep Dive into Reasoning by Komal Kumar, Tajamul Ashraf‚ÄØet‚ÄØal[1].\n\n\n\n\n\n\n\n\n\nMethod\nGoal & Common Use Cases\nBenefits\nLimitations\n\n\n\n\nRLHF (Reinforcement Learning from Human Feedback)\nAlign model outputs with human expectations using preference comparisons from human annotators to train a reward model, then optimize with RL (e.g., PPO).\nProduces helpful, safe, and human-aligned responses; can optimize for nuanced objectives; widely adopted in practice.\nExpensive and time‚Äëconsuming to collect human preference data; reward models can be overfit or gamed; dependent on noisy human judgments.\n\n\nRLAIF (Reinforcement Learning from AI Feedback)\nReplace human annotation with AI‚Äëgenerated feedback to create preference labels for training the reward model.\nReduces cost and time; scalable to large datasets; avoids bottleneck of human labeling.\nQuality depends on feedback model; risk of propagating biases or errors from the AI judge; less diversity than human feedback.\n\n\nDPO (Direct Preference Optimization)\nLearn directly from preference pairs without training a separate reward model or running PPO, by optimizing likelihood ratios to favor preferred responses.\nSimpler and more stable than RLHF; no online sampling; scalable; increasingly popular in open‚Äësource LLMs.\nLacks per‚Äëstep credit assignment; may underperform RLHF for complex reasoning tasks; dependent on high‚Äëquality preference data.\n\n\nOREO (Online Reasoning Optimization)\nRL method to improve multi‚Äëstep reasoning by refining policies based on reasoning‚Äëstep evaluations rather than just final answers.\nFine‚Äëgrained feedback at reasoning step level; boosts reasoning accuracy and interpretability.\nComputationally intensive; domain‚Äëspecific; requires curated reasoning traces.\n\n\nGRPO (Group Relative Policy Optimization)\nRL variant that scores multiple outputs for the same query relative to each other, eliminating the need for a critic model.\nReduces memory usage; stabilizes training; enables fine‚Äëgrained rewards for complex reasoning tasks.\nRequires large groups of candidate responses; effectiveness depends on diversity and quality of generated outputs.\n\n\nPure RL‚ÄëBased LLM Refinement\nMulti‚Äëstage RL pipelines (e.g., DeepSeek-R1) that refine models without or with minimal SFT, often incorporating distillation and curated reasoning traces.\nCan achieve high performance without large SFT datasets; distillation improves efficiency; robust reasoning capabilities.\nComplex to implement; computationally expensive; requires large curated datasets for stability and quality."
  },
  {
    "objectID": "post-training.html#conclusion",
    "href": "post-training.html#conclusion",
    "title": "Post-Training LLMs",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nPost-training combines complementary strategies that transform raw LLMs into capable assistants. Prompting and test-time scaling enable quick improvements without retraining, while supervised fine-tuning and preference optimization build deeper alignment. The key is to match the method to the goal‚Äîbalancing speed, cost, and alignment needs.\n\nPrompting: quick results without model changes\nTest-time scaling: boosts reasoning within compute budget\nSupervised fine-tuning: persistent behavior/domain expertise\nPreference optimization: safety, alignment, human-like responses\n\nThe strongest systems integrate these methods‚Äîprompting for fast control, test-time scaling for complex reasoning, fine-tuning for skills, and preference optimization for safety. OpenAI‚Äôs evolution from GPT-3 to ChatGPT layered tuning, RLHF, and CoT to transform raw capability into reliable assistance. Similarly, Klarna‚Äôs AI assistant blends fine-tuning, preference optimization, and TTS to achieve human-level workloads with consistent quality.\n\n\n\n\n\n\nKey takeaway\n\n\n\nEffective post-training is about matching the method to the goal. Understanding the trade-offs of each approach empowers practitioners to build LLMs that are not only more capable, but also safer, more reliable, and better aligned with human intent."
  },
  {
    "objectID": "post-training.html#references-further-reading",
    "href": "post-training.html#references-further-reading",
    "title": "Post-Training LLMs",
    "section": "7 References & Further Reading",
    "text": "7 References & Further Reading\n[1] Kumar, K., Ashraf, T., Thawakar, O., et al.¬†(2025). LLM Post-Training: A Deep Dive into Reasoning Large Language Models. https://arxiv.org/abs/2502.21321\n[2] Brown, T., Mann, B., Ryder, N., et al.¬†(2020). Language Models are Few-Shot Learners (GPT-3). https://arxiv.org/abs/2005.14165\n[3] Wei, J., Wang, X., Schuurmans, D., et al.¬†(2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. https://arxiv.org/abs/2201.11903\n[4] Ouyang, L., Wu, J., Jiang, X., et al.¬†(2022). Training Language Models to Follow Instructions with Human Feedback (InstructGPT). https://arxiv.org/abs/2203.02155\n[5] Chung, H. W., Hou, L., Longpre, S., et al.¬†(2022). Scaling Instruction-Finetuned Language Models (Flan). https://arxiv.org/abs/2210.11416\n[6] Singhal, K., Azizi, S., Tu, T., et al.¬†(2022). Large Language Models Encode Clinical Knowledge. https://arxiv.org/abs/2212.13138\n[7] Stiennon, N., Ouyang, L., Wu, J., et al.¬†(2020). Learning to Summarize with Human Feedback. https://arxiv.org/abs/2009.01325\n[8] Rafailov, R., Sharma, A., Mitchell, E., et al.¬†(2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. https://arxiv.org/abs/2305.18290\n[9] Wei, J., Wang, X., Schuurmans, D., et al.¬†(2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. https://arxiv.org/abs/2201.11903\n[10] Wang, X., Wei, J., Schuurmans, D., et al.¬†(2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models. https://arxiv.org/abs/2203.11171\n[11] Yao, S., Yu, D., Zhao, J., et al.¬†(2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models. https://arxiv.org/abs/2305.10601\n[12] Houlsby, N., Giurgiu, A., Jastrzebski, S., et al.¬†(2019). Parameter-Efficient Transfer Learning for NLP. https://arxiv.org/abs/1902.00751\n[13] Radford, A., Wu, J., Child, R., et al.¬†(2019). Language Models are Unsupervised Multitask Learners. https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n[14] Kojima, T., Gu, S. S., Reid, M., et al.¬†(2022). Large Language Models are Zero-Shot Reasoners. https://arxiv.org/abs/2205.11916\n[15] Zhou, X., Santurkar, S., Bau, D., et al.¬†(2022). Large Language Models Are Human-Level Prompt Engineers. https://arxiv.org/abs/2211.01910\n[16] Huyen, C. (2024). AI Engineering: Building Applications with Foundation Models. O‚ÄôReilly Media.\n[17] Alammar, J., & Grootendorst, M. (2023). Hands-On Large Language Models: Language Understanding and Generation. O‚ÄôReilly Media."
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Variational Autoencoders (VAEs) combine the power of neural networks with probabilistic inference to model complex data distributions. This blog unpacks the intuition, math, and implementation of VAEs ‚Äî from KL divergence and the ELBO to PyTorch code that generates to generate new images."
  },
  {
    "objectID": "vae.html#autoencoders-vs-variational-autoencoders",
    "href": "vae.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational Autoencoders",
    "section": "1 Autoencoders vs Variational Autoencoders",
    "text": "1 Autoencoders vs Variational Autoencoders\nTraditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities ‚Äî they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) were introduced to overcome these limitations. Rather than encoding inputs into fixed latent vectors, VAEs learn a probabilistic latent space by modeling each input as a distribution ‚Äî typically a Gaussian with a learned mean \\(\\\\mu\\) and standard deviation \\(\\\\sigma\\). This approach enables the model to sample latent variables \\(z\\) using the reparameterization trick, allowing the entire architecture to remain differentiable and trainable. By doing so, VAEs not only enable reconstruction, but also promote the learning of a continuous, interpretable latent space ‚Äî a key enabler for generation and interpolation.\nThe diagram below illustrates this process:\n\nSource: Wikimedia Commons, licensed under CC BY-SA 4.0."
  },
  {
    "objectID": "vae.html#probabilistic-framework",
    "href": "vae.html#probabilistic-framework",
    "title": "Variational Autoencoders",
    "section": "2 Probabilistic Framework",
    "text": "2 Probabilistic Framework\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\nHere, \\(\\mathbf{z}\\) acts as a hidden or latent variable, which is unobserved during training. The model thus defines a mixture of infinitely many Gaussians ‚Äî one for each \\(\\mathbf{z}\\).\nTo compute the likelihood of a data point \\(\\mathbf{x}\\), we must marginalize over all possible latent variables: \\[\n  p(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n  \\]\nThis integral requires integrating over all possible values of the latent variable \\(\\mathbf{z}\\), which is often high-dimensional and affects the likelihood in a non-linear way through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of techniques like variational inference and ELBO.\n\n2.1 Computational Challenge\nThis integral requires integrating over:\n\nAll possible values of \\(\\mathbf{z}\\) (often high-dimensional)\nNon-linear transformations through neural networks\n\nResult: Exact computation is intractable, motivating techniques like variational inference and ELBO (developed next)."
  },
  {
    "objectID": "vae.html#estimating-the-marginal-likelihood",
    "href": "vae.html#estimating-the-marginal-likelihood",
    "title": "Variational Autoencoders",
    "section": "3 Estimating the Marginal Likelihood",
    "text": "3 Estimating the Marginal Likelihood\n\n3.1 Naive Monte Carlo Estimation\nOne natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nHowever, this fails in practice. For most values of \\(z\\), the joint probability \\(p_\\theta(x, z)\\) is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely ‚Äúhits‚Äù likely values of \\(z\\).\n\n\n3.2 Importance Sampling\nTo address this, we use importance sampling, introducing a proposal distribution \\(q(z)\\):\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nThis gives an unbiased estimator of \\(p(x)\\) if \\(q(z)\\) is well-chosen (ideally close to \\(p_\\theta(z|x)\\)). Intuitively, we sample \\(z\\) more frequently in regions where \\(p_\\theta(x, z)\\) is high.\n\n\n\n3.3 Log likelihood\nOur goal is to optimize the log-likelihood, and the log of an expectation is not the same as the expectation of the log. That is,\n\\[\n\\log p(x) = log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\neq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nWhile the marginal likelihood p(x) can be estimated unbiasedly using importance sampling, estimating its logarithm \\(p(x)\\) introduces bias due to the concavity of the log function. This is captured by Jensen‚Äôs Inequality, which tells us:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\geq \\underbrace{\\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]}_{\\text{ELBO}}\n\\]\nThis means that the expected log of the estimator underestimates the true log-likelihood. The right-hand side provides a tractable surrogate objective known as the Evidence Lower Bound (ELBO), which is a biased lower bound to \\(\\log p(x)\\). Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood.\nIn the next section, we formally derive this bound and explore its components in detail."
  },
  {
    "objectID": "vae.html#why-variational-inference",
    "href": "vae.html#why-variational-inference",
    "title": "Variational Autoencoders",
    "section": "4 Why Variational Inference?",
    "text": "4 Why Variational Inference?\nComputing the true posterior distribution \\(p(z \\mid x)\\) is intractable in most cases, because it requires evaluating the marginal likelihood \\(p(x)\\), which involves integrating over all possible values of \\(z\\):\n\\[\np(x) = \\int p(x, z) \\, dz\n\\]\nVariational inference tackles this by introducing a tractable, parameterized distribution \\(q(z)\\) to approximate \\(p(z|x)\\). We aim to make \\(q(z)\\) as close as possible to the true posterior by minimizing the KL divergence:\n\\[\nD_{\\text{KL}}(q(z) \\| p(z|x))\n\\]\nThis turns inference into an optimization problem. A key result is the Evidence Lower Bound (ELBO). See next section."
  },
  {
    "objectID": "vae.html#training-a-vae",
    "href": "vae.html#training-a-vae",
    "title": "Variational Autoencoders",
    "section": "5 Training a VAE",
    "text": "5 Training a VAE\n\n5.1 ELBO Objective\nNow that we‚Äôve introduced the challenge of approximating the intractable posterior using variational inference, we turn our attention to deriving the Evidence Lower Bound (ELBO). This derivation reveals how optimizing a surrogate objective allows us to approximate the true log-likelihood of the data while keeping the approximate posterior close to the prior. The steps below walk through this formulation.\n\n5.1.1 KL Divergence Objective\n\\[\\begin{equation}\nD_{KL}(q(z)\\|p(z|x; \\theta)) = \\sum_z q(z) \\log \\frac{q(z)}{p(z|x; \\theta)}\n\\end{equation}\\]\n\n\n5.1.2 Apply Bayes‚Äô Rule\nSubstitute \\(p(z|x; \\theta) = \\frac{p(z,x;\\theta)}{p(x;\\theta)}\\): \\[\\begin{equation}\n= \\sum_z q(z) \\log \\left( \\frac{q(z) \\cdot p(x; \\theta)}{p(z, x; \\theta)} \\right)\n\\end{equation}\\]\n\n\n5.1.3 Decompose Terms\n\\[\\begin{align}\n&= \\sum_z q(z) \\log q(z) + \\sum_z q(z) \\log p(x; \\theta) \\nonumber \\\\\n&\\quad - \\sum_z q(z) \\log p(z, x; \\theta) \\\\\n&= -H(q) + \\log p(x; \\theta) - \\mathbb{E}_q[\\log p(z,x;\\theta)]\n\\end{align}\\]\n\nNote: The term \\(\\mathcal{H}(q)\\) represents the entropy of the variational distribution \\(q(z|x)\\). Entropy is defined as:\n\\[\n\\mathcal{H}(q) = -\\sum_z q(z) \\log q(z) = -\\mathbb{E}_{q(z)}[\\log q(z)]\n\\]\nEntropy measures the amount of uncertainty or ‚Äúspread‚Äù in a distribution. A high-entropy \\(q(z)\\) places probability mass across a wide region of the latent space, while a low-entropy \\(q(z)\\) is more concentrated. This decomposition is key to understanding the KL divergence term in the ELBO.\n\n\n\n5.1.4 Rearrange for ELBO\n\\[\n\\log p(x; \\theta) =\n\\underbrace{\n    \\mathbb{E}_q[\\log p(z, x; \\theta)] + \\mathcal{H}(q)\n}_{\\text{ELBO}}\n+D_{KL}(q(z)\\|p(z|x; \\theta))\n\\]\nThis equation shows that the log-likelihood \\(\\log p(x)\\) can be decomposed into the ELBO and the KL divergence between the approximate posterior and the true posterior. Since the KL divergence is always non-negative, the ELBO serves as a lower bound to the log-likelihood. By maximizing the ELBO, we indirectly minimize the KL divergence, bringing \\(q(z)\\) closer to \\(p(z|x)\\).\n Visualizing how \\(\\log p(x)\\) decomposes into the ELBO and KL divergence.\nSource: deepgenerativemodels.github.io\n\n\n5.1.5 Key Results\n\nEvidence Lower Bound (ELBO): \\[\\begin{equation}\n\\mathcal{L}(\\theta,\\phi) = \\mathbb{E}_{q(z;\\phi)}[\\log p(x,z;\\theta)] + H(q(z;\\phi))\n\\end{equation}\\]\nOptimization: \\[\\begin{equation}\n\\max_{\\theta,\\phi} \\mathcal{L}(\\theta,\\phi) \\Rightarrow\n\\begin{cases}\n\\text{Maximizes data likelihood} \\\\\n\\text{Minimizes } D_{KL}(q\\|p)\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "href": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "title": "Variational Autoencoders",
    "section": "6 Understanding the KL Divergence Term in the VAE Loss",
    "text": "6 Understanding the KL Divergence Term in the VAE Loss\nIn a VAE, the KL divergence term penalizes the encoder for producing latent distributions that deviate too far from the standard normal prior. This regularization has several important benefits:\n\nIt ensures that the latent space has a consistent structure, enabling meaningful sampling and interpolation.\nIt helps avoid large gaps between clusters in latent space by encouraging the encoder to distribute representations more uniformly.\nIt pushes the model to use the space around the origin more symmetrically and efficiently.\n\n\n6.1 Balancing KL Divergence and Reconstruction\nIn a Variational Autoencoder, the loss balances two goals:\n\nReconstruction ‚Äî making the output resemble the input\nRegularization ‚Äî keeping the latent space close to a standard normal distribution\n\nThis is captured by the loss function:\n\\[\n\\mathcal{L}_{\\text{VAE}} = \\text{Reconstruction Loss} + \\beta \\cdot D_{\\text{KL}}(q(z|x) \\,\\|\\, p(z))\n\\]\nThe parameter \\(\\beta\\) controls how strongly we enforce this regularization. Getting its value right is critical.\n\n6.1.1 When \\(\\beta\\) is too low:\n\nThe model mostly ignores the KL term, behaving like a plain autoencoder\nThe latent space becomes disorganized or fragmented\nSampling from the prior \\(p(z) = \\mathcal{N}(0, I)\\) results in unrealistic or broken outputs\n\n\n\n6.1.2 When \\(\\beta\\) is too high:\n\nThe encoder is forced to keep \\(q(z|x)\\) too close to the prior\nIt encodes less information about the input\nReconstructions become blurry or generic, since the decoder gets little to work with\n\n\nChoosing \\(\\beta\\) carefully is essential for balancing generalization and fidelity.\nA well-tuned \\(\\beta\\) helps the VAE both reconstruct accurately and generate new samples that resemble the training data.\n\n\n\n\n6.2 Gradient Challenge\nIn variational inference, we approximate the true posterior \\(p(z|x)\\) with a tractable distribution \\(q_\\phi(z)\\). This allows us to optimize the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q(z; \\phi)} \\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nOur goal is to maximize this objective with respect to both \\(\\theta\\) and \\(\\phi\\). While computing the gradient with respect to \\(\\theta\\) is straightforward, optimizing with respect to \\(\\phi\\) presents a challenge.\nThe complication arises because \\(\\phi\\) appears both in the density \\(q_\\phi(z|x)\\) and in the expectation operator. That is:\n\\[\n\\nabla_\\phi \\mathbb{E}_{q(z; \\phi)} \\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nThis gradient is hard to compute directly because we‚Äôre sampling from a distribution that depends on the parameters we‚Äôre trying to update.\n\n\n\n6.3 The Reparameterization Trick\nTo make this expression differentiable, we reparameterize the random variable \\(z\\) as a deterministic transformation of a parameter-free noise variable \\(\\epsilon\\):\n\\[\n\\epsilon \\sim \\mathcal{N}(0, I), \\quad z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n\\]\nThis turns the expectation into:\n\\[\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nwhere \\(z\\) is now a differentiable function of \\(\\phi\\).\n\n\n\nReparameterization Trick Diagram\n\n\nImage source: Wikipedia (CC BY-SA 4.0)\nThis diagram illustrates how the reparameterization trick enables differentiable sampling:\n\nIn the original formulation, \\(z\\) is sampled directly from a learned distribution, breaking the gradient flow.\nIn the reparameterized formulation, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\), and compute \\(z = \\mu + \\sigma \\cdot \\epsilon\\), making the sampling path fully differentiable.\n\n\n6.3.1 Monte Carlo Approximation\nWe approximate the expectation using Monte Carlo sampling:\n\\[\n\\mathbb{E}_{\\epsilon}[\\log p_\\theta(x, z) - \\log q_\\phi(z)] \\approx \\frac{1}{K} \\sum_{k=1}^K \\left[\\log p_\\theta(x, z^{(k)}) - \\log q_\\phi(z^{(k)})\\right]\n\\]\nwith:\n\\[\nz^{(k)} = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon^{(k)}, \\quad \\epsilon^{(k)} \\sim \\mathcal{N}(0, I)\n\\]\nThis enables us to compute gradients using backpropagation.\n\n\n\n6.3.2 Summary\n\nVariational inference introduces a gradient challenge because \\(q_\\phi(z)\\) depends on \\(\\phi\\)\nThe reparameterization trick expresses \\(z\\) as a differentiable function of noise and \\(\\phi\\)\nThis allows us to use backpropagation to optimize the ELBO efficiently\n\n\n\n\n\n6.4 Amortized Inference\nIn classical variational inference, we introduce a separate set of variational parameters \\(\\phi^i\\) for each datapoint \\(x^i\\) to approximate the true posterior \\(p(z|x^i)\\). However:\n\nOptimizing a separate \\(\\phi^i\\) for every datapoint is computationally expensive and does not scale to large datasets.\n\n\n\n6.4.1 The Key Idea: Amortization\nInstead of learning and storing a separate \\(\\phi^i\\) for every datapoint, we learn a single parametric function \\(f_\\phi(x)\\) ‚Äî typically a neural network ‚Äî that maps each input \\(x\\) to the parameters of the approximate posterior:\n\\[\nq_\\phi(z|x) = \\mathcal{N}\\left(\\mu_\\phi(x), \\sigma^2_\\phi(x)\\right)\n\\]\nHere, \\(\\phi\\) are the shared parameters of the encoder network, and \\(\\mu_\\phi(x), \\sigma_\\phi(x)\\) are its outputs.\nThis is like learning a regression function that predicts the optimal variational parameters for any input \\(x\\).\n\n\n\n\n6.5 Training with Amortized Inference\nOur training objective remains the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nWe optimize both \\(\\theta\\) (decoder parameters) and \\(\\phi\\) (encoder parameters) using stochastic gradient descent.\n\n6.5.1 Algorithm:\n\nInitialize \\(\\theta^{(0)}, \\phi^{(0)}\\)\nSample a datapoint \\(x^i\\)\nUse \\(f_\\phi(x^i)\\) to produce \\(\\mu^i, \\sigma^i\\)\nSample \\(z^i = \\mu^i + \\sigma^i \\cdot \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)\nEstimate the ELBO and compute gradients w.r.t. \\(\\theta, \\phi\\)\nUpdate \\(\\theta, \\phi\\) using gradient descent\nUpdate \\(\\theta\\), \\(\\phi\\) using gradient descent:\n\n\\[\n\\phi \\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\n\\[\n\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\nwhere \\(\\mathcal{B}\\) is the current minibatch and \\(\\tilde{\\nabla}\\) indicates a stochastic gradient approximation.\n\n\n\n\n6.6 Summary\n\nAmortized inference replaces per-datapoint optimization with a single learned mapping \\(f_\\phi(x)\\)\nThis makes variational inference scalable and efficient\nThe model can generalize to unseen inputs by predicting variational parameters on-the-fly\n\n\nNote: Following common practice in the literature, we use \\(\\phi\\) to denote the parameters of the encoder network, even though it now defines a function rather than individual variational parameters."
  },
  {
    "objectID": "vae.html#applications-of-vaes",
    "href": "vae.html#applications-of-vaes",
    "title": "Variational Autoencoders",
    "section": "7 Applications of VAEs",
    "text": "7 Applications of VAEs\nVariational Autoencoders are widely used in:\n\nImage Generation: VAEs can generate new images similar to the training data (e.g., MNIST digits)\n\nAnomaly Detection: High reconstruction error flags unusual data points\n\nRepresentation Learning: Latent space captures features for downstream tasks\n\n\n7.1 üòé Face Generation with Convolutional VAE\nTo complement the theory, I‚Äôve built a full PyTorch implementation of a Variational Autoencoder trained on the CelebA dataset.\nüìò The notebook walks through:\n\nDefining the encoder, decoder, and reparameterization trick\n\nImplementing the ELBO loss function (reconstruction + KL divergence)\n\nTraining the model on face images\n\nGenerating new faces from random latent vectors\n\n\nüìì View on GitHub"
  },
  {
    "objectID": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections.",
    "href": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections.",
    "title": "Variational Autoencoders",
    "section": "8 This example is designed to reinforce the theoretical concepts from earlier sections.",
    "text": "8 This example is designed to reinforce the theoretical concepts from earlier sections."
  },
  {
    "objectID": "vae.html#further-reading",
    "href": "vae.html#further-reading",
    "title": "Variational Autoencoders",
    "section": "9 Further Reading",
    "text": "9 Further Reading\nFor readers interested in diving deeper into the theory and applications of variational autoencoders, the following resources are recommended:\n\nTutorial on Variational Autoencoders\nCarl Doersch (2016)\nhttps://arxiv.org/pdf/1606.05908\nAuto-Encoding Variational Bayes\nKingma & Welling (2014) ‚Äî the original VAE paper\nhttps://arxiv.org/pdf/1312.6114\nThe Challenges of Amortized Inference for Structured Prediction\nCremer, Li, & Duvenaud (2019)\nhttps://arxiv.org/pdf/1906.02691\nDeep Generative Models course notes\nhttps://deepgenerativemodels.github.io/notes/vae/"
  },
  {
    "objectID": "coming-soon.html",
    "href": "coming-soon.html",
    "title": "Coming Soon",
    "section": "",
    "text": "This page is under construction and will be available soon. Stay tuned!"
  },
  {
    "objectID": "ebm_mnist.html",
    "href": "ebm_mnist.html",
    "title": "Energy-Based Model (EBM) on MNIST",
    "section": "",
    "text": "This notebook implements a basic EBM using PyTorch and MNIST.\n\n# Install and import dependencies\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport torch.optim as optim\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nBATCH_SIZE = 128\nLEARNING_RATE = 0.0001\nnum_epochs = 10\n\n\n# ------------------------------------------\n# Load the MNIST dataset in PyTorch\n# ------------------------------------------\n\n# Step 1: Define a transform to convert images to PyTorch tensors\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Converts images to range [0, 1] as float32 tensors\n])\n\n# Step 2: Download and load the training and test sets\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n# Step 3: Create DataLoaders for easy batch access\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n\n# ------------------------------------------\n# Custom preprocessing function (if needed)\n# ------------------------------------------\n\ndef preprocess_pytorch(images):\n    \"\"\"\n    Normalize and reshape the images similar to your original TensorFlow preprocessing.\n\n    - Normalize pixel values from [0, 255] ‚Üí [-1, 1]\n    - Pad from 28x28 ‚Üí 32x32 with constant value -1\n    - Add a channel dimension if not present\n    \"\"\"\n\n    # Scale from [0, 1] to [-1, 1]\n    images = images * 2 - 1\n\n    # images: shape (batch_size, 1, 28, 28) ‚Üí pad to (batch_size, 1, 32, 32)\n    images = F.pad(images, pad=(2, 2, 2, 2), mode='constant', value=-1.0)\n\n    return images\n\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),         # Converts image to [0, 1] tensor\n    transforms.Lambda(preprocess_pytorch)  # Then normalize and pad\n])\n\n\n# Download MNIST dataset\nmnist_data = datasets.MNIST(root='.', train=True, download=True)\nx_train = mnist_data.data.numpy()\n\nmnist_test = datasets.MNIST(root='.', train=False, download=True)\nx_test = mnist_test.data.numpy()\n\n\n# Convert numpy arrays to PyTorch tensors and add channel dimension\nx_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)  # shape: [B, 1, 28, 28]\nx_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1)    # shape: [B, 1, 28, 28]\n\n# Create TensorDatasets from tensors\ntrain_dataset = TensorDataset(x_train_tensor)\ntest_dataset = TensorDataset(x_test_tensor)\n\n# Wrap datasets in DataLoader to enable batching and shuffling\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# Define a PyTorch EBM model similar to your TensorFlow implementation\nclass EBM(nn.Module):\n    def __init__(self, image_size=28, channels=1):\n        super(EBM, self).__init__()\n\n        # Convolutional layers with Swish (SiLU) activations\n        self.conv1 = nn.Conv2d(channels, 16, kernel_size=5, stride=2, padding=2)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n\n        # Calculate the flattened output size after 4 conv layers\n        conv_output_size = image_size // (2**4)  # 4 strides of 2\n        flattened_dim = 64 * conv_output_size * conv_output_size\n\n        # Dense layers\n        self.fc1 = nn.Linear(flattened_dim, 64)\n        self.fc2 = nn.Linear(64, 1)  # Single energy output\n\n    def forward(self, x):\n        # Swish activation is available as F.silu in PyTorch\n        x = F.silu(self.conv1(x))\n        x = F.silu(self.conv2(x))\n        x = F.silu(self.conv3(x))\n        x = F.silu(self.conv4(x))\n\n        x = x.view(x.size(0), -1)  # Flatten for dense layers\n        x = F.silu(self.fc1(x))\n        energy = self.fc2(x)  # Output energy (unnormalized score)\n        return energy\n\n\ndef generate_samples(\n    model,                 # The energy-based model\n    inp_imgs,              # Initial images (random noise or seeds)\n    steps,                 # Number of Langevin steps\n    step_size,             # Step size (learning rate)\n    noise,                 # Stddev of added Gaussian noise\n    return_img_per_step=False,  # Whether to save images at each step\n):\n    imgs_per_step = []\n\n    inp_imgs = inp_imgs.clone().detach().to(device).requires_grad_(True)\n\n    for _ in range(steps):\n        # Step 1: Add Gaussian noise to encourage exploration\n        inp_imgs.data += torch.randn_like(inp_imgs) * noise\n\n        # Step 2: Clamp values to stay in [-1, 1] range (MNIST normalized)\n        inp_imgs.data = torch.clamp(inp_imgs.data, -1.0, 1.0)\n\n        # Step 3: Forward pass to compute score (energy)\n        out_score = model(inp_imgs)\n\n        # Step 4: Compute gradient of score w.r.t. input image\n        grads = torch.autograd.grad(outputs=out_score.sum(), inputs=inp_imgs)[0]\n\n        # Step 5: Clip gradients for stability\n        grads = torch.clamp(grads, -GRADIENT_CLIP, GRADIENT_CLIP)\n\n        # Step 6: Gradient ascent step on input image\n        inp_imgs.data += step_size * grads\n\n        # Step 7: Clamp again to stay in valid range\n        inp_imgs.data = torch.clamp(inp_imgs.data, -1.0, 1.0)\n\n        if return_img_per_step:\n            imgs_per_step.append(inp_imgs.detach().clone())\n\n    if return_img_per_step:\n        return torch.stack(imgs_per_step, dim=0)\n    else:\n        return inp_imgs.detach()\n\n\nclass EBM(nn.Module):\n    def __init__(self, base_model, alpha=0.1):\n        super().__init__()\n        self.model = base_model  # scoring network\n        self.alpha = alpha       # regularization weight\n\n    def forward(self, x):\n        return self.model(x).squeeze()\n\ndef compute_loss(model, real_imgs, steps, step_size, noise_scale):\n    \"\"\"\n    Contrastive divergence loss between real data and fake (noise) samples.\n    \"\"\"\n    batch_size = real_imgs.size(0)\n\n    # Generate fake images from random noise\n    fake_imgs = torch.empty_like(real_imgs).uniform_(-1, 1).to(real_imgs.device)\n    fake_imgs.requires_grad = True\n\n    # Langevin dynamics steps (optional: here 0 steps means no update)\n    for _ in range(steps):\n        fake_imgs.data += torch.randn_like(fake_imgs) * noise_scale\n        fake_imgs.data = torch.clamp(fake_imgs.data, -1, 1)\n\n        energy = model(fake_imgs)\n        grads = torch.autograd.grad(energy.sum(), fake_imgs, create_graph=True)[0]\n        fake_imgs.data += step_size * grads\n        fake_imgs.data = torch.clamp(fake_imgs.data, -1, 1)\n\n    # Get scores\n    real_scores = model(real_imgs)\n    fake_scores = model(fake_imgs.detach())\n\n    # Contrastive Divergence (CD-1) Loss\n    cdiv_loss = fake_scores.mean() - real_scores.mean()\n\n    # Regularization: penalize high scores\n    reg_loss = model.alpha * ((real_scores ** 2).mean() + (fake_scores ** 2).mean())\n\n    total_loss = cdiv_loss + reg_loss\n\n    return total_loss, cdiv_loss.item(), reg_loss.item(), real_scores.mean().item(), fake_scores.mean().item()\n\n\nclass ScoreNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # [batch, 32, 14, 14]\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # [batch, 64, 7, 7]\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(64 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)  # Final scalar output (energy score)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nbase_model = ScoreNet()\nebm = EBM(base_model=base_model, alpha=0.1)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nebm = ebm.to(device)  # Move the model to the device\n\n\noptimizer = optim.Adam(ebm.parameters(), lr=LEARNING_RATE)\n\n\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        if isinstance(batch, (list, tuple)):\n            real_images = batch[0]\n        else:\n            real_images = batch\n\n        real_images = real_images.to(device)\n        real_images.requires_grad = True\n\n        # Forward pass\n        scores = ebm(real_images)\n        loss = -scores.mean()  # Basic negative log-score loss\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n\nEpoch 1/10, Loss: -1949711.2500\nEpoch 2/10, Loss: -34621388.0000\nEpoch 3/10, Loss: -168554992.0000\nEpoch 4/10, Loss: -460923584.0000\nEpoch 5/10, Loss: -974023040.0000\nEpoch 6/10, Loss: -1813268352.0000\nEpoch 7/10, Loss: -2972573696.0000\nEpoch 8/10, Loss: -4526898176.0000\nEpoch 9/10, Loss: -6693363200.0000\nEpoch 10/10, Loss: -9541715968.0000\n\n\n\n# üß† Langevin Dynamics Sampling Function\ndef generate_samples(model, inp_imgs, steps, step_size, noise, return_img_per_step=False):\n    inp_imgs = inp_imgs.clone().detach().to(device).requires_grad_()  # Make it a leaf tensor with grad\n    imgs_per_step = []\n\n    for _ in range(steps):\n        # Add Gaussian noise and clip to valid range\n        inp_imgs = inp_imgs + noise * torch.randn_like(inp_imgs)\n        inp_imgs = torch.clamp(inp_imgs, -1.0, 1.0)\n        inp_imgs.requires_grad_()  # Re-enable grad tracking (necessary after in-place ops)\n\n        # Compute gradients of score wrt input\n        scores = model(inp_imgs)\n        grads = torch.autograd.grad(scores.sum(), inp_imgs)[0]\n\n        # Langevin update\n        inp_imgs = inp_imgs + step_size * grads\n        inp_imgs = torch.clamp(inp_imgs, -1.0, 1.0)\n\n        if return_img_per_step:\n            imgs_per_step.append(inp_imgs.detach().cpu())\n\n    if return_img_per_step:\n        return torch.stack(imgs_per_step)\n    else:\n        return inp_imgs.detach().cpu()\n\n\n# Starting noise image (e.g., 10 images of 28x28x1)\nstart_imgs = torch.rand(10, 1, 28, 28).to(device) * 2 - 1\n\n# Run Langevin sampling\ngen_imgs = generate_samples(\n    model=ebm,\n    inp_imgs=start_imgs,\n    steps=1000,\n    step_size=0.1,\n    noise=0.005,\n    return_img_per_step=True\n)\n\n# Visualize final images\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 10, figsize=(12, 2))\nfor i, ax in enumerate(axes):\n    ax.imshow(gen_imgs[-1][i][0], cmap=\"gray\")\n    ax.axis(\"off\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "slm.html",
    "href": "slm.html",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "",
    "text": "For years, the AI industry chased a single dream ‚Äî one massive model that could do everything. We went from 7 billion parameters to a trillion, racing toward a ‚ÄúGod Model.‚Äù But in 2025, the wind has shifted. We are entering the era of Small Language Models (SLMs).\nSmall Language Models (SLMs)‚Äîcompact, specialized AI systems typically under 10 billion parameters‚Äîare reshaping how organizations deploy AI. They‚Äôre not cheaper versions of GPT-4. They‚Äôre purpose-built engines designed for specific tasks, offering faster performance, lower costs, and better privacy controls.\nThis matters because the next wave of AI isn‚Äôt about chatbots. It‚Äôs about autonomous agents that make hundreds of decisions per transaction. And those agents need a different architecture‚Äîone where small, specialized models handle routine work while large models tackle genuine complexity."
  },
  {
    "objectID": "slm.html#the-trade-off-breadth-vs.-depth",
    "href": "slm.html#the-trade-off-breadth-vs.-depth",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "2.1 The Trade-Off: Breadth vs.¬†Depth",
    "text": "2.1 The Trade-Off: Breadth vs.¬†Depth\nLarge models excel at versatility. GPT-4 can write poetry, debug code, and explain quantum physics in the same conversation. But using a 400-billion parameter model to extract dates from invoices is like hiring a philosophy professor to sort your mail ‚Äî effective, but absurdly expensive.\nSmall models excel at efficiency. A 3-billion parameter model trained specifically on financial documents can‚Äôt discuss medieval history, but it will process loan applications faster and more accurately than a generalist model. Since it focuses on one domain, it delivers results in milliseconds rather than seconds‚Äîand at a fraction of the cost.\nThe economics are straightforward: if you‚Äôre running the same type of task thousands of times daily, specialized models make sense. If you need broad capabilities for unpredictable queries, large models remain essential.\n\n\n\nFigure: Large models excel at versatility; small models excel at efficiency. The choice depends on your use case."
  },
  {
    "objectID": "slm.html#the-solution-heterogeneous-architecture",
    "href": "slm.html#the-solution-heterogeneous-architecture",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "4.1 The Solution: Heterogeneous Architecture",
    "text": "4.1 The Solution: Heterogeneous Architecture\nThe answer isn‚Äôt using one model for everything. It‚Äôs using the right model for each task. In this approach, a large model acts as an orchestrator‚Äîunderstanding vague requests, planning the workflow, handling edge cases. Small models execute the routine steps: running database queries, formatting outputs, validating results.\nThis mixed approach cuts costs by 10-30√ó while improving reliability. Small models fine-tuned for specific tasks ‚Äúhallucinate‚Äù less than generalist models attempting the same work.1"
  },
  {
    "objectID": "slm.html#why-agents-need-slms",
    "href": "slm.html#why-agents-need-slms",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "4.2 Why Agents Need SLMs",
    "text": "4.2 Why Agents Need SLMs\nHere‚Äôs the economic challenge ‚Äî Unlike traditional applications where a user submits a single query, agents make dozens of calls per request. A single request might trigger 20-50 model calls. When each call goes to a large model, costs compound rapidly.\nAnalysis of real agents (MetaGPT, Open Operator, Cradle) reveals that 60-70% of their model calls are repetitive, narrow tasks: parsing JSON, formatting API parameters, validating outputs, simple decision logic. These don‚Äôt need a 400-billion parameter model‚Äôs ‚Äúworld knowledge.‚Äù A 3-billion parameter model trained specifically for the task performs better and costs 30√ó less.1"
  },
  {
    "objectID": "slm.html#loan-document-processing",
    "href": "slm.html#loan-document-processing",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "7.1 Loan Document Processing",
    "text": "7.1 Loan Document Processing\nA regional bank processing 10,000 loan applications monthly has two options:\n\nThe Old Way (LLM): GPT-4 at ~$2.50 per application costs $25,000/month.\n\nThe SLM Way: A fine-tuned 3B model costs ~$0.10 per application, totaling $1,000/month.\n\nThe SLM, trained specifically on lending documentation, often delivers higher accuracy on domain-specific extractions like debt-to-income ratios, collateral valuations, and employment verification."
  },
  {
    "objectID": "slm.html#real-time-fraud-compliance-monitoring",
    "href": "slm.html#real-time-fraud-compliance-monitoring",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "7.2 Real-Time Fraud & Compliance Monitoring",
    "text": "7.2 Real-Time Fraud & Compliance Monitoring\nBanks need to screen every transaction for fraud. A massive cloud model is too slow and too expensive for this. An SLM can:\n\nFlag suspicious transactions in milliseconds\n\nRun entirely on-premise (no data leaves the bank‚Äôs secure network).\n\nProcess 100,000+ transactions daily at a fraction of cloud API costs.\n\nAdapt overnight to new regulations via fine-tuning.\n\nFor privacy-sensitive operations, on-premise deployment eliminates the risk of sending customer data to external API endpoints."
  },
  {
    "objectID": "slm.html#customer-service-triage",
    "href": "slm.html#customer-service-triage",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "7.3 Customer Service Triage",
    "text": "7.3 Customer Service Triage\nInstead of forcing every customer to wait for a ‚Äúgenius‚Äù AI, banks use a tiered system:\n\nThe Front Line (SLM): A fast local model answers routine questions (‚ÄúWhat‚Äôs my balance?‚Äù) instantly.\nThe Escalation (LLM): Complex problems are routed to a larger reasoning model.\n\nThis hybrid approach delivers sub-second responses for most customers while maintaining quality for complex cases‚Äîall while reducing inference costs by 10-15√ó."
  },
  {
    "objectID": "slm.html#the-deeper-economics-of-slms",
    "href": "slm.html#the-deeper-economics-of-slms",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "8.1 The Deeper Economics of SLMs",
    "text": "8.1 The Deeper Economics of SLMs\nThe economic advantages extend beyond per-token pricing:\nInference Efficiency: Serving a 7B SLM requires 10-30√ó fewer FLOPs than a 70-175B LLM, enabling real-time responses at scale with dramatically lower energy consumption.\nFine-Tuning Agility: Full parameter fine-tuning for SLMs requires only GPU-hours versus GPU-weeks for LLMs. This means behaviors can be added, fixed, or specialized overnight rather than over weeks‚Äîcritical for rapidly evolving business requirements.\nEdge Deployment: SLMs run on consumer-grade GPUs, smartphones, and edge devices. For banks, this means processing sensitive data locally without cloud dependencies, reducing latency and strengthening data control.\nInfrastructure Simplicity: SLMs require less or no parallelization across GPUs and nodes, lowering both capital expenditure for hardware and operational costs for maintenance."
  },
  {
    "objectID": "slm.html#understanding-the-barriers",
    "href": "slm.html#understanding-the-barriers",
    "title": "The Future of AI Isn‚Äôt Bigger ‚Äî It‚Äôs Smaller",
    "section": "8.2 Understanding the Barriers",
    "text": "8.2 Understanding the Barriers\nFinancial institutions face different barriers depending on their AI maturity:\n\n8.2.1 For Early Adopters (Already Using LLMs)\nOrganizations that deployed LLM-based solutions face organizational inertia ‚Äî they‚Äôve built teams, workflows, and expertise around centralized models. Shifting to heterogeneous architectures requires retooling processes and adapting governance frameworks, organizational changes that take time.\n\n\n8.2.2 For Institutions Just Starting\n1. Governance & Compliance Uncertainty: Financial institutions face regulatory requirements for model explainability, validation, and risk management. While SLMs are actually more transparent than massive LLMs, most banks haven‚Äôt yet built AI governance frameworks. Starting with smaller, more auditable models is actually easier, but the perceived complexity of ‚ÄúAI governance‚Äù creates hesitation.\n2. Lack of Banking-Specific Validation: Most published SLM case studies focus on tech companies or consumer applications. Regional banks lack peer examples showing successful SLM deployments in banking operations, credit decisioning, or compliance workflows. This slows adoption in risk-averse institutions.\n3. Skills & Vendor Ecosystem: Unlike mature LLM platforms (OpenAI API, Anthropic), the SLM tooling ecosystem is less developed. Banks face uncertainty about deployment platforms, monitoring tools, and vendor support‚Äîincreasing perceived implementation risk.\nAs organizations adapt their processes and banking-specific benchmarks emerge, these barriers continue to diminish.\nThe Strategic Choice: For banks beginning their AI journey, SLMs offer a clear path forward‚Äîlower costs, easier governance, on-premise deployment. For institutions already using LLM-based solutions, the transition to heterogeneous architectures requires systematic planning but delivers compelling ROI. The next section provides a practical implementation roadmap for both scenarios."
  },
  {
    "objectID": "flows.html",
    "href": "flows.html",
    "title": "Normalizing Flow Models",
    "section": "",
    "text": "In generative modeling, the objective is to learn a probability distribution over data that allows us to both generate new examples and evaluate the likelihood of observed ones. For a model to be practically useful, it must support efficient sampling and enable exact or tractable likelihood computation during training.\nA Variational Autoencoder (VAE) is a type of generative model that introduces latent variables \\(z\\), allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood \\(p(x)\\) is often intractable. To address this, VAEs use variational inference to approximate the posterior \\(p(z \\mid x)\\) and optimize a surrogate objective known as the Evidence Lower Bound (ELBO). This is made possible by the reparameterization trick, which enables gradients to flow through stochastic latent variables during training.\nNormalizing flows address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point \\(x\\) is generated by applying a function \\(x = f(z)\\) to a latent variable \\(z\\) sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so \\(z\\) can be exactly recovered as \\(z = f^{-1}(x)\\). This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.\nThis structure offers several advantages. First, each \\(x\\) maps to a unique \\(z\\), eliminating the need to marginalize over latent variables as in VAEs. Second, the change-of-variables formula enables exact computation of the likelihood, rather than approximations. Third, sampling is straightforward: draw \\(z \\sim p_Z(z)\\) from the base distribution and apply the transformation \\(x = f(z)\\).\nDespite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn lower-dimensional latent representations, flows require the latent and data spaces to have equal dimensionality to preserve invertibility. This means flow-based models do not perform dimensionality reduction, which can be a disadvantage in tasks where compact representations are important.\n\n\n\nComparison of VAE and Flow-based Models\n\n\nVAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.\nTo understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We‚Äôll begin with the 1D case and build up to the multivariate formulation."
  },
  {
    "objectID": "flows.html#introduction",
    "href": "flows.html#introduction",
    "title": "Normalizing Flow Models",
    "section": "",
    "text": "In generative modeling, the objective is to learn a probability distribution over data that allows us to both generate new examples and evaluate the likelihood of observed ones. For a model to be practically useful, it must support efficient sampling and enable exact or tractable likelihood computation during training.\nA Variational Autoencoder (VAE) is a type of generative model that introduces latent variables \\(z\\), allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood \\(p(x)\\) is often intractable. To address this, VAEs use variational inference to approximate the posterior \\(p(z \\mid x)\\) and optimize a surrogate objective known as the Evidence Lower Bound (ELBO). This is made possible by the reparameterization trick, which enables gradients to flow through stochastic latent variables during training.\nNormalizing flows address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point \\(x\\) is generated by applying a function \\(x = f(z)\\) to a latent variable \\(z\\) sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so \\(z\\) can be exactly recovered as \\(z = f^{-1}(x)\\). This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.\nThis structure offers several advantages. First, each \\(x\\) maps to a unique \\(z\\), eliminating the need to marginalize over latent variables as in VAEs. Second, the change-of-variables formula enables exact computation of the likelihood, rather than approximations. Third, sampling is straightforward: draw \\(z \\sim p_Z(z)\\) from the base distribution and apply the transformation \\(x = f(z)\\).\nDespite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn lower-dimensional latent representations, flows require the latent and data spaces to have equal dimensionality to preserve invertibility. This means flow-based models do not perform dimensionality reduction, which can be a disadvantage in tasks where compact representations are important.\n\n\n\nComparison of VAE and Flow-based Models\n\n\nVAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.\nTo understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We‚Äôll begin with the 1D case and build up to the multivariate formulation."
  },
  {
    "objectID": "flows.html#math-review",
    "href": "flows.html#math-review",
    "title": "Normalizing Flow Models",
    "section": "2 Math Review",
    "text": "2 Math Review\nThis section builds the mathematical foundation for understanding flow models, starting with change-of-variable and extending to multivariate transformations and Jacobians.\n\n2.1 Change of Variables in 1D\nSuppose we have a random variable \\(z\\) with a known distribution \\(p_Z(z)\\), and we define a new variable:\n\\[\nx = f(z)\n\\]\nwhere \\(f\\) is a monotonic, differentiable function with an inverse:\n\\[\nz = f^{-1}(x) = h(x)\n\\]\nOur goal is to compute the probability density function (PDF) of \\(x\\), denoted \\(p_X(x)\\), in terms of the known PDF \\(p_Z(z)\\).\n\n2.1.1 Cumulative Distribution Function (CDF)\nWe begin with the cumulative distribution function of \\(x\\):\n\\[\nF_X(x) = P(X \\leq x) = P(f(Z) \\leq x)\n\\]\nSince \\(f\\) is monotonic and invertible, this becomes:\n\\[\nP(f(Z) \\leq x) = P(Z \\leq f^{-1}(x)) = F_Z(h(x))\n\\]\n\n\n2.1.2 Deriving the PDF via Chain Rule\nTo obtain the PDF, we differentiate the CDF:\n\\[\np_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx} F_Z(h(x))\n\\]\nApplying the chain rule:\n\\[\np_X(x) = F_Z'(h(x)) \\cdot h'(x) = p_Z(h(x)) \\cdot h'(x)\n\\]\n\n\n2.1.3 Rewrite in Terms of \\(z\\)\nFrom the previous step:\n\\[\np_X(x) = p_Z(h(x)) \\cdot h'(x)\n\\]\nSince \\(z = h(x)\\), we can rewrite:\n\\[\np_X(x) = p_Z(z) \\cdot h'(x)\n\\]\nNow, using the inverse function theorem, we express \\(h'(x)\\) as:\n\\[\nh'(x) = \\frac{d}{dx} f^{-1}(x) = \\frac{1}{f'(z)}\n\\]\nSo the final expression becomes:\n\\[\np_X(x) = p_Z(z) \\cdot \\left| \\frac{1}{f'(z)} \\right|\n\\]\nThe absolute value ensures the density remains non-negative, as required for any valid probability distribution.\nThis is the fundamental concept normalizing flows use to model complex distributions by transforming simple ones.\n\n\n\n2.2 Geometry: Determinants and Volume Changes\nTo further understand the multivariate change-of-variable formula, it‚Äôs helpful to first explore how linear transformations affect volume in high-dimensional spaces.\nLet \\(\\mathbf{Z}\\) be a random vector uniformly distributed in the unit cube \\([0,1]^n\\), and let \\(\\mathbf{X} = A\\mathbf{Z}\\), where \\(A\\) is a square, invertible matrix. Geometrically, the matrix \\(A\\) maps the unit hypercube to a parallelogram in 2D or a parallelotope in higher dimensions.\nThe determinant of a square matrix tells us how the transformation scales volume. For instance, if the determinant of a \\(2 \\times 2\\) matrix is 3, applying that matrix will stretch the area of a region by a factor of 3. A negative determinant indicates a reflection, meaning the transformation also flips the orientation. When measuring volume, we care about the absolute value of the determinant.\nThe volume of the resulting parallelotope is given by:\n\\[\n\\text{Volume} = |\\det(A)|\n\\]\nThis expression tells us how much the transformation \\(A\\) scales space. For example, if \\(|\\det(A)| = 2\\), the transformation doubles the volume.\nTo make this idea concrete, consider the illustration below. The left figure shows a uniform distribution over the unit square \\([0, 1]^2\\). When we apply the linear transformation \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\), each point in the square is mapped to a new location, stretching the square into a parallelogram. The area of this parallelogram ‚Äî and hence the volume scaling ‚Äî is given by the absolute value of the determinant \\(|\\det(A)| = |ad - bc|\\).\n\n\n\n A linear transformation maps a unit square to a parallelogram. \n\n\nThis geometric intuition becomes essential when we apply the same logic to probability densities. The area of the parallelogram equals the absolute value of the determinant, |det(A)|, indicating how the transformation scales area.\n\n\n2.3 Determinants and Probability Density\nPreviously, we saw how a linear transformation scales volume. Now we apply the same idea to probability densities ‚Äî since density is defined per unit volume, scaling the volume also affects the density.\nTo transform the density from \\(\\mathbf{Z}\\) to \\(\\mathbf{X}\\), we use the change-of-variable formula. Since \\(\\mathbf{X} = A\\mathbf{Z}\\), the inverse transformation is \\(\\mathbf{Z} = A^{-1} \\mathbf{X}\\). This tells us how to evaluate the density at \\(\\mathbf{x}\\) by ‚Äúpulling it back‚Äù through the inverse mapping. Applying the multivariate change-of-variable rule:\n\\[\np_X(\\mathbf{x}) = p_Z(W \\mathbf{x}) \\cdot \\left| \\det(W) \\right| \\quad \\text{where } W = A^{-1}\n\\]\nThis is directly analogous to the 1D change-of-variable rule:\n\\[\np_X(x) = p_Z(h(x)) \\cdot |h'(x)|\n\\]\nbut now in multiple dimensions using the determinant of the inverse transformation.\nTo make this more concrete, here‚Äôs a simple 2D example demonstrating how linear transformations affect probability density.\nLet \\(\\mathbf{Z}\\) be a random vector uniformly distributed over the unit square \\([0, 1]^2\\). Suppose we apply the transformation \\(\\mathbf{X} = A\\mathbf{Z}\\), where\n\\[\nA = \\begin{bmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\quad \\text{so that} \\quad\nW = A^{-1} =\n\\begin{bmatrix}\n\\frac{1}{2} & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\nThis transformation stretches the square horizontally, doubling its width while keeping the height unchanged. As a result, the area is doubled:\n\\[\n|\\det(A)| = 2 \\quad \\text{and} \\quad |\\det(W)| = \\frac{1}{2}\n\\] Since the same total probability must be spread over a larger area, the density decreases, meaning the probability per unit area is reduced due to the increased area over which the same total probability is distributed.\nNow, let‚Äôs say \\(p_Z(z) = 1\\) inside the unit square (a uniform distribution). To compute \\(p_X(\\mathbf{x})\\) at a point \\(\\mathbf{x}\\) in the transformed space, we use:\n\\[\np_X(\\mathbf{x}) = p_Z(W\\mathbf{x}) \\cdot |\\det(W)| = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}\n\\]\nSo, the transformed density is halved ‚Äî the same total probability (which must remain 1) is now spread over an area that is twice as large.\n\n\n2.4 Generalizing to Nonlinear Transformations\nFor nonlinear transformations \\(\\mathbf{x} = f(\\mathbf{z})\\), the idea is similar. But instead of a constant matrix \\(A\\), we now consider the Jacobian matrix of the function \\(f\\):\n\\[\nJ_f(\\mathbf{z}) = \\frac{\\partial f}{\\partial \\mathbf{z}}\n\\]\nThe Jacobian matrix generalizes derivatives to multivariable functions, capturing how a transformation scales and rotates space locally through all partial derivatives. Its determinant tells us how much the transformation stretches or compresses space ‚Äî acting as a local volume scaling factor.\n\n\n2.5 Multivariate Change-of-Variable\nGiven an invertible transformation \\(\\mathbf{x} = f(\\mathbf{z})\\), the probability density transforms as:\n\\[\np_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\cdot \\left| \\det \\left( \\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nAlternatively, in the forward form (often used during training):\n\\[\np_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\cdot \\left| \\det \\left( \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}} \\right) \\right|^{-1}\n\\]\nThis generalizes the 1D rule and enables us to compute exact likelihoods for complex distributions as long as the transformation is invertible and differentiable. This formula is pivotal in machine learning, where transformations of probability distributions are common ‚Äî such as in the implementation of normalizing flows for generative modeling."
  },
  {
    "objectID": "flows.html#flow-model",
    "href": "flows.html#flow-model",
    "title": "Normalizing Flow Models",
    "section": "3 Flow Model",
    "text": "3 Flow Model\nA normalizing flow model defines a one-to-one and reversible transformation between observed variables \\(\\mathbf{x}\\) and latent variables \\(\\mathbf{z}\\). This transformation is given by an invertible, differentiable function \\(f_\\theta\\), parameterized by \\(\\theta\\):\n\\[\n\\mathbf{x} = f_\\theta(\\mathbf{z}) \\quad \\text{and} \\quad \\mathbf{z} = f_\\theta^{-1}(\\mathbf{x})\n\\]\n\n\n\nFlow model showing forward and inverse transformations\n\n\nFigure: A flow-based model uses a forward transformation \\(f_\\theta\\) to map from latent variables (\\(\\mathbf{z}\\)) to data (\\(\\mathbf{x}\\)), and an inverse transformation \\(f_\\theta^{-1}\\) to compute likelihoods. Adapted from class notes (XCS236, Stanford).\nBecause the transformation is invertible, we can apply the change-of-variable formula to compute the exact probability of \\(\\mathbf{x}\\):\n\\[\np_X(\\mathbf{x}; \\theta) = p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nThis makes it possible to evaluate exact likelihoods and learn the model via maximum likelihood estimation (MLE).\n\nNote: Both \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) must be continuous and have the same dimensionality since the transformation must be invertible.\n\n\n3.1 Model Architecture: A Sequence of Invertible Transformations\nThe term flow refers to the fact that we can compose multiple invertible functions to form a more expressive transformation:\n\\[\n\\mathbf{z}_m = f_\\theta^{(m)} \\circ f_\\theta^{(m-1)} \\circ \\cdots \\circ f_\\theta^{(1)}(\\mathbf{z}_0)\n\\]\nIn this setup:\n\n\\(\\mathbf{z}_0 \\sim p_Z\\) is sampled from a simple base distribution (e.g., standard Gaussian)\n\\(\\mathbf{x} = \\mathbf{z}_M\\) is the final transformed variable\nThe full transformation \\(f_\\theta\\) is the composition of \\(M\\) sequential invertible functions. Each function slightly reshapes the distribution, and together they produce a highly expressive mapping from a simple base distribution to a complex one.\n\nThe visuals below illustrate this idea from two angles. The first diagram illustrates the structure of a normalizing flow as a composition of invertible steps, while the second shows how this architecture reshapes simple distributions into complex ones through repeated transformations.\n\n\n\n Adapted from Wikipedia: Mapping simple distributions to complex ones via invertible transformations. \n\n\n\n\n\n Adapted from class notes (XCS236, Stanford), originally based on Rezende & Mohamed, 2016. \n\n\nThe density of \\(\\mathbf{x}\\) is given by the change-of-variable formula:\n\\[\np_X(\\mathbf{x}; \\theta) = p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot \\prod_{m=1}^M \\left| \\det \\left( \\frac{\\partial (f_\\theta^{(m)})^{-1}(\\mathbf{z}_m)}{\\partial \\mathbf{z}_m} \\right) \\right|\n\\]\nThis approach allows the model to approximate highly complex distributions using simple building blocks."
  },
  {
    "objectID": "flows.html#learning-and-inference",
    "href": "flows.html#learning-and-inference",
    "title": "Normalizing Flow Models",
    "section": "4 Learning and Inference",
    "text": "4 Learning and Inference\nTraining a flow-based model is done by maximizing the log-likelihood over the dataset \\(\\mathcal{D}\\):\n\\[\n\\max_\\theta \\log p_X(\\mathcal{D}; \\theta) = \\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p_Z(f_\\theta^{-1}(\\mathbf{x})) + \\log \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nKey advantages of normalizing flows:\n\nExact likelihoods: No approximation needed ‚Äî just apply the change-of-variable rule\nEfficient sampling: Generate new data by drawing \\(\\mathbf{z} \\sim p_Z\\) and computing \\(\\mathbf{x} = f_\\theta(\\mathbf{z})\\)\nLatent inference: Invert \\(f_\\theta\\) to compute latent codes \\(\\mathbf{z} = f_\\theta^{-1}(\\mathbf{x})\\), without needing a separate encoder\n\n\n4.1 Computational Considerations\nOne challenge in training normalizing flow models is that computing the exact likelihood requires evaluating the determinant of the Jacobian matrix of the transformation:\n\nFor a transformation \\(f : \\mathbb{R}^n \\to \\mathbb{R}^n\\), the Jacobian is an \\(n \\times n\\) matrix.\nComputing its determinant has a cost of \\(\\mathcal{O}(n^3)\\), which is computationally expensive during training ‚Äî especially in high dimensions.\n\n\n4.1.1 Key Insight\nTo make normalizing flows scalable, we design transformations where the Jacobian has a special structure that makes the determinant easy to compute.\nFor example: - If the Jacobian is a triangular matrix, the determinant is just the product of the diagonal entries, which can be computed in \\(\\mathcal{O}(n)\\) time. - This works because in a triangular matrix, all the off-diagonal elements are zero ‚Äî so the determinant simplifies significantly.\nIn practice, flow models like RealNVP and MAF are designed so that each output dimension \\(x_i\\) depends only on some subset of the input dimensions \\(z_{\\leq i}\\) (for lower triangular structure) or \\(z_{\\geq i}\\) (for upper triangular structure). This results in a Jacobian of the form:\n\\[\nJ = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial z_1} & 0 & \\cdots & 0 \\\\\n\\ast & \\frac{\\partial f_2}{\\partial z_2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\ast & \\ast & \\cdots & \\frac{\\partial f_n}{\\partial z_n}\n\\end{pmatrix}\n\\]\nBecause of this triangular structure, computing the determinant becomes as simple as multiplying the diagonal terms:\n\\[\n\\det(J) = \\prod_{i=1}^{n} \\frac{\\partial f_i}{\\partial z_i}\n\\]\nThis is why many modern flow models rely on coupling layers or autoregressive masking: they preserve invertibility and enable efficient, exact likelihood computation."
  },
  {
    "objectID": "flows.html#types-of-flow-architectures",
    "href": "flows.html#types-of-flow-architectures",
    "title": "Normalizing Flow Models",
    "section": "5 Types of Flow Architectures",
    "text": "5 Types of Flow Architectures\nThis section introduces common architectural families used in normalizing flows, highlighting their core ideas, strengths, and limitations.\n\n5.1 Elementwise Flows\n\nIdea: Apply a simple invertible function to each variable independently.\nExamples: Leaky ReLU, Softplus, ELU.\nStrengths: Extremely fast; easy to implement; analytically tractable.\nLimitations: Cannot model interactions or dependencies between variables.\n\n\n\n5.2 Linear Flows\n\nIdea: Apply a linear transformation using an invertible matrix (e.g., permutation, rotation, LU decomposition).\nExamples: Glow‚Äôs 1x1 Convolution, LU flows.\nStrengths: Efficiently models global dependencies; can be used to permute variables.\nLimitations: Limited expressiveness when used alone.\n\n\n\n5.3 Coupling Flows\n\nIdea: Split the input into two parts. One half remains unchanged while the other is transformed based on it.\nExamples: NICE (additive), RealNVP (affine).\nStrengths: Easy to invert and compute Jacobians; scalable to high dimensions.\nLimitations: Requires stacking multiple layers to mix information across all dimensions.\n\n\n\n5.4 Autoregressive Flows\n\nIdea: Model the transformation of each variable conditioned on the previous ones in a fixed order.\nExamples: Masked Autoregressive Flow (MAF), Inverse Autoregressive Flow (IAF).\nStrengths: Highly expressive; models arbitrary dependencies.\nLimitations: Slower sampling or density evaluation depending on flow direction.\n\n\n\n5.5 Residual Flows\n\nIdea: Add residual connections while enforcing invertibility (e.g., using constraints on Jacobian eigenvalues).\nExamples: Planar flows, Radial flows, Residual Flows (Behrmann et al.).\nStrengths: Flexible and capable of complex transformations.\nLimitations: May require care to ensure invertibility; harder to train.\n\n\n\n5.6 Continuous Flows\n\nIdea: Model the transformation as the solution to a differential equation parameterized by a neural network.\nExamples: Neural ODEs, FFJORD.\nStrengths: Highly flexible; enables continuous-time modeling.\nLimitations: Computationally expensive; uses ODE solvers during training and inference.\n\n\nThese architectures can be mixed and matched in real-world models to balance expressiveness, efficiency, and tractability. Each comes with trade-offs, and their selection often depends on the task and data at hand.\nIn the rest of this article, we focus on Coupling Flows, briefly introducing NICE and then diving deeper into the structure, intuition, and implementation details of RealNVP.\n\n\n5.7 NICE: Nonlinear Independent Components Estimation\nThe NICE (Nonlinear Independent Components Estimation) model, introduced by Laurent Dinh, David Krueger, and Yoshua Bengio in 2014, is a foundational work in the development of normalizing flows.\nIt provides a framework for transforming complex high-dimensional data into a simpler latent space (often a standard Gaussian), enabling both exact likelihood estimation and sampling ‚Äî two fundamental goals in generative modeling.\n\n5.7.1 Core Concepts\n\nInvertible Transformations:\nNICE constructs a chain of invertible functions to map inputs to latent variables. This ensures that both the forward and inverse transformations are tractable.\nAdditive Coupling Layers:\nThe model partitions the input into two parts and applies an additive transformation to one part using a function of the other. This design yields a triangular Jacobian with determinant 1, making log-likelihood computation efficient.\nVolume-Preserving Mapping:\nBecause additive coupling layers do not scale the space, NICE preserves volume ‚Äî i.e., the Jacobian determinant is exactly 1. While this limits expressiveness, it simplifies training and inference.\nScaling Layer (Optional):\nThe original NICE paper includes an optional scaling layer at the end to allow some volume change per dimension.\nExact Log-Likelihood:\nUnlike VAEs or GANs, which rely on approximations, NICE enables exact evaluation of the log-likelihood, making it a fully probabilistic, likelihood-based model.\n\n\n\n5.7.2 Additive Coupling Layer\nTo make the transformation invertible and computationally efficient, NICE splits the input vector into two parts. One part is kept unchanged, while the other part is modified using a function of the unchanged part. This way, we can easily reverse the process because we always know what was kept intact.\nLet‚Äôs partition the input \\(\\mathbf{z} \\in \\mathbb{R}^n\\) into two subsets: \\(\\mathbf{z}_{1:d}\\) and \\(\\mathbf{z}_{d+1:n}\\) for some \\(1 \\leq d &lt; n\\).\n\nForward Mapping \\(\\mathbf{z} \\mapsto \\mathbf{x}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)} \\\\\n\\mathbf{x}_{d+1:n} &= \\mathbf{z}_{d+1:n} + m_\\theta(\\mathbf{z}_{1:d})\n\\end{aligned}\n\\]\nwhere \\(m_\\theta(\\cdot)\\) is a neural network with parameters \\(\\theta\\), \\(d\\) input units, and \\(n - d\\) output units.\n\nInverse Mapping \\(\\mathbf{x} \\mapsto \\mathbf{z}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)} \\\\\n\\mathbf{z}_{d+1:n} &= \\mathbf{x}_{d+1:n} - m_\\theta(\\mathbf{x}_{1:d})\n\\end{aligned}\n\\]\n\nJacobian of the forward mapping:\n\nThe Jacobian matrix captures how much the transformation stretches or compresses space. Because the unchanged subset passes through as-is and the transformation is purely additive (no scaling), the Jacobian is triangular with 1s on the diagonal ‚Äî so its determinant is 1 ‚Äî meaning the transformation preserves volume.\n\\[\nJ = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\n\\frac{\\partial m_\\theta}{\\partial \\mathbf{z}_{1:d}} & I_{n-d}\n\\end{pmatrix}\n\\]\n\\[\n\\det(J) = 1\n\\]\nHence, additive coupling is a volume-preserving transformation.\n\n\n5.7.3 Rescaling Layer\nTo overcome the limitation of fixed volume, NICE adds a diagonal scaling transformation at the end, allowing the model to contract or expand space. This is not part of the coupling layers, but is crucial to increase flexibility.\n\nForward Mapping:\n\n\\[\nx_i = s_i z_i \\quad \\text{with} \\quad s_i &gt; 0\n\\]\n\nInverse Mapping:\n\n\\[\nz_i = \\frac{x_i}{s_i}\n\\]\n\nJacobian:\n\n\\[\nJ = \\text{diag}(\\mathbf{s})\n\\quad \\Rightarrow \\quad\n\\det(J) = \\prod_{i=1}^n s_i\n\\]\nHowever, the volume-preserving property of NICE limits its expressiveness. RealNVP extends this idea by introducing affine coupling layers that enable volume changes during transformation.\n\n\n\n5.8 Real-NVP: Non-Volume Preserving Extension of NICE\nReal-NVP (Dinh et al., 2017) extends NICE by introducing a scaling function that allows the model to change volume, enabling more expressive transformations. This is achieved using affine coupling layers that apply learned scaling and translation functions to part of the input while keeping the rest unchanged.\n\n\n\n Visualization of a single affine coupling layer in RealNVP. The identity path and affine transform structure allow exact inversion and efficient computation. \n\n\nWe partition the input \\(\\mathbf{z} \\in \\mathbb{R}^n\\) into two subsets: \\(\\mathbf{z}_{1:d}\\) and \\(\\mathbf{z}_{d+1:n}\\).\n\nForward Mapping \\(\\mathbf{z} \\mapsto \\mathbf{x}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)} \\\\\\\\\n\\mathbf{x}_{d+1:n} &= \\mathbf{z}_{d+1:n} \\odot \\exp(\\alpha_\\theta(\\mathbf{z}_{1:d})) + \\mu_\\theta(\\mathbf{z}_{1:d})\n\\end{aligned}\n\\]\nHere, \\(\\boldsymbol{\\alpha}_\\theta(\\cdot)\\) and \\(\\boldsymbol{\\mu}_\\theta(\\cdot)\\) are neural networks with parameters \\(\\theta\\) that take the unchanged subset \\(\\mathbf{z}_{1:d}\\) as input and produce scale and shift parameters, respectively, for the transformed subset \\(\\mathbf{z}_{d+1:n}\\). These functions enable flexible, learnable affine transformations while preserving invertibility.\n\nInverse Mapping \\(\\mathbf{x} \\mapsto \\mathbf{z}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)} \\\\\\\\\n\\mathbf{z}_{d+1:n} &= \\left( \\mathbf{x}_{d+1:n} - \\mu_\\theta(\\mathbf{x}_{1:d}) \\right) \\odot \\exp(-\\alpha_\\theta(\\mathbf{x}_{1:d}))\n\\end{aligned}\n\\]\nThe inverse mapping recovers the latent variable \\(\\mathbf{z}\\) from the data \\(\\mathbf{x}\\). The first subset \\(\\mathbf{x}_{1:d}\\) remains unchanged and directly becomes \\(\\mathbf{z}_{1:d}\\). To reconstruct \\(\\mathbf{z}_{d+1:n}\\), we first subtract the shift \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_{1:d})\\) from \\(\\mathbf{x}_{d+1:n}\\), and then apply an elementwise rescaling using \\(\\exp(-\\boldsymbol{\\alpha}_\\theta(\\mathbf{x}_{1:d}))\\). This inversion relies on the same neural networks used in the forward pass and ensures that the transformation is exactly reversible.\n\nJacobian of Forward Mapping:\n\n\\[\nJ = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\\\\\n\\frac{\\partial \\mathbf{x}_{d+1:n}}{\\partial \\mathbf{z}_{1:d}} & \\operatorname{diag}\\left(\\exp(\\alpha_\\theta(\\mathbf{z}_{1:d}))\\right)\n\\end{pmatrix}\n\\]\nThe Jacobian matrix of the RealNVP forward transformation has a special block structure due to the design of the affine coupling layer:\n\nUpper left block: \\(\\mathbf{I}_d\\)\nThis corresponds to the partial derivatives of \\(\\mathbf{x}_{1:d}\\) with respect to \\(\\mathbf{z}_{1:d}\\). Since the first \\(d\\) variables are passed through unchanged (\\(\\mathbf{x}_{1:d} = \\mathbf{z}_{1:d}\\)), their derivatives form an identity matrix.\nUpper right block: \\(0\\)\nThese derivatives are zero because \\(\\mathbf{x}_{1:d}\\) does not depend on \\(\\mathbf{z}_{d+1:n}\\) at all ‚Äî they‚Äôre completely decoupled.\nLower right block: (diagonal)\nEach element of \\(\\mathbf{x}_{d+1:n}\\) is scaled elementwise by \\(\\exp\\left(\\left[\\alpha_\\theta(\\mathbf{z}_{1:d})\\right]_i\\right)\\). This means the Jacobian of this part is a diagonal matrix, where each diagonal entry is the corresponding scale factor.\nLower left block:\nThis part can contain non-zero values because \\(\\mathbf{x}_{d+1:n}\\) depends on \\(\\mathbf{z}_{1:d}\\) via the neural networks. But thanks to the triangular structure of the Jacobian, we don‚Äôt need this block to compute the determinant.\n\n\n\n\n Jacobian of the RealNVP forward transformation. Upper triangular structure arises because the first subset is unchanged, while the second is scaled and shifted based on the first. \n\n\n\n5.8.1 Why This Structure Matters\nBecause the Jacobian is triangular, its determinant is simply the product of the diagonal entries.\n\\[\n\\det(J) = \\prod_{i=d+1}^{n} \\exp\\left( \\alpha_\\theta(\\mathbf{z}_{1:d})_i \\right)\n= \\exp\\left( \\sum_{i=d+1}^{n} \\alpha_\\theta(\\mathbf{z}_{1:d})_i \\right)\n\\]\nIn log-space, this becomes a sum:\n\\[\n\\log \\det(J) = \\sum_{i=d+1}^{n} \\alpha_\\theta(\\mathbf{z}_{1:d})_i\n\\]\nThis makes the computation of log-likelihoods fast and tractable.\nTaking the product of the diagonal entries gives us a measure of how much the transformation expands or contracts local volume. If the determinant is greater than 1, the transformation expands space; if it‚Äôs less than 1, it contracts space. Since the determinant is not fixed, RealNVP performs a non-volume preserving transformation ‚Äî allowing it to model more complex distributions than NICE, which preserves volume by design.\n\n\n5.8.2 Stacking Coupling Layers\nEach coupling layer only transforms part of the input. To ensure that every dimension is eventually updated, RealNVP stacks multiple coupling layers and alternates the masking pattern between them.\n\nIn one layer, the first half is fixed, and the second half is transformed.\nIn the next layer, the roles are reversed.\n\nThis alternating structure ensures: - All input dimensions are updated across layers - The full transformation remains invertible - The total log-determinant is the sum of the log-determinants of each layer\n\n\n5.8.3 RealNVP in Action (Two Moons)\nThe following plots illustrate how RealNVP transforms data in practice:\n\n\n\n Top-left: Original two-moons data (X)\nTop-right: Encoded latent space (Z) Bottom-left: Latent samples from base distribution\nBottom-right: Generated samples mapped back to (X) space\n\n\n\n\n\n5.8.4 Summary\nTo recap the key distinctions between NICE and RealNVP, here‚Äôs a side-by-side comparison:\n\n\n\n\n\n\n\n\nAspect\nNICE\nRealNVP\n\n\n\n\nType of coupling\nAdditive\nAffine (scaling + shift)\n\n\nVolume change\nOnly possible with rescaling layer\nBuilt into each coupling layer\n\n\nJacobian determinant\n1 (in coupling layers)\nVaries (depends on learned scale)\n\n\nExpressiveness\nLimited (volume-preserving layers)\nHigher (learns scale & shift)\n\n\nLog-likelihood\nExact\nExact"
  },
  {
    "objectID": "flows.html#try-it-yourself-flow-model-in-pytorch",
    "href": "flows.html#try-it-yourself-flow-model-in-pytorch",
    "title": "Normalizing Flow Models",
    "section": "6 üß™ Try It Yourself: Flow Model in Pytorch",
    "text": "6 üß™ Try It Yourself: Flow Model in Pytorch\nYou can explore a minimal PyTorch implementation of a normalizing flow model:\n\nüìò View Notebook on GitHub\nüöÄ Run in Google Colab"
  },
  {
    "objectID": "flows.html#references",
    "href": "flows.html#references",
    "title": "Normalizing Flow Models",
    "section": "7 References",
    "text": "7 References\n[1] Stanford CS236 Notes. ‚ÄúNormalizing Flows‚Äù\n[2] UT Austin Calculus Notes. ‚ÄúJacobian and Change of Variables‚Äù\n[3] Danilo Jimenez Rezende, and Shakir Mohamed. ‚ÄúVariational Inference with Normalizing Flows‚Äù\n[4] Kobyzev, Prince, and Brubaker. ‚ÄúNormalizing Flows: An Introduction and Review of Current Methods‚Äù\n[5] Wikipedia. ‚ÄúNormalizing Flow‚Äù"
  },
  {
    "objectID": "flows.html#further-reading",
    "href": "flows.html#further-reading",
    "title": "Normalizing Flow Models",
    "section": "8 Further Reading",
    "text": "8 Further Reading\n[1] George Papamakarios et al.¬†‚ÄúNormalizing Flows for Probabilistic Modeling and Inference‚Äù\n[2] Lilian Weng. ‚ÄúFlow-based Models‚Äù\n[3] Eric Jang. ‚ÄúNormalizing Flows Tutorial ‚Äì Part 1‚Äù\n[4] Eric Jang. ‚ÄúNormalizing Flows Tutorial ‚Äì Part 2‚Äù"
  },
  {
    "objectID": "transformers.html",
    "href": "transformers.html",
    "title": "Understanding Transformers, Step by Step",
    "section": "",
    "text": "Transformers are a neural network architecture designed for processing sequential data. Introduced in the 2017 paper Attention is All You Need, transformers replaced recurrent and convolutional architectures in many NLP tasks. They leverage a novel mechanism called self-attention, which allows for parallel computation and effectively models long-range dependencies.\nThe architecture consists of an encoder and decoder, as illustrated below.\n\n\n\nFigure: Figure: High-level schematic of the Transformer architecture, showing the encoder and decoder with self-attention and masked attention mechanisms (Vaswani et al., 2017, Attention Is All You Need)."
  },
  {
    "objectID": "transformers.html#introduction",
    "href": "transformers.html#introduction",
    "title": "Understanding Transformers, Step by Step",
    "section": "",
    "text": "Transformers are a neural network architecture designed for processing sequential data. Introduced in the 2017 paper Attention is All You Need, transformers replaced recurrent and convolutional architectures in many NLP tasks. They leverage a novel mechanism called self-attention, which allows for parallel computation and effectively models long-range dependencies.\nThe architecture consists of an encoder and decoder, as illustrated below.\n\n\n\nFigure: Figure: High-level schematic of the Transformer architecture, showing the encoder and decoder with self-attention and masked attention mechanisms (Vaswani et al., 2017, Attention Is All You Need)."
  },
  {
    "objectID": "transformers.html#limitations-of-recurrent-neural-networks-rnns",
    "href": "transformers.html#limitations-of-recurrent-neural-networks-rnns",
    "title": "Understanding Transformers, Step by Step",
    "section": "2 Limitations of Recurrent Neural Networks (RNNs)",
    "text": "2 Limitations of Recurrent Neural Networks (RNNs)\nWhile transformers have become the standard for sequence modeling, they were developed to overcome key challenges in earlier architectures such as Recurrent Neural Networks (RNNs). Before the advent of transformers, Recurrent Neural Networks (RNNs) were the dominant architecture for sequence modeling tasks. Their ability to process sequences of variable length and maintain a memory of past inputs made them widely used. In an RNN, the hidden state \\(h_t\\) at time \\(t\\) is computed based on the current input \\(x_t\\) and the previous hidden state \\(h_{t-1}\\). This allows RNNs to process sequences by carrying information forward over time. The hidden state at time t can be computed recursively as:\n\\[\nh_t = \\tanh(W_h h_{t-1} + W_x x_t + b)\n\\]\nwhere:\n\n\\(W_h\\) ‚Äî weight matrix for the hidden state\n\\(W_x\\) ‚Äî weight matrix for the input\n\\(b\\) ‚Äî bias term\n\\(\\tanh\\) ‚Äî activation function\n\nThe figure below shows a single RNN cell (left) and the unrolled RNN over multiple time steps (right).\n\n\n\nFigure: RNN unrolled over time, showing the recurrence of hidden states and outputs across time steps.\n\n\nHowever, RNNs suffer from:\n\nSequential computation: Cannot fully parallelize processing of sequences.\nVanishing/exploding gradients: Gradients through many time steps can vanish or blow up, making it difficult to learn long-term dependencies."
  },
  {
    "objectID": "transformers.html#transformer-architecture-overview",
    "href": "transformers.html#transformer-architecture-overview",
    "title": "Understanding Transformers, Step by Step",
    "section": "3 Transformer Architecture Overview",
    "text": "3 Transformer Architecture Overview\nThe Transformer architecture can be configured in different ways depending on the application. At a high level, there are three common variants:\n\nEncoder‚ÄìDecoder (original transformer) ‚Äî used in translation.\nEncoder-only (e.g., BERT) ‚Äî for classification, masked language modeling.\nDecoder-only (e.g., GPT) ‚Äî for autoregressive text generation.\n\nAt the heart of these architectures is the self-attention mechanism, which enables each token to attend to all others in the sequence ‚Äî a key innovation that we‚Äôll explain shortly.\nIn the following sections, we‚Äôll break down the encoder and decoder architectures in detail.\n\n3.1 Key Variables\nBefore we dive into the equations and diagrams, here is a quick reference table summarizing the commonly used variables, their shapes, and their meanings. This will make the subsequent sections easier to follow.\n\n\n\nNotation Table"
  },
  {
    "objectID": "transformers.html#encoder",
    "href": "transformers.html#encoder",
    "title": "Understanding Transformers, Step by Step",
    "section": "4 Encoder",
    "text": "4 Encoder\nThe encoder consists of a stack of identical layers, each containing:\n\nSelf-Attention\n\nFeedforward Network\n\nResidual Connections + Layer Norm\n\nPositional Encoding\n\n\n\n4.1 Input Representation\nBefore the attention mechanism can work, each input token is mapped to a continuous vector representation and enriched with positional information.\nToken Embedding\nWe start with a sequence of token indices:\n\\[\nT \\in \\{1, \\dots, V\\}^n\n\\] where \\(n\\) is the sequence length and \\(V\\) is the vocabulary size.\nThese indices are mapped to continuous vectors using a learnable embedding matrix \\(E \\in \\mathbb{R}^{V \\times d_e}\\):\n\\[\nX_\\text{tokens} = E[T] \\in \\mathbb{R}^{n \\times d_e}\n\\]\nPositional Encoding\nThe self-attention mechanism processes the input sequence as a set of vectors without any notion of order.\nHowever, the meaning of a sentence depends on the order of its words.\n\n‚ÄúThe cat chased the dog‚Äù ‚â† ‚ÄúThe dog chased the cat‚Äù\n\nWithout positional information, self-attention would treat these sentences as identical because it has no notion of token order. To address this, we add a position vector \\(p_i\\) to each token embedding \\(x_i\\), producing a position-aware representation:\n\\[\n\\tilde{x}_i = x_i + p_i\n\\]\nwhere:\n\n\\(x_i\\) ‚Äî embedding of the token at position \\(i\\)\n\\(p_i\\) ‚Äî positional encoding vector\n\\(\\tilde{x}_i\\) ‚Äî position-aware embedding passed to the model\n\nThe figure below illustrates this process:\n\n\n\nFigure: Token and positional embeddings are summed element-wise to produce the final input representation passed to the encoder.\n\n\nThere are two common methods for generating positional embeddings, detailed below.\n(A) Sinusoidal Positional Encoding\nIn the original Transformer paper, \\(p_i\\) is defined using sinusoidal functions of varying frequencies:\n\\[\np_i =\n\\begin{pmatrix}\n\\sin\\big(i / 10000^{\\frac{2j}{d_e}}\\big) \\\\\n\\cos\\big(i / 10000^{\\frac{2j}{d_e}}\\big)\n\\end{pmatrix}\n\\]\nWhy do trigonometric functions work?\n\nSine and cosine provide unique, smooth, and continuous encodings for each position.\nDifferent frequencies let the model capture both short- and long-range position relationships.\nNo learnable parameters ‚Äî generalizes better to unseen sequence lengths.\nPros:\n\nDoes not introduce additional parameters.\nPeriodicity allows some generalization to longer sequences.\n\nCons:\n\nNot learnable ‚Äî fixed at initialization.\nLimited extrapolation in practice.\n\n\n(B) Learned Positional Encoding\nAn alternative is to treat \\(p_i\\) as a learnable parameter:\nWe define a matrix \\(P \\in \\mathbb{R}^{d_e \\times n}\\), where each \\(p_i\\) is a column of \\(P\\).\n\\[\nP = [p_1, p_2, \\dots, p_n], \\quad P \\in \\mathbb{R}^{d_e \\times n}\n\\]\n\nPros:\n\nFully learnable; each position can adapt to the data.\nMost modern systems (e.g., GPT) use this.\n\nCons:\n\nCannot extrapolate to sequences longer than seen in training.\n\n\nIn practice, most architectures today use learned positional encodings because of their flexibility and performance.\nSummary\n\nPositional encoding ensures the model is aware of token order.\n\nSinusoidal encodings are fixed & periodic.\n\nLearned encodings are flexible & widely used today.\n\nWith the input representation enriched by positional information, the transformer encoder can now apply self-attention and feedforward layers to process the sequence effectively.\n\n\n\n4.2 Attention Mechanism\nAt the heart of the transformer is the scaled dot-product attention mechanism, which allows the model to weigh the relevance of each token in the sequence when processing a given token. This enables the model to capture relationships between tokens regardless of their distance in the sequence.\nWhat is Attention?\nEach token is projected into three vectors:\n\nQuery (\\(Q\\)): represents the token we‚Äôre focusing on.\nKey (\\(K\\)): represents the tokens we compare against.\nValue (\\(V\\)): represents the information we retrieve if the key is relevant.\n\nFor a given query \\(Q\\), the attention weights over all keys \\(K\\) are computed by taking the dot product of \\(Q\\) with \\(K\\), scaling, and passing through a softmax:\n\\[\n\\text{Attention}(Q, K, V) =\n\\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n\\]\nwhere:\n\n\\(Q\\) is the matrix of query vectors, shape \\([n \\times d_k]\\)\n\\(K\\) is the matrix of key vectors, shape \\([n \\times d_k]\\)\n\\(V\\) is the matrix of value vectors, shape \\([n \\times d_v]\\)\n\\(d_k\\) is the dimension of the keys (used for scaling)\n\nStep-by-step Computation\nThe figure below illustrates the computation flow of scaled dot-product attention, including the dimensions of each variable at every stage:\n\n\n\nFigure: Attention ‚Äî step-by-step computation, with variables, shapes, and operations.\n\n\nBreakdown of the Steps\n1: Input Representation: The input tokens are embedded into a continuous vector space and positional encodings are added to inject sequence order information:\n\\[\nX = \\text{Embedding}(X_{\\text{tokens}}) + \\text{PositionalEncoding}\n\\]\n2: Linear Projections: The embedded input \\(X\\) is projected into three different spaces to produce the query (Q), key (K), and value (V) matrices, using learnable weight matrices \\(W_q\\), \\(W_k\\), \\(W_v\\):\n\\[\nQ = X W_q, \\quad K = X W_k, \\quad V = X W_v\n\\]\nThese projections allow the model to attend to different aspects of the input.\n3: Compute Similarity Scores: We compute the dot product between the queries and the transposed keys to measure similarity (or relevance) between tokens:\n\\[\nQ K^T\n\\]\nThis gives a matrix of raw attention scores.\n4: Scale the Scores: To avoid extremely large values when the dimensionality \\(d_k\\) is high, the similarity scores are scaled by \\(\\sqrt{d_k}\\):\n\\[\n\\frac{Q K^T}{\\sqrt{d_k}}\n\\]\nThis stabilizes gradients during training.\n5: Softmax to get Attention Weights: The scaled scores are passed through a softmax function to convert them into probabilities that sum to 1. These are the attention weights \\(A\\), indicating how much focus to put on each token:\n\\[\n\\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right)\n\\]\n6: Weighted Sum: Finally, the attention weights are multiplied with the value vectors \\(V\\) to produce a weighted sum, which is the attention output:\n\\[\n\\text{Attention Weights} \\cdot V\n\\]\nWhy is Attention Powerful?\n\nCaptures long-range dependencies.\n\nLearns which tokens are most relevant to each other.\n\nFully parallelizable since it operates on the entire sequence at once.\n\nWhile single-head attention is effective, it can focus only on one type of relationship at a time. Multi-Head Attention extends this idea by allowing the model to attend to information from multiple representation subspaces in parallel.\n\n\n\n4.3 Multi-Head Attention\nWhile a single attention head can focus on certain aspects of the input sequence, it may miss other patterns. The multi-head attention (MHA) mechanism allows the model to attend to information from multiple representation subspaces at different positions simultaneously. This helps the transformer to capture more nuanced patterns in the input sequence.\nThe figure below shows how multiple independent attention heads are computed in parallel, concatenated, and linearly transformed to produce the final output of the multi-head attention layer.\n\n\n\nFigure: Multi-Head Attention ‚Äî four parallel heads, concatenated and projected by \\(W_o\\) to form the final output.\n\n\nWhat is Multi-Head Attention?\nInstead of computing a single set of \\(Q\\), \\(K\\), \\(V\\), the model projects the input into \\(h\\) different sets of \\(Q\\), \\(K\\), \\(V\\), called heads. Each head performs scaled dot-product attention independently, and their outputs are concatenated and linearly transformed.\nFor head \\(i\\): \\[\n\\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) =\n\\text{softmax} \\left( \\frac{Q_i K_i^T}{\\sqrt{d_k}} \\right) V_i\n\\]\nwhere:\n\n\\(Q_i = X W_q^{(i)}\\)\n\\(K_i = X W_k^{(i)}\\)\n\\(V_i = X W_v^{(i)}\\)\n\nHere, \\(W_q^{(i)}, W_k^{(i)}, W_v^{(i)}\\) are separate learnable weights for each head.\nCombining the Heads\nThe outputs of all \\(h\\) heads are concatenated along the feature dimension and projected back into \\(d_e\\) dimensions: \\[\n\\text{MultiHead}(Q, K, V) =\n\\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_o\n\\]\nwhere \\(W_o\\) is a learnable weight matrix of shape \\([h \\cdot d_v \\times d_e]\\).\nThe figure below illustrates how multi-head self-attention captures relationships between tokens. The word ‚Äúmaking‚Äù attends strongly to itself, as well as to the distant tokens ‚Äúmore‚Äù and ‚Äúdifficult‚Äù. Different colors represent different attention heads, each learning distinct patterns, while thicker lines indicate stronger attention weights, showing which tokens are most relevant for the given token.\n\n\n\nFigure: Visualization of the attention mechanism showing long-range dependencies from Vaswani et al., 2017\n\n\nWhy Multi-Head Attention?\n\nAllows the model to jointly attend to information from different representation subspaces.\n\nProvides richer and more diverse attention patterns.\n\nEmpirically improves performance compared to a single head.\n\nAfter computing the attention outputs, each token passes through a feedforward network to introduce non-linear transformations.\n\n\n\n4.4 Feedforward Network\nAfter self-attention, the transformer applies a position-wise feedforward network (FFN) to each token independently ‚Äî adding non-linearity and transforming the attended information further.\nWhy is this needed?\n\nSelf-attention by itself is a linear operation ‚Äî it just computes weighted sums of the value vectors.\nStacking more self-attention layers without nonlinearity simply re-averages the values, limiting expressiveness.\nTo address this, each output vector of the self-attention layer is passed through a multi-layer perceptron (MLP).\n\nComputation\nFor each token output vector \\(output_i\\) from the self-attention: \\[\nm_i = MLP(output_i) = W_2 \\cdot ReLU(W_1 \\cdot output_i + b_1) + b_2\n\\]\nwhere:\n\n\\(W_1\\), \\(W_2\\) ‚Äî learnable weight matrices.\n\\(b_1\\), \\(b_2\\) ‚Äî learnable biases.\n\\(ReLU\\) ‚Äî non-linear activation function applied element-wise.\n\nThis is done independently for each position.\nSummary:\n\nAdds nonlinearity and expressiveness.\n\nProcesses each token independently after attention.\n\nHelps the model learn complex transformations beyond weighted averages.\n\n\n\n\n4.5 Residual Connections & Layer Normalization\nTo further improve optimization and stability, transformers also incorporate two additional techniques: residual connections and layer normalization. These are often written together as Add & Norm in most diagrams.\nWhy use these techniques?\n\nResidual connections make the optimization smoother and help gradients flow better.\n\nLayer normalization stabilizes training and helps each layer produce consistent, normalized outputs.\n\n\n4.5.1 Residual Connections\nResidual connections help the model learn better by allowing gradients to flow more easily through the network and biasing it towards the identity function.\nInstead of simply applying a layer transformation:\n\\[\nX^{(i)} = \\text{Layer}(X^{(i-1)})\n\\]\nwe add the input back to the output of the layer:\n\\[\nX^{(i)} = X^{(i-1)} + \\text{Layer}(X^{(i-1)})\n\\]\nThis ensures that the model only needs to learn the residual (the difference from the input), which improves optimization.\n\n\n4.5.2 Layer Normalization\nLayer normalization helps by reducing uninformative variation in the hidden states of each layer. It normalizes each token embedding to have zero mean and unit variance within each layer, and within the feature dimension of a single token ‚Äî independent of other tokens or samples in the batch.\nFor a vector \\(x \\in \\mathbb{R}^d\\), layer norm is computed as:\n\\[\n\\mu = \\frac{1}{d} \\sum_{j=1}^{d} x_j, \\quad \\sigma = \\sqrt{\\frac{1}{d} \\sum_{j=1}^{d} (x_j - \\mu)^2}\n\\]\nThen:\n\\[\n\\text{output} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta\n\\]\nwhere:\n\n\\(\\gamma \\in \\mathbb{R}^d\\), \\(\\beta \\in \\mathbb{R}^d\\) are learnable gain and bias parameters.\n\\(\\epsilon\\) is a small constant for numerical stability.\n\nIn conventional neural networks, such as convolutional neural networks (CNNs), it is common to use Batch Normalization to stabilize training. However, in sequence models like Transformers, Layer Normalization is preferred because it normalizes each token independently of the batch and sequence length ‚Äî making it more suitable for NLP tasks and small batches.\nThe figure and the table below shows the difference between Layer Normalization and Batch Normalization, highlighting which dimensions they operate over.\n\n\n\nFigure: Layer Norm normalizes each token‚Äôs features (independently of other tokens and samples), while Batch Norm normalizes each feature across the batch.\n\n\n\n\n\n\n\n\n\n\nAspect\nLayer Normalization\nBatch Normalization\n\n\n\n\nWhat it normalizes over\nFeature dimension (per token)\nFeature dimension (across the batch)\n\n\nSuitable for\nSequence models, NLP, Transformers (small batch sizes)\nCNNs, vision tasks (large batch sizes)\n\n\nHandles variable batch sizes?\nYes\nNo (depends on batch size)\n\n\nWhy in Transformers?\nStable for per-token operations & small batches\nUnstable in small batches & sequences\n\n\n\nIn summary, Layer Normalization normalizes each token‚Äôs features independently of other tokens and samples, making it ideal for sequence-to-sequence and language tasks. Batch Normalization normalizes each feature across the batch, which is less suitable for NLP. Layer normalization‚Äôs independence from batch size and sequence structure makes it ideal for NLP and sequence models.\nWith these components ‚Äî self-attention, feedforward networks, residual connections, and layer normalization ‚Äî the encoder stack is complete. We now turn to the decoder, which builds on the encoder‚Äôs output to generate predictions."
  },
  {
    "objectID": "transformers.html#decoder",
    "href": "transformers.html#decoder",
    "title": "Understanding Transformers, Step by Step",
    "section": "5 Decoder",
    "text": "5 Decoder\nLike the encoder, the decoder is composed of stacked layers, but with three key components:\n\nMasked Multi-Head Self-Attention ‚Äî only attends to past tokens.\n\nEncoder‚ÄìDecoder Attention ‚Äî attends to the encoder output.\n\nFeedforward + normalization + residuals\n\n\n\n5.1 Attention Mask for Decoder\nIn the decoder‚Äôs self-attention, it is critical to prevent the model from ‚Äúlooking into the future‚Äù ‚Äî attending to tokens that come later in the sequence. This ensures proper causal sequence generation (left-to-right).\nWhy do we need masking?\n\nIn sequence prediction tasks (like machine translation or language modeling), the model should predict the next token based only on past and current tokens ‚Äî not on future tokens.\nWithout masking, the self-attention mechanism would compute attention over all tokens in the sequence, violating the causal order.\nThis is solved by masking out the attention to future tokens.\n\nHow does masking work?\nAt each position \\(i\\), we: - Keep attention scores for tokens at \\(j \\leq i\\) (past & current positions). - Set attention scores to \\(-\\infty\\) (or equivalently, weights to 0 after softmax) for \\(j &gt; i\\) (future positions).\nMathematically, the masked attention score \\(e_{ij}\\) is:\n\\[\ne_{ij} =\n\\begin{cases}\nq_i^\\top k_j, & j \\leq i \\\\\\\\\n-\\infty, & j &gt; i\n\\end{cases}\n\\]\nAfter applying the softmax, the model assigns non-zero attention only to tokens at positions ‚â§ \\(i\\).\nThe figure below illustrates how the attention mask is applied to \\(QK^\\top\\), blocking future tokens by assigning them \\(-\\infty\\), and producing the masked attention matrix.\n\n\n\nFigure: Visualization of the attention mask applied to \\(QK^\\top\\), setting future positions to \\(-\\infty\\), and producing the masked attention matrix.\n\n\nBenefits:\n\nEnables parallel computation ‚Äî the mask is precomputed and applied during training.\n\nPrevents information leakage from future tokens.\n\nAllows the decoder to autoregressively predict tokens during inference.\n\n\n\n\n5.2 Cross-Attention in the Decoder\nAfter the decoder‚Äôs self-attention step, the transformer uses a cross-attention layer (sometimes called encoder-decoder attention) to incorporate information from the encoder‚Äôs output into each decoder token representation.\nIn this layer:\n\nThe queries (\\(x_1\\)) come from the decoder hidden states at the current layer.\nThe keys (\\(x_2\\)) and values (\\(x_2\\)) come from the encoder output.\n\nThis allows the decoder to attend to the entire input sequence (from the encoder) for each token it generates.\nThe figure below shows the computation flow of the cross-attention mechanism, including the dimensions of each variable at every stage:\n\n\n\nFigure: Cross-attention mechanism in the decoder. Queries (\\(x_1\\)) come from the decoder, while keys and values (\\(x_2\\)) come from the encoder output. The attention weights determine how much each encoder token contributes to each decoder token.\n\n\n\n5.2.1 Step-by-step\n\nThe decoder hidden states \\(x_1 \\in \\mathbb{R}^{n \\times d_e}\\) (where \\(n\\) is the number of decoder tokens) are projected into queries: \\[\nQ = x_1 W_q \\in \\mathbb{R}^{n \\times d_q}\n\\]\nThe encoder output \\(x_2 \\in \\mathbb{R}^{m \\times d_e}\\) (where \\(m\\) is the number of encoder tokens) is projected into keys and values: \\[\nK = x_2 W_k \\in \\mathbb{R}^{m \\times d_q}, \\quad V = x_2 W_v \\in \\mathbb{R}^{m \\times d_v}\n\\]\nCompute similarity scores between each query and all keys: \\[\nQ K^T \\in \\mathbb{R}^{n \\times m}\n\\]\nScale the scores by \\(\\sqrt{d_k}\\) and apply softmax to get the attention weights: \\[\nA = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) \\in \\mathbb{R}^{n \\times m}\n\\]\nCompute the weighted sum over the values to get the Cross-Attention Output \\(Z\\): \\[\nZ = A V \\in \\mathbb{R}^{n \\times d_v}\n\\]\n\nThis mechanism allows each decoder token to attend over the entire encoder output sequence and incorporate the most relevant source information into its prediction."
  },
  {
    "objectID": "transformers.html#putting-it-all-together",
    "href": "transformers.html#putting-it-all-together",
    "title": "Understanding Transformers, Step by Step",
    "section": "6 Putting it All Together",
    "text": "6 Putting it All Together\nWe‚Äôve seen how the encoder and decoder each work internally ‚Äî but how do they interact in a complete Transformer model?\nIn a sequence-to-sequence task, the encoder processes the entire input sequence (the source sentence) and produces a sequence of hidden states ‚Äî a rich, contextual representation of the source.\nThe decoder then uses these encoder outputs to generate the target sequence one token at a time.\nAt each decoding step, the decoder:\n\nAttends to its own previously generated tokens through masked self-attention, ensuring predictions depend only on known context and not future tokens.\nAttends to the encoder‚Äôs hidden states through cross-attention, incorporating relevant information from the source sequence.\n\nThis interaction allows the decoder to generate each token based on both the source sequence (via the encoder) and the target sequence generated so far, making the Transformer effective for tasks like translation, summarization, and question answering."
  },
  {
    "objectID": "transformers.html#training-tips",
    "href": "transformers.html#training-tips",
    "title": "Understanding Transformers, Step by Step",
    "section": "7 Training Tips",
    "text": "7 Training Tips\n\nLarge batches & data: Helps stabilize gradients and improve generalization by exposing the model to more diverse examples.\nLearning rate warm-up: Gradually increases the learning rate at the start of training to avoid unstable updates.\nLabel smoothing: Prevents the model from becoming overconfident by softening the target labels.\nGradient clipping: Limits the magnitude of gradients to avoid exploding gradients and stabilize training."
  },
  {
    "objectID": "transformers.html#variants-and-evolutions",
    "href": "transformers.html#variants-and-evolutions",
    "title": "Understanding Transformers, Step by Step",
    "section": "8 Variants and Evolutions",
    "text": "8 Variants and Evolutions\n\n\n\nModel\nType\nUse Case\n\n\n\n\nBERT\nEncoder-only\nClassification, QA\n\n\nGPT\nDecoder-only\nText generation\n\n\nT5\nEncoder‚ÄìDecoder\nTranslation, summarization\n\n\nBART\nEncoder‚ÄìDecoder\nSummarization, text generation\n\n\nViT\nEncoder-only\nImage classification"
  },
  {
    "objectID": "transformers.html#applications",
    "href": "transformers.html#applications",
    "title": "Understanding Transformers, Step by Step",
    "section": "9 Applications",
    "text": "9 Applications\n\nNLP: translation, summarization, question answering.\nVision: Vision Transformers (ViTs).\nMultimodal: CLIP (image-text), Flamingo.\nOther: Protein folding (AlphaFold)."
  },
  {
    "objectID": "transformers.html#strengths-and-limitations",
    "href": "transformers.html#strengths-and-limitations",
    "title": "Understanding Transformers, Step by Step",
    "section": "10 Strengths and Limitations",
    "text": "10 Strengths and Limitations\nPros:\n\nCaptures long dependencies\nParallelizable\nState-of-the-art results\n\nCons:\n\nRequires huge compute resources\nData-hungry\nLess interpretable\n\n\nTransformers have revolutionized sequence modeling by replacing recurrence with parallelizable self-attention. Their ability to capture long-range dependencies and scale to large data has established them as the foundation of modern NLP and beyond."
  },
  {
    "objectID": "transformers.html#references-further-reading",
    "href": "transformers.html#references-further-reading",
    "title": "Understanding Transformers, Step by Step",
    "section": "11 References & Further Reading",
    "text": "11 References & Further Reading\n[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is All You Need. https://arxiv.org/abs/1706.03762\n[2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://arxiv.org/abs/1810.04805\n[3] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ‚Ä¶ Amodei, D. (2020). Language Models are Few-Shot Learners (GPT-3). https://arxiv.org/abs/2005.14165\n[4] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ‚Ä¶ Houlsby, N. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Vision Transformer). https://arxiv.org/abs/2010.11929"
  },
  {
    "objectID": "ebm.html",
    "href": "ebm.html",
    "title": "Energy Based Models (EBM)",
    "section": "",
    "text": "Modern generative models impose strict architectural requirements:\n\nVAEs need encoder-decoder pairs\n\nGANs require adversarial networks\n\nNormalizing flows must use invertible transforms\n\nWhile each model family has strengths, they also carry trade-offs:\n\nVAEs often produce blurry samples due to reliance on Gaussian assumptions.\n\nGANs can generate high-quality images but are notoriously unstable and suffer from mode collapse.\n\nNormalizing Flows guarantee exact likelihoods but restrict architecture design due to invertibility constraints.\n\n\nEnergy-Based Models (EBMs) break this pattern by:\n\nLearning a scoring function (energy) where it assigns low energy to likely or observed data (\\(x \\sim p_{data}\\)), and high energy to unlikely or unobserved inputs.\nLinking to probability via: \\(p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z(\\theta)}, \\quad Z(\\theta) = \\int e^{-E_\\theta(x)}\\,dx\\)\nwhere low energy = high probability\nUsing generic neural nets‚Äîno specialized architectures needed\n\nThis approach provides unique advantages:\n‚úì Simpler setup - works with any neural net\n‚úì Explicit scoring - \\(p(x) \\propto e^{-E(x)}\\) (energy)\n‚úì Stable training - No adversarial balancing act like GANs\n\n\n\n\n\n\nCore Principle\n\n\n\nEBMs replace architectural constraints with a single principle:\n‚ÄúLearn to distinguish real data from noise through energy scoring.‚Äù\n\n\nTo understand how this works mathematically, we must first answer: What defines a valid probability distribution?\nThe next section derives EBMs‚Äô theoretical foundation‚Äîrevealing how they bypass traditional limitations while introducing new challenges (like partition function estimation)."
  },
  {
    "objectID": "ebm.html#introduction",
    "href": "ebm.html#introduction",
    "title": "Energy Based Models (EBM)",
    "section": "",
    "text": "Modern generative models impose strict architectural requirements:\n\nVAEs need encoder-decoder pairs\n\nGANs require adversarial networks\n\nNormalizing flows must use invertible transforms\n\nWhile each model family has strengths, they also carry trade-offs:\n\nVAEs often produce blurry samples due to reliance on Gaussian assumptions.\n\nGANs can generate high-quality images but are notoriously unstable and suffer from mode collapse.\n\nNormalizing Flows guarantee exact likelihoods but restrict architecture design due to invertibility constraints.\n\n\nEnergy-Based Models (EBMs) break this pattern by:\n\nLearning a scoring function (energy) where it assigns low energy to likely or observed data (\\(x \\sim p_{data}\\)), and high energy to unlikely or unobserved inputs.\nLinking to probability via: \\(p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z(\\theta)}, \\quad Z(\\theta) = \\int e^{-E_\\theta(x)}\\,dx\\)\nwhere low energy = high probability\nUsing generic neural nets‚Äîno specialized architectures needed\n\nThis approach provides unique advantages:\n‚úì Simpler setup - works with any neural net\n‚úì Explicit scoring - \\(p(x) \\propto e^{-E(x)}\\) (energy)\n‚úì Stable training - No adversarial balancing act like GANs\n\n\n\n\n\n\nCore Principle\n\n\n\nEBMs replace architectural constraints with a single principle:\n‚ÄúLearn to distinguish real data from noise through energy scoring.‚Äù\n\n\nTo understand how this works mathematically, we must first answer: What defines a valid probability distribution?\nThe next section derives EBMs‚Äô theoretical foundation‚Äîrevealing how they bypass traditional limitations while introducing new challenges (like partition function estimation)."
  },
  {
    "objectID": "ebm.html#math-review",
    "href": "ebm.html#math-review",
    "title": "Energy Based Models (EBM)",
    "section": "2 Math Review",
    "text": "2 Math Review\n\n2.1 Understanding the Probability Foundation Behind EBMs\nIn generative modeling, a valid probability distribution \\(p(x)\\) must satisfy:\n\nNon-negativity:\n\\[\np(x) \\geq 0\n\\]\nNormalization:\n\\[\n\\int p(x)\\, dx = 1\n\\]\n\nWhile it‚Äôs easy to define a function that satisfies \\(p(x) \\geq 0\\) (e.g., using exponentials), ensuring that it also sums to 1 ‚Äî i.e., \\(\\int p(x)\\, dx = 1\\) ‚Äî is much more difficult, especially for flexible functions like neural networks.\n\n\n\n2.2 Why do we introduce \\(g(x)\\)?\nInstead of modeling \\(p(x)\\) directly, we define a non-negative function \\(g(x) \\geq 0\\) and turn it into a probability distribution by normalizing:\n\\[\np_\\theta(x) = \\frac{g_\\theta(x)}{Z(\\theta)}, \\quad \\text{where} \\quad Z(\\theta) = \\int g_\\theta(x)\\, dx\n\\]\nThis trick simplifies the problem by separating the two requirements:\n\n\\(g_\\theta(x)\\) ensures non-negativity\n\n\\(Z(\\theta)\\) enforces normalization\n\nThe normalization constant \\(Z(\\theta)\\) is also known as the partition function.\nThis trick allows us to use any expressive function for \\(g_\\theta(x)\\) ‚Äî including deep neural networks.\n\n\n2.2.1 Intuition\nThink of \\(g_\\theta(x)\\) as a scoring function:\n\nHigher \\(g_\\theta(x)\\) means more likely\n\nDividing by \\(Z(\\theta)\\) rescales these scores to form a valid probability distribution\n\n\n\n\n\n2.3 Energy-Based Parameterization\nWe‚Äôve just seen how \\(g_\\theta(x)\\) can be any non-negative function. A common and powerful choice is to define it using an exponential transformation:\n\\[\ng_\\theta(x) = \\exp(f_\\theta(x))\n\\]\nThis ensures that \\(g_\\theta(x) \\geq 0\\) and allows us to interpret \\(f_\\theta(x)\\) as an unnormalized log-probability score.\nWe then normalize using the partition function ( Z() ) to obtain a valid probability distribution:\n\\[\np_\\theta(x) = \\frac{g_\\theta(x)}{Z(\\theta)} = \\frac{\\exp(f_\\theta(x))}{Z(\\theta)},\n\\quad \\text{where} \\quad Z(\\theta) = \\int \\exp(f_\\theta(x)) \\, dx\n\\]\nThis leads to the energy-based formulation, where \\(f_\\theta(x)\\) is interpreted as the negative energy. We can equivalently write:\n\\[\nE_\\theta(x) = -f_\\theta(x)\n\\quad \\Rightarrow \\quad\np_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z(\\theta)}\n\\]\nEnergy-Based Models (EBMs) follow this foundational idea: define a scoring function \\(f_\\theta(x)\\) that assigns high values to likely data points, then convert those scores into probabilities using exponentiation and normalization.\nThis perspective offers four key advantages:\n\nIt allows us to use flexible models (like deep neural networks) to assign unnormalized scores.\n\nIt separates concerns: one function ensures non-negativity (via exponentiation), and the partition function enforces normalization.\n\nIt lets us interpret \\(f_\\theta(x)\\) as an unnormalized log-probability, improving interpretability.\n\nIt connects naturally to well-known distributions (e.g., exponential family, Boltzmann distribution), making the formulation more general.\n\nThis formulation gives us the freedom to model complex distributions with any differentiable scoring function \\(f_\\theta(x)\\), while only requiring that we can compute or approximate its gradients.\n\n\n2.3.1 Summary\nWhether we express scores directly via \\(f_\\theta(x)\\) or indirectly through energy \\(E_\\theta(x)\\), the core idea remains the same:\n\nUse a flexible model to assign unnormalized scores\n\nNormalize those scores using the partition function \\(Z(\\theta)\\)\n\nThis gives EBMs the freedom to model complex data distributions using any differentiable function for \\(f_\\theta(x)\\) ‚Äî without requiring invertibility or exact likelihoods.\n\n\n\n2.3.2 Visualizing Energy Functions\nLet‚Äôs now build visual intuition for what these energy landscapes look like.\nEnergy-based models capture compatibility between variables using an energy function. In this example (adapted from Atcold, 2020), the energy function assigns lower values to pairs of variables \\((x, y)\\) that are more likely to co-occur ‚Äî and higher energy elsewhere.\n\n\n\nVisualizing Energy Field\n\n\nVisualizing the energy landscape \\(E(x_1, x_2)\\) ‚Äî lower energy corresponds to higher probability.\nThis shows how EBMs encode knowledge: valleys indicate high-likelihood (real) data, while peaks represent unlikely (noise) regions.\nThis kind of energy landscape is especially useful in practical tasks like image generation, where the model learns to assign low energy to realistic images and high energy to unrealistic ones. During inference, the model can generate new samples by exploring low-energy regions of this landscape."
  },
  {
    "objectID": "ebm.html#practical-applications",
    "href": "ebm.html#practical-applications",
    "title": "Energy Based Models (EBM)",
    "section": "3 Practical Applications",
    "text": "3 Practical Applications\nEnergy-Based Models (EBMs) offer unique benefits in scenarios where traditional models struggle. Below are two practical applications that demonstrate how EBMs shine in real-world settings.\n\n3.1 When You Don‚Äôt Need the Partition Function\nIn general, evaluating the full probability \\(p_\\theta(x)\\) requires computing the partition function \\(Z(\\theta)\\):\n\\[\np_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp(f_\\theta(x))\n\\]\n\n\n\n\n\n\nKey Insight\nIn some applications, we don‚Äôt need the exact probability ‚Äî we only need to compare scores. This allows EBMs to be useful even when the partition function is intractable.\n\n\n\nWhen comparing two samples \\(x\\) and \\(x'\\), we can compute the ratio of their probabilities:\n\\[\n\\frac{p_\\theta(x)}{p_\\theta(x')} = \\exp(f_\\theta(x) - f_\\theta(x'))\n\\]\nThis lets us determine which input is more likely ‚Äî without ever computing \\(Z(\\theta)\\) ‚Äî a powerful advantage of EBMs.\nPractical Applications:\n\nAnomaly detection: Identify inputs with unusually low likelihood.\nDenoising: Prefer cleaner versions of corrupted data by comparing likelihoods.\nObject recognition: Assign the most likely label to an input image.\nSequence labeling: Predict tags (e.g., part-of-speech) for input tokens.\nImage restoration: Recover clean images from noisy inputs.\n\n\n\n\n\n\n\nNote\n\n\n\nReal-World Example\nIn anomaly detection, EBMs have been used to identify outliers in high-dimensional sensor data without computing exact probabilities ‚Äî simply by scoring input configurations and flagging those with unusually high energy.\n\n\n\n\n3.2 Product of Experts (Compositional Generation)\nIn some cases, we want to combine multiple expert models that each score different attributes of an input \\(\\mathbf{x}\\) ‚Äî for example, age, gender, or hairstyle. This is where Energy-Based Models (EBMs) shine through Product of Experts (PoE).\nSuppose you have three trained expert models \\(f_{\\theta_1}(x)\\), \\(f_{\\theta_2}(x)\\), and \\(f_{\\theta_3}(x)\\). A tempting idea is to combine their scores additively and exponentiate:\n\\[\n\\exp\\left(f_{\\theta_1}(x) + f_{\\theta_2}(x) + f_{\\theta_3}(x)\\right)\n\\]\nTo make this a valid probability distribution, we normalize:\n\\[\np_{\\theta_1, \\theta_2, \\theta_3}(x) = \\frac{1}{Z(\\theta_1, \\theta_2, \\theta_3)} \\exp\\left(f_{\\theta_1}(x) + f_{\\theta_2}(x) + f_{\\theta_3}(x)\\right)\n\\]\nThis behaves like a logical AND: if any expert assigns low score, the overall likelihood drops. This contrasts with mixture models (like Mixture of Gaussians), which behave more like OR.\n\n\n\n\n\n\nNote\n\n\n\nReal-World Example\nIn the figure below (Du et al., 2020), EBMs were used to model attributes like ‚Äúyoung‚Äù, ‚Äúfemale‚Äù, ‚Äúsmiling‚Äù, and ‚Äúwavy hair‚Äù. By combining these via Product of Experts, the model generated faces that satisfied multiple specific attributes simultaneously.\n\n\n\n\n\nSource: Du et al., 2020. Compositional Visual Generation with Energy Based Models"
  },
  {
    "objectID": "ebm.html#benefits-and-limitations-of-ebms",
    "href": "ebm.html#benefits-and-limitations-of-ebms",
    "title": "Energy Based Models (EBM)",
    "section": "4 Benefits and Limitations of EBMs",
    "text": "4 Benefits and Limitations of EBMs\n\n4.1 Key Benefits\nVery flexible model architectures\nNo need for invertibility, autoregressive factorization, or adversarial design.\nStable training\nCompared to GANs, EBMs can be more robust and easier to optimize.\nHigh sample quality\nCapable of modeling complex, multi-modal data distributions.\nFlexible composition\nEnergies can be combined to support multi-task objectives or structured learning.\n\n\n\n4.2 Limitations\nDespite their strengths, EBMs come with notable challenges:\nSampling is expensive\nNo direct way to sample from \\(p_\\theta(x)\\); MCMC methods are slow and scale poorly.\nLikelihood is intractable\nPartition function \\(Z(\\theta)\\) is hard to compute, and we can‚Äôt directly evaluate log-likelihood.\nTraining is indirect\nLearning requires reducing energy of incorrect samples, not just increasing for correct ones.\nNo feature learning (by default)\nEBMs don‚Äôt learn latent features unless explicitly structured (e.g., RBMs)."
  },
  {
    "objectID": "ebm.html#training-energy-based-models",
    "href": "ebm.html#training-energy-based-models",
    "title": "Energy Based Models (EBM)",
    "section": "5 Training Energy-Based Models",
    "text": "5 Training Energy-Based Models\nEBMs are trained to assign higher scores (lower energy) to observed data points, and lower scores (higher energy) to unobserved ones. This corresponds to maximizing the likelihood of training data.\n\\[\np_\\theta(x_{\\text{train}}) = \\frac{\\exp(f_\\theta(x_{\\text{train}}))}{Z(\\theta)}\n\\]\nThis expression tells us that increasing the score for \\(x_{\\text{train}}\\) is not enough ‚Äî we must also decrease scores for other \\(x\\) to reduce \\(Z(\\theta)\\) and make the probability higher relatively.\n\n\n5.1 Log-Likelihood and Its Gradient\nTo train EBMs, we compute the gradient of the log-likelihood with respect to parameters \\(\\theta\\).\nWe start by writing the log-likelihood:\n\\[\n\\log p_\\theta(x_{\\text{train}}) = f_\\theta(x_{\\text{train}}) - \\log Z(\\theta)\n\\]\nTaking the gradient with respect to \\(\\theta\\):\n\\[\n\\nabla_\\theta \\log p_\\theta(x_{\\text{train}}) = \\nabla_\\theta f_\\theta(x_{\\text{train}}) - \\nabla_\\theta \\log Z(\\theta)\n\\]\nTo compute this, we need the gradient of the log partition function:\n\\[\nZ(\\theta) = \\int \\exp(f_\\theta(x))\\, dx\n\\]\nApplying the chain rule:\n\\[\n\\nabla_\\theta \\log Z(\\theta)\n= \\frac{1}{Z(\\theta)} \\int \\exp(f_\\theta(x)) \\nabla_\\theta f_\\theta(x)\\, dx\n= \\mathbb{E}_{x \\sim p_\\theta} \\left[ \\nabla_\\theta f_\\theta(x) \\right]\n\\]\nSubstitute this back:\n\\[\n\\nabla_\\theta \\log p_\\theta(x_{\\text{train}})\n= \\nabla_\\theta f_\\theta(x_{\\text{train}}) - \\mathbb{E}_{x \\sim p_\\theta} \\left[ \\nabla_\\theta f_\\theta(x) \\right]\n\\]\nThe first term is straightforward ‚Äî it‚Äôs the gradient of the model‚Äôs score on the training point.\nBut the second term, the expectation over model samples, is difficult. It requires drawing samples from \\(p_\\theta(x)\\), which in turn depends on the intractable normalization constant \\(Z(\\theta)\\).\nTo deal with this, we use sampling-based approximations like Contrastive Divergence.\n\n\n\n5.2 Contrastive Divergence\nSince computing the true gradient requires sampling from \\(p_\\theta(x)\\), which is intractable due to the partition function \\(Z(\\theta)\\), we use Contrastive Divergence as an efficient approximation.\nWe approximate the expectation:\n\\[\n\\mathbb{E}_{x \\sim p_\\theta} \\left[ \\nabla_\\theta f_\\theta(x) \\right] \\approx \\nabla_\\theta f_\\theta(x_{\\text{sample}})\n\\]\nThis gives:\n\\[\n\\nabla_\\theta \\log p_\\theta(x_{\\text{train}}) \\approx \\nabla_\\theta f_\\theta(x_{\\text{train}}) - \\nabla_\\theta f_\\theta(x_{\\text{sample}})\n= \\nabla_\\theta \\left( f_\\theta(x_{\\text{train}}) - f_\\theta(x_{\\text{sample}}) \\right)\n\\]\nContrastive Divergence Algorithm:\n\nSample \\(x_{\\text{sample}} \\sim p_\\theta\\) (typically via MCMC)\nTake a gradient step on:\n\n\\[\n\\nabla_\\theta \\left( f_\\theta(x_{\\text{train}}) - f_\\theta(x_{\\text{sample}}) \\right)\n\\]\nThis encourages the model to increase the score of the training sample and decrease the score of samples it currently believes are likely.\n\n\n\n\n\n\nEBM Training Recap\n- Want: High scores (low energy) for real data\n- Avoid: High scores for incorrect data\n- Can‚Äôt compute exact gradient due to \\(Z(\\theta)\\)\n- So: Approximate using Monte Carlo sample \\(\\sim p_\\theta(x)\\)\n\n\n\n\n\n5.2.1 Intuition Recap\n\nPull up: Increase the score (lower the energy) of the real training sample \\(\\nabla_\\theta f_\\theta(x_{\\text{train}})\\)\nPush down: Decrease the score of samples from the model \\(\\nabla_\\theta f_\\theta(x_{\\text{sample}})\\)\nThis sharpens the model‚Äôs belief in real data and corrects high-scoring regions where it is currently overconfident.\n\n During training, EBMs increase the score of correct samples and decrease the score of incorrect ones.\n\nSource: course material from CS236: Deep Generative Models"
  },
  {
    "objectID": "ebm.html#sampling-from-energy-based-models",
    "href": "ebm.html#sampling-from-energy-based-models",
    "title": "Energy Based Models (EBM)",
    "section": "6 Sampling from Energy-Based Models",
    "text": "6 Sampling from Energy-Based Models\nRecall that EBMs define a probability distribution as:\n\\[\np_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp(f_\\theta(x))\n\\]\nUnlike autoregressive or flow models, there is no direct way to sample from \\(p_\\theta(x)\\) because we cannot easily compute how likely each possible sample is. That‚Äôs because the normalization term \\(Z(\\theta)\\) is intractable.\nHowever, we can still compare two samples using a key insight:\n\n\n\n\n\n\nKey Insight\nWe can still compare two samples \\(x\\) and \\(x'\\) without needing \\(Z(\\theta)\\):\n\\[\n\\frac{p_\\theta(x)}{p_\\theta(x')} = \\exp(f_\\theta(x) - f_\\theta(x'))\n\\]\nThis property is useful for tasks like ranking, anomaly detection, and denoising.\n\n\n\nWhile we can‚Äôt sample from \\(p_\\theta(x)\\) directly due to the intractable \\(Z(\\theta)\\), we can still generate approximate samples using Markov Chain Monte Carlo (MCMC) methods.\n\n\n6.1 Metropolis-Hastings (MH) MCMC\nMetropolis-Hastings proposes samples and accepts or rejects them based on how much they improve the energy score.\nTo sample from \\(p_\\theta(x)\\), we use an iterative approach like MCMC:\n\nInitialize \\(x^0\\) randomly\n\nPropose a new sample: \\(x' = x^t + \\text{noise}\\)\n\nAccept or reject based on scores:\n\nIf \\(f_\\theta(x') &gt; f_\\theta(x^t)\\), set \\(x^{t+1} = x'\\)\nElse set \\(x^{t+1} = x'\\) with probability \\(\\exp(f_\\theta(x') - f_\\theta(x^t))\\)\nOtherwise, set \\(x^{t+1} = x^t\\)\n\nRepeat this process until the chain converges\n\nPros:\n- General-purpose\n- Guaranteed to converge to \\(p_\\theta(x)\\) under mild conditions\nCons:\n- Can take a very long time to convergence - Sensitive to proposal distribution\n- Computationally expensive in high dimensions\n\n\n6.2 Unadjusted Langevin MCMC (ULA)\nULA improves over random-walk MH by using gradient information to guide proposals.\nTo sample from \\(p_\\theta(x)\\), Unadjusted Langevin MCMC uses gradient information to guide proposals:\n\nInitialize \\(x^0 \\sim \\pi(x)\\)\n\nRepeat for \\(t = 0, 1, 2, \\dots, T - 1\\):\n\nSample \\(z^t \\sim \\mathcal{N}(0, I)\\)\n\nUpdate: \\(x^{t+1} = x^t + \\epsilon \\nabla_x \\log p_\\theta(x^t) + \\sqrt{2\\epsilon} z^t\\)\n\n\nFor EBMs, since \\(\\nabla_x \\log p_\\theta(x) = \\nabla_x f_\\theta(x)\\) the update becomes:\n\\[\nx^{t+1} = x^t + \\epsilon \\nabla_x f_\\theta(x^t) + \\sqrt{2\\epsilon} z^t\n\\]\nPros:\n- Uses gradient to improve proposal\n- Often faster mixing than random-walk methods\nCons:\n- Still requires many steps for good convergence\n- Sensitive to step size \\(\\epsilon\\)\n\n\n6.3 Adjusted Langevin MCMC (ALA)\nALA fixes the bias in ULA by using Metropolis-Hastings-style acceptance to ensure correct sampling.\nTo sample from \\(p_\\theta(x)\\), Adjusted Langevin MCMC applies a step after each Langevin update to ensure samples follow the correct stationary distribution.\nThis makes it a corrected version of ULA with proper stationary distribution.\n\n \\(x^0 \\sim \\pi(x)\\)\n\n for \\(t = 0, 1, 2, \\dots, T - 1\\):\n\nSample \\(z^t \\sim \\mathcal{N}(0, I)\\)\n\nPropose: \\(x' = x^t + \\epsilon \\nabla_x \\log p_\\theta(x^t) + \\sqrt{2\\epsilon} z^t\\)\nForward proposal: \\(q(x^{t+1} \\mid x^t) = \\mathcal{N}\\left(x^{t+1} \\mid x^t + \\epsilon \\nabla_x \\log p_\\theta(x^t),\\ 2\\epsilon I\\right)\\)\nReverse proposal: \\(q(x^t \\mid x^{t+1}) = \\mathcal{N}\\left(x^t \\mid x^{t+1} + \\epsilon \\nabla_x \\log p_\\theta(x^{t+1}),\\ 2\\epsilon I\\right)\\)\nAccept \\(x\\) with probability \\(\\alpha = \\min\\left(1, \\frac{p_\\theta(x') \\cdot q(x^t \\mid x')}{p_\\theta(x^t) \\cdot q(x' \\mid x^t)}\\right)\\)\n\nIf accepted: \\(x^{t+1} = x'\\)\n\nOtherwise: \\(x^{t+1} = x^t\\)\n\n\n\nFor EBMs, since\n\\[\n\\nabla_x \\log p_\\theta(x) = \\nabla_x f_\\theta(x)\n\\] and \\[  \nq(x' \\mid x^t) = \\mathcal{N}\\left(x' \\mid x^t + \\epsilon \\nabla_x f_\\theta(x^t), 2\\epsilon I\\right)\n\\]\nthe proposal becomes:\n\\[\nx' = x^t + \\epsilon \\nabla_x f_\\theta(x^t) + \\sqrt{2\\epsilon} z^t\n\\]\n\n\n\n6.4 Summary\nAll these methods aim to sample from \\(p_\\theta(x)\\), but differ in how they explore the space:\n\nMH is simple but can be inefficient.\n\nULA is gradient-guided and faster but biased.\n\nAdjusted Langevin corrects ULA using MH-style acceptance.\n\n\n\n\n\n\n\n\n\nMethod\nKey Advantage\nKey Limitation\n\n\n\n\nMH (Metropolis-Hastings)\nGeneral-purpose; unbiased under mild conditions\nCan be inefficient and slow to converge\n\n\nULA (Unadjusted Langevin)\nGradient-guided; faster mixing than MH\nBiased sampling; sensitive to step size\n\n\nALA (Adjusted Langevin)\nCombines ULA with MH for correctness\nMore complex and computationally expensive\n\n\n\nSampling is a core challenge in EBMs ‚Äî especially because we need to sample during every training step when using contrastive divergence."
  },
  {
    "objectID": "ebm.html#ebm-recap",
    "href": "ebm.html#ebm-recap",
    "title": "Energy Based Models (EBM)",
    "section": "7 üß© EBM Recap",
    "text": "7 üß© EBM Recap\nEnergy-Based Models (EBMs) offer a flexible and elegant alternative to traditional generative models by focusing on scoring functions rather than prescribing fixed model architectures. By defining an unnormalized score over inputs and converting it into a probability via a partition function, EBMs avoid the structural constraints of VAEs, GANs, or flows.\nKey ideas: - EBMs define probabilities as:\n\\[\n  p_\\theta(x) = \\frac{1}{Z(\\theta)} \\exp(f_\\theta(x))\n  \\] - The energy function \\(E_\\theta(x) = -f_\\theta(x)\\) creates a landscape where low energy = high probability.\n- EBMs excel when exact likelihood is unnecessary (e.g., denoising, anomaly detection, structured generation).\n- They support modular composition through techniques like Product of Experts (PoE).\n- Training involves score-based contrastive learning, and sampling is powered by MCMC methods.\nDespite challenges like intractable sampling and likelihood computation, EBMs continue to gain traction due to their conceptual clarity, compositional flexibility, and alignment with physics-inspired learning paradigms."
  },
  {
    "objectID": "ebm.html#references",
    "href": "ebm.html#references",
    "title": "Energy Based Models (EBM)",
    "section": "8 üìö References",
    "text": "8 üìö References\n[1] Atcold, Y. (2020). NYU Deep Learning Spring 2020 ‚Äì Week 07: Energy-Based Models. Retrieved from https://atcold.github.io/NYU-DLSP20/en/week07/07-1/\n[2] LeCun, Y., Hinton, G., & Bengio, Y. (2021). A Path Towards Autonomous Machine Intelligence. arXiv. Retrieved from https://arxiv.org/pdf/2101.03288\n[3] MIT. (2022). Energy-Based Models ‚Äì MIT Class Project. Retrieved from https://energy-based-model.github.io/Energy-based-Model-MIT/\n[4] University of Amsterdam. (2021). Deep Energy Models ‚Äì UvA DL Notebooks. Retrieved from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html\n[5] MIT. (2022). Compositional Generation and Inference with Energy-Based Models. Retrieved from https://energy-based-model.github.io/compositional-generation-inference/"
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html",
    "href": "gen-ai-use-cases/healthcare-use-cases.html",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "",
    "text": "Generative AI is accelerating transformation in healthcare by automating documentation, enhancing diagnostic imaging, improving drug discovery, and supporting clinical and administrative workflows. Below are key use cases currently being deployed by healthcare systems and life sciences firms."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#summary-of-use-cases",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#summary-of-use-cases",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "1 üìã Summary of Use Cases",
    "text": "1 üìã Summary of Use Cases\n\n\n\n\n\n\n\n\n\nUse Case\nExamples\nImpact\nUnderlying Models\n\n\n\n\nClinical Documentation\nNuance DAX, AWS HealthScribe\nNuance DAX reduces physician documentation time by up to 50%\nLLM, Speech-to-Text\n\n\nMedical Imaging & Diagnostics\nMIT CSAIL, DeepMind‚Äôs EyeDiff\nMIT CSAIL reduced false positives by 37%, biopsies by 27%, and surgeries by 30%\nCNN, Diffusion models\n\n\nDrug Discovery & Molecule Design\nInsilico, BenevolentAI\nCuts lead time from 2 years to 6 months (Insilico); improves success rate to 25% (BenevolentAI)\nVAEs, GNNs, RNNs, Transformers, RL\n\n\nPatient Communication\nMayo Copilot, Ada Health\nImproves patient experience (Mayo); 66% feel more confident, 40% less anxious (Ada); 81% clinician adoption (UCSD)\nLLM, RAG, RL\n\n\nAdmin Workflow Automation\nMedLM, Abridge, NexusMD\nReduces admin burden by ~28‚Äì36 hrs/week; reduces cognitive load on clinicians by 78%)\nLLM, IDP, Workflow Automation Agents"
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#detailed-use-cases",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#detailed-use-cases",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "2 Detailed Use Cases",
    "text": "2 Detailed Use Cases\n\n2.1 Clinical Documentation\n\nProblem: Clinicians spend significant time on documentation, contributing to burnout and reducing time available for patient care. Manual entry can also introduce delays and inconsistencies.\nSolution: Generative AI tools like ambient scribing (e.g., Nuance DAX) and real-time note generation (e.g., AWS HealthScribe) automatically generate clinical notes, transcripts, and summaries from conversations ‚Äî streamlining EHR workflows.\nImpact: Nuance DAX reduces physician documentation time by up to 50%, saving an average of 7 minutes per patient encounter and enabling 3‚Äì5 additional appointments per day[1].\n\n\n\n2.2 Medical Imaging and Diagnostics\n\nProblem: Medical imaging analysis for rare conditions or low-resolution scans can be difficult, time-consuming, and resource-intensive for radiologists.\n\nSolution: Generative AI improves diagnostic workflows in two key ways. First, it creates high-quality medical images that represent a wide range of conditions, patients, and scan types‚Äîusing techniques like diffusion models (e.g., DeepMind‚Äôs EyeDiff). Second, it enhances image interpretation and helps reduce false positives through advanced pattern recognition models such as convolutional neural networks (CNNs), as demonstrated by MIT CSAIL.\n\nImpact: MIT CSAIL‚Äôs AI reduced false positives by 37.3%, biopsy rates by 27.8%, and unnecessary breast surgeries by over 30%[2][3]; DeepMind‚Äôs diffusion-based EyeDiff model improved rare disease classification accuracy on retinal OCT scans by enhancing detection of minority classes and outperforming traditional oversampling techniques[4].\n\n\n\n2.3 Drug Discovery and Molecular Design\n\nProblem: Traditional drug discovery is time-consuming and expensive, often taking over a decade and billions of dollars to bring a new therapy to market.\n\nSolution: Generative AI platforms accelerate this process by using models like graph neural networks (GNNs), variational autoencoders (VAEs), recurrent neural networks (RNNs), and transformers to generate novel compounds, optimize drug-like properties, and prioritize candidates for development.\n\nImpact: Insilico Medicine advanced a novel drug candidate for idiopathic pulmonary fibrosis (IPF) to Phase II trials in under 30 months, including less than 18 months from target identification to candidate nomination[5]; BenevolentAI has used GenAI to generate molecules targeting resistant diseases, with multiple candidates advancing to preclinical and clinical stages[6].\n\n\n\n2.4 Patient Communication and Education\n\nProblem: Patients often find it difficult to understand complex medical language, follow care instructions, or manage their treatment after discharge‚Äîleading to confusion, low adherence, and worse health outcomes.\nSolution: GenAI-powered assistants use large language models (LLMs) with retrieval-augmented generation (RAG) to simplify medical information, answer patient questions in plain language, and deliver personalized health education at scale.\n\nImpact: Mayo Clinic reports improved patient experience and more efficient discharge workflows through AI-assisted communication[7]; Ada Health users showed significant engagement benefits‚Äî66% felt more certain about the care they needed, 40% reported reduced anxiety, and 80% felt better prepared for doctor consultations[8]; At UC San Diego Health, physicians using AI-generated replies wrote longer, more empathetic messages with less cognitive effort [9].\n\n\n\n2.5 Administrative Workflow Automation\n\nProblem: Healthcare organizations face rising administrative workloads‚Äîfrom prior authorizations to claims processing and medical coding‚Äîthat consume staff time, and drive up costs.\nSolution: Generative AI streamlines these tasks by combining LLMs and document intelligence platforms to automate medical coding, summarize prior authorization requests, extract insurance policy details, and generate routine correspondence.\nImpact: Google‚Äôs MedLM, used in tools like Augmedix, automates clinical documentation to deliver faster, more accurate notes and reduce administrative burden. Google Cloud estimates that staff spend 28‚Äì36 hours per week on administrative tasks‚Äîtime that GenAI can help reclaim[10]; Abridge reports up to 78% reduction in clinician cognitive load and 86% drop in after-hours documentation [11]."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#ethical-and-regulatory-considerations",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#ethical-and-regulatory-considerations",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "3 Ethical and Regulatory Considerations",
    "text": "3 Ethical and Regulatory Considerations\nThe use of Generative AI in healthcare offers immense potential, but it also raises complex ethical and regulatory challenges that must be addressed to ensure safety, trust, and fairness. Below are the key pillars guiding responsible deployment:\n\n3.1 Transparency and Explainability\nGenerative models like large language models (LLMs) often operate as black boxes, making it difficult to trace how decisions are made. In healthcare, this lack of explainability is a critical barrier to adoption. The U.S. FDA‚Äôs AI/ML Software as a Medical Device (SaMD) Action Plan calls for real-world performance monitoring, clear labeling, and transparency in algorithm logic, especially for high-risk clinical applications [12]. Similarly, the European Commission‚Äôs ethics guidelines designate transparency as a foundational requirement for AI systems in medicine [13].\nBest practice: Use explainable models or complement black-box systems with post-hoc interpretability tools (e.g., SHAP, LIME), and communicate limitations clearly to clinicians.\n\n\n3.2 Bias Mitigation and Fairness\nAI systems trained on healthcare data can inherit or amplify existing disparities. A landmark study published in Science revealed that a commercial algorithm systematically underestimated the needs of Black patients, leading to reduced care referrals [14]. The WHO urges developers to adopt inclusive data sourcing, perform subgroup analysis, and use fairness-aware learning techniques to ensure equitable performance [15].\nBest practice: Conduct bias audits, stratify model evaluation by race, gender, and age, and iteratively improve fairness during retraining.\n\n\n3.3 Privacy and Consent\nGenerative AI models may memorize and regurgitate sensitive patient information, posing serious risks under HIPAA and GDPR. Experts emphasize the need for differential privacy, federated learning, and strong access controls to protect PHI [16][17]. In addition, explicit informed consent should be obtained when AI is used in patient-facing workflows or influences clinical decisions [15].\nBest practice: Train models on de-identified data, disclose AI involvement to patients, and obtain consent when appropriate.\n\n\n3.4 Governance and Accountability\nAs AI becomes embedded in clinical workflows, clear accountability mechanisms are essential. Who is responsible if an AI-driven recommendation is wrong? The FDA‚Äôs Predetermined Change Control Plans and Total Product Lifecycle (TPLC) framework recommend ongoing oversight, version control, and human review [18].\nBest practice: Maintain audit trails, define roles and responsibilities, and require human-in-the-loop verification for all high-impact decisions.\n\n\n3.5 Model Drift and Continuous Validation\nAI systems may become less reliable over time as clinical practices or populations evolve‚Äîa phenomenon known as model drift. Without regular validation, GenAI tools risk producing inaccurate or even harmful outputs.\nBest practice: Set up monitoring pipelines to detect performance degradation and retrain models periodically using current data [18].\n\n\n3.6 Human-AI Collaboration\nGenerative AI should augment, not replace, clinical judgment. Over-automation can erode clinician trust and lead to overreliance on AI outputs. Leading health AI researchers emphasize the importance of human-in-the-loop design to ensure clinicians remain central to the decision-making process [17][19].\nBest practice: Embed AI into workflows in a way that supports clinician expertise, rather than bypassing it.\n\n\n3.7 Conclusion\nTaken together, these ethical pillars‚Äîtransparency, fairness, privacy, governance, drift monitoring, interoperability, and human-AI collaboration‚Äîform the foundation for trustworthy and effective use of GenAI in healthcare. Responsible design and deployment not only reduce risks but also build the confidence needed for widespread adoption."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#key-takeaways",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#key-takeaways",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "4 üßæ Key Takeaways",
    "text": "4 üßæ Key Takeaways\n\nGenerative AI is transitioning from pilots to production across healthcare, with mature applications in clinical documentation, medical imaging, and drug discovery.\n\nProviders, insurers, and biotech firms are leveraging GenAI to automate back-office tasks, accelerate research, and improve decision-making‚Äîdelivering measurable time and cost savings.\n\nPatient-facing tools for education, triage, and communication are enabling more personalized, accessible, and scalable care experiences.\nAs adoption increases, organizations must address critical concerns around transparency, bias, privacy, and accountability to ensure responsible and equitable deployment of GenAI in clinical settings."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#references",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#references",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "5 üìö References",
    "text": "5 üìö References\n[1] Microsoft Nuance. Move beyond scribes to automatically document care https://www.nuance.com/content/dam/nuance/en_us/collateral/healthcare/infographic/ig-move-beyond-scribes-to-automatically-document-care-en-us.pdf\n[2] MIT CSAIL News. Using AI to improve early breast cancer detection. https://www.csail.mit.edu/news/using-artificial-intelligence-improve-early-breast-cancer-detection\n[3] Liu et al.¬†(2021). Artificial intelligence system reduces false-positive findings in the interpretation of breast ultrasound exams. https://pmc.ncbi.nlm.nih.gov/articles/PMC8463596/pdf/41467_2021_Article_26023.pdf\n[4] De Fauw et al.¬†(2024). EyeDiff: text-to-image diffusion model improves rare eye disease diagnosis. https://arxiv.org/abs/2411.10004\n[5] Insilico Medicine. First Generative AI Drug Begins Phase II Trials with Patients. https://insilico.com/blog/first_phase2\n[6] The Pharmaceutical Journal. How AI is transforming drug discovery. https://pharmaceutical-journal.com/article/feature/how-ai-is-transforming-drug-discovery\n[7] Mayo Clinic. AI Improves Patient Experience. https://mayomagazine.mayoclinic.org/2025/04/ai-improves-patient-experience/\n[8] Ada Health. Improving patient pathways with AI. https://about.ada.com/improving-patient-pathways-with-ada-digital-triage/\n[9] UC San Diego Health. Study Reveals AI Enhances Physician-Patient Communication. https://health.ucsd.edu/news/press-releases/2024-04-15-study-reveals-ai-enhances-physician-patient-communication/\n[10] Google Cloud. MedLM: Foundation Models for Healthcare. https://cloud.google.com/blog/topics/healthcare-life-sciences/introducing-medlm-for-the-healthcare-industry\n[11] Christus Health reduces cognitive load on clinicians by 78% with Abridge. https://www.abridge.com/press-release/christus-health-announcement\n[12] U.S. FDA. Artificial Intelligence and Machine Learning Software as a Medical Device Action Plan. https://www.fda.gov/media/145022/download\n[13] European Commission. Ethics Guidelines for Trustworthy AI. https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai\n[14] Obermeyer Z, Powers B, Vogeli C, Mullainathan S. (2019). Dissecting racial bias in an algorithm used to manage the health of populations. Science. https://www.science.org/doi/10.1126/science.aax2342\n[15] World Health Organization. Ethics and Governance of Artificial Intelligence for Health. https://www.who.int/publications/i/item/9789240029200\n[16] Momani A. Implications of Artificial Intelligence on Health Data Privacy and Confidentiality. arXiv, January 2025. https://arxiv.org/abs/2501.01639\n[17] Stanford HAI. On the Opportunities and Risks of Foundation Models. https://crfm.stanford.edu/report.html\n[18] U.S. FDA. Artificial Intelligence and Machine Learning Discussion Paper. https://www.fda.gov/files/medical%20devices/published/US-FDA-Artificial-Intelligence-and-Machine-Learning-Discussion-Paper.pdf\n[19] JAMA. Artificial Intelligence in Health Care: Anticipating Challenges to Ethics, Privacy, and Bias. https://jamanetwork.com/journals/jama/fullarticle/2765681"
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html",
    "href": "gen-ai-use-cases/ai_first_bank.html",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "",
    "text": "Relationship managers at most banks spend only 25‚Äì30% of their time in actual client conversations. The rest goes to prospecting, meeting preparation, documentation, and follow-ups. The result is predictable: frustrated bankers, missed opportunities, and churn rates that often reach 15‚Äì35%.1\nAI is now changing not just what banks can do‚Äîbut who does the work. Today, most banks use Generative AI as a productivity aid: drafting emails, summarizing meetings, or answering basic questions. The bigger opportunity is Agentic AI‚Äîsystems designed to carry out work, not just assist with it.\nThe difference is operational. Generative AI might help draft an email. An agentic system can review incoming messages, decide which ones matter, follow up with clients, and update CRM records. It can flag exceptions for human review‚Äîall while operating within defined rules and oversight.\nThis enables an ‚Äúalways-on‚Äù operating model. While relationship managers focus on judgment, relationships, and complex negotiations, agents handle preparation, follow-ups, and routine coordination in the background. The banker‚Äôs role shifts away from execution and toward decision-making and client engagement.\nBanks that are already moving in this direction report 30% pipeline growth and twice the conversion rates of traditional approaches. These results don‚Äôt come from chatbots or meeting summaries. They come from redesigning workflows so that routine work runs automatically and people focus on where they add the most value.1\nThe end state is what industry leaders call ‚Äúinvisible intelligence‚Äù‚ÄîAI that operates seamlessly beneath the surface of banking operations rather than as a separate layer bolted on top. Agents become embedded infrastructure, not standalone tools. The technology recedes into the background while the outcomes‚Äîfaster decisions, better service, lower costs‚Äîmove to the foreground.\nTo see where banking is headed, it helps to look at how banking architecture evolved to support increasingly sophisticated automation‚Äîand why this moment is different from earlier technology transitions."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#introduction",
    "href": "gen-ai-use-cases/ai_first_bank.html#introduction",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "",
    "text": "Relationship managers at most banks spend only 25‚Äì30% of their time in actual client conversations. The rest goes to prospecting, meeting preparation, documentation, and follow-ups. The result is predictable: frustrated bankers, missed opportunities, and churn rates that often reach 15‚Äì35%.1\nAI is now changing not just what banks can do‚Äîbut who does the work. Today, most banks use Generative AI as a productivity aid: drafting emails, summarizing meetings, or answering basic questions. The bigger opportunity is Agentic AI‚Äîsystems designed to carry out work, not just assist with it.\nThe difference is operational. Generative AI might help draft an email. An agentic system can review incoming messages, decide which ones matter, follow up with clients, and update CRM records. It can flag exceptions for human review‚Äîall while operating within defined rules and oversight.\nThis enables an ‚Äúalways-on‚Äù operating model. While relationship managers focus on judgment, relationships, and complex negotiations, agents handle preparation, follow-ups, and routine coordination in the background. The banker‚Äôs role shifts away from execution and toward decision-making and client engagement.\nBanks that are already moving in this direction report 30% pipeline growth and twice the conversion rates of traditional approaches. These results don‚Äôt come from chatbots or meeting summaries. They come from redesigning workflows so that routine work runs automatically and people focus on where they add the most value.1\nThe end state is what industry leaders call ‚Äúinvisible intelligence‚Äù‚ÄîAI that operates seamlessly beneath the surface of banking operations rather than as a separate layer bolted on top. Agents become embedded infrastructure, not standalone tools. The technology recedes into the background while the outcomes‚Äîfaster decisions, better service, lower costs‚Äîmove to the foreground.\nTo see where banking is headed, it helps to look at how banking architecture evolved to support increasingly sophisticated automation‚Äîand why this moment is different from earlier technology transitions."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#technology-transitions-in-banking-architecture",
    "href": "gen-ai-use-cases/ai_first_bank.html#technology-transitions-in-banking-architecture",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "2 Technology Transitions in Banking Architecture",
    "text": "2 Technology Transitions in Banking Architecture\n1960s‚Äì2000s: From Mainframes to Services\nBanking evolved from centralized mainframe systems to distributed architectures. The internet brought digital banking. Banks separated monolithic systems into reusable components, creating the layered architecture that modern systems build on today.\n2010s: Cloud and API Ecosystems\nCloud platforms introduced elastic scaling and microservices, allowing banks to build and deploy products faster. This era enabled open banking, partner integrations, and modern data platforms. More importantly, it established API-based connectivity as the standard way systems communicate‚Äîa prerequisite for agent-driven architectures.\n2022‚ÄìPresent: AI as Operating Infrastructure\nThe emergence of large language models changed what banks can automate. Earlier AI handled narrow tasks like fraud scoring or chatbot responses. Today‚Äôs systems combine language models with enterprise data to support complex decision-making and multi-step workflows.\nThis architectural evolution sets the foundation for the next operating model in banking ‚Äî AI agents."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#the-three-levels-of-agents-in-banking",
    "href": "gen-ai-use-cases/ai_first_bank.html#the-three-levels-of-agents-in-banking",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "3 The Three Levels of Agents in Banking",
    "text": "3 The Three Levels of Agents in Banking\nAI agents differ by autonomy level. L1 agents provide intelligence, L2 agents execute processes, and L3 agents make decisions independently. Understanding these levels helps banks deploy agents appropriately‚Äîmatching capability to risk tolerance and regulatory requirements.\nL1 ‚Äì Insight Agents (Assistive Intelligence)\nL1 agents extract and synthesize information to support human decision-making. They assist with planning, discovery, and knowledge retrieval but do not take actions independently. Humans control all execution.\n\n\n\nFigure: L1 agents provide assistive intelligence through single request-response interactions. Users submit queries, the LLM generates insights, and humans decide what actions to take. No autonomous execution.\n\n\nIn banking, L1 agents appear most commonly in advisor tools and research summarization.\nMorgan Stanley‚Äôs AI @ Morgan Stanley Debrief tool automates meeting notes, analyzes client profiles and transaction histories, and generates tailored recommendations before advisor meetings. The tool saves approximately 30 minutes per meeting, freeing 10‚Äì15 hours per week for higher-value client work.8\nMorgan Stanley‚Äôs AskResearchGPT synthesizes insights from over 70,000 proprietary research reports. Similar tools have reduced document review time by up to 90% while improving accuracy by 25%.9\nL2 ‚Äì Process Orchestration Agents (Semi-Autonomous)\nL2 agents execute multi-step, rule-governed business processes. They decompose tasks, orchestrate workflows across systems, and automate structured operations while operating under human supervision and regulatory controls.\n\n\n\nFigure: L2 agents orchestrate multi-step workflows using specialized agents coordinated by a central orchestrator. Draft outputs are generated automatically, but human reviewers approve before execution‚Äîbalancing automation efficiency with accountability and control.\n\n\nConsider compliance documentation. Writing suspicious activity reports (SARs), regulatory memos, and audit summaries traditionally requires manual effort and creates consistency problems.\nL2 agents automate classification, extraction, summarization, and drafting while enforcing compliance standards. These systems have cut turnaround time by 80% and improved processing accuracy, allowing compliance teams to handle higher volumes with better quality.3\nThe key distinction from L1: these agents don‚Äôt just provide recommendations‚Äîthey generate final work products and execute regulated processes within defined guardrails.\nL3 ‚Äì Complex Decisioning Agents (Fully Autonomous)\nL3 agents continuously monitor their environment, make decisions, and trigger actions without constant human involvement. They coordinate L1 and L2 agents to operate as closed-loop decision systems.\n\n\n\nFigure: L3 agents operate autonomously in continuous observe-plan-act loops. They monitor environments, make decisions, execute actions, and adapt based on outcomes‚Äîall within governance boundaries. Unlike L2, humans are notified after decisions rather than approving before.\n\n\nRather than waiting for borrowers or advisors to identify refinancing opportunities, an L3 agent monitors market rates, borrower credit profiles, loan performance, and property valuations continuously.\nWhen conditions align, the agent triggers eligibility checks through L2 agents, generates personalized offers, initiates pre-approval workflows, and notifies customers and relationship managers. Work that traditionally requires weeks of coordination compresses into hours.\n\nImplementation Reality: Most banks operate primarily at L1 today and are scaling L2 capabilities. L1 agents prepare intelligence, L2 agents execute regulated processes, and L3 agents make and act on decisions independently. Governed L3 use cases represent the next competitive frontier and the next major governance challenge.\n\n\n\n3.1 Governing Autonomous Agents: Beyond Traditional Controls\nTraditional IT governance frameworks weren‚Äôt built for systems that make decisions autonomously and adapt continuously. When an agent can approve transactions, escalate compliance alerts, or reallocate capital based on real-time conditions, the governance challenge fundamentally changes.\nWhy Agent Governance Is Different\nThe core risk is behavioral drift. Unlike static rule-based systems, agents optimize their actions dynamically based on feedback loops and outcome data. Research by Jiang et al.¬†(2025) demonstrates that when agents and their supporting tools adapt simultaneously without coordination, systems can enter unstable ‚ÄúRed Queen‚Äù dynamics‚Äîcontinuous change without improvement. Without explicit controls regulating when and how adaptation occurs, these systems can develop unintended behaviors over time. Rather than stemming from a single incorrect decision, the risk accumulates as agents operate continuously across changing conditions.\nBanks must also guard against optimization misalignment, a more immediate and visible failure mode, where an agent optimizes against a narrow metric (closing AML alerts quickly) while undermining the actual business goal (accurate fraud detection). This happens when targets are too specific and governance is too hands-off.\nThese risks are not theoretical‚Äîthey stem from how autonomous agents interact with data, systems, and each other in production environments.\nAgent-Specific Risks\nAutonomous agents can exhibit unexpected behaviors that traditional software cannot. Corrupted training data can cause agents to make systematically wrong decisions. Poorly calibrated algorithms can trigger regulatory violations or discriminatory outcomes. In extreme cases, agents can enter feedback loops where one agent‚Äôs output continuously triggers another agent‚Äôs action in an uncontrolled cycle.\nThe deeper issue: as agents interact within large, interconnected systems, behaviors can emerge from the complexity itself. Agents might exploit programming loopholes, misapply learned patterns in new contexts, or demonstrate different objectives in production than they showed during testing.\nBuilding Control Mechanisms\nEffective agent governance requires three foundational controls:\nAgent Registry: Every autonomous agent must be catalogued with clear ownership, defined scope, approved data sources, and explicit risk exposure limits‚Äîwhether financial, reputational, or operational. This registry becomes the authoritative source for monitoring agent proliferation and ensuring accountability.\nDigital Wallets and Action Limits: Advanced controls, such as digital wallets with spending limits or transaction caps, can constrain agent actions to predefined boundaries. If an agent attempts to execute a trade, approve a loan, or transfer funds beyond its authorized limit, the action is automatically blocked and escalated to human review.\nContinuous Monitoring and Override: Real-time dashboards track agent performance, decision accuracy, and anomaly detection. Human operators must retain the ability to pause, override, or terminate any agent at any point‚Äîregardless of how autonomous the system appears.\nThe institutions that successfully scale agentic AI will be those that build governance into agent design from day one, not as an afterthought."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#experience-agents-vs.-domain-agents",
    "href": "gen-ai-use-cases/ai_first_bank.html#experience-agents-vs.-domain-agents",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "4 Experience Agents vs.¬†Domain Agents",
    "text": "4 Experience Agents vs.¬†Domain Agents\nBeyond the L1/L2/L3 autonomy levels, agents also differ by who‚Äîor what‚Äîthey interact with.\nExperience Agents interact directly with people: customers, relationship managers, and operations staff. They handle conversations, provide recommendations, and guide users through tasks. These agents remember context from earlier in a session and personalize responses based on user history and intent. Think of chatbots, advisor copilots, and customer service assistants.\nDomain Agents interact directly with systems: enterprise data, applications, and workflows. They execute backend processes such as validation, risk assessment, and transaction orchestration. These agents operate autonomously within defined business rules and execute core banking processes across credit, fraud, compliance, and operations. They work behind the scenes, often invisible to end users.\nExperience agents must handle ambiguity and communicate clearly. Domain agents must execute with precision and maintain audit trails. In practice, most banking workflows combine both types‚ÄîExperience agents at the front end capturing user needs, Domain agents in the back end executing the work."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#the-ai-first-bank-architecture",
    "href": "gen-ai-use-cases/ai_first_bank.html#the-ai-first-bank-architecture",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "5 The AI-First Bank Architecture",
    "text": "5 The AI-First Bank Architecture\nNow that we‚Äôve defined agent types, where do they actually operate? An AI-first bank architecture is organized into six layers, each serving a distinct purpose. Experience Agents live in the customer-facing layer, Domain Agents execute work in the action layer, and the intelligence layer coordinates both.\n\n\n\nFigure: The AI-First Bank Architecture is organized into six functional layers. Experience Agents handle customer interaction, Domain Agents execute business processes, and the Intelligence Layer coordinates workflows‚Äîall wrapped in a unified Control Plane for governance.\n\n\n1: Customer Layer: Web, mobile, contact centers, and advisor desktops make up the Customer Layer. Experience Agents operate here‚Äîthe chatbots, copilots, and assistants that customers and bankers interact with directly. This layer handles identity, personalization, and real-time responsiveness.\n2: Action Layer: The Action Layer executes core banking work: loan origination, payments, fraud checks, cash management, and servicing. Domain Agents orchestrate these processes, connecting multiple systems and enforcing business rules. What used to be rigid, sequential workflows become flexible, policy-governed sequences that adapt based on data and context.\n3: Intelligence Layer: The Intelligence Layer coordinates all AI activity across the bank. It manages model deployment, monitors performance, controls access to data, and enforces risk boundaries. This is where L1, L2, and L3 agents are orchestrated‚Äîensuring an insight agent pulling research doesn‚Äôt access the same systems as a decisioning agent approving loans. The layer also handles prompt management, output validation, and model risk controls that regulators expect. Think of this as a coordination layer that connects modern agents with existing core systems, allowing both to work together without requiring a complete technology replacement.\n4: Data Layer: Agents need access to enterprise data‚Äîcustomer profiles, transaction histories, risk scores, and market data. The Data Layer provides this through traditional data warehouses and lakes, plus newer retrieval systems optimized for AI search. Rather than training separate models on private data, banks feed structured context to foundation models through governed pipelines. This approach maintains control while enabling agents to work with proprietary information.\n5: Foundational Layer: Core banking systems‚Äîdeposits, lending, payments, general ledger, and customer master data‚Äîanchor the Foundational Layer. These remain the authoritative source for balances, transactions, and financial positions. Agents read from and write to these systems but never replace them.\n6: Control Plane and Cloud Native Fabric: Two elements cut across all layers. The Control Plane provides enterprise-wide security, governance, compliance, and auditability. The Cloud Native Fabric enables flexible deployment across on-premise and cloud environments. Together they ensure agents can innovate without compromising regulatory discipline or data protection.\nThis layered approach allows banks to experiment with AI in customer-facing applications while maintaining strict controls over core banking operations."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#use-cases-agents-in-practice",
    "href": "gen-ai-use-cases/ai_first_bank.html#use-cases-agents-in-practice",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "6 Use Cases: Agents in Practice",
    "text": "6 Use Cases: Agents in Practice\nThe architecture comes to life through specific banking workflows. These four use cases demonstrate how Experience and Domain agents collaborate across retail, corporate, risk, and compliance functions.\n\n6.1 Retail Banking (Agent Collaboration)\nScenario: A customer interacts with her bank while a relationship manager oversees the account. Multiple Experience Agents and one Domain Agent work together.\nCustomer experience: A Spend Analyzer Agent tracks spending patterns. When the customer plans a major expense, a Product Recommendation Agent identifies a likely cash shortfall, suggests lending products, and creates a personalized repayment schedule based on income and expenses.\nBanker experience: The relationship manager works through an AI-powered dashboard. An Application Tracker Agent summarizes the loan application by risk tier and approval probability. A Credit Decisioning Agent handles due diligence and issues automated approval within defined risk thresholds, routing exceptions to human credit officers.\nOutcome: AI integrates into the customer‚Äôs financial life while providing bankers with actionable, policy-compliant intelligence.\nBeyond product recommendations, agents reshape daily banker workflows. AI coaches analyze call transcripts and provide real-time guidance, improving customer satisfaction scores by 7 percentage points and helping new bankers ramp up 20% faster.1 Meeting preparation that once consumed half a day now takes minutes, with agents synthesizing account plans from CRM data, emails, and reports automatically. One banker noted, ‚ÄúI used to spend half my week chasing leads that went nowhere. Now I only talk to clients who already want to meet.‚Äù\nThese workflow improvements demonstrate how agents compound value‚Äînot just through individual tasks, but by rebalancing how relationship managers allocate time between administrative work and client relationships.\n\n\n6.2 Corporate Banking (Agent-to-Agent Automation)\nScenario: A multinational corporation expands into Europe, requiring coordination between the corporate client and bank across multiple geographies.\nForecasting: The CFO uses a Cash Flow Forecasting Agent that consolidates data from Balance Inquiry Agents and Document Extraction Agents reading invoice data. Together they project a working capital shortfall. The client‚Äôs agent then communicates directly with the bank‚Äôs domain agents to request a ‚Ç¨50 million credit facility.\nExecution: The bank‚Äôs Risk Agent generates a credit write-up by analyzing financial statements, market conditions, and reports. Once approved, the Supply Chain Finance Smart Program Agent reads supplier contracts and automatically onboards European suppliers into the financing program.\nAll approvals and onboarding operate within regulatory, legal, and credit policy constraints, with automatic escalation to regional credit officers when cross-border risk thresholds are exceeded.\nOutcome: Work that traditionally requires months of email exchanges and document reviews executes rapidly through interoperable domain agents.\nDeal structuring uses pricing agents that analyze discount patterns across relationship managers, customer attributes, and willingness to pay. Rather than relying on banker instinct‚Äî‚ÄúSometimes I discount because I‚Äôm unsure what‚Äôs fair‚Äù‚Äîthese agents recommend pricing in real time with clear rationales that risk teams trust. Early results show 10% margin gains and faster quote cycles, with one bank cutting deal turnaround from five days to two.1\n\n\n6.3 Risk Management (Stress Testing and CCAR)\nScenario: Regulatory stress testing and scenario analysis for CCAR compliance.\nThe problem: Preparing for regulatory stress tests traditionally requires months of data gathering, model execution, and scenario construction to answer questions like ‚ÄúWhat happens if interest rates rise by 2 percent?‚Äù\nThe solution: An AI Assistant retrieves Federal Reserve guidance and maps regulatory stress scenarios into calibrated baseline and adverse frameworks. A Stress Testing Domain Agent applies these scenarios to the bank‚Äôs portfolio using enterprise calculation engines.\nOutcome: The Chief Risk Officer reviews capital adequacy, liquidity impact, and profitability projections in one day rather than multiple months.\n\n\n6.4 Financial Crime (AI Investigator)\nScenario: Anti-money laundering alerts with high false-positive rates that traditionally require large investigation teams.\nThe process: The AI Investigator Agent reviews transaction alerts in real time, assigns a quantitative risk score (for example, 98.85), and generates a transparent narrative explaining suspicious behavior such as rapid fund movement or transaction structuring.\nHuman oversight: Investigators review the AI-generated narrative and visual fund flow diagrams rather than spending hours gathering evidence manually. They validate insights and mark cases as credible or non-credible, feeding this feedback into continuous model improvement.\nOutcome: Work that previously required days of manual evidence gathering condenses into minutes, producing regulator-ready, auditable cases.\n\n\n6.5 Commercial Banking Operations (Email Triage)\nScenario: A bank‚Äôs Commercial Banking division fields 60,000-70,000 client emails monthly. Requests range from urgent credit line draws to complex legal inquiries‚Äîeach requiring specialized expertise.\nThe Challenge: A team manually reviewed every email: reading the request, determining urgency, identifying the right internal expert, routing it correctly, and logging it for tracking. For complex requests‚Äîa CFO asking about cross-border regulatory requirements or a treasurer requesting covenant waivers‚Äîthis process took hours. Clients waited. Opportunities were missed.\nScotiabank deployed an agentic AI system to handle this complexity autonomously. The agent reads incoming emails, interprets client intent across dozens of request types, routes messages to the appropriate specialists, and creates tracking cases‚Äîall without human intervention. Edge cases that fall outside normal patterns are flagged for human review.5\nOutcome: The system now processes 90% of emails autonomously. Response times dropped from hours to minutes. The bank redeployed 70% of the routing team to client-facing roles. Client satisfaction improved, and the system operates continuously‚Äînot just during business hours.5\nBanks historically considered unstructured client communications ‚Äútoo variable to automate.‚Äù Agentic AI proved otherwise. The agent handles irregularity that would break traditional automation‚Äîinterpreting ambiguous requests, understanding context, and making routing decisions that previously required human judgment.\n\n\n6.6 Capital Markets (Analyst Research Automation)\nScenario: Investment analysts at RBC Capital Markets cover hundreds of public companies. When companies release SEC filings or earnings reports, analysts must review the information and update investment recommendations‚Äîoften within hours of disclosure.\nThe solution: RBC deployed Aiden, a multi-agent research platform. When a covered company files with the SEC, three specialized agents activate: an SEC Filing Agent extracts financial data and risk factors, an Earnings Agent compares results to forecasts, and a News Agent monitors real-time media coverage. An orchestration agent synthesizes these inputs and generates preliminary research notes for analyst review.\nOutcome: Analysis that previously required 2-3 hours of manual document review now completes in minutes. Analysts receive structured summaries highlighting what changed and why it matters, allowing them to focus on generating investment insights rather than gathering data.4"
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#three-pathways-to-agentic-banking",
    "href": "gen-ai-use-cases/ai_first_bank.html#three-pathways-to-agentic-banking",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "7 Three Pathways to Agentic Banking",
    "text": "7 Three Pathways to Agentic Banking\nMost regional banks still run core systems on decades-old mainframes. The good news: banks don‚Äôt need full modernization before deploying agents. Insight-focused agents layer onto legacy systems through APIs. However, agents that execute processes or make autonomous decisions require modern data platforms and real-time connectivity.\nMoving from vision to execution requires a practical roadmap. Banks can introduce agentic AI through three distinct approaches, each suited to different organizational contexts, risk appetites, and strategic goals.\n\n\n\nFigure: The strategic progression of agentic banking. While Smart Overlays deliver immediate efficiency gains with low technical debt, true Reinvention requires deeper integration and fundamental process redesign.\n\n\nPathway 1: Smart Overlay\nThe fastest way to see results: wrap AI agents around existing processes without requiring a complete technology overhaul. Rather than replacing legacy infrastructure, banks add an intelligent layer on top of current workflows.\nConsider treasury operations. Most banks already use robotic process automation (RPA) for routine cash sweeps and balance reconciliation. A smart overlay elevates this foundation with an AI agent that makes real-time decisions on liquidity optimization, pricing, and hedging based on market conditions.\nThis works best when banks have clearly documented standard operating procedures. The AI agent follows these documented steps, ensuring consistency and compliance while adding intelligence. The benefit is speed: banks unlock near-term productivity gains without large-scale system replacements.\nPathway 2: Agentic by Design\nSome processes can‚Äôt be optimized by overlaying intelligence‚Äîthey need to be rebuilt from the ground up. The agentic by design approach creates new, purpose-built autonomous applications specifically tailored for each banking function.\nThink of it like microservices. Instead of one monolithic system, banks develop smaller, specialized agentic services that handle specific functions yet integrate smoothly into the broader infrastructure. For example, rather than retrofitting a 30-year-old loan origination system, a bank might build a new autonomous credit decisioning service that coordinates multiple agents: one for document verification, another for creditworthiness analysis, and a third for regulatory compliance checks.\nThis approach requires more upfront investment but delivers greater long-term flexibility. As regulations change or customer expectations evolve, individual agent services can be updated or replaced without disrupting the entire technology stack.\nPathway 3: Process Redesign\nThe boldest approach‚Äîand the hardest‚Äîinvolves fundamentally reengineering workflows to embed agents at their core. This goes beyond automation to reimagine how work actually gets done.\nTraditional mortgage servicing assumes a human reviews each refinancing opportunity. Process redesign flips this model: autonomous agents continuously monitor market conditions, borrower profiles, and property valuations, proactively triggering refinance workflows when optimal conditions emerge. The process isn‚Äôt just faster‚Äîit‚Äôs fundamentally different.\nThis pathway works best for strategically important processes where current automation is low but business impact is high. It demands close collaboration between business leaders, process engineers, and AI developers to map new workflows that leverage agent capabilities rather than simply replicating existing manual steps.\nChoosing Your Path\nMost banks will use all three approaches simultaneously. Smart overlays deliver quick wins in well-documented processes. Agentic by design creates competitive differentiation in customer-facing services. Process redesign transforms core operations with high strategic value.\nThe key is matching the approach to the specific context: organizational readiness, regulatory constraints, technical debt, and strategic importance of the process being transformed."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#business-impact-the-economics-of-agent-driven-banking",
    "href": "gen-ai-use-cases/ai_first_bank.html#business-impact-the-economics-of-agent-driven-banking",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "8 Business Impact: The Economics of Agent-Driven Banking",
    "text": "8 Business Impact: The Economics of Agent-Driven Banking\nThe shift to agent-driven architectures delivers measurable returns. McKinsey research shows banks implementing agentic AI across frontline operations achieve:1\n\n3-15% revenue increase per relationship manager through better lead conversion and coverage\n20-40% reduction in cost to serve by automating administrative workflows\n30% growth in pipeline from AI-driven prospecting and lead qualification\n10-12 hours per week returned to each banker for client-facing activities\n\nThese gains stem from rebalancing how bankers spend time. At many commercial banks, relationship managers spend just 25-30% of their time in client dialogue. Agents handling prospecting, meeting preparation, and documentation push that figure toward 70%, changing the banker‚Äôs role from administrator to advisor.\nThe impact compounds across use cases: prospecting agents deliver twice the conversion rate of traditional lead sources, lead nurturing agents increase qualified leads 2-3x, and deal scoring agents improves margins by 10% while cutting approval cycles from five days to two.\nHowever, early evidence shows banks often chase low-value use cases first. Meeting summarization and basic chatbots are easier to implement but deliver modest returns. The highest-value applications‚Äîprospecting agents, lead nurturing, and deal optimization‚Äîrequire deeper integration with core systems and workflow redesign, but they deliver the returns that matter: revenue growth, not just efficiency.\nAdoption is accelerating. Customer service applications of generative AI in financial services doubled over the past year, rising from 25% to 60% of institutions. More than 90% of banks deploying AI report positive revenue impact, with customer experience metrics improving by 26% on average.4\nThese improvements compound. Better customer experience drives retention, retention increases lifetime value, and lifetime value justifies further AI investment. Banks that deployed AI early are now scaling L2 and L3 agents across departments. Late adopters face a widening competitive gap."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#scaling-agentic-ai-six-critical-principles",
    "href": "gen-ai-use-cases/ai_first_bank.html#scaling-agentic-ai-six-critical-principles",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "9 Scaling Agentic AI: Six Critical Principles",
    "text": "9 Scaling Agentic AI: Six Critical Principles\nMoving from pilots to enterprise-scale requires deliberate choices about how agents integrate into banking operations:\n1. Define the value creation thesis. Leading banks are intentional about which use cases to lead versus follow, and about expected impact through frontline productivity gains, customer experience improvements, and competitive differentiation.\n2. Reimagine workflows end-to-end. Leading banks don‚Äôt just layer AI onto existing processes‚Äîthey redesign entire workflows for human-agent collaboration. Because agents work across departments, this often means rethinking organizational boundaries.\n3. Build reusable technology foundations. Three capabilities distinguish successful implementations: an agentic mesh (composable orchestration layer), a strong ontology capturing workflows and decisions, and robust trust and security frameworks including end-to-end encryption.\n4. Establish governance guardrails. Define agent autonomy levels, set limits on independent decisions (such as credit increases), implement monitoring protocols, and create audit mechanisms before scaling.\n5. Reskill the workforce. As agents handle more administrative work, new roles emerge‚Äîbusiness engineers who design agent workflows and agent orchestrators who manage human-AI collaboration. Meanwhile, relationship managers evolve to focus on delivering insights and coordinating expertise across the bank.\n6. Organize for Federated Development. At scale, centralized AI teams become bottlenecks, while fully decentralized development leads to fragmentation. Successful banks define shared standards, interfaces, and protocols that allow business units to build domain-specific agents independently while remaining interoperable.\n\nBlackRock‚Äôs Aladdin Copilot illustrates how a federated model works in practice. A central AI platform team defines shared communication standards, integration patterns, and governance controls. Individual business units then develop specialized agents‚Äîsuch as portfolio analysis, risk management, or compliance‚Äîon top of this common foundation.4 This federated model becomes essential at scale. When you have five agents, coordination is manageable. When you have fifty, you need standardized protocols."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#ai-governance-the-banking-imperative",
    "href": "gen-ai-use-cases/ai_first_bank.html#ai-governance-the-banking-imperative",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "10 AI Governance: The Banking Imperative",
    "text": "10 AI Governance: The Banking Imperative\nBanking operates under governance requirements that differ from other industries. While technology companies can iterate and learn from failures, banks must demonstrate explainability, fairness, and accountability before deployment‚Äînot after. The question isn‚Äôt whether to govern AI, but how to govern it in ways that enable rather than block innovation.\n\n10.1 Three Regulatory Pillars\nSR 11-7 (Model Risk Management) - The Federal Reserve‚Äôs 2011 guidance applies to AI and machine learning. Banks must demonstrate models are fit for purpose through independent validation, maintain documentation others can understand, and ensure board-level oversight. The challenge: AI models that learn and evolve resist validation approaches designed for static statistical models.\nECOA and Regulation B (Fair Lending) - The CFPB established a clear requirement: if you cannot explain why you denied credit, you cannot use the model. ‚ÄúThere is no special exemption for artificial intelligence.‚Äù When agents approve or deny loan applications, banks must provide specific reasons. ‚ÄúBehavioral spending patterns‚Äù fails the standard. ‚ÄúFrequent overdrafts in the past 90 days combined with three late utility payments‚Äù meets it.\nSR 13-19 (Third-Party Risk Management) - Most banks purchase vendor AI solutions. The rule: outsourcing execution does not transfer accountability. Banks remain responsible for vendor model outcomes despite limited access to proprietary algorithms.\n\n\n10.2 The Explainability Requirement\nTraditional credit models use techniques where you can trace how each input affects output. Modern AI‚Äîparticularly deep learning‚Äîidentifies patterns that predict outcomes more accurately but makes explaining the logic difficult.\nBanks address this through explainability methods that approximate which inputs influenced specific decisions. These enable adverse action notices satisfying regulators, though imperfect approximations risk providing inaccurate reasons to customers.\nThe practical outcome: banks often limit complex AI to L1 applications (insights) where explainability matters less, and use simpler models for L2 and L3 applications (execution and decisions) where regulatory scrutiny is higher.\n\n\n10.3 Continuous Monitoring Requirements\nAI models drift. Customer behavior changes, economic conditions shift, and patterns models learned become less predictive. Banks must implement monitoring that tracks model performance in production, tests for bias across protected classes regularly, validates models remain within design parameters, and maintains human oversight for high-stakes decisions.\nFair lending testing extends monitoring further. Banks must test whether models produce disparate impact across protected classes‚Äîeven when models don‚Äôt use protected class as an input. If a model approves loans for different groups at significantly different rates, banks must adjust the model or demonstrate legitimate business justification.\n\n\n10.4 Governance Structure\nEffective governance requires organization, not just policy. Banks deploying AI at scale establish:\nModel Risk Committees - Board-level oversight of AI deployment, performance monitoring, and risk exposure. These committees review new models before deployment, monitor production model performance, and ensure compliance with regulatory guidance. Senior management reports on individual model risk and aggregate exposure across all AI systems.\nIndependent Validation Teams - Critical analysis by parties separate from development. For AI, this includes bias testing across customer segments, validating explainability methods produce accurate reasons for decisions, and confirming models perform as designed under different conditions. Validation rigor scales with model risk‚Äîhigh-stakes credit decisions require more comprehensive validation than marketing recommendations.\nVendor Management Processes - Due diligence on third-party AI tools. Banks require vendors to provide methodology documentation, conduct independent outcome analysis comparing vendor predictions to actual results, and implement controls such as human review of edge cases or secondary validation models.\nAI Ethics Review Boards - Independent review of AI models before production deployment. Every model undergoes scrutiny for data usage, algorithmic transparency, and bias. Scotiabank implements tiered approval based on risk: low-risk models clear at the department level, high-risk models require C-suite sign-off. This ensures ethical considerations aren‚Äôt an afterthought‚Äîthey‚Äôre embedded in the approval process from day one.\n\n\n10.5 Governance as Enabler\nBanks scaling AI must build governance alongside technical capabilities. This means investing in explainability tools and talent who can interpret complex models, establishing monitoring infrastructure that tracks model performance and drift, training validation teams on AI-specific techniques including bias testing and fairness evaluation, and creating escalation procedures when models behave unexpectedly.\nThe payoff: strong governance enables confident deployment of L3 autonomous agents. When controls, monitoring, and oversight ensure agents operate within defined boundaries, banks can move faster because risk is managed, not avoided.\nIn a regulated industry, treating governance as a capability rather than an obstacle becomes a competitive advantage that enables faster, safer AI scaling.\n\n\n\n\n\n\n\n\n\n\n\nFurther Reading: Agentic AI Security\n\n\n\n\nThis governance framework addresses regulatory compliance and model risk management. However, agentic AI also introduces operational security risks‚Äîprompt injection attacks, tool hijacking, and adversarial manipulation‚Äîthat traditional IT controls weren‚Äôt designed to handle. For detailed analysis of these threats and layered defense strategies, see Building Safe and Secure Agentic AI."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#the-ai-first-bank-a-vision-for-2030",
    "href": "gen-ai-use-cases/ai_first_bank.html#the-ai-first-bank-a-vision-for-2030",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "11 The AI-First Bank: A Vision for 2030",
    "text": "11 The AI-First Bank: A Vision for 2030\nBanks that successfully scale agents won‚Äôt just improve efficiency‚Äîthey‚Äôll redefine banking itself.\nCiti calls this the ‚ÄúDo It For Me‚Äù (DIFM) economy. Customers won‚Äôt use tools to make decisions‚ÄîAI agents will make and execute decisions on their behalf. A customer won‚Äôt search for mortgage rates. Their agent monitors the market continuously, identifies optimal refinancing opportunities, and initiates the process without being asked. The bank‚Äôs role shifts from providing tools to providing trusted agents that act in the customer‚Äôs best interest.6\nSix characteristics will distinguish these institutions:\n1. Always-On Financial Partner\nInstead of periodic check-ins with a branch manager, customers have an AI agent continuously monitoring their financial life. When mortgage rates drop below the customer‚Äôs current rate, the agent initiates refinancing. When spending patterns suggest cash flow issues, the agent offers solutions before the customer asks. This shifts banking from reactive to proactive.\n2. Flexible Products That Adapt\nRather than separate checking, savings, credit card, and loan products, customers access adaptive financial capacity that adjusts to their needs. Purchase groceries and the system functions as a debit card. Buy a car and it converts to installment financing with optimized terms. Build savings and it shifts funds to higher-yield options‚Äîall within a single account managed by agents.\n3. Banking Without Banks\nFinancial services embed into platforms where customers already spend time‚Äîshopping apps, ride-sharing services, social media. Customers access credit, payments, and savings without opening separate banking apps or visiting websites. The bank becomes infrastructure, not a destination.\n4. Automated Operations at Scale\nAgents handle service inquiries, compliance checks, and exception processing from start to finish within defined guardrails. A customer service team of 500 shrinks to 50 specialists handling complex cases while agents manage routine work at near-zero cost. Staff focus shifts from processing to strategy and relationship building.\n5. Dynamic Balance Sheet Management\nTraditional banks allocate capital quarterly through manual processes. AI-first banks shift liquidity and risk exposure continuously across customers and portfolios, maximizing returns while maintaining regulatory compliance. A customer‚Äôs credit limit adjusts in real time based on behavior, market conditions, and the bank‚Äôs capital position.\n6. Small Teams, Massive Scale\nThe AI-first bank may operate with far fewer people than today while serving millions of customers through highly automated, policy-constrained workflows.\nThese aren‚Äôt incremental improvements‚Äîthey represent a different operating model for banking. Banks pursuing this transformation systematically will create competitive advantages that traditional institutions cannot match."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#conclusion",
    "href": "gen-ai-use-cases/ai_first_bank.html#conclusion",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "12 Conclusion",
    "text": "12 Conclusion\nBanks are moving from AI that creates content to AI that executes work. This shift changes what‚Äôs possible in financial services‚Äîfrom reactive customer service to proactive financial guidance, from manual compliance workflows to automated regulatory reporting, from quarterly stress tests to continuous risk monitoring.\nThe architecture outlined here‚Äîlayered, governed, and designed for agent collaboration‚Äîprovides a framework for this transition. Experience agents improve customer and employee interactions. Domain agents automate complex backend processes. The intelligence layer ensures both operate within risk boundaries.\nEarly movers are already seeing results: relationship managers save 10-15 hours per week, compliance automation cuts turnaround time by 80%, and stress testing that took months now completes in days. Banks implementing agentic AI across frontline operations report 3-15% revenue increases per relationship manager and 30% pipeline growth‚Äîreturns too material to ignore.\nThe competitive advantage goes to banks that deploy agents systematically rather than experimentally‚Äîtreating AI as core infrastructure, not a feature. The question isn‚Äôt whether to build agent-driven architectures, but how quickly to scale them while maintaining the governance and controls that banking requires."
  },
  {
    "objectID": "gen-ai-use-cases/ai_first_bank.html#references",
    "href": "gen-ai-use-cases/ai_first_bank.html#references",
    "title": "Building the AI-First Bank: A Strategic Guide",
    "section": "13 References",
    "text": "13 References\n[1] Mckinsey. (2025). Agentic AI is here. Is your bank‚Äôs frontline team ready?\n[2] BCG. (2025). From Branches to Bots: Will AI Agents Transform Retail Banking?\n[3] Deloitte / WSJ CFO Journal. (2025). Agentic AI in Banks: Supercharging Intelligent Automation\n[4] NVIDIA. (2025). AI On: How Financial Services Companies Use Agentic AI to Enhance Productivity, Efficiency and Security\n[5] Scotiabank. (2025). How Agentic AI is the ‚Äònext big wave‚Äô in artificial intelligence\n[6] Citi. (2025). Agentic AI - Finance & the ‚ÄòDo It For Me‚Äô Economy\n[7] Bank of America. (2025). The new wave: Agentic AI - Agentic AI represents a generation of increasingly powerful foundation models that may spark a corporate efficiency revolution.\n[8] Morgan Stanley. (2024). AI @ Morgan Stanley Debrief Launch.\n[9] Morgan Stanley. (2024). AskResearchGPT helps advisors access 70,000+ research reports.\n[10] Jiang et al.¬†(2025). Adaptation of Agentic AI."
  }
]