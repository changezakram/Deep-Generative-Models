[
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html",
    "href": "gen-ai-use-cases/banking-use-cases.html",
    "title": "Generative AI Use Cases in Banking",
    "section": "",
    "text": "Generative AI is redefining how banks operate — from automating customer service to streamlining underwriting. Below are practical examples showing how leading financial institutions are applying GenAI for real business value."
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#summary-of-use-cases",
    "href": "gen-ai-use-cases/banking-use-cases.html#summary-of-use-cases",
    "title": "Generative AI Use Cases in Banking",
    "section": "1 📋 Summary of Use Cases",
    "text": "1 📋 Summary of Use Cases\n\n\n\n\n\n\n\n\nUse Case Area\nExamples\nBenefits\n\n\n\n\nCustomer Engagement\nErica, Cora+, Fargo, Eliza\n24/7 support, lower call volume\n\n\nOperational Efficiency\nLLM Suite, AskResearchGPT, PwC\nProcess streamlining, reduced manual work\n\n\nRisk Management\nMastercard DI, Wells Fargo Fraud\nEarly fraud detection, loss prevention\n\n\nCredit Underwriting\nCreditLens, COiN Platform\nQuicker approvals, lower compliance risk\n\n\nPersonalized Marketing\nJPMorgan Campaigns\nBetter targeting, higher engagement\n\n\nGenerative BI\nCopilot, ThoughtSpot, Custom Assistants\nSelf-service insights, exec summaries"
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#customer-engagement",
    "href": "gen-ai-use-cases/banking-use-cases.html#customer-engagement",
    "title": "Generative AI Use Cases in Banking",
    "section": "2 🏦 Customer Engagement",
    "text": "2 🏦 Customer Engagement\n\n\n\n\n\n\nWhy it matters:\nGenerative AI enables banks to deliver faster, more intelligent, and more personalized customer service. By automating routine interactions and offering real-time assistance, banks can improve customer satisfaction, reduce support costs, and scale their service capabilities without increasing headcount.\n\n\n\nExamples:\n- Bank of America – Erica: Offers financial guidance through natural language interactions, helping users with budgeting, transactions, and account management.\n- NatWest – Cora+: Anticipates customer needs and provides proactive responses using conversational AI.\n- Wells Fargo – Fargo: Supports English and Spanish to help customers manage cards and access banking services via natural language.\n- BNY Mellon – Eliza: Internal chatbot and tooling platform used by 52,000 employees to build GenAI-powered apps like lead generation tools."
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#operational-efficiency",
    "href": "gen-ai-use-cases/banking-use-cases.html#operational-efficiency",
    "title": "Generative AI Use Cases in Banking",
    "section": "3 📊 Operational Efficiency",
    "text": "3 📊 Operational Efficiency\n\n\n\n\n\n\nWhy it matters:\nBanks deal with vast volumes of reports, compliance documentation, and internal knowledge. Generative AI improves productivity by summarizing documents, automating workflows, and assisting developers — freeing employees to focus on higher-value tasks and accelerating project delivery.\n\n\n\nExamples:\n- JPMorgan – LLM Suite: Automates document summarization, content creation, and reporting.\n- Morgan Stanley – AskResearchGPT: Provides natural language access to 70,000+ research reports.\n- PwC – Compliance Automation: Uses GenAI to summarize regulatory documents and extract clauses.\n- Citi – Reporting Automation: Streamlines compliance reporting and data extraction.\n- Citigroup – AI Tools for Developers: Boosts engineering productivity with GenAI code assistance."
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#risk-management",
    "href": "gen-ai-use-cases/banking-use-cases.html#risk-management",
    "title": "Generative AI Use Cases in Banking",
    "section": "4 🔐 Risk Management",
    "text": "4 🔐 Risk Management\n\n\n\n\n\n\nWhy it matters:\nFraud detection and risk modeling are core to banking safety. GenAI enhances traditional models by spotting subtle anomalies, synthesizing potential fraud scenarios, and enabling faster investigation of suspicious transactions.\n\n\n\nExamples:\n- Mastercard – Decision Intelligence: Evaluates over 1 trillion data points in real time to approve 143+ billion transactions annually.\n- Wells Fargo – AI for Fraud: Uses GenAI for enhanced anomaly detection in transactional data."
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#credit-approval-and-loan-underwriting",
    "href": "gen-ai-use-cases/banking-use-cases.html#credit-approval-and-loan-underwriting",
    "title": "Generative AI Use Cases in Banking",
    "section": "5 📝 Credit Approval and Loan Underwriting",
    "text": "5 📝 Credit Approval and Loan Underwriting\n\n\n\n\n\n\nWhy it matters:\nCredit underwriting involves reviewing complex financial documents and assessing risk within tight timeframes. Generative AI helps automate the analysis of financial statements, legal documents, and industry benchmarks, enabling faster loan decisions, improved accuracy, and streamlined compliance.\n\n\n\nExamples:\n- Moody’s Analytics – CreditLens: Automates credit memo creation and industry analysis with generative tools.\n- JPMorgan Chase – COiN Platform: Uses GenAI to analyze legal contracts in seconds, reducing loan underwriting time."
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#personalized-marketing",
    "href": "gen-ai-use-cases/banking-use-cases.html#personalized-marketing",
    "title": "Generative AI Use Cases in Banking",
    "section": "6 📣 Personalized Marketing",
    "text": "6 📣 Personalized Marketing\n\n\n\n\n\n\nWhy it matters:\nGenerative AI enables banks to move beyond mass marketing and deliver personalized, data-driven messaging. This improves customer engagement, increases conversion rates, and enhances long-term loyalty.\n\n\n\nExamples:\n- JPMorgan Chase – Hyper-personalized Campaigns: Uses GenAI to create individualized marketing materials."
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#generative-business-intelligence-gen-bi",
    "href": "gen-ai-use-cases/banking-use-cases.html#generative-business-intelligence-gen-bi",
    "title": "Generative AI Use Cases in Banking",
    "section": "7 📈 Generative Business Intelligence (Gen BI)",
    "text": "7 📈 Generative Business Intelligence (Gen BI)\n\n\n\n\n\n\nWhy it matters:\nGenerative BI transforms data consumption by allowing non-technical users to explore reports, uncover insights, and ask ad hoc questions using natural language — all without waiting on analysts. This boosts data literacy and speeds up decision-making across business units.\n\n\n\nExamples:\n- Microsoft Copilot in Power BI: Enables users to ask natural-language questions and instantly generate charts or summaries.\n- ThoughtSpot Sage: Combines search with GenAI to explain patterns and alert users to anomalies.\n- Internal Gen BI tools: Many banks are building custom assistants on top of Snowflake, BigQuery, or Lakehouse environments to support executive dashboards and ops reviews."
  },
  {
    "objectID": "gen-ai-use-cases/banking-use-cases.html#key-takeaways",
    "href": "gen-ai-use-cases/banking-use-cases.html#key-takeaways",
    "title": "Generative AI Use Cases in Banking",
    "section": "8 🧾 Key Takeaways",
    "text": "8 🧾 Key Takeaways\n\nGenAI is no longer experimental in banking — it’s powering real customer-facing and internal systems.\nUse cases like customer engagement, underwriting, and Gen BI deliver measurable productivity and experience gains.\nAs the industry matures, GenAI will likely integrate deeper into core banking platforms and regulatory processes."
  },
  {
    "objectID": "ebm.html",
    "href": "ebm.html",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "",
    "text": "In the world of generative models, techniques like VAEs, GANs, and normalizing flows have each carved out their niche—but all of them come with specific constraints. Energy-Based Models (EBMs) offer a powerful alternative that’s architecturally flexible, conceptually elegant, and growing in popularity in modern deep learning research."
  },
  {
    "objectID": "ebm.html#energy-based-models-a-flexible-approach-to-generative-modeling",
    "href": "ebm.html#energy-based-models-a-flexible-approach-to-generative-modeling",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "",
    "text": "In the world of generative models, techniques like VAEs, GANs, and normalizing flows have each carved out their niche—but all of them come with specific constraints. Energy-Based Models (EBMs) offer a powerful alternative that’s architecturally flexible, conceptually elegant, and growing in popularity in modern deep learning research."
  },
  {
    "objectID": "ebm.html#limitations-of-mainstream-generative-models",
    "href": "ebm.html#limitations-of-mainstream-generative-models",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "🔍 Limitations of Mainstream Generative Models",
    "text": "🔍 Limitations of Mainstream Generative Models\n\n\n\n\n\n\n\n\nModel\nPros\nCons\n\n\n\n\nVAEs\nProbabilistic framework, tractable ELBO\nModel architecture restrictions; blurry samples\n\n\nNormalizing Flows\nExact likelihood, invertibility\nRestricted to invertible architectures; expensive Jacobian computation\n\n\nAutoregressive Models\nExact likelihood\nSlow sampling; autoregressive dependency limits parallelism\n\n\nGANs\nHigh-quality samples; flexible\nNo likelihood; unstable training; mode collapse\n\n\n\nThese models attempt to approximate the true data distribution ( P_{} ) by selecting a model ( P_) from a constrained family, often limited by the need for tractable likelihoods, invertible mappings, or adversarial training stability."
  },
  {
    "objectID": "ebm.html#enter-energy-based-models-ebms",
    "href": "ebm.html#enter-energy-based-models-ebms",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "⚡ Enter Energy-Based Models (EBMs)",
    "text": "⚡ Enter Energy-Based Models (EBMs)\nEnergy-Based Models drop the assumption of tractable density. Instead of directly modeling the probability distribution ( P(x) ), EBMs define an energy function ( E_(x) ) that assigns lower energy to more likely data points:\n[ p_(x) = ]\nHere, ( Z_= e^{-E_(x)} dx ) is the partition function, which normalizes the probability distribution. This implicit formulation gives EBMs the flexibility to use any neural architecture for ( E_), without needing invertibility, autoregressive factorization, or adversarial setup."
  },
  {
    "objectID": "ebm.html#math-review-from-scores-to-probabilities-in-energy-based-models",
    "href": "ebm.html#math-review-from-scores-to-probabilities-in-energy-based-models",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "📐 Math Review: From Scores to Probabilities in Energy-Based Models",
    "text": "📐 Math Review: From Scores to Probabilities in Energy-Based Models\nIn generative modeling, a valid probability distribution ( p(x) ) must satisfy:\n\nNon-negativity:\n[ p(x) ]\nNormalization (sum-to-one):\n[ _x p(x) = 1 p(x)dx = 1 ]\n\nCreating functions ( g_(x) ) is easy, but they often fail to normalize. Instead, we define:\n[ p_(x) = Z() = g_(x) dx ]\nIn EBMs, we use an exponential form:\n[ g_(x) = (f_(x)) p_(x) = ]\nand define energy as:\n[ E_(x) := -f_(x) ]\nSo the model becomes:\n[ p_(x) = ]\nThis is flexible, expressive, and connected to exponential family distributions."
  },
  {
    "objectID": "ebm.html#key-benefits-of-ebms",
    "href": "ebm.html#key-benefits-of-ebms",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "✅ Key Benefits of EBMs",
    "text": "✅ Key Benefits of EBMs\n\nVery flexible model architectures: No need for invertibility, factorization, or adversarial design.\nStable training: Compared to GANs, EBMs can be more robust.\nRelatively high sample quality: Can model multi-modal data well.\nFlexible composition: Combine energies for multi-task objectives."
  },
  {
    "objectID": "ebm.html#limitations-of-energy-based-models",
    "href": "ebm.html#limitations-of-energy-based-models",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "⚠️ Limitations of Energy-Based Models",
    "text": "⚠️ Limitations of Energy-Based Models\nDespite their elegance, EBMs pose serious challenges:\n\n🔄 Hard Sampling\n\nNo direct way to draw samples from ( p_(x) )\nRequires iterative MCMC methods\nSampling cost is high and scales poorly in high dimensions\n\n\n\n📉 Hard Likelihood Evaluation and Learning\n\nPartition function ( Z() ) is intractable\nEvaluating ( p_(x) ) is not feasible\nNeed to push down energy of incorrect configurations to make learning effective\n\n\n\n🚫 No Feature Learning (by default)\n\nEBMs do not learn latent features unless explicitly modeled (e.g., RBMs)"
  },
  {
    "objectID": "ebm.html#learning-and-inference-in-energy-based-models",
    "href": "ebm.html#learning-and-inference-in-energy-based-models",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "🧪 Learning and Inference in Energy-Based Models",
    "text": "🧪 Learning and Inference in Energy-Based Models\nWe optimize the likelihood:\n[ p_(x_{}) = f_(x_{}) - Z() ]\nGradient:\n[ f(x_{}) - {x p}[f(x)] ]\nSince ( Z() ) is intractable, this expectation is approximated using sampling."
  },
  {
    "objectID": "ebm.html#contrastive-divergence",
    "href": "ebm.html#contrastive-divergence",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "🔁 Contrastive Divergence",
    "text": "🔁 Contrastive Divergence\nContrastive Divergence approximates the gradient using:\n[ f(x_{}) - f(x_{}) ]\nWhere ( x_{} p_) (via MCMC). The goal is to increase ( f ) on training data and decrease it elsewhere."
  },
  {
    "objectID": "ebm.html#sampling-from-energy-based-models",
    "href": "ebm.html#sampling-from-energy-based-models",
    "title": "Energy-Based Models: A Flexible Approach to Generative Modeling",
    "section": "🎲 Sampling from Energy-Based Models",
    "text": "🎲 Sampling from Energy-Based Models\nWe can’t sample directly from EBMs, but we can use MCMC methods.\n\n🔄 Metropolis-Hastings\n\nPropose ( x’ = x + )\nAccept:\n\nAlways if ( f(x’) f(x) )\nOtherwise with probability ( (f(x’) - f(x)) )\n\n\nConverges to ( p_(x) ) as ( T ), but is slow in high dimensions.\n\n\n\n🌪 Langevin Dynamics\nUses gradients:\n[ x^{t+1} = x^t + x f(x^t) + z^t z^t (0, I) ]\nWorks well when ( x f(x) ) is tractable (which it is in EBMs).\n\n\n\n⚠️ Challenges of MCMC Sampling\n\nConvergence is slow, especially in high dimensions\nRequires multiple steps for every training iteration\nCan dominate training time in contrastive divergence\n\n\n\n⚠️ Sampling is the key bottleneck in training EBMs. Effective MCMC strategies are critical to making them practical in real-world settings."
  },
  {
    "objectID": "vae_mnist.html",
    "href": "vae_mnist.html",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "",
    "text": "This notebook explains a Variational Autoencoder (VAE) trained on the MNIST dataset using PyTorch.\nEach step is annotated with detailed comments to help beginners understand what’s happening.\n!pip install torch torchvision"
  },
  {
    "objectID": "vae_mnist.html#import-required-libraries",
    "href": "vae_mnist.html#import-required-libraries",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "1. Import Required Libraries",
    "text": "1. Import Required Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "href": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "2. Load and Prepare the MNIST Dataset",
    "text": "2. Load and Prepare the MNIST Dataset\n\n# We transform MNIST images into tensors.\ntransform = transforms.ToTensor()\n\n# Download and load the training data\ntrain_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n\n# DataLoader for batching and shuffling\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
  },
  {
    "objectID": "vae_mnist.html#define-the-vae-model",
    "href": "vae_mnist.html#define-the-vae-model",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "3. Define the VAE Model",
    "text": "3. Define the VAE Model\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):\n        super(VAE, self).__init__()\n        # Encoder layers: input -&gt; hidden -&gt; (mu, logvar)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # outputs mean of q(z|x)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # outputs log-variance of q(z|x)\n\n        # Decoder layers: latent -&gt; hidden -&gt; reconstruction\n        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, input_dim)\n\n    def encode(self, x):\n        # Apply a hidden layer then split into mean and logvar\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        # Apply the reparameterization trick\n        std = torch.exp(0.5 * logvar)      # standard deviation\n        eps = torch.randn_like(std)        # random normal noise\n        return mu + eps * std              # sample z\n\n    def decode(self, z):\n        # Reconstruct input from latent representation\n        h = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h))  # Output in [0, 1] range for binary MNIST\n\n    def forward(self, x):\n        # Full VAE forward pass\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        return recon_x, mu, logvar"
  },
  {
    "objectID": "vae_mnist.html#define-the-elbo-loss",
    "href": "vae_mnist.html#define-the-elbo-loss",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "4. Define the ELBO Loss",
    "text": "4. Define the ELBO Loss\n\ndef elbo_loss(recon_x, x, mu, logvar):\n    # Binary cross-entropy for reconstruction\n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n\n    # KL divergence term to regularize q(z|x) against standard normal p(z)\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    # Total loss is negative ELBO\n    return BCE + KLD"
  },
  {
    "objectID": "vae_mnist.html#train-the-vae",
    "href": "vae_mnist.html#train-the-vae",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "5. Train the VAE",
    "text": "5. Train the VAE\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = VAE().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for x, _ in train_loader:\n        x = x.view(-1, 784).to(device)               # Flatten 28x28 images into 784 vectors\n        recon_x, mu, logvar = model(x)               # Forward pass\n        loss = elbo_loss(recon_x, x, mu, logvar)     # Compute loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader.dataset):.2f}\")"
  },
  {
    "objectID": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "href": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "6. Visualize Original and Reconstructed Digits",
    "text": "6. Visualize Original and Reconstructed Digits\n\nmodel.eval()\nwith torch.no_grad():\n    x, _ = next(iter(train_loader))\n    x = x.view(-1, 784).to(device)\n    recon_x, _, _ = model(x)\n\n    # Convert back to image format\n    x = x.view(-1, 1, 28, 28).cpu()\n    recon_x = recon_x.view(-1, 1, 28, 28).cpu()\n\n    fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n    for i in range(10):\n        axs[0, i].imshow(x[i][0], cmap='gray')\n        axs[0, i].axis('off')\n        axs[1, i].imshow(recon_x[i][0], cmap='gray')\n        axs[1, i].axis('off')\n    axs[0, 0].set_ylabel(\"Original\", fontsize=12)\n    axs[1, 0].set_ylabel(\"Reconstruction\", fontsize=12)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "vae_mnist.html#visualize-latent-space",
    "href": "vae_mnist.html#visualize-latent-space",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "7. Visualize Latent Space",
    "text": "7. Visualize Latent Space\n\nimport seaborn as sns\n\nmodel.eval()\nall_z = []\nall_labels = []\n\n# Go through a few batches and collect latent representations\nwith torch.no_grad():\n    for x, y in train_loader:\n        x = x.view(-1, 784).to(device)\n        mu, _ = model.encode(x)  # use the mean as representation\n        all_z.append(mu.cpu())\n        all_labels.append(y)\n\n# Concatenate all batches\nz = torch.cat(all_z, dim=0).numpy()\nlabels = torch.cat(all_labels, dim=0).numpy()\n\n# Plot with seaborn\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=z[:, 0], y=z[:, 1], hue=labels, palette=\"tab10\", s=15)\nplt.title(\"Latent Space Visualization (using μ)\")\nplt.xlabel(\"z[0]\")\nplt.ylabel(\"z[1]\")\nplt.legend(title=\"Digit\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Variational Autoencoders (VAEs) combine the power of neural networks with probabilistic inference to model complex data distributions. This blog unpacks the intuition, math, and implementation of VAEs — from KL divergence and the ELBO to PyTorch code that generates to generate new images."
  },
  {
    "objectID": "vae.html#autoencoders-vs-variational-autoencoders",
    "href": "vae.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational Autoencoders",
    "section": "Autoencoders vs Variational Autoencoders",
    "text": "Autoencoders vs Variational Autoencoders\nTraditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) were introduced to overcome these limitations. Rather than encoding inputs into fixed latent vectors, VAEs learn a probabilistic latent space by modeling each input as a distribution — typically a Gaussian with a learned mean \\(\\\\mu\\) and standard deviation \\(\\\\sigma\\). This approach enables the model to sample latent variables \\(z\\) using the reparameterization trick, allowing the entire architecture to remain differentiable and trainable. By doing so, VAEs not only enable reconstruction, but also promote the learning of a continuous, interpretable latent space — a key enabler for generation and interpolation.\nThe diagram below illustrates this process:\n\nSource: Wikimedia Commons, licensed under CC BY-SA 4.0."
  },
  {
    "objectID": "vae.html#probabilistic-framework",
    "href": "vae.html#probabilistic-framework",
    "title": "Variational Autoencoders",
    "section": "Probabilistic Framework",
    "text": "Probabilistic Framework\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\nHere, \\(\\mathbf{z}\\) acts as a hidden or latent variable, which is unobserved during training. The model thus defines a mixture of infinitely many Gaussians — one for each \\(\\mathbf{z}\\).\nTo compute the likelihood of a data point \\(\\mathbf{x}\\), we must marginalize over all possible latent variables: \\[\n  p(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n  \\]\nThis integral requires integrating over all possible values of the latent variable \\(\\mathbf{z}\\), which is often high-dimensional and affects the likelihood in a non-linear way through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of techniques like variational inference and ELBO.\n\nComputational Challenge\nThis integral requires integrating over:\n\nAll possible values of \\(\\mathbf{z}\\) (often high-dimensional)\nNon-linear transformations through neural networks\n\nResult: Exact computation is intractable, motivating techniques like variational inference and ELBO (developed next)."
  },
  {
    "objectID": "vae.html#estimating-the-marginal-likelihood",
    "href": "vae.html#estimating-the-marginal-likelihood",
    "title": "Variational Autoencoders",
    "section": "Estimating the Marginal Likelihood",
    "text": "Estimating the Marginal Likelihood\n\nNaive Monte Carlo Estimation\nOne natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nHowever, this fails in practice. For most values of \\(z\\), the joint probability \\(p_\\theta(x, z)\\) is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely “hits” likely values of \\(z\\).\n\n\nImportance Sampling\nTo address this, we use importance sampling, introducing a proposal distribution \\(q(z)\\):\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nThis gives an unbiased estimator of \\(p(x)\\) if \\(q(z)\\) is well-chosen (ideally close to \\(p_\\theta(z|x)\\)). Intuitively, we sample \\(z\\) more frequently in regions where \\(p_\\theta(x, z)\\) is high.\n\n\n\nLog likelihood\nOur goal is to optimize the log-likelihood, and the log of an expectation is not the same as the expectation of the log. That is,\n\\[\n\\log p(x) = log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\neq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nWhile the marginal likelihood p(x) can be estimated unbiasedly using importance sampling, estimating its logarithm \\(p(x)\\) introduces bias due to the concavity of the log function. This is captured by Jensen’s Inequality, which tells us:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\geq \\underbrace{\\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]}_{\\text{ELBO}}\n\\]\nThis means that the expected log of the estimator underestimates the true log-likelihood. The right-hand side provides a tractable surrogate objective known as the Evidence Lower Bound (ELBO), which is a biased lower bound to \\(\\log p(x)\\). Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood.\nIn the next section, we formally derive this bound and explore its components in detail."
  },
  {
    "objectID": "vae.html#why-variational-inference",
    "href": "vae.html#why-variational-inference",
    "title": "Variational Autoencoders",
    "section": "Why Variational Inference?",
    "text": "Why Variational Inference?\nComputing the true posterior distribution \\(p(z \\mid x)\\) is intractable in most cases, because it requires evaluating the marginal likelihood \\(p(x)\\), which involves integrating over all possible values of \\(z\\):\n\\[\np(x) = \\int p(x, z) \\, dz\n\\]\nVariational inference tackles this by introducing a tractable, parameterized distribution \\(q(z)\\) to approximate \\(p(z|x)\\). We aim to make \\(q(z)\\) as close as possible to the true posterior by minimizing the KL divergence:\n\\[\nD_{\\text{KL}}(q(z) \\| p(z|x))\n\\]\nThis turns inference into an optimization problem. A key result is the Evidence Lower Bound (ELBO). See next section."
  },
  {
    "objectID": "vae.html#training-a-vae",
    "href": "vae.html#training-a-vae",
    "title": "Variational Autoencoders",
    "section": "Training a VAE",
    "text": "Training a VAE\n\nELBO Objective\nNow that we’ve introduced the challenge of approximating the intractable posterior using variational inference, we turn our attention to deriving the Evidence Lower Bound (ELBO). This derivation reveals how optimizing a surrogate objective allows us to approximate the true log-likelihood of the data while keeping the approximate posterior close to the prior. The steps below walk through this formulation.\n\nStep 1: KL Divergence Objective\n\\[\\begin{equation}\nD_{KL}(q(z)\\|p(z|x; \\theta)) = \\sum_z q(z) \\log \\frac{q(z)}{p(z|x; \\theta)}\n\\end{equation}\\]\n\n\nStep 2: Apply Bayes’ Rule\nSubstitute \\(p(z|x; \\theta) = \\frac{p(z,x;\\theta)}{p(x;\\theta)}\\): \\[\\begin{equation}\n= \\sum_z q(z) \\log \\left( \\frac{q(z) \\cdot p(x; \\theta)}{p(z, x; \\theta)} \\right)\n\\end{equation}\\]\n\n\nStep 3: Decompose Terms\n\\[\\begin{align}\n&= \\sum_z q(z) \\log q(z) + \\sum_z q(z) \\log p(x; \\theta) \\nonumber \\\\\n&\\quad - \\sum_z q(z) \\log p(z, x; \\theta) \\\\\n&= -H(q) + \\log p(x; \\theta) - \\mathbb{E}_q[\\log p(z,x;\\theta)]\n\\end{align}\\]\n\nNote: The term \\(\\mathcal{H}(q)\\) represents the entropy of the variational distribution \\(q(z|x)\\). Entropy is defined as:\n\\[\n\\mathcal{H}(q) = -\\sum_z q(z) \\log q(z) = -\\mathbb{E}_{q(z)}[\\log q(z)]\n\\]\nEntropy measures the amount of uncertainty or “spread” in a distribution. A high-entropy \\(q(z)\\) places probability mass across a wide region of the latent space, while a low-entropy \\(q(z)\\) is more concentrated. This decomposition is key to understanding the KL divergence term in the ELBO.\n\n\n\nStep 4: Rearrange for ELBO\n\\[\n\\log p(x; \\theta) =\n\\underbrace{\n    \\mathbb{E}_q[\\log p(z, x; \\theta)] + \\mathcal{H}(q)\n}_{\\text{ELBO}}\n+D_{KL}(q(z)\\|p(z|x; \\theta))\n\\]\nThis equation shows that the log-likelihood \\(\\log p(x)\\) can be decomposed into the ELBO and the KL divergence between the approximate posterior and the true posterior. Since the KL divergence is always non-negative, the ELBO serves as a lower bound to the log-likelihood. By maximizing the ELBO, we indirectly minimize the KL divergence, bringing \\(q(z)\\) closer to \\(p(z|x)\\).\n Visualizing how \\(\\log p(x)\\) decomposes into the ELBO and KL divergence.\nSource: deepgenerativemodels.github.io\n\n\nKey Results\n\nEvidence Lower Bound (ELBO): \\[\\begin{equation}\n\\mathcal{L}(\\theta,\\phi) = \\mathbb{E}_{q(z;\\phi)}[\\log p(x,z;\\theta)] + H(q(z;\\phi))\n\\end{equation}\\]\nOptimization: \\[\\begin{equation}\n\\max_{\\theta,\\phi} \\mathcal{L}(\\theta,\\phi) \\Rightarrow\n\\begin{cases}\n\\text{Maximizes data likelihood} \\\\\n\\text{Minimizes } D_{KL}(q\\|p)\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "href": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "title": "Variational Autoencoders",
    "section": "Understanding the KL Divergence Term in the VAE Loss",
    "text": "Understanding the KL Divergence Term in the VAE Loss\nIn a VAE, the KL divergence term penalizes the encoder for producing latent distributions that deviate too far from the standard normal prior. This regularization has several important benefits:\n\nIt ensures that the latent space has a consistent structure, enabling meaningful sampling and interpolation.\nIt helps avoid large gaps between clusters in latent space by encouraging the encoder to distribute representations more uniformly.\nIt pushes the model to use the space around the origin more symmetrically and efficiently.\n\n\nBalancing KL Divergence and Reconstruction\nIn a Variational Autoencoder, the loss balances two goals:\n\nReconstruction — making the output resemble the input\nRegularization — keeping the latent space close to a standard normal distribution\n\nThis is captured by the loss function:\n\\[\n\\mathcal{L}_{\\text{VAE}} = \\text{Reconstruction Loss} + \\beta \\cdot D_{\\text{KL}}(q(z|x) \\,\\|\\, p(z))\n\\]\nThe parameter \\(\\beta\\) controls how strongly we enforce this regularization. Getting its value right is critical.\n\nWhen \\(\\beta\\) is too low:\n\nThe model mostly ignores the KL term, behaving like a plain autoencoder\nThe latent space becomes disorganized or fragmented\nSampling from the prior \\(p(z) = \\mathcal{N}(0, I)\\) results in unrealistic or broken outputs\n\n\n\nWhen \\(\\beta\\) is too high:\n\nThe encoder is forced to keep \\(q(z|x)\\) too close to the prior\nIt encodes less information about the input\nReconstructions become blurry or generic, since the decoder gets little to work with\n\n\nChoosing \\(\\beta\\) carefully is essential for balancing generalization and fidelity.\nA well-tuned \\(\\beta\\) helps the VAE both reconstruct accurately and generate new samples that resemble the training data.\n\n\n\n\nGradient Challenge\nIn variational inference, we approximate the true posterior \\(p(z|x)\\) with a tractable distribution \\(q_\\phi(z)\\). This allows us to optimize the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q(z; \\phi)} \\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nOur goal is to maximize this objective with respect to both \\(\\theta\\) and \\(\\phi\\). While computing the gradient with respect to \\(\\theta\\) is straightforward, optimizing with respect to \\(\\phi\\) presents a challenge.\nThe complication arises because \\(\\phi\\) appears both in the density \\(q_\\phi(z|x)\\) and in the expectation operator. That is:\n\\[\n\\nabla_\\phi \\mathbb{E}_{q(z; \\phi)} \\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nThis gradient is hard to compute directly because we’re sampling from a distribution that depends on the parameters we’re trying to update.\n\n\n\nThe Reparameterization Trick\nTo make this expression differentiable, we reparameterize the random variable \\(z\\) as a deterministic transformation of a parameter-free noise variable \\(\\epsilon\\):\n\\[\n\\epsilon \\sim \\mathcal{N}(0, I), \\quad z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n\\]\nThis turns the expectation into:\n\\[\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nwhere \\(z\\) is now a differentiable function of \\(\\phi\\).\n\n\n\nReparameterization Trick Diagram\n\n\nImage source: Wikipedia (CC BY-SA 4.0)\nThis diagram illustrates how the reparameterization trick enables differentiable sampling:\n\nIn the original formulation, \\(z\\) is sampled directly from a learned distribution, breaking the gradient flow.\nIn the reparameterized formulation, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\), and compute \\(z = \\mu + \\sigma \\cdot \\epsilon\\), making the sampling path fully differentiable.\n\n\nMonte Carlo Approximation\nWe approximate the expectation using Monte Carlo sampling:\n\\[\n\\mathbb{E}_{\\epsilon}[\\log p_\\theta(x, z) - \\log q_\\phi(z)] \\approx \\frac{1}{K} \\sum_{k=1}^K \\left[\\log p_\\theta(x, z^{(k)}) - \\log q_\\phi(z^{(k)})\\right]\n\\]\nwith:\n\\[\nz^{(k)} = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon^{(k)}, \\quad \\epsilon^{(k)} \\sim \\mathcal{N}(0, I)\n\\]\nThis enables us to compute gradients using backpropagation.\n\n\n\nSummary\n\nVariational inference introduces a gradient challenge because \\(q_\\phi(z)\\) depends on \\(\\phi\\)\nThe reparameterization trick expresses \\(z\\) as a differentiable function of noise and \\(\\phi\\)\nThis allows us to use backpropagation to optimize the ELBO efficiently\n\n\n\n\n\nAmortized Inference\nIn classical variational inference, we introduce a separate set of variational parameters \\(\\phi^i\\) for each datapoint \\(x^i\\) to approximate the true posterior \\(p(z|x^i)\\). However:\n\nOptimizing a separate \\(\\phi^i\\) for every datapoint is computationally expensive and does not scale to large datasets.\n\n\n\nThe Key Idea: Amortization\nInstead of learning and storing a separate \\(\\phi^i\\) for every datapoint, we learn a single parametric function \\(f_\\phi(x)\\) — typically a neural network — that maps each input \\(x\\) to the parameters of the approximate posterior:\n\\[\nq_\\phi(z|x) = \\mathcal{N}\\left(\\mu_\\phi(x), \\sigma^2_\\phi(x)\\right)\n\\]\nHere, \\(\\phi\\) are the shared parameters of the encoder network, and \\(\\mu_\\phi(x), \\sigma_\\phi(x)\\) are its outputs.\nThis is like learning a regression function that predicts the optimal variational parameters for any input \\(x\\).\n\n\n\n\nTraining with Amortized Inference\nOur training objective remains the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nWe optimize both \\(\\theta\\) (decoder parameters) and \\(\\phi\\) (encoder parameters) using stochastic gradient descent.\n\nAlgorithm:\n\nInitialize \\(\\theta^{(0)}, \\phi^{(0)}\\)\nSample a datapoint \\(x^i\\)\nUse \\(f_\\phi(x^i)\\) to produce \\(\\mu^i, \\sigma^i\\)\nSample \\(z^i = \\mu^i + \\sigma^i \\cdot \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)\nEstimate the ELBO and compute gradients w.r.t. \\(\\theta, \\phi\\)\nUpdate \\(\\theta, \\phi\\) using gradient descent\nUpdate \\(\\theta\\), \\(\\phi\\) using gradient descent:\n\n\\[\n\\phi \\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\n\\[\n\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\nwhere \\(\\mathcal{B}\\) is the current minibatch and \\(\\tilde{\\nabla}\\) indicates a stochastic gradient approximation.\n\n\n\n\nSummary\n\nAmortized inference replaces per-datapoint optimization with a single learned mapping \\(f_\\phi(x)\\)\nThis makes variational inference scalable and efficient\nThe model can generalize to unseen inputs by predicting variational parameters on-the-fly\n\n\nNote: Following common practice in the literature, we use \\(\\phi\\) to denote the parameters of the encoder network, even though it now defines a function rather than individual variational parameters."
  },
  {
    "objectID": "vae.html#applications-of-vaes",
    "href": "vae.html#applications-of-vaes",
    "title": "Variational Autoencoders",
    "section": "Applications of VAEs",
    "text": "Applications of VAEs\nVariational Autoencoders are widely used in:\n\nImage Generation: VAEs can generate new images similar to the training data (e.g., MNIST digits)\n\nAnomaly Detection: High reconstruction error flags unusual data points\n\nRepresentation Learning: Latent space captures features for downstream tasks\n\n\n😎 Face Generation with Convolutional VAE\nTo complement the theory, I’ve built a full PyTorch implementation of a Variational Autoencoder trained on the CelebA dataset.\n📘 The notebook walks through:\n\nDefining the encoder, decoder, and reparameterization trick\n\nImplementing the ELBO loss function (reconstruction + KL divergence)\n\nTraining the model on face images\n\nGenerating new faces from random latent vectors\n\n\n📓 View on GitHub"
  },
  {
    "objectID": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections.",
    "href": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections.",
    "title": "Variational Autoencoders",
    "section": "This example is designed to reinforce the theoretical concepts from earlier sections.",
    "text": "This example is designed to reinforce the theoretical concepts from earlier sections."
  },
  {
    "objectID": "vae.html#further-reading",
    "href": "vae.html#further-reading",
    "title": "Variational Autoencoders",
    "section": "Further Reading",
    "text": "Further Reading\nFor readers interested in diving deeper into the theory and applications of variational autoencoders, the following resources are recommended:\n\nTutorial on Variational Autoencoders\nCarl Doersch (2016)\nhttps://arxiv.org/pdf/1606.05908\nAuto-Encoding Variational Bayes\nKingma & Welling (2014) — the original VAE paper\nhttps://arxiv.org/pdf/1312.6114\nThe Challenges of Amortized Inference for Structured Prediction\nCremer, Li, & Duvenaud (2019)\nhttps://arxiv.org/pdf/1906.02691\nDeep Generative Models course notes\nhttps://deepgenerativemodels.github.io/notes/vae/"
  },
  {
    "objectID": "vae_faces.html",
    "href": "vae_faces.html",
    "title": "Face Generation with Convolutional VAE",
    "section": "",
    "text": "This notebook implements a convolutional variational autoencoder (VAE) trained on the CelebA face dataset using PyTorch.\nIt uses convolutional layers to encode and decode 64x64 face images, and demonstrates generation by sampling from the latent space.\n!pip install torch torchvision matplotlib\n# Enable autoreloading in Jupyter (if applicable)\n%load_ext autoreload\n%autoreload 2\n\n# Core libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import norm  # Used for visualizing latent distributions\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms, utils\n# Hyperparameters\n\nIMAGE_SIZE = 32        # Input image resolution (32x32)\nCHANNELS = 3           # Number of color channels (RGB)\nBATCH_SIZE = 128       # Batch size for training\nNUM_FEATURES = 128     # Hidden layer size or intermediate feature size\nZ_DIM = 200            # Dimensionality of the latent space\nLEARNING_RATE = 5e-4   # Learning rate for optimizer\nEPOCHS = 1             # Number of training epochs\nBETA = 2000            # Weight for KL divergence term in loss (used in beta-VAE)"
  },
  {
    "objectID": "vae_faces.html#loading-and-preprocessing-the-celeba-dataset",
    "href": "vae_faces.html#loading-and-preprocessing-the-celeba-dataset",
    "title": "Face Generation with Convolutional VAE",
    "section": "Loading and Preprocessing the CelebA Dataset",
    "text": "Loading and Preprocessing the CelebA Dataset\n\nfrom torchvision.datasets import CelebA\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Transform pipeline: resize and convert to tensor\ntransform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor()\n])\n\n# Load CelebA dataset\ntrain_data = CelebA(\n    root=\"./data\",                      # Where to download/save the data\n    split=\"train\",                      # Options: \"train\", \"valid\", \"test\", or \"all\"\n    download=True,                      # Download if not already there\n    transform=transform\n)\n\n# DataLoader\ntrain_loader = DataLoader(\n    train_data,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\n\nval_data = CelebA(\n    root=\"./data\",\n    split=\"valid\",\n    download=True,\n    transform=transform\n)\n\nval_loader = DataLoader(\n    val_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)"
  },
  {
    "objectID": "vae_faces.html#visualizing-training-data",
    "href": "vae_faces.html#visualizing-training-data",
    "title": "Face Generation with Convolutional VAE",
    "section": "Visualizing Training Data",
    "text": "Visualizing Training Data\n\ndef sample_batch(dataloader, num_samples=16):\n    \"\"\"\n    Sample one batch from the dataloader and return the first `num_samples` images.\n    \"\"\"\n    for images, _ in dataloader:\n        return images[:num_samples]\n\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\ndef display(images, cmap=None, nrow=8):\n    \"\"\"\n    Display a grid of images using matplotlib.\n    \"\"\"\n    grid = make_grid(images, nrow=nrow, padding=2, normalize=True)\n    np_img = grid.permute(1, 2, 0).cpu().numpy()\n\n    plt.figure(figsize=(10, 5))\n    plt.imshow(np_img, cmap=cmap)\n    plt.axis('off')\n    plt.show()\n\n\ntrain_sample = sample_batch(train_loader, num_samples=16)\ndisplay(train_sample)"
  },
  {
    "objectID": "vae_faces.html#reparameterization-trick",
    "href": "vae_faces.html#reparameterization-trick",
    "title": "Face Generation with Convolutional VAE",
    "section": "Reparameterization Trick",
    "text": "Reparameterization Trick\n\ndef reparameterize(z_mean, z_log_var):\n    \"\"\"\n    Reparameterization trick: z = mu + sigma * epsilon\n    where epsilon ~ N(0, I)\n    \"\"\"\n    std = torch.exp(0.5 * z_log_var)\n    eps = torch.randn_like(std)\n    return z_mean + eps * std"
  },
  {
    "objectID": "vae_faces.html#encoder",
    "href": "vae_faces.html#encoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "Encoder",
    "text": "Encoder\n\nclass Encoder(nn.Module):\n    def __init__(self, z_dim):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(CHANNELS, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 32 → 16\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 16 → 8\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 8 → 4\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 4 → 2\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n        )\n\n        self.flatten = nn.Flatten()\n        self.feature_shape = (NUM_FEATURES, IMAGE_SIZE // 16, IMAGE_SIZE // 16)\n        self.flat_dim = NUM_FEATURES * (IMAGE_SIZE // 16) ** 2\n\n        self.fc_mu = nn.Linear(self.flat_dim, z_dim)\n        self.fc_logvar = nn.Linear(self.flat_dim, z_dim)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_flat = self.flatten(x)\n        mu = self.fc_mu(x_flat)\n        logvar = self.fc_logvar(x_flat)\n        return mu, logvar"
  },
  {
    "objectID": "vae_faces.html#vae-model-wrapper-encoder-reparameterization-and-decoder",
    "href": "vae_faces.html#vae-model-wrapper-encoder-reparameterization-and-decoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "VAE Model Wrapper: Encoder, Reparameterization, and Decoder",
    "text": "VAE Model Wrapper: Encoder, Reparameterization, and Decoder\n\nclass VAE(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"\n        z = mu + sigma * epsilon\n        where epsilon ~ N(0, I)\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass:\n        - Encode input to get mu and logvar\n        - Sample z using reparameterization trick\n        - Decode z to reconstruct image\n        \"\"\"\n        mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        x_recon = self.decoder(z)\n        return x_recon, mu, logvar"
  },
  {
    "objectID": "vae_faces.html#vae-loss",
    "href": "vae_faces.html#vae-loss",
    "title": "Face Generation with Convolutional VAE",
    "section": "VAE Loss",
    "text": "VAE Loss\n\ndef vae_loss(x, x_recon, mu, logvar, beta=BETA):\n    \"\"\"\n    VAE loss = beta * reconstruction loss (MSE) + KL divergence\n    \"\"\"\n    # Reconstruction loss (MSE)\n    recon_loss = F.mse_loss(x_recon, x, reduction='mean') * beta\n\n    # KL divergence between q(z|x) and N(0, I)\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    kl_loss = kl_loss / x.size(0)  # average over batch\n\n    return recon_loss + kl_loss, recon_loss, kl_loss"
  },
  {
    "objectID": "vae_faces.html#generating-and-saving-sample-images-from-the-latent-space",
    "href": "vae_faces.html#generating-and-saving-sample-images-from-the-latent-space",
    "title": "Face Generation with Convolutional VAE",
    "section": "Generating and Saving Sample Images from the Latent Space",
    "text": "Generating and Saving Sample Images from the Latent Space\n\nfrom torchvision.utils import save_image\nimport os\n\ndef generate_images(model, epoch, z_dim, num_img=8, path=\"./output\"):\n    \"\"\"\n    Generate and save images from random latent vectors after each epoch.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    model.eval()\n    z = torch.randn(num_img, z_dim).to(next(model.parameters()).device)\n    with torch.no_grad():\n        generated = model.decoder(z)\n    for i in range(num_img):\n        save_image(generated[i], f\"{path}/generated_img_{epoch:03d}_{i}.png\")"
  },
  {
    "objectID": "vae_faces.html#save-checkpoint",
    "href": "vae_faces.html#save-checkpoint",
    "title": "Face Generation with Convolutional VAE",
    "section": "Save Checkpoint",
    "text": "Save Checkpoint\n\ndef save_checkpoint(model, epoch, loss, best_loss, path=\"./checkpoint.pt\"):\n    if loss &lt; best_loss:\n        print(f\"Saving new best model at epoch {epoch} with loss {loss:.4f}\")\n        torch.save(model.state_dict(), path)\n        return loss\n    return best_loss"
  },
  {
    "objectID": "vae_faces.html#training-step",
    "href": "vae_faces.html#training-step",
    "title": "Face Generation with Convolutional VAE",
    "section": "Training Step",
    "text": "Training Step\n\ndef train_step(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n\n    for batch in dataloader:\n        x, _ = batch if isinstance(batch, (tuple, list)) else (batch, None)\n        x = x.to(device)\n\n        optimizer.zero_grad()\n\n        x_recon, mu, logvar = model(x)\n        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_recon += recon_loss.item()\n        total_kl += kl_loss.item()\n\n    num_batches = len(dataloader)\n    return {\n        \"loss\": total_loss / num_batches,\n        \"reconstruction_loss\": total_recon / num_batches,\n        \"kl_loss\": total_kl / num_batches\n    }"
  },
  {
    "objectID": "vae_faces.html#validation-step",
    "href": "vae_faces.html#validation-step",
    "title": "Face Generation with Convolutional VAE",
    "section": "Validation Step",
    "text": "Validation Step\n\n@torch.no_grad()\ndef validate_epoch(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n\n    for batch in dataloader:\n        x, _ = batch if isinstance(batch, (tuple, list)) else (batch, None)\n        x = x.to(device)\n        x_recon, mu, logvar = model(x)\n        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar)\n        total_loss += loss.item()\n        total_recon += recon_loss.item()\n        total_kl += kl_loss.item()\n\n    num_batches = len(dataloader)\n    return {\n        \"loss\": total_loss / num_batches,\n        \"reconstruction_loss\": total_recon / num_batches,\n        \"kl_loss\": total_kl / num_batches\n    }"
  },
  {
    "objectID": "vae_faces.html#instantiate-model-optimizer-writer-device",
    "href": "vae_faces.html#instantiate-model-optimizer-writer-device",
    "title": "Face Generation with Convolutional VAE",
    "section": "Instantiate model, optimizer, writer, device",
    "text": "Instantiate model, optimizer, writer, device\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nencoder = Encoder(z_dim=Z_DIM)\ndecoder = Decoder(z_dim=Z_DIM)\nvae = VAE(encoder, decoder).to(device)\n\noptimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n\nwriter = SummaryWriter(log_dir=\"./logs\")"
  },
  {
    "objectID": "vae_faces.html#training-the-vae-model",
    "href": "vae_faces.html#training-the-vae-model",
    "title": "Face Generation with Convolutional VAE",
    "section": "Training the VAE Model",
    "text": "Training the VAE Model\n\nbest_loss = float(\"inf\")\n\nfor epoch in range(EPOCHS):\n    train_logs = train_step(vae, train_loader, optimizer, device)\n    val_logs = validate_epoch(vae, val_loader, device)\n\n    print(f\"Epoch {epoch+1:02d} | \"\n          f\"Train Loss: {train_logs['loss']:.4f} | \"\n          f\"Val Loss: {val_logs['loss']:.4f}\")\n\n    # Save best model\n    best_loss = save_checkpoint(vae, epoch, val_logs[\"loss\"], best_loss)\n\n    # Generate and save sample images\n    generate_images(vae, epoch, Z_DIM)\n\n    # Log to TensorBoard\n    writer.add_scalar(\"Loss/train\", train_logs[\"loss\"], epoch)\n    writer.add_scalar(\"Loss/val\", val_logs[\"loss\"], epoch)\n    writer.add_scalar(\"KL/train\", train_logs[\"kl_loss\"], epoch)\n    writer.add_scalar(\"Recon/train\", train_logs[\"reconstruction_loss\"], epoch)\n\nEpoch 01 | Train Loss: 72.4671 | Val Loss: 58.4050\nSaving new best model at epoch 0 with loss 58.4050"
  },
  {
    "objectID": "vae_faces.html#reconstruct-using-the-variational-autoencoder",
    "href": "vae_faces.html#reconstruct-using-the-variational-autoencoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "Reconstruct using the variational autoencoder",
    "text": "Reconstruct using the variational autoencoder\n\n# Select a batch of images from the training set\nexample_images, _ = next(iter(train_loader))  # Ignore labels\nexample_images = example_images[:8].to(device)  # Select first 8 images\n\n# Set model to evaluation mode\nvae.eval()\n\n# Forward pass through the VAE to get reconstructions\nwith torch.no_grad():\n    reconstructions, _, _ = vae(example_images)\n\n# Move tensors to CPU for display\nexample_images = example_images.cpu()\nreconstructions = reconstructions.cpu()\n\n# Display original images\nprint(\"Example real faces\")\ndisplay(example_images)\n\n# Display reconstructed images\nprint(\"Reconstructions\")\ndisplay(reconstructions)\n\nExample real faces\n\n\n\n\n\n\n\n\n\nReconstructions"
  },
  {
    "objectID": "vae_faces.html#generate-new-faces",
    "href": "vae_faces.html#generate-new-faces",
    "title": "Face Generation with Convolutional VAE",
    "section": "Generate New Faces",
    "text": "Generate New Faces\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample latent vectors from standard normal distribution\ngrid_width, grid_height = 10, 3\nnum_samples = grid_width * grid_height\nz_sample = torch.randn(num_samples, Z_DIM).to(device)\n\n# Decode to generate new faces\nvae.eval()\nwith torch.no_grad():\n    generated_faces = vae.decoder(z_sample).cpu()  # Shape: [N, C, H, W]\n\n# Plot grid of generated images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(num_samples):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    img = generated_faces[i].permute(1, 2, 0).numpy()  # CHW → HWC\n    ax.imshow(img)\n\nplt.suptitle(\"Generated Faces from Sampled Latent Vectors\", fontsize=16)\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Generative Models",
    "section": "",
    "text": "This site explores the foundations and frontiers of deep generative models — from core mathematical principles to the architectures powering today’s most advanced AI systems.\nIt’s designed to document concepts clearly, walk through implementations step by step, and connect theory with real-world relevance. Topics range from early probabilistic frameworks to cutting-edge diffusion models driving breakthroughs in image, text, and multimodal generation.\n\nLearn by building · Backed by math · Inspired by real-world use cases"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Generative Models",
    "section": "",
    "text": "This site explores the foundations and frontiers of deep generative models — from core mathematical principles to the architectures powering today’s most advanced AI systems.\nIt’s designed to document concepts clearly, walk through implementations step by step, and connect theory with real-world relevance. Topics range from early probabilistic frameworks to cutting-edge diffusion models driving breakthroughs in image, text, and multimodal generation.\n\nLearn by building · Backed by math · Inspired by real-world use cases"
  },
  {
    "objectID": "index.html#key-model-families",
    "href": "index.html#key-model-families",
    "title": "Deep Generative Models",
    "section": "Key Model Families",
    "text": "Key Model Families\n\n1. Variational Autoencoders (VAEs)\n\nProbabilistic encoder–decoder architecture\n\nLearns latent representations using Gaussian priors\n\n\n\n2. Autoregressive Models\n\nModels sequences element-by-element (e.g., PixelCNN, WaveNet)\n\nStrong in text, audio, and image generation with local dependencies\n\n\n\n3. Generative Adversarial Networks (GANs)\n\nGenerator vs. discriminator adversarial training\n\nCapable of generating highly realistic images\n\n\n\n4. Normalizing Flows\n\nUses invertible transformations for exact likelihood\n\nFlexible for density estimation in high dimensions\nEnergy-Based Models: A Flexible Approach to Generative Modeling\nA comprehensive introduction to EBMs, covering math, learning, sampling, and their pros & cons.\n\n\n\n6. Diffusion Models\n\nIterative denoising process\n\nState-of-the-art in high-quality image generation"
  },
  {
    "objectID": "index.html#explore-real-world-genai-use-cases",
    "href": "index.html#explore-real-world-genai-use-cases",
    "title": "Deep Generative Models",
    "section": "✨ Explore Real-World GenAI Use Cases",
    "text": "✨ Explore Real-World GenAI Use Cases\nWe are tracking real-world implementations of Generative AI across industries.\n\nBanking Use Cases →\nHealthcare Use Cases →"
  },
  {
    "objectID": "flows.html",
    "href": "flows.html",
    "title": "Normalizing Flow Models",
    "section": "",
    "text": "In generative modeling, the objective is to learn a probability distribution over data that allows us to both generate new examples and evaluate the likelihood of observed ones. For a model to be practically useful, it must support efficient sampling and enable exact or tractable likelihood computation during training.\nA Variational Autoencoder (VAE) is a type of generative model that introduces latent variables \\(z\\), allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood \\(p(x)\\) is often intractable. To address this, VAEs use variational inference to approximate the posterior \\(p(z \\mid x)\\) and optimize a surrogate objective known as the Evidence Lower Bound (ELBO). This is made possible by the reparameterization trick, which enables gradients to flow through stochastic latent variables during training.\nNormalizing flows address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point \\(x\\) is generated by applying a function \\(x = f(z)\\) to a latent variable \\(z\\) sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so \\(z\\) can be exactly recovered as \\(z = f^{-1}(x)\\). This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.\nThis structure offers several advantages. First, each \\(x\\) maps to a unique \\(z\\), eliminating the need to marginalize over latent variables as in VAEs. Second, the change-of-variables formula enables exact computation of the likelihood, rather than approximations. Third, sampling is straightforward: draw \\(z \\sim p_Z(z)\\) from the base distribution and apply the transformation \\(x = f(z)\\).\nDespite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn lower-dimensional latent representations, flows require the latent and data spaces to have equal dimensionality to preserve invertibility. This means flow-based models do not perform dimensionality reduction, which can be a disadvantage in tasks where compact representations are important.\n\n\n\nComparison of VAE and Flow-based Models\n\n\nVAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.\nTo understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We’ll begin with the 1D case and build up to the multivariate formulation."
  },
  {
    "objectID": "flows.html#introduction",
    "href": "flows.html#introduction",
    "title": "Normalizing Flow Models",
    "section": "",
    "text": "In generative modeling, the objective is to learn a probability distribution over data that allows us to both generate new examples and evaluate the likelihood of observed ones. For a model to be practically useful, it must support efficient sampling and enable exact or tractable likelihood computation during training.\nA Variational Autoencoder (VAE) is a type of generative model that introduces latent variables \\(z\\), allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood \\(p(x)\\) is often intractable. To address this, VAEs use variational inference to approximate the posterior \\(p(z \\mid x)\\) and optimize a surrogate objective known as the Evidence Lower Bound (ELBO). This is made possible by the reparameterization trick, which enables gradients to flow through stochastic latent variables during training.\nNormalizing flows address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point \\(x\\) is generated by applying a function \\(x = f(z)\\) to a latent variable \\(z\\) sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so \\(z\\) can be exactly recovered as \\(z = f^{-1}(x)\\). This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.\nThis structure offers several advantages. First, each \\(x\\) maps to a unique \\(z\\), eliminating the need to marginalize over latent variables as in VAEs. Second, the change-of-variables formula enables exact computation of the likelihood, rather than approximations. Third, sampling is straightforward: draw \\(z \\sim p_Z(z)\\) from the base distribution and apply the transformation \\(x = f(z)\\).\nDespite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn lower-dimensional latent representations, flows require the latent and data spaces to have equal dimensionality to preserve invertibility. This means flow-based models do not perform dimensionality reduction, which can be a disadvantage in tasks where compact representations are important.\n\n\n\nComparison of VAE and Flow-based Models\n\n\nVAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.\nTo understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We’ll begin with the 1D case and build up to the multivariate formulation."
  },
  {
    "objectID": "flows.html#math-review",
    "href": "flows.html#math-review",
    "title": "Normalizing Flow Models",
    "section": "Math Review",
    "text": "Math Review\nThis section builds the mathematical foundation for understanding flow models, starting with change-of-variable and extending to multivariate transformations and Jacobians.\n\nChange of Variables in 1D\nSuppose we have a random variable \\(z\\) with a known distribution \\(p_Z(z)\\), and we define a new variable:\n\\[\nx = f(z)\n\\]\nwhere \\(f\\) is a monotonic, differentiable function with an inverse:\n\\[\nz = f^{-1}(x) = h(x)\n\\]\nOur goal is to compute the probability density function (PDF) of \\(x\\), denoted \\(p_X(x)\\), in terms of the known PDF \\(p_Z(z)\\).\n\nStep 1: Cumulative Distribution Function (CDF)\nWe begin with the cumulative distribution function of \\(x\\):\n\\[\nF_X(x) = P(X \\leq x) = P(f(Z) \\leq x)\n\\]\nSince \\(f\\) is monotonic and invertible, this becomes:\n\\[\nP(f(Z) \\leq x) = P(Z \\leq f^{-1}(x)) = F_Z(h(x))\n\\]\n\n\nStep 2: Deriving the PDF via Chain Rule\nTo obtain the PDF, we differentiate the CDF:\n\\[\np_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx} F_Z(h(x))\n\\]\nApplying the chain rule:\n\\[\np_X(x) = F_Z'(h(x)) \\cdot h'(x) = p_Z(h(x)) \\cdot h'(x)\n\\]\n\n\nStep 3: Rewrite in Terms of \\(z\\)\nFrom the previous step:\n\\[\np_X(x) = p_Z(h(x)) \\cdot h'(x)\n\\]\nSince \\(z = h(x)\\), we can rewrite:\n\\[\np_X(x) = p_Z(z) \\cdot h'(x)\n\\]\nNow, using the inverse function theorem, we express \\(h'(x)\\) as:\n\\[\nh'(x) = \\frac{d}{dx} f^{-1}(x) = \\frac{1}{f'(z)}\n\\]\nSo the final expression becomes:\n\\[\np_X(x) = p_Z(z) \\cdot \\left| \\frac{1}{f'(z)} \\right|\n\\]\nThe absolute value ensures the density remains non-negative, as required for any valid probability distribution.\nThis is the fundamental concept normalizing flows use to model complex distributions by transforming simple ones.\n\n\n\nGeometry: Determinants and Volume Changes\nTo further understand the multivariate change-of-variable formula, it’s helpful to first explore how linear transformations affect volume in high-dimensional spaces.\nLet \\(\\mathbf{Z}\\) be a random vector uniformly distributed in the unit cube \\([0,1]^n\\), and let \\(\\mathbf{X} = A\\mathbf{Z}\\), where \\(A\\) is a square, invertible matrix. Geometrically, the matrix \\(A\\) maps the unit hypercube to a parallelogram in 2D or a parallelotope in higher dimensions.\nThe determinant of a square matrix tells us how the transformation scales volume. For instance, if the determinant of a \\(2 \\times 2\\) matrix is 3, applying that matrix will stretch the area of a region by a factor of 3. A negative determinant indicates a reflection, meaning the transformation also flips the orientation. When measuring volume, we care about the absolute value of the determinant.\nThe volume of the resulting parallelotope is given by:\n\\[\n\\text{Volume} = |\\det(A)|\n\\]\nThis expression tells us how much the transformation \\(A\\) scales space. For example, if \\(|\\det(A)| = 2\\), the transformation doubles the volume.\nTo make this idea concrete, consider the illustration below. The left figure shows a uniform distribution over the unit square \\([0, 1]^2\\). When we apply the linear transformation \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\), each point in the square is mapped to a new location, stretching the square into a parallelogram. The area of this parallelogram — and hence the volume scaling — is given by the absolute value of the determinant \\(|\\det(A)| = |ad - bc|\\).\n\n\n\n A linear transformation maps a unit square to a parallelogram. \n\n\nThis geometric intuition becomes essential when we apply the same logic to probability densities. The area of the parallelogram equals the absolute value of the determinant, |det(A)|, indicating how the transformation scales area.\n\n\nDeterminants and Probability Density\nPreviously, we saw how a linear transformation scales volume. Now we apply the same idea to probability densities — since density is defined per unit volume, scaling the volume also affects the density.\nTo transform the density from \\(\\mathbf{Z}\\) to \\(\\mathbf{X}\\), we use the change-of-variable formula. Since \\(\\mathbf{X} = A\\mathbf{Z}\\), the inverse transformation is \\(\\mathbf{Z} = A^{-1} \\mathbf{X}\\). This tells us how to evaluate the density at \\(\\mathbf{x}\\) by “pulling it back” through the inverse mapping. Applying the multivariate change-of-variable rule:\n\\[\np_X(\\mathbf{x}) = p_Z(W \\mathbf{x}) \\cdot \\left| \\det(W) \\right| \\quad \\text{where } W = A^{-1}\n\\]\nThis is directly analogous to the 1D change-of-variable rule:\n\\[\np_X(x) = p_Z(h(x)) \\cdot |h'(x)|\n\\]\nbut now in multiple dimensions using the determinant of the inverse transformation.\nTo make this more concrete, here’s a simple 2D example demonstrating how linear transformations affect probability density.\nLet \\(\\mathbf{Z}\\) be a random vector uniformly distributed over the unit square \\([0, 1]^2\\). Suppose we apply the transformation \\(\\mathbf{X} = A\\mathbf{Z}\\), where\n\\[\nA = \\begin{bmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\quad \\text{so that} \\quad\nW = A^{-1} =\n\\begin{bmatrix}\n\\frac{1}{2} & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\nThis transformation stretches the square horizontally, doubling its width while keeping the height unchanged. As a result, the area is doubled:\n\\[\n|\\det(A)| = 2 \\quad \\text{and} \\quad |\\det(W)| = \\frac{1}{2}\n\\] Since the same total probability must be spread over a larger area, the density decreases, meaning the probability per unit area is reduced due to the increased area over which the same total probability is distributed.\nNow, let’s say \\(p_Z(z) = 1\\) inside the unit square (a uniform distribution). To compute \\(p_X(\\mathbf{x})\\) at a point \\(\\mathbf{x}\\) in the transformed space, we use:\n\\[\np_X(\\mathbf{x}) = p_Z(W\\mathbf{x}) \\cdot |\\det(W)| = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}\n\\]\nSo, the transformed density is halved — the same total probability (which must remain 1) is now spread over an area that is twice as large.\n\n\nGeneralizing to Nonlinear Transformations\nFor nonlinear transformations \\(\\mathbf{x} = f(\\mathbf{z})\\), the idea is similar. But instead of a constant matrix \\(A\\), we now consider the Jacobian matrix of the function \\(f\\):\n\\[\nJ_f(\\mathbf{z}) = \\frac{\\partial f}{\\partial \\mathbf{z}}\n\\]\nThe Jacobian matrix generalizes derivatives to multivariable functions, capturing how a transformation scales and rotates space locally through all partial derivatives. Its determinant tells us how much the transformation stretches or compresses space — acting as a local volume scaling factor.\n\n\nMultivariate Change-of-Variable\nGiven an invertible transformation \\(\\mathbf{x} = f(\\mathbf{z})\\), the probability density transforms as:\n\\[\np_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\cdot \\left| \\det \\left( \\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nAlternatively, in the forward form (often used during training):\n\\[\np_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\cdot \\left| \\det \\left( \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}} \\right) \\right|^{-1}\n\\]\nThis generalizes the 1D rule and enables us to compute exact likelihoods for complex distributions as long as the transformation is invertible and differentiable. This formula is pivotal in machine learning, where transformations of probability distributions are common — such as in the implementation of normalizing flows for generative modeling."
  },
  {
    "objectID": "flows.html#flow-model",
    "href": "flows.html#flow-model",
    "title": "Normalizing Flow Models",
    "section": "Flow Model",
    "text": "Flow Model\nA normalizing flow model defines a one-to-one and reversible transformation between observed variables \\(\\mathbf{x}\\) and latent variables \\(\\mathbf{z}\\). This transformation is given by an invertible, differentiable function \\(f_\\theta\\), parameterized by \\(\\theta\\):\n\\[\n\\mathbf{x} = f_\\theta(\\mathbf{z}) \\quad \\text{and} \\quad \\mathbf{z} = f_\\theta^{-1}(\\mathbf{x})\n\\]\n\n\n\nFlow model showing forward and inverse transformations\n\n\nFigure: A flow-based model uses a forward transformation \\(f_\\theta\\) to map from latent variables (\\(\\mathbf{z}\\)) to data (\\(\\mathbf{x}\\)), and an inverse transformation \\(f_\\theta^{-1}\\) to compute likelihoods. Adapted from class notes (XCS236, Stanford).\nBecause the transformation is invertible, we can apply the change-of-variable formula to compute the exact probability of \\(\\mathbf{x}\\):\n\\[\np_X(\\mathbf{x}; \\theta) = p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nThis makes it possible to evaluate exact likelihoods and learn the model via maximum likelihood estimation (MLE).\n\nNote: Both \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) must be continuous and have the same dimensionality since the transformation must be invertible.\n\n\nModel Architecture: A Sequence of Invertible Transformations\nThe term flow refers to the fact that we can compose multiple invertible functions to form a more expressive transformation:\n\\[\n\\mathbf{z}_m = f_\\theta^{(m)} \\circ f_\\theta^{(m-1)} \\circ \\cdots \\circ f_\\theta^{(1)}(\\mathbf{z}_0)\n\\]\nIn this setup:\n\n\\(\\mathbf{z}_0 \\sim p_Z\\) is sampled from a simple base distribution (e.g., standard Gaussian)\n\\(\\mathbf{x} = \\mathbf{z}_M\\) is the final transformed variable\nThe full transformation \\(f_\\theta\\) is the composition of \\(M\\) sequential invertible functions. Each function slightly reshapes the distribution, and together they produce a highly expressive mapping from a simple base distribution to a complex one.\n\nThe visuals below illustrate this idea from two angles. The first diagram illustrates the structure of a normalizing flow as a composition of invertible steps, while the second shows how this architecture reshapes simple distributions into complex ones through repeated transformations.\n\n\n\n Adapted from Wikipedia: Mapping simple distributions to complex ones via invertible transformations. \n\n\n\n\n\n Adapted from class notes (XCS236, Stanford), originally based on Rezende & Mohamed, 2016. \n\n\nThe density of \\(\\mathbf{x}\\) is given by the change-of-variable formula:\n\\[\np_X(\\mathbf{x}; \\theta) = p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot \\prod_{m=1}^M \\left| \\det \\left( \\frac{\\partial (f_\\theta^{(m)})^{-1}(\\mathbf{z}_m)}{\\partial \\mathbf{z}_m} \\right) \\right|\n\\]\nThis approach allows the model to approximate highly complex distributions using simple building blocks."
  },
  {
    "objectID": "flows.html#learning-and-inference",
    "href": "flows.html#learning-and-inference",
    "title": "Normalizing Flow Models",
    "section": "Learning and Inference",
    "text": "Learning and Inference\nTraining a flow-based model is done by maximizing the log-likelihood over the dataset \\(\\mathcal{D}\\):\n\\[\n\\max_\\theta \\log p_X(\\mathcal{D}; \\theta) = \\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p_Z(f_\\theta^{-1}(\\mathbf{x})) + \\log \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nKey advantages of normalizing flows:\n\nExact likelihoods: No approximation needed — just apply the change-of-variable rule\nEfficient sampling: Generate new data by drawing \\(\\mathbf{z} \\sim p_Z\\) and computing \\(\\mathbf{x} = f_\\theta(\\mathbf{z})\\)\nLatent inference: Invert \\(f_\\theta\\) to compute latent codes \\(\\mathbf{z} = f_\\theta^{-1}(\\mathbf{x})\\), without needing a separate encoder\n\n\nComputational Considerations\nOne challenge in training normalizing flow models is that computing the exact likelihood requires evaluating the determinant of the Jacobian matrix of the transformation:\n\nFor a transformation \\(f : \\mathbb{R}^n \\to \\mathbb{R}^n\\), the Jacobian is an \\(n \\times n\\) matrix.\nComputing its determinant has a cost of \\(\\mathcal{O}(n^3)\\), which is computationally expensive during training — especially in high dimensions.\n\n\nKey Insight\nTo make normalizing flows scalable, we design transformations where the Jacobian has a special structure that makes the determinant easy to compute.\nFor example: - If the Jacobian is a triangular matrix, the determinant is just the product of the diagonal entries, which can be computed in \\(\\mathcal{O}(n)\\) time. - This works because in a triangular matrix, all the off-diagonal elements are zero — so the determinant simplifies significantly.\nIn practice, flow models like RealNVP and MAF are designed so that each output dimension \\(x_i\\) depends only on some subset of the input dimensions \\(z_{\\leq i}\\) (for lower triangular structure) or \\(z_{\\geq i}\\) (for upper triangular structure). This results in a Jacobian of the form:\n\\[\nJ = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial z_1} & 0 & \\cdots & 0 \\\\\n\\ast & \\frac{\\partial f_2}{\\partial z_2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\ast & \\ast & \\cdots & \\frac{\\partial f_n}{\\partial z_n}\n\\end{pmatrix}\n\\]\nBecause of this triangular structure, computing the determinant becomes as simple as multiplying the diagonal terms:\n\\[\n\\det(J) = \\prod_{i=1}^{n} \\frac{\\partial f_i}{\\partial z_i}\n\\]\nThis is why many modern flow models rely on coupling layers or autoregressive masking: they preserve invertibility and enable efficient, exact likelihood computation."
  },
  {
    "objectID": "flows.html#types-of-flow-architectures",
    "href": "flows.html#types-of-flow-architectures",
    "title": "Normalizing Flow Models",
    "section": "Types of Flow Architectures",
    "text": "Types of Flow Architectures\nThis section introduces common architectural families used in normalizing flows, highlighting their core ideas, strengths, and limitations.\n\n1. Elementwise Flows\n\nIdea: Apply a simple invertible function to each variable independently.\nExamples: Leaky ReLU, Softplus, ELU.\nStrengths: Extremely fast; easy to implement; analytically tractable.\nLimitations: Cannot model interactions or dependencies between variables.\n\n\n\n2. Linear Flows\n\nIdea: Apply a linear transformation using an invertible matrix (e.g., permutation, rotation, LU decomposition).\nExamples: Glow’s 1x1 Convolution, LU flows.\nStrengths: Efficiently models global dependencies; can be used to permute variables.\nLimitations: Limited expressiveness when used alone.\n\n\n\n3. Coupling Flows\n\nIdea: Split the input into two parts. One half remains unchanged while the other is transformed based on it.\nExamples: NICE (additive), RealNVP (affine).\nStrengths: Easy to invert and compute Jacobians; scalable to high dimensions.\nLimitations: Requires stacking multiple layers to mix information across all dimensions.\n\n\n\n4. Autoregressive Flows\n\nIdea: Model the transformation of each variable conditioned on the previous ones in a fixed order.\nExamples: Masked Autoregressive Flow (MAF), Inverse Autoregressive Flow (IAF).\nStrengths: Highly expressive; models arbitrary dependencies.\nLimitations: Slower sampling or density evaluation depending on flow direction.\n\n\n\n5. Residual Flows\n\nIdea: Add residual connections while enforcing invertibility (e.g., using constraints on Jacobian eigenvalues).\nExamples: Planar flows, Radial flows, Residual Flows (Behrmann et al.).\nStrengths: Flexible and capable of complex transformations.\nLimitations: May require care to ensure invertibility; harder to train.\n\n\n\n6. Continuous Flows\n\nIdea: Model the transformation as the solution to a differential equation parameterized by a neural network.\nExamples: Neural ODEs, FFJORD.\nStrengths: Highly flexible; enables continuous-time modeling.\nLimitations: Computationally expensive; uses ODE solvers during training and inference.\n\n\nThese architectures can be mixed and matched in real-world models to balance expressiveness, efficiency, and tractability. Each comes with trade-offs, and their selection often depends on the task and data at hand.\nIn the rest of this article, we focus on Coupling Flows, briefly introducing NICE and then diving deeper into the structure, intuition, and implementation details of RealNVP.\n\n\nNICE: Nonlinear Independent Components Estimation\nThe NICE (Nonlinear Independent Components Estimation) model, introduced by Laurent Dinh, David Krueger, and Yoshua Bengio in 2014, is a foundational work in the development of normalizing flows.\nIt provides a framework for transforming complex high-dimensional data into a simpler latent space (often a standard Gaussian), enabling both exact likelihood estimation and sampling — two fundamental goals in generative modeling.\n\nCore Concepts\n\nInvertible Transformations:\nNICE constructs a chain of invertible functions to map inputs to latent variables. This ensures that both the forward and inverse transformations are tractable.\nAdditive Coupling Layers:\nThe model partitions the input into two parts and applies an additive transformation to one part using a function of the other. This design yields a triangular Jacobian with determinant 1, making log-likelihood computation efficient.\nVolume-Preserving Mapping:\nBecause additive coupling layers do not scale the space, NICE preserves volume — i.e., the Jacobian determinant is exactly 1. While this limits expressiveness, it simplifies training and inference.\nScaling Layer (Optional):\nThe original NICE paper includes an optional scaling layer at the end to allow some volume change per dimension.\nExact Log-Likelihood:\nUnlike VAEs or GANs, which rely on approximations, NICE enables exact evaluation of the log-likelihood, making it a fully probabilistic, likelihood-based model.\n\n\n\nAdditive Coupling Layer\nTo make the transformation invertible and computationally efficient, NICE splits the input vector into two parts. One part is kept unchanged, while the other part is modified using a function of the unchanged part. This way, we can easily reverse the process because we always know what was kept intact.\nLet’s partition the input \\(\\mathbf{z} \\in \\mathbb{R}^n\\) into two subsets: \\(\\mathbf{z}_{1:d}\\) and \\(\\mathbf{z}_{d+1:n}\\) for some \\(1 \\leq d &lt; n\\).\n\nForward Mapping \\(\\mathbf{z} \\mapsto \\mathbf{x}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)} \\\\\n\\mathbf{x}_{d+1:n} &= \\mathbf{z}_{d+1:n} + m_\\theta(\\mathbf{z}_{1:d})\n\\end{aligned}\n\\]\nwhere \\(m_\\theta(\\cdot)\\) is a neural network with parameters \\(\\theta\\), \\(d\\) input units, and \\(n - d\\) output units.\n\nInverse Mapping \\(\\mathbf{x} \\mapsto \\mathbf{z}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)} \\\\\n\\mathbf{z}_{d+1:n} &= \\mathbf{x}_{d+1:n} - m_\\theta(\\mathbf{x}_{1:d})\n\\end{aligned}\n\\]\n\nJacobian of the forward mapping:\n\nThe Jacobian matrix captures how much the transformation stretches or compresses space. Because the unchanged subset passes through as-is and the transformation is purely additive (no scaling), the Jacobian is triangular with 1s on the diagonal — so its determinant is 1 — meaning the transformation preserves volume.\n\\[\nJ = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\n\\frac{\\partial m_\\theta}{\\partial \\mathbf{z}_{1:d}} & I_{n-d}\n\\end{pmatrix}\n\\]\n\\[\n\\det(J) = 1\n\\]\nHence, additive coupling is a volume-preserving transformation.\n\n\nRescaling Layer\nTo overcome the limitation of fixed volume, NICE adds a diagonal scaling transformation at the end, allowing the model to contract or expand space. This is not part of the coupling layers, but is crucial to increase flexibility.\n\nForward Mapping:\n\n\\[\nx_i = s_i z_i \\quad \\text{with} \\quad s_i &gt; 0\n\\]\n\nInverse Mapping:\n\n\\[\nz_i = \\frac{x_i}{s_i}\n\\]\n\nJacobian:\n\n\\[\nJ = \\text{diag}(\\mathbf{s})\n\\quad \\Rightarrow \\quad\n\\det(J) = \\prod_{i=1}^n s_i\n\\]\nHowever, the volume-preserving property of NICE limits its expressiveness. RealNVP extends this idea by introducing affine coupling layers that enable volume changes during transformation.\n\n\n\nReal-NVP: Non-Volume Preserving Extension of NICE\nReal-NVP (Dinh et al., 2017) extends NICE by introducing a scaling function that allows the model to change volume, enabling more expressive transformations. This is achieved using affine coupling layers that apply learned scaling and translation functions to part of the input while keeping the rest unchanged.\n\n\n\n Visualization of a single affine coupling layer in RealNVP. The identity path and affine transform structure allow exact inversion and efficient computation. \n\n\nWe partition the input \\(\\mathbf{z} \\in \\mathbb{R}^n\\) into two subsets: \\(\\mathbf{z}_{1:d}\\) and \\(\\mathbf{z}_{d+1:n}\\).\n\nForward Mapping \\(\\mathbf{z} \\mapsto \\mathbf{x}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)} \\\\\\\\\n\\mathbf{x}_{d+1:n} &= \\mathbf{z}_{d+1:n} \\odot \\exp(\\alpha_\\theta(\\mathbf{z}_{1:d})) + \\mu_\\theta(\\mathbf{z}_{1:d})\n\\end{aligned}\n\\]\nHere, \\(\\boldsymbol{\\alpha}_\\theta(\\cdot)\\) and \\(\\boldsymbol{\\mu}_\\theta(\\cdot)\\) are neural networks with parameters \\(\\theta\\) that take the unchanged subset \\(\\mathbf{z}_{1:d}\\) as input and produce scale and shift parameters, respectively, for the transformed subset \\(\\mathbf{z}_{d+1:n}\\). These functions enable flexible, learnable affine transformations while preserving invertibility.\n\nInverse Mapping \\(\\mathbf{x} \\mapsto \\mathbf{z}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)} \\\\\\\\\n\\mathbf{z}_{d+1:n} &= \\left( \\mathbf{x}_{d+1:n} - \\mu_\\theta(\\mathbf{x}_{1:d}) \\right) \\odot \\exp(-\\alpha_\\theta(\\mathbf{x}_{1:d}))\n\\end{aligned}\n\\]\nThe inverse mapping recovers the latent variable \\(\\mathbf{z}\\) from the data \\(\\mathbf{x}\\). The first subset \\(\\mathbf{x}_{1:d}\\) remains unchanged and directly becomes \\(\\mathbf{z}_{1:d}\\). To reconstruct \\(\\mathbf{z}_{d+1:n}\\), we first subtract the shift \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_{1:d})\\) from \\(\\mathbf{x}_{d+1:n}\\), and then apply an elementwise rescaling using \\(\\exp(-\\boldsymbol{\\alpha}_\\theta(\\mathbf{x}_{1:d}))\\). This inversion relies on the same neural networks used in the forward pass and ensures that the transformation is exactly reversible.\n\nJacobian of Forward Mapping:\n\n\\[\nJ = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\\\\\n\\frac{\\partial \\mathbf{x}_{d+1:n}}{\\partial \\mathbf{z}_{1:d}} & \\operatorname{diag}\\left(\\exp(\\alpha_\\theta(\\mathbf{z}_{1:d}))\\right)\n\\end{pmatrix}\n\\]\nThe Jacobian matrix of the RealNVP forward transformation has a special block structure due to the design of the affine coupling layer:\n\nUpper left block: \\(\\mathbf{I}_d\\)\nThis corresponds to the partial derivatives of \\(\\mathbf{x}_{1:d}\\) with respect to \\(\\mathbf{z}_{1:d}\\). Since the first \\(d\\) variables are passed through unchanged (\\(\\mathbf{x}_{1:d} = \\mathbf{z}_{1:d}\\)), their derivatives form an identity matrix.\nUpper right block: \\(0\\)\nThese derivatives are zero because \\(\\mathbf{x}_{1:d}\\) does not depend on \\(\\mathbf{z}_{d+1:n}\\) at all — they’re completely decoupled.\nLower right block: (diagonal)\nEach element of \\(\\mathbf{x}_{d+1:n}\\) is scaled elementwise by \\(\\exp\\left(\\left[\\alpha_\\theta(\\mathbf{z}_{1:d})\\right]_i\\right)\\). This means the Jacobian of this part is a diagonal matrix, where each diagonal entry is the corresponding scale factor.\nLower left block:\nThis part can contain non-zero values because \\(\\mathbf{x}_{d+1:n}\\) depends on \\(\\mathbf{z}_{1:d}\\) via the neural networks. But thanks to the triangular structure of the Jacobian, we don’t need this block to compute the determinant.\n\n\n\n\n Jacobian of the RealNVP forward transformation. Upper triangular structure arises because the first subset is unchanged, while the second is scaled and shifted based on the first. \n\n\n\nWhy This Structure Matters\nBecause the Jacobian is triangular, its determinant is simply the product of the diagonal entries.\n\\[\n\\det(J) = \\prod_{i=d+1}^{n} \\exp\\left( \\alpha_\\theta(\\mathbf{z}_{1:d})_i \\right)\n= \\exp\\left( \\sum_{i=d+1}^{n} \\alpha_\\theta(\\mathbf{z}_{1:d})_i \\right)\n\\]\nIn log-space, this becomes a sum:\n\\[\n\\log \\det(J) = \\sum_{i=d+1}^{n} \\alpha_\\theta(\\mathbf{z}_{1:d})_i\n\\]\nThis makes the computation of log-likelihoods fast and tractable.\nTaking the product of the diagonal entries gives us a measure of how much the transformation expands or contracts local volume. If the determinant is greater than 1, the transformation expands space; if it’s less than 1, it contracts space. Since the determinant is not fixed, RealNVP performs a non-volume preserving transformation — allowing it to model more complex distributions than NICE, which preserves volume by design.\n\n\nStacking Coupling Layers\nEach coupling layer only transforms part of the input. To ensure that every dimension is eventually updated, RealNVP stacks multiple coupling layers and alternates the masking pattern between them.\n\nIn one layer, the first half is fixed, and the second half is transformed.\nIn the next layer, the roles are reversed.\n\nThis alternating structure ensures: - All input dimensions are updated across layers - The full transformation remains invertible - The total log-determinant is the sum of the log-determinants of each layer\n\n\nRealNVP in Action (Two Moons)\nThe following plots illustrate how RealNVP transforms data in practice:\n\n\n\n Top-left: Original two-moons data (X)\nTop-right: Encoded latent space (Z) Bottom-left: Latent samples from base distribution\nBottom-right: Generated samples mapped back to (X) space\n\n\n\n\n\nSummary\nTo recap the key distinctions between NICE and RealNVP, here’s a side-by-side comparison:\n\n\n\n\n\n\n\n\nAspect\nNICE\nRealNVP\n\n\n\n\nType of coupling\nAdditive\nAffine (scaling + shift)\n\n\nVolume change\nOnly possible with rescaling layer\nBuilt into each coupling layer\n\n\nJacobian determinant\n1 (in coupling layers)\nVaries (depends on learned scale)\n\n\nExpressiveness\nLimited (volume-preserving layers)\nHigher (learns scale & shift)\n\n\nLog-likelihood\nExact\nExact"
  },
  {
    "objectID": "flows.html#try-it-yourself-flow-model-in-pytorch",
    "href": "flows.html#try-it-yourself-flow-model-in-pytorch",
    "title": "Normalizing Flow Models",
    "section": "🧪 Try It Yourself: Flow Model in Pytorch",
    "text": "🧪 Try It Yourself: Flow Model in Pytorch\nYou can explore a minimal PyTorch implementation of a normalizing flow model:\n\n📘 View Notebook on GitHub\n🚀 Run in Google Colab"
  },
  {
    "objectID": "flows.html#references",
    "href": "flows.html#references",
    "title": "Normalizing Flow Models",
    "section": "References",
    "text": "References\n[1] Stanford CS236 Notes. “Normalizing Flows”\n[2] UT Austin Calculus Notes. “Jacobian and Change of Variables”\n[3] Danilo Jimenez Rezende, and Shakir Mohamed. “Variational Inference with Normalizing Flows”\n[4] Kobyzev, Prince, and Brubaker. “Normalizing Flows: An Introduction and Review of Current Methods”\n[5] Wikipedia. “Normalizing Flow”"
  },
  {
    "objectID": "flows.html#further-reading",
    "href": "flows.html#further-reading",
    "title": "Normalizing Flow Models",
    "section": "Further Reading",
    "text": "Further Reading\n[1] George Papamakarios et al. “Normalizing Flows for Probabilistic Modeling and Inference”\n[2] Lilian Weng. “Flow-based Models”\n[3] Eric Jang. “Normalizing Flows Tutorial – Part 1”\n[4] Eric Jang. “Normalizing Flows Tutorial – Part 2”"
  },
  {
    "objectID": "normalizing_flow_pytorch.html",
    "href": "normalizing_flow_pytorch.html",
    "title": "RealNVP Model in PyTorch",
    "section": "",
    "text": "This notebook provides a simple implementation of a RealNVP flow model.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nfrom torch.distributions import MultivariateNormal\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nUsing device: cuda\n\n\n\nCreate a Normalize the Two Moons Dataset\n\n# Generate the two moons dataset\nn_samples = 30000\ndata, _ = make_moons(n_samples=n_samples, noise=0.05)\ndata = data.astype(np.float32)\n\n# Normalize the dataset\nscaler = StandardScaler()\nnormalized_data = scaler.fit_transform(data)\n\n# Convert to PyTorch tensor and move to device\nnormalized_data = torch.tensor(normalized_data, dtype=torch.float32).to(device)\n\n# Visualize the data\nplt.figure(figsize=(6, 6))\nplt.scatter(normalized_data[:, 0].cpu(), normalized_data[:, 1].cpu(), s=1, c='green')\nplt.title(\"Normalized Two Moons Dataset\")\nplt.axis(\"equal\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nclass CouplingMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n\n        # Scale output: initialized near zero with tanh for stability\n        self.scale = nn.Linear(hidden_dim, input_dim)\n        self.translate = nn.Linear(hidden_dim, input_dim)\n\n        # Apply Xavier initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        h = self.net(x)\n        s = torch.tanh(self.scale(h)) * 2.0  # bounded scaling\n        t = self.translate(h)\n        return s, t\n\n\nclass RealNVP(nn.Module):\n    def __init__(self, input_dim=2, num_coupling_layers=6):\n        super().__init__()\n        self.input_dim = input_dim\n        self.num_coupling_layers = num_coupling_layers\n\n        # Prior distribution in latent space\n        self.prior = torch.distributions.MultivariateNormal(\n            torch.zeros(input_dim).to(device),\n            torch.eye(input_dim).to(device)\n        )\n\n        # Alternating binary masks, e.g., [0,1], [1,0], ...\n        self.masks = torch.tensor(\n            [[0, 1], [1, 0]] * (num_coupling_layers // 2),\n            dtype=torch.float32\n        ).to(device)\n\n        # Create coupling layers\n        self.coupling_layers = nn.ModuleList([\n            CouplingMLP(input_dim=input_dim) for _ in range(num_coupling_layers)\n        ])\n\n    def forward(self, x, reverse=False):\n        \"\"\"\n        If reverse=False: maps data → latent (x → z)\n        If reverse=True:  maps latent → data (z → x)\n        \"\"\"\n        log_det = torch.zeros(x.shape[0], device=x.device)\n        direction = -1 if not reverse else 1\n        layers = range(self.num_coupling_layers)[::direction]\n\n        for i in layers:\n            mask = self.masks[i]\n            x_masked = x * mask\n            s, t = self.coupling_layers[i](x_masked)\n\n            s = s * (1 - mask)\n            t = t * (1 - mask)\n\n            if reverse:\n                x = (x - t) * torch.exp(-s) * (1 - mask) + x_masked\n            else:\n                x = (x * torch.exp(s) + t) * (1 - mask) + x_masked\n                log_det += torch.sum(s, dim=1)\n\n        return x, log_det\n\n    def log_prob(self, x):\n        \"\"\"\n        Log probability of input x under the model.\n        \"\"\"\n        z, log_det = self.forward(x, reverse=False)\n        log_prob_z = self.prior.log_prob(z)\n        return log_prob_z + log_det\n\n    def loss(self, x):\n        \"\"\"\n        Negative log-likelihood loss.\n        \"\"\"\n        return -self.log_prob(x).mean()\n\n\nbatch_size = 256\nepochs = 100\nlearning_rate = 1e-4\n\n\ndataset = TensorDataset(normalized_data)\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n\nmodel = RealNVP(input_dim=2, num_coupling_layers=6).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n\nlosses = []\n\nfor epoch in range(epochs):\n    epoch_loss = 0.0\n    model.train()\n\n    for batch in train_loader:\n        x = batch[0].to(device)\n\n        optimizer.zero_grad()\n        loss = model.loss(x)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * x.size(0)\n\n    avg_loss = epoch_loss / len(train_loader.dataset)\n    losses.append(avg_loss)\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n\nEpoch [10/100], Loss: 1.3349\nEpoch [20/100], Loss: 1.2672\nEpoch [30/100], Loss: 1.2408\nEpoch [40/100], Loss: 1.2296\nEpoch [50/100], Loss: 1.2191\nEpoch [60/100], Loss: 1.2145\nEpoch [70/100], Loss: 1.1950\nEpoch [80/100], Loss: 1.1911\nEpoch [90/100], Loss: 1.1978\nEpoch [100/100], Loss: 1.1867\n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(losses, label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Negative Log-Likelihood\")\nplt.title(\"RealNVP Training Loss\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    z, _ = model(normalized_data, reverse=False)\n\n\nwith torch.no_grad():\n    z_samples = model.prior.sample((3000,))\n    x_samples, _ = model(z_samples, reverse=True)\n\n\nf, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Original data\naxes[0, 0].scatter(normalized_data[:, 0].cpu(), normalized_data[:, 1].cpu(), s=1, c='green')\naxes[0, 0].set_title(\"Original Data (X)\")\naxes[0, 0].axis(\"equal\")\n\n# Latent representation of data\naxes[0, 1].scatter(z[:, 0].cpu(), z[:, 1].cpu(), s=1, c='blue')\naxes[0, 1].set_title(\"Encoded Latent Space (Z)\")\naxes[0, 1].axis(\"equal\")\n\n# Random latent samples\naxes[1, 0].scatter(z_samples[:, 0].cpu(), z_samples[:, 1].cpu(), s=1, c='orange')\naxes[1, 0].set_title(\"Sampled Latent Z\")\naxes[1, 0].axis(\"equal\")\n\n# Transformed back to data\naxes[1, 1].scatter(x_samples[:, 0].cpu(), x_samples[:, 1].cpu(), s=1, c='red')\naxes[1, 1].set_title(\"Generated Samples (X)\")\naxes[1, 1].axis(\"equal\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html",
    "href": "gen-ai-use-cases/healthcare-use-cases.html",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "",
    "text": "Generative AI is accelerating transformation in healthcare by automating documentation, enhancing diagnostic imaging, improving drug discovery, and supporting clinical and administrative workflows. Below are key use cases currently being deployed by healthcare systems and life sciences firms."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#summary-of-use-cases",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#summary-of-use-cases",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "1 📋 Summary of Use Cases",
    "text": "1 📋 Summary of Use Cases\n\n\n\n\n\n\n\n\nUse Case Area\nExamples\nBenefits\n\n\n\n\nClinical Documentation\nNuance DAX, AWS HealthScribe\nReduced burden, faster note capture\n\n\nMedical Imaging & Diagnostics\nDeepMind, MIT CSAIL\nEnhanced resolution, rare disease training\n\n\nDrug Discovery & Molecule Design\nInsilico, BenevolentAI\nFaster R&D cycles, AI-generated molecules\n\n\nPatient Communication\nMayo Copilot, Ada Health\nSimplified education, automated triage\n\n\nAdmin Workflow Automation\nMedLM, Abridge\nProcess improvement, reduced manual work"
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#clinical-documentation",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#clinical-documentation",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "2 🩺 Clinical Documentation",
    "text": "2 🩺 Clinical Documentation\n\n\n\n\n\n\nWhy it matters:\nGenerative AI is significantly reducing the administrative burden on clinicians by automating the creation of clinical notes and summaries. AI tools can transcribe doctor-patient conversations, extract relevant medical details, and generate structured notes in real time or post-visit.\n\n\n\nExamples:\n- Nuance DAX Copilot (Microsoft): Automatically generates clinical notes during consultations, reducing documentation time for physicians.\n- AWS HealthScribe: Extracts structured data and generates medical summaries from recorded conversations between patients and providers."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#medical-imaging-and-diagnostics",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#medical-imaging-and-diagnostics",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "3 🧠 Medical Imaging and Diagnostics",
    "text": "3 🧠 Medical Imaging and Diagnostics\n\n\n\n\n\n\nWhy it matters:\nGenerative models such as GANs and diffusion are being used to enhance medical images, synthesize missing views, and even generate realistic training data for rare conditions. These techniques improve diagnostic quality and accelerate model development.\n\n\n\nExamples:\n- Google DeepMind: Uses generative models to enhance resolution in retinal scans and chest X-rays.\n- MIT CSAIL: Trains diagnostic AI systems using synthetic images generated via diffusion models, improving accuracy for rare diseases."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#drug-discovery-and-molecular-design",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#drug-discovery-and-molecular-design",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "4 💊 Drug Discovery and Molecular Design",
    "text": "4 💊 Drug Discovery and Molecular Design\n\n\n\n\n\n\nWhy it matters:\nPharmaceutical companies and AI research labs are using GenAI to generate novel molecular structures that satisfy biological and chemical constraints. This reduces the time and cost of early-stage drug discovery.\n\n\n\nExamples:\n- Insilico Medicine: Uses generative chemistry platforms to propose new drug candidates and recently advanced one into Phase II trials.\n- BenevolentAI: Uses AI models to generate molecules with desired properties based on disease pathways and target proteins."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#patient-communication-and-education",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#patient-communication-and-education",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "5 🗣️ Patient Communication and Education",
    "text": "5 🗣️ Patient Communication and Education\n\n\n\n\n\n\nWhy it matters:\nHealthcare providers are using GenAI to generate personalized educational materials, explain complex medical conditions in plain language, and respond to patient questions through virtual assistants.\n\n\n\nExamples:\n- Mayo Clinic and Microsoft Copilot: Exploring AI to generate patient-friendly summaries of lab results and discharge instructions.\n- Ada Health: Uses generative agents to guide patients through symptom checking and triage using everyday language."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#administrative-workflow-automation",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#administrative-workflow-automation",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "6 🏥 Administrative Workflow Automation",
    "text": "6 🏥 Administrative Workflow Automation\n\n\n\n\n\n\nWhy it matters:\nHospitals and health systems are adopting GenAI to streamline operational processes such as prior authorization, billing, appeals, and scheduling. This improves staff productivity and reduces manual paperwork.\n\n\n\nExamples:\n- Google MedLM: Supports automation of prior authorization workflows and claim denials through natural language understanding and document generation.\n- Abridge: Not only transcribes medical conversations, but also classifies, summarizes, and codes them for billing and documentation purposes."
  },
  {
    "objectID": "gen-ai-use-cases/healthcare-use-cases.html#key-takeaways",
    "href": "gen-ai-use-cases/healthcare-use-cases.html#key-takeaways",
    "title": "Generative AI Use Cases in Healthcare",
    "section": "7 🧾 Key Takeaways",
    "text": "7 🧾 Key Takeaways\n\nGenAI in healthcare is moving from pilots to production, especially in clinical documentation and imaging.\nHospitals, insurers, and biotech firms are exploring AI-powered assistants to streamline manual workflows and improve decision support.\nPatient-facing use cases like education and triage are opening the door for more accessible, scalable care delivery."
  }
]