[
  {
    "objectID": "normalizing_flow_pytorch.html",
    "href": "normalizing_flow_pytorch.html",
    "title": "RealNVP Model in PyTorch",
    "section": "",
    "text": "This notebook provides a simple implementation of a RealNVP flow model.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nfrom torch.distributions import MultivariateNormal\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nUsing device: cuda\n\n\n\nCreate a Normalize the Two Moons Dataset\n\n# Generate the two moons dataset\nn_samples = 30000\ndata, _ = make_moons(n_samples=n_samples, noise=0.05)\ndata = data.astype(np.float32)\n\n# Normalize the dataset\nscaler = StandardScaler()\nnormalized_data = scaler.fit_transform(data)\n\n# Convert to PyTorch tensor and move to device\nnormalized_data = torch.tensor(normalized_data, dtype=torch.float32).to(device)\n\n# Visualize the data\nplt.figure(figsize=(6, 6))\nplt.scatter(normalized_data[:, 0].cpu(), normalized_data[:, 1].cpu(), s=1, c='green')\nplt.title(\"Normalized Two Moons Dataset\")\nplt.axis(\"equal\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nclass CouplingMLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim=256):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n\n        # Scale output: initialized near zero with tanh for stability\n        self.scale = nn.Linear(hidden_dim, input_dim)\n        self.translate = nn.Linear(hidden_dim, input_dim)\n\n        # Apply Xavier initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        h = self.net(x)\n        s = torch.tanh(self.scale(h)) * 2.0  # bounded scaling\n        t = self.translate(h)\n        return s, t\n\n\nclass RealNVP(nn.Module):\n    def __init__(self, input_dim=2, num_coupling_layers=6):\n        super().__init__()\n        self.input_dim = input_dim\n        self.num_coupling_layers = num_coupling_layers\n\n        # Prior distribution in latent space\n        self.prior = torch.distributions.MultivariateNormal(\n            torch.zeros(input_dim).to(device),\n            torch.eye(input_dim).to(device)\n        )\n\n        # Alternating binary masks, e.g., [0,1], [1,0], ...\n        self.masks = torch.tensor(\n            [[0, 1], [1, 0]] * (num_coupling_layers // 2),\n            dtype=torch.float32\n        ).to(device)\n\n        # Create coupling layers\n        self.coupling_layers = nn.ModuleList([\n            CouplingMLP(input_dim=input_dim) for _ in range(num_coupling_layers)\n        ])\n\n    def forward(self, x, reverse=False):\n        \"\"\"\n        If reverse=False: maps data → latent (x → z)\n        If reverse=True:  maps latent → data (z → x)\n        \"\"\"\n        log_det = torch.zeros(x.shape[0], device=x.device)\n        direction = -1 if not reverse else 1\n        layers = range(self.num_coupling_layers)[::direction]\n\n        for i in layers:\n            mask = self.masks[i]\n            x_masked = x * mask\n            s, t = self.coupling_layers[i](x_masked)\n\n            s = s * (1 - mask)\n            t = t * (1 - mask)\n\n            if reverse:\n                x = (x - t) * torch.exp(-s) * (1 - mask) + x_masked\n            else:\n                x = (x * torch.exp(s) + t) * (1 - mask) + x_masked\n                log_det += torch.sum(s, dim=1)\n\n        return x, log_det\n\n    def log_prob(self, x):\n        \"\"\"\n        Log probability of input x under the model.\n        \"\"\"\n        z, log_det = self.forward(x, reverse=False)\n        log_prob_z = self.prior.log_prob(z)\n        return log_prob_z + log_det\n\n    def loss(self, x):\n        \"\"\"\n        Negative log-likelihood loss.\n        \"\"\"\n        return -self.log_prob(x).mean()\n\n\nbatch_size = 256\nepochs = 100\nlearning_rate = 1e-4\n\n\ndataset = TensorDataset(normalized_data)\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n\nmodel = RealNVP(input_dim=2, num_coupling_layers=6).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n\nlosses = []\n\nfor epoch in range(epochs):\n    epoch_loss = 0.0\n    model.train()\n\n    for batch in train_loader:\n        x = batch[0].to(device)\n\n        optimizer.zero_grad()\n        loss = model.loss(x)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item() * x.size(0)\n\n    avg_loss = epoch_loss / len(train_loader.dataset)\n    losses.append(avg_loss)\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n\nEpoch [10/100], Loss: 1.3349\nEpoch [20/100], Loss: 1.2672\nEpoch [30/100], Loss: 1.2408\nEpoch [40/100], Loss: 1.2296\nEpoch [50/100], Loss: 1.2191\nEpoch [60/100], Loss: 1.2145\nEpoch [70/100], Loss: 1.1950\nEpoch [80/100], Loss: 1.1911\nEpoch [90/100], Loss: 1.1978\nEpoch [100/100], Loss: 1.1867\n\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(losses, label=\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Negative Log-Likelihood\")\nplt.title(\"RealNVP Training Loss\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    z, _ = model(normalized_data, reverse=False)\n\n\nwith torch.no_grad():\n    z_samples = model.prior.sample((3000,))\n    x_samples, _ = model(z_samples, reverse=True)\n\n\nf, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Original data\naxes[0, 0].scatter(normalized_data[:, 0].cpu(), normalized_data[:, 1].cpu(), s=1, c='green')\naxes[0, 0].set_title(\"Original Data (X)\")\naxes[0, 0].axis(\"equal\")\n\n# Latent representation of data\naxes[0, 1].scatter(z[:, 0].cpu(), z[:, 1].cpu(), s=1, c='blue')\naxes[0, 1].set_title(\"Encoded Latent Space (Z)\")\naxes[0, 1].axis(\"equal\")\n\n# Random latent samples\naxes[1, 0].scatter(z_samples[:, 0].cpu(), z_samples[:, 1].cpu(), s=1, c='orange')\naxes[1, 0].set_title(\"Sampled Latent Z\")\naxes[1, 0].axis(\"equal\")\n\n# Transformed back to data\naxes[1, 1].scatter(x_samples[:, 0].cpu(), x_samples[:, 1].cpu(), s=1, c='red')\naxes[1, 1].set_title(\"Generated Samples (X)\")\naxes[1, 1].axis(\"equal\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "flows.html",
    "href": "flows.html",
    "title": "Normalizing Flow Models",
    "section": "",
    "text": "In generative modeling, the objective is to learn a probability distribution over data that allows us to both generate new examples and evaluate the likelihood of observed ones. For a model to be practically useful, it must support efficient sampling and enable exact or tractable likelihood computation during training.\nA Variational Autoencoder (VAE) is a type of generative model that introduces latent variables \\(z\\), allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood \\(p(x)\\) is often intractable. To address this, VAEs use variational inference to approximate the posterior \\(p(z \\mid x)\\) and optimize a surrogate objective known as the Evidence Lower Bound (ELBO). This is made possible by the reparameterization trick, which enables gradients to flow through stochastic latent variables during training.\nNormalizing flows address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point \\(x\\) is generated by applying a function \\(x = f(z)\\) to a latent variable \\(z\\) sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so \\(z\\) can be exactly recovered as \\(z = f^{-1}(x)\\). This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.\nThis structure offers several advantages. First, each \\(x\\) maps to a unique \\(z\\), eliminating the need to marginalize over latent variables as in VAEs. Second, the change-of-variables formula enables exact computation of the likelihood, rather than approximations. Third, sampling is straightforward: draw \\(z \\sim p_Z(z)\\) from the base distribution and apply the transformation \\(x = f(z)\\).\nDespite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn lower-dimensional latent representations, flows require the latent and data spaces to have equal dimensionality to preserve invertibility. This means flow-based models do not perform dimensionality reduction, which can be a disadvantage in tasks where compact representations are important.\n\n\n\nComparison of VAE and Flow-based Models\n\n\nVAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.\nTo understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We’ll begin with the 1D case and build up to the multivariate formulation."
  },
  {
    "objectID": "flows.html#introduction",
    "href": "flows.html#introduction",
    "title": "Normalizing Flow Models",
    "section": "",
    "text": "In generative modeling, the objective is to learn a probability distribution over data that allows us to both generate new examples and evaluate the likelihood of observed ones. For a model to be practically useful, it must support efficient sampling and enable exact or tractable likelihood computation during training.\nA Variational Autoencoder (VAE) is a type of generative model that introduces latent variables \\(z\\), allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood \\(p(x)\\) is often intractable. To address this, VAEs use variational inference to approximate the posterior \\(p(z \\mid x)\\) and optimize a surrogate objective known as the Evidence Lower Bound (ELBO). This is made possible by the reparameterization trick, which enables gradients to flow through stochastic latent variables during training.\nNormalizing flows address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point \\(x\\) is generated by applying a function \\(x = f(z)\\) to a latent variable \\(z\\) sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so \\(z\\) can be exactly recovered as \\(z = f^{-1}(x)\\). This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.\nThis structure offers several advantages. First, each \\(x\\) maps to a unique \\(z\\), eliminating the need to marginalize over latent variables as in VAEs. Second, the change-of-variables formula enables exact computation of the likelihood, rather than approximations. Third, sampling is straightforward: draw \\(z \\sim p_Z(z)\\) from the base distribution and apply the transformation \\(x = f(z)\\).\nDespite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn lower-dimensional latent representations, flows require the latent and data spaces to have equal dimensionality to preserve invertibility. This means flow-based models do not perform dimensionality reduction, which can be a disadvantage in tasks where compact representations are important.\n\n\n\nComparison of VAE and Flow-based Models\n\n\nVAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.\nTo understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We’ll begin with the 1D case and build up to the multivariate formulation."
  },
  {
    "objectID": "flows.html#math-review",
    "href": "flows.html#math-review",
    "title": "Normalizing Flow Models",
    "section": "Math Review",
    "text": "Math Review\nThis section builds the mathematical foundation for understanding flow models, starting with change-of-variable and extending to multivariate transformations and Jacobians.\n\nChange of Variables in 1D\nSuppose we have a random variable \\(z\\) with a known distribution \\(p_Z(z)\\), and we define a new variable:\n\\[\nx = f(z)\n\\]\nwhere \\(f\\) is a monotonic, differentiable function with an inverse:\n\\[\nz = f^{-1}(x) = h(x)\n\\]\nOur goal is to compute the probability density function (PDF) of \\(x\\), denoted \\(p_X(x)\\), in terms of the known PDF \\(p_Z(z)\\).\n\nStep 1: Cumulative Distribution Function (CDF)\nWe begin with the cumulative distribution function of \\(x\\):\n\\[\nF_X(x) = P(X \\leq x) = P(f(Z) \\leq x)\n\\]\nSince \\(f\\) is monotonic and invertible, this becomes:\n\\[\nP(f(Z) \\leq x) = P(Z \\leq f^{-1}(x)) = F_Z(h(x))\n\\]\n\n\nStep 2: Deriving the PDF via Chain Rule\nTo obtain the PDF, we differentiate the CDF:\n\\[\np_X(x) = \\frac{d}{dx} F_X(x) = \\frac{d}{dx} F_Z(h(x))\n\\]\nApplying the chain rule:\n\\[\np_X(x) = F_Z'(h(x)) \\cdot h'(x) = p_Z(h(x)) \\cdot h'(x)\n\\]\n\n\nStep 3: Rewrite in Terms of \\(z\\)\nFrom the previous step:\n\\[\np_X(x) = p_Z(h(x)) \\cdot h'(x)\n\\]\nSince \\(z = h(x)\\), we can rewrite:\n\\[\np_X(x) = p_Z(z) \\cdot h'(x)\n\\]\nNow, using the inverse function theorem, we express \\(h'(x)\\) as:\n\\[\nh'(x) = \\frac{d}{dx} f^{-1}(x) = \\frac{1}{f'(z)}\n\\]\nSo the final expression becomes:\n\\[\np_X(x) = p_Z(z) \\cdot \\left| \\frac{1}{f'(z)} \\right|\n\\]\nThe absolute value ensures the density remains non-negative, as required for any valid probability distribution.\nThis is the fundamental concept normalizing flows use to model complex distributions by transforming simple ones.\n\n\n\nGeometry: Determinants and Volume Changes\nTo further understand the multivariate change-of-variable formula, it’s helpful to first explore how linear transformations affect volume in high-dimensional spaces.\nLet \\(\\mathbf{Z}\\) be a random vector uniformly distributed in the unit cube \\([0,1]^n\\), and let \\(\\mathbf{X} = A\\mathbf{Z}\\), where \\(A\\) is a square, invertible matrix. Geometrically, the matrix \\(A\\) maps the unit hypercube to a parallelogram in 2D or a parallelotope in higher dimensions.\nThe determinant of a square matrix tells us how the transformation scales volume. For instance, if the determinant of a \\(2 \\times 2\\) matrix is 3, applying that matrix will stretch the area of a region by a factor of 3. A negative determinant indicates a reflection, meaning the transformation also flips the orientation. When measuring volume, we care about the absolute value of the determinant.\nThe volume of the resulting parallelotope is given by:\n\\[\n\\text{Volume} = |\\det(A)|\n\\]\nThis expression tells us how much the transformation \\(A\\) scales space. For example, if \\(|\\det(A)| = 2\\), the transformation doubles the volume.\nTo make this idea concrete, consider the illustration below. The left figure shows a uniform distribution over the unit square \\([0, 1]^2\\). When we apply the linear transformation \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\), each point in the square is mapped to a new location, stretching the square into a parallelogram. The area of this parallelogram — and hence the volume scaling — is given by the absolute value of the determinant \\(|\\det(A)| = |ad - bc|\\).\n\n\n\n A linear transformation maps a unit square to a parallelogram. \n\n\nThis geometric intuition becomes essential when we apply the same logic to probability densities. The area of the parallelogram equals the absolute value of the determinant, |det(A)|, indicating how the transformation scales area.\n\n\nDeterminants and Probability Density\nPreviously, we saw how a linear transformation scales volume. Now we apply the same idea to probability densities — since density is defined per unit volume, scaling the volume also affects the density.\nTo transform the density from \\(\\mathbf{Z}\\) to \\(\\mathbf{X}\\), we use the change-of-variable formula. Since \\(\\mathbf{X} = A\\mathbf{Z}\\), the inverse transformation is \\(\\mathbf{Z} = A^{-1} \\mathbf{X}\\). This tells us how to evaluate the density at \\(\\mathbf{x}\\) by “pulling it back” through the inverse mapping. Applying the multivariate change-of-variable rule:\n\\[\np_X(\\mathbf{x}) = p_Z(W \\mathbf{x}) \\cdot \\left| \\det(W) \\right| \\quad \\text{where } W = A^{-1}\n\\]\nThis is directly analogous to the 1D change-of-variable rule:\n\\[\np_X(x) = p_Z(h(x)) \\cdot |h'(x)|\n\\]\nbut now in multiple dimensions using the determinant of the inverse transformation.\nTo make this more concrete, here’s a simple 2D example demonstrating how linear transformations affect probability density.\nLet \\(\\mathbf{Z}\\) be a random vector uniformly distributed over the unit square \\([0, 1]^2\\). Suppose we apply the transformation \\(\\mathbf{X} = A\\mathbf{Z}\\), where\n\\[\nA = \\begin{bmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\quad \\text{so that} \\quad\nW = A^{-1} =\n\\begin{bmatrix}\n\\frac{1}{2} & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\nThis transformation stretches the square horizontally, doubling its width while keeping the height unchanged. As a result, the area is doubled:\n\\[\n|\\det(A)| = 2 \\quad \\text{and} \\quad |\\det(W)| = \\frac{1}{2}\n\\] Since the same total probability must be spread over a larger area, the density decreases, meaning the probability per unit area is reduced due to the increased area over which the same total probability is distributed.\nNow, let’s say \\(p_Z(z) = 1\\) inside the unit square (a uniform distribution). To compute \\(p_X(\\mathbf{x})\\) at a point \\(\\mathbf{x}\\) in the transformed space, we use:\n\\[\np_X(\\mathbf{x}) = p_Z(W\\mathbf{x}) \\cdot |\\det(W)| = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}\n\\]\nSo, the transformed density is halved — the same total probability (which must remain 1) is now spread over an area that is twice as large.\n\n\nGeneralizing to Nonlinear Transformations\nFor nonlinear transformations \\(\\mathbf{x} = f(\\mathbf{z})\\), the idea is similar. But instead of a constant matrix \\(A\\), we now consider the Jacobian matrix of the function \\(f\\):\n\\[\nJ_f(\\mathbf{z}) = \\frac{\\partial f}{\\partial \\mathbf{z}}\n\\]\nThe Jacobian matrix generalizes derivatives to multivariable functions, capturing how a transformation scales and rotates space locally through all partial derivatives. Its determinant tells us how much the transformation stretches or compresses space — acting as a local volume scaling factor.\n\n\nMultivariate Change-of-Variable\nGiven an invertible transformation \\(\\mathbf{x} = f(\\mathbf{z})\\), the probability density transforms as:\n\\[\np_X(\\mathbf{x}) = p_Z(f^{-1}(\\mathbf{x})) \\cdot \\left| \\det \\left( \\frac{\\partial f^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nAlternatively, in the forward form (often used during training):\n\\[\np_X(\\mathbf{x}) = p_Z(\\mathbf{z}) \\cdot \\left| \\det \\left( \\frac{\\partial f(\\mathbf{z})}{\\partial \\mathbf{z}} \\right) \\right|^{-1}\n\\]\nThis generalizes the 1D rule and enables us to compute exact likelihoods for complex distributions as long as the transformation is invertible and differentiable. This formula is pivotal in machine learning, where transformations of probability distributions are common — such as in the implementation of normalizing flows for generative modeling."
  },
  {
    "objectID": "flows.html#flow-model",
    "href": "flows.html#flow-model",
    "title": "Normalizing Flow Models",
    "section": "Flow Model",
    "text": "Flow Model\nA normalizing flow model defines a one-to-one and reversible transformation between observed variables \\(\\mathbf{x}\\) and latent variables \\(\\mathbf{z}\\). This transformation is given by an invertible, differentiable function \\(f_\\theta\\), parameterized by \\(\\theta\\):\n\\[\n\\mathbf{x} = f_\\theta(\\mathbf{z}) \\quad \\text{and} \\quad \\mathbf{z} = f_\\theta^{-1}(\\mathbf{x})\n\\]\n\n\n\nFlow model showing forward and inverse transformations\n\n\nFigure: A flow-based model uses a forward transformation \\(f_\\theta\\) to map from latent variables (\\(\\mathbf{z}\\)) to data (\\(\\mathbf{x}\\)), and an inverse transformation \\(f_\\theta^{-1}\\) to compute likelihoods. Adapted from class notes (XCS236, Stanford).\nBecause the transformation is invertible, we can apply the change-of-variable formula to compute the exact probability of \\(\\mathbf{x}\\):\n\\[\np_X(\\mathbf{x}; \\theta) = p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nThis makes it possible to evaluate exact likelihoods and learn the model via maximum likelihood estimation (MLE).\n\nNote: Both \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) must be continuous and have the same dimensionality since the transformation must be invertible.\n\n\nModel Architecture: A Sequence of Invertible Transformations\nThe term flow refers to the fact that we can compose multiple invertible functions to form a more expressive transformation:\n\\[\n\\mathbf{z}_m = f_\\theta^{(m)} \\circ f_\\theta^{(m-1)} \\circ \\cdots \\circ f_\\theta^{(1)}(\\mathbf{z}_0)\n\\]\nIn this setup:\n\n\\(\\mathbf{z}_0 \\sim p_Z\\) is sampled from a simple base distribution (e.g., standard Gaussian)\n\\(\\mathbf{x} = \\mathbf{z}_M\\) is the final transformed variable\nThe full transformation \\(f_\\theta\\) is the composition of \\(M\\) sequential invertible functions. Each function slightly reshapes the distribution, and together they produce a highly expressive mapping from a simple base distribution to a complex one.\n\nThe visuals below illustrate this idea from two angles. The first diagram illustrates the structure of a normalizing flow as a composition of invertible steps, while the second shows how this architecture reshapes simple distributions into complex ones through repeated transformations.\n\n\n\n Adapted from Wikipedia: Mapping simple distributions to complex ones via invertible transformations. \n\n\n\n\n\n Adapted from class notes (XCS236, Stanford), originally based on Rezende & Mohamed, 2016. \n\n\nThe density of \\(\\mathbf{x}\\) is given by the change-of-variable formula:\n\\[\np_X(\\mathbf{x}; \\theta) = p_Z(f_\\theta^{-1}(\\mathbf{x})) \\cdot \\prod_{m=1}^M \\left| \\det \\left( \\frac{\\partial (f_\\theta^{(m)})^{-1}(\\mathbf{z}_m)}{\\partial \\mathbf{z}_m} \\right) \\right|\n\\]\nThis approach allows the model to approximate highly complex distributions using simple building blocks."
  },
  {
    "objectID": "flows.html#learning-and-inference",
    "href": "flows.html#learning-and-inference",
    "title": "Normalizing Flow Models",
    "section": "Learning and Inference",
    "text": "Learning and Inference\nTraining a flow-based model is done by maximizing the log-likelihood over the dataset \\(\\mathcal{D}\\):\n\\[\n\\max_\\theta \\log p_X(\\mathcal{D}; \\theta) = \\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p_Z(f_\\theta^{-1}(\\mathbf{x})) + \\log \\left| \\det \\left( \\frac{\\partial f_\\theta^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\right|\n\\]\nKey advantages of normalizing flows:\n\nExact likelihoods: No approximation needed — just apply the change-of-variable rule\nEfficient sampling: Generate new data by drawing \\(\\mathbf{z} \\sim p_Z\\) and computing \\(\\mathbf{x} = f_\\theta(\\mathbf{z})\\)\nLatent inference: Invert \\(f_\\theta\\) to compute latent codes \\(\\mathbf{z} = f_\\theta^{-1}(\\mathbf{x})\\), without needing a separate encoder\n\n\nComputational Considerations\nOne challenge in training normalizing flow models is that computing the exact likelihood requires evaluating the determinant of the Jacobian matrix of the transformation:\n\nFor a transformation \\(f : \\mathbb{R}^n \\to \\mathbb{R}^n\\), the Jacobian is an \\(n \\times n\\) matrix.\nComputing its determinant has a cost of \\(\\mathcal{O}(n^3)\\), which is computationally expensive during training — especially in high dimensions.\n\n\nKey Insight\nTo make normalizing flows scalable, we design transformations where the Jacobian has a special structure that makes the determinant easy to compute.\nFor example: - If the Jacobian is a triangular matrix, the determinant is just the product of the diagonal entries, which can be computed in \\(\\mathcal{O}(n)\\) time. - This works because in a triangular matrix, all the off-diagonal elements are zero — so the determinant simplifies significantly.\nIn practice, flow models like RealNVP and MAF are designed so that each output dimension \\(x_i\\) depends only on some subset of the input dimensions \\(z_{\\leq i}\\) (for lower triangular structure) or \\(z_{\\geq i}\\) (for upper triangular structure). This results in a Jacobian of the form:\n\\[\nJ = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial z_1} & 0 & \\cdots & 0 \\\\\n\\ast & \\frac{\\partial f_2}{\\partial z_2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\ast & \\ast & \\cdots & \\frac{\\partial f_n}{\\partial z_n}\n\\end{pmatrix}\n\\]\nBecause of this triangular structure, computing the determinant becomes as simple as multiplying the diagonal terms:\n\\[\n\\det(J) = \\prod_{i=1}^{n} \\frac{\\partial f_i}{\\partial z_i}\n\\]\nThis is why many modern flow models rely on coupling layers or autoregressive masking: they preserve invertibility and enable efficient, exact likelihood computation."
  },
  {
    "objectID": "flows.html#popular-flow-models",
    "href": "flows.html#popular-flow-models",
    "title": "Normalizing Flow Models",
    "section": "Popular Flow Models",
    "text": "Popular Flow Models\nTo implement scalable and expressive normalizing flows, researchers have designed several model families based on clever transformation strategies—particularly coupling layers and autoregressive masking.\nIn this section, we’ll introduce few of these architectures:\n\nNICE: Introduces additive coupling layers for tractable Jacobians.\nReal-NVP: Extends NICE with scaling and better expressivity.\nInverse Autoregressive Flow (IAF): Uses autoregressive masking for high flexibility.\nMasked Autoregressive Flow (MAF): Reverses the computation flow of IAF for efficient density evaluation.\n\nWe’ll walk through the core design behind each and how they achieve invertibility and efficient learning.\n\nNICE: Nonlinear Independent Components Estimation\nThe NICE model introduces two key components to construct invertible transformations:\n\nAdditive Coupling Layers: These preserve volume and maintain a simple Jacobian structure.\nRescaling Layers: These adjust volume and allow the model to capture more expressive distributions.\n\n\nAdditive Coupling Layer\nTo make the transformation invertible and computationally efficient, NICE splits the input vector into two parts. One part is kept unchanged, while the other part is modified using a function of the unchanged part. This way, we can easily reverse the process because we always know what was kept intact.\nLet’s partition the input \\(\\mathbf{z} \\in \\mathbb{R}^n\\) into two subsets: \\(\\mathbf{z}_{1:d}\\) and \\(\\mathbf{z}_{d+1:n}\\) for some \\(1 \\leq d &lt; n\\).\n\nForward Mapping \\(\\mathbf{z} \\mapsto \\mathbf{x}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)} \\\\\n\\mathbf{x}_{d+1:n} &= \\mathbf{z}_{d+1:n} + m_\\theta(\\mathbf{z}_{1:d})\n\\end{aligned}\n\\]\nwhere \\(m_\\theta(\\cdot)\\) is a neural network with parameters \\(\\theta\\), \\(d\\) input units, and \\(n - d\\) output units.\n\nInverse Mapping \\(\\mathbf{x} \\mapsto \\mathbf{z}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)} \\\\\n\\mathbf{z}_{d+1:n} &= \\mathbf{x}_{d+1:n} - m_\\theta(\\mathbf{x}_{1:d})\n\\end{aligned}\n\\]\n\nJacobian of the forward mapping:\n\nThe Jacobian matrix captures how much the transformation stretches or compresses space. Because part of the input is unchanged and the other part is only shifted (not scaled), the determinant of the Jacobian is 1 — meaning the transformation preserves volume.\n\\[\nJ = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\n\\frac{\\partial m_\\theta}{\\partial \\mathbf{z}_{1:d}} & I_{n-d}\n\\end{pmatrix}\n\\]\n\\[\n\\det(J) = 1\n\\]\nHence, additive coupling is a volume-preserving transformation.\n\n\nRescaling Layer\nTo enable volume changes, NICE appends a final rescaling layer:\n\nForward Mapping:\n\n\\[\nx_i = s_i z_i \\quad \\text{with} \\quad s_i &gt; 0\n\\]\n\nInverse Mapping:\n\n\\[\nz_i = \\frac{x_i}{s_i}\n\\]\n\nJacobian:\n\n\\[\nJ = \\text{diag}(\\mathbf{s})\n\\quad \\Rightarrow \\quad\n\\det(J) = \\prod_{i=1}^n s_i\n\\]\n\n\n\nReal-NVP: Non-Volume Preserving Extension of NICE\nReal-NVP (Dinh et al., 2017) extends NICE by introducing a scaling function that allows the model to change volume, enabling more expressive transformations.\nWe again partition the input \\(\\mathbf{z} \\in \\mathbb{R}^n\\) into two subsets: \\(\\mathbf{z}_{1:d}\\) and \\(\\mathbf{z}_{d+1:n}\\).\n\nForward Mapping \\(\\mathbf{z} \\mapsto \\mathbf{x}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{x}_{1:d} &= \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)} \\\\\\\\\n\\mathbf{x}_{d+1:n} &= \\mathbf{z}_{d+1:n} \\odot \\exp(\\alpha_\\theta(\\mathbf{z}_{1:d})) + \\mu_\\theta(\\mathbf{z}_{1:d})\n\\end{aligned}\n\\]\nThis equation modifies the second half of the input by scaling and shifting it, based on the first half. The use of the exponential function ensures that scaling is always positive, which is important for invertibility.\n\nInverse Mapping \\(\\mathbf{x} \\mapsto \\mathbf{z}\\):\n\n\\[\n\\begin{aligned}\n\\mathbf{z}_{1:d} &= \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)} \\\\\\\\\n\\mathbf{z}_{d+1:n} &= \\left( \\mathbf{x}_{d+1:n} - \\mu_\\theta(\\mathbf{x}_{1:d}) \\right) \\odot \\exp(-\\alpha_\\theta(\\mathbf{x}_{1:d}))\n\\end{aligned}\n\\]\n\nJacobian of Forward Mapping:\n\n\\[\nJ = \\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\\\\\n\\frac{\\partial \\mathbf{x}_{d+1:n}}{\\partial \\mathbf{z}_{1:d}} & \\operatorname{diag}\\left(\\exp(\\alpha_\\theta(\\mathbf{z}_{1:d}))\\right)\n\\end{pmatrix}\n\\]\n\nDeterminant of the Jacobian:\n\n\\[\n\\det(J) = \\prod_{i=d+1}^{n} \\exp\\left( \\alpha_\\theta(\\mathbf{z}_{1:d})_i \\right)\n= \\exp\\left( \\sum_{i=d+1}^{n} \\alpha_\\theta(\\mathbf{z}_{1:d})_i \\right)\n\\]\nThis makes Real-NVP a non-volume preserving transformation, allowing for both expansion and contraction of volume.\n\n\n\n\n\n\n## 🔄 Real-NVP: Non-Volume Preserving Extension of NICE\n\n\nReal-NVP (Dinh et al., 2017) extends NICE by introducing a scaling function that allows the model to change volume, enabling more expressive transformations. This is achieved using affine coupling layers that apply learned scaling and translation functions to part of the input while keeping the rest unchanged.\n\n\n### 🔀 Coupling Layer Transformation\n\n\nWe again partition the input ( ^n ) into two disjoint subsets: ( {1:d} ) and ( {d+1:n} ). The transformation is defined as follows:\n\n\n\\[\n\\mathbf{x}_{1:d} = \\mathbf{z}_{1:d} \\quad \\text{(identity transformation)}\n\\]\n\n\n\\[\n\\mathbf{x}_{d+1:n} = \\mathbf{z}_{d+1:n} \\odot \\exp(s(\\mathbf{z}_{1:d})) + t(\\mathbf{z}_{1:d})\n\\]\n\n\nThis equation modifies the second half of the input by scaling and shifting it based on the first half. The exponential ensures scaling is always positive (important for invertibility), while the identity path allows efficient inversion.\n\n\n\n\n\n\n\n\n📐 Efficient Jacobian\nBecause the first half ( {1:d} ) is copied directly and only ( {d+1:n} ) is transformed element-wise, the Jacobian of this transformation has a triangular structure:\n\\[\n\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{z}} =\n\\begin{pmatrix}\nI_d & 0 \\\\\n\\frac{\\partial \\mathbf{x}_{d+1:n}}{\\partial \\mathbf{z}_{1:d}} & \\text{diag} \\left( \\exp(s(\\mathbf{z}_{1:d})) \\right)\n\\end{pmatrix}\n\\]\nThe lower-right block is a diagonal matrix because each element of ( _{d+1:n} ) is scaled independently. This makes the Jacobian determinant simply the product of diagonal entries:\n\\[\n\\det(J) = \\prod_{i=d+1}^{n} \\exp(s(\\mathbf{z}_{1:d})_i) = \\exp \\left( \\sum_{i=d+1}^{n} s(\\mathbf{z}_{1:d})_i \\right)\n\\]\n\n💡 Why this matters: The design ensures that the transformation is invertible and the log-determinant is easy to compute — both are essential for tractable likelihood estimation.\n\n\n\n\n🔁 Inverse Mapping\nTo recover ( ) from ( ), the process is symmetric:\n\\[\n\\mathbf{z}_{1:d} = \\mathbf{x}_{1:d} \\quad \\text{(identity transformation)}\n\\]\n\\[\n\\mathbf{z}_{d+1:n} = \\left( \\mathbf{x}_{d+1:n} - t(\\mathbf{x}_{1:d}) \\right) \\odot \\exp(-s(\\mathbf{x}_{1:d}))\n\\]\n\n\n\n🧱 Stacking Coupling Layers\nEach coupling layer only transforms part of the input. To ensure that every dimension is eventually updated, RealNVP stacks multiple coupling layers and alternates the masking pattern between them.\n\nIn one layer, the first half is fixed, and the second half is transformed.\nIn the next layer, the roles are reversed.\n\nThis alternating structure ensures: - All input dimensions are updated across layers - The full transformation remains invertible - The total log-determinant is the sum of the log-determinants of each layer\n\n\n\n🖼 Affine Coupling Layer Visual\n\n\n\n Visualization of a single affine coupling layer in RealNVP. The identity path and affine transform structure allow exact inversion and efficient computation. \n\n\n\n\n\n🧪 RealNVP in Action (Two Moons)\nThe following plots illustrate how RealNVP transforms data in practice:\n\n\n\n Top-left: Original two-moons data ((X))\nTop-right: Encoded latent space ((Z))\nBottom-left: Latent samples from base distribution\nBottom-right: Generated samples mapped back to (X) space"
  },
  {
    "objectID": "flows.html#try-it-yourself-flow-model-in-pytorch",
    "href": "flows.html#try-it-yourself-flow-model-in-pytorch",
    "title": "Normalizing Flow Models",
    "section": "🧪 Try It Yourself: Flow Model in Pytorch",
    "text": "🧪 Try It Yourself: Flow Model in Pytorch\nYou can explore a minimal PyTorch implementation of a normalizing flow model:\n\n📘 View Notebook on GitHub\n🚀 Run in Google Colab"
  },
  {
    "objectID": "flows.html#references",
    "href": "flows.html#references",
    "title": "Normalizing Flow Models",
    "section": "📚 References",
    "text": "📚 References\n\nStanford CS236 Notes. Normalizing Flows. deepgenerativemodels.github.io\nRezende, D. J., & Mohamed, S. (2015). Variational Inference with Normalizing Flows. arXiv:1505.05770\n\nWikipedia. Normalizing Flow. Link"
  },
  {
    "objectID": "flows.html#further-reading",
    "href": "flows.html#further-reading",
    "title": "Normalizing Flow Models",
    "section": "🔍 Further Reading",
    "text": "🔍 Further Reading\n\nPapamakarios et al. (2019). Normalizing Flows for Probabilistic Modeling and Inference. arXiv:1912.02762\n\nLilian Weng. (2018). Flow-based Models. Blog\n\nEric Jang. (2018). Normalizing Flows Tutorial – Part 1. Blog\n\nEric Jang. (2018). Normalizing Flows Tutorial – Part 2. Blog\n\nUT Austin Calculus Notes. Jacobian and Change of Variables. Web\nKobyzev, Prince, & Brubaker. (2019). Normalizing Flows: An Introduction and Review of Current Methods. arXiv PDF"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Deep Generative Models",
    "section": "",
    "text": "Welcome to my deep generative models resource. This site covers:"
  },
  {
    "objectID": "index.html#key-model-families",
    "href": "index.html#key-model-families",
    "title": "Deep Generative Models",
    "section": "Key Model Families",
    "text": "Key Model Families\n\n1. Variational Autoencoders (VAEs)\n\nProbabilistic encoder-decoder architecture\nLearn latent spaces with Gaussian distributions\nLearn more about VAEs\n\n\n\n2. Autoregressive Models\n\nGenerate sequences element-by-element\nExamples: PixelCNN, WaveNet\n\n\n\n3. Generative Adversarial Networks (GANs)\n\nAdversarial training with generator/discriminator\nHigh-quality image generation\n\n\n\n4. Normalizing Flows\n\nInvertible transformations for exact likelihood\n\nFlexible density estimation\n\nLearn more about Normalizing Flows\n\n\n\n5. Energy-Based Models (EBMs)\n\nLearn energy functions for data distribution\nFlexible for discrete/continuous data\n\n\n\n6. Diffusion Models\n\nIterative denoising process\nState-of-the-art image generation"
  },
  {
    "objectID": "vae_faces.html",
    "href": "vae_faces.html",
    "title": "Face Generation with Convolutional VAE",
    "section": "",
    "text": "This notebook implements a convolutional variational autoencoder (VAE) trained on the CelebA face dataset using PyTorch.\nIt uses convolutional layers to encode and decode 64x64 face images, and demonstrates generation by sampling from the latent space.\n!pip install torch torchvision matplotlib\n# Enable autoreloading in Jupyter (if applicable)\n%load_ext autoreload\n%autoreload 2\n\n# Core libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import norm  # Used for visualizing latent distributions\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms, utils\n# Hyperparameters\n\nIMAGE_SIZE = 32        # Input image resolution (32x32)\nCHANNELS = 3           # Number of color channels (RGB)\nBATCH_SIZE = 128       # Batch size for training\nNUM_FEATURES = 128     # Hidden layer size or intermediate feature size\nZ_DIM = 200            # Dimensionality of the latent space\nLEARNING_RATE = 5e-4   # Learning rate for optimizer\nEPOCHS = 1             # Number of training epochs\nBETA = 2000            # Weight for KL divergence term in loss (used in beta-VAE)"
  },
  {
    "objectID": "vae_faces.html#loading-and-preprocessing-the-celeba-dataset",
    "href": "vae_faces.html#loading-and-preprocessing-the-celeba-dataset",
    "title": "Face Generation with Convolutional VAE",
    "section": "Loading and Preprocessing the CelebA Dataset",
    "text": "Loading and Preprocessing the CelebA Dataset\n\nfrom torchvision.datasets import CelebA\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n# Transform pipeline: resize and convert to tensor\ntransform = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor()\n])\n\n# Load CelebA dataset\ntrain_data = CelebA(\n    root=\"./data\",                      # Where to download/save the data\n    split=\"train\",                      # Options: \"train\", \"valid\", \"test\", or \"all\"\n    download=True,                      # Download if not already there\n    transform=transform\n)\n\n# DataLoader\ntrain_loader = DataLoader(\n    train_data,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\n\nval_data = CelebA(\n    root=\"./data\",\n    split=\"valid\",\n    download=True,\n    transform=transform\n)\n\nval_loader = DataLoader(\n    val_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)"
  },
  {
    "objectID": "vae_faces.html#visualizing-training-data",
    "href": "vae_faces.html#visualizing-training-data",
    "title": "Face Generation with Convolutional VAE",
    "section": "Visualizing Training Data",
    "text": "Visualizing Training Data\n\ndef sample_batch(dataloader, num_samples=16):\n    \"\"\"\n    Sample one batch from the dataloader and return the first `num_samples` images.\n    \"\"\"\n    for images, _ in dataloader:\n        return images[:num_samples]\n\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\ndef display(images, cmap=None, nrow=8):\n    \"\"\"\n    Display a grid of images using matplotlib.\n    \"\"\"\n    grid = make_grid(images, nrow=nrow, padding=2, normalize=True)\n    np_img = grid.permute(1, 2, 0).cpu().numpy()\n\n    plt.figure(figsize=(10, 5))\n    plt.imshow(np_img, cmap=cmap)\n    plt.axis('off')\n    plt.show()\n\n\ntrain_sample = sample_batch(train_loader, num_samples=16)\ndisplay(train_sample)"
  },
  {
    "objectID": "vae_faces.html#reparameterization-trick",
    "href": "vae_faces.html#reparameterization-trick",
    "title": "Face Generation with Convolutional VAE",
    "section": "Reparameterization Trick",
    "text": "Reparameterization Trick\n\ndef reparameterize(z_mean, z_log_var):\n    \"\"\"\n    Reparameterization trick: z = mu + sigma * epsilon\n    where epsilon ~ N(0, I)\n    \"\"\"\n    std = torch.exp(0.5 * z_log_var)\n    eps = torch.randn_like(std)\n    return z_mean + eps * std"
  },
  {
    "objectID": "vae_faces.html#encoder",
    "href": "vae_faces.html#encoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "Encoder",
    "text": "Encoder\n\nclass Encoder(nn.Module):\n    def __init__(self, z_dim):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(CHANNELS, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 32 → 16\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 16 → 8\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 8 → 4\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n\n            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, kernel_size=3, stride=2, padding=1),  # 4 → 2\n            nn.BatchNorm2d(NUM_FEATURES),\n            nn.LeakyReLU(),\n        )\n\n        self.flatten = nn.Flatten()\n        self.feature_shape = (NUM_FEATURES, IMAGE_SIZE // 16, IMAGE_SIZE // 16)\n        self.flat_dim = NUM_FEATURES * (IMAGE_SIZE // 16) ** 2\n\n        self.fc_mu = nn.Linear(self.flat_dim, z_dim)\n        self.fc_logvar = nn.Linear(self.flat_dim, z_dim)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x_flat = self.flatten(x)\n        mu = self.fc_mu(x_flat)\n        logvar = self.fc_logvar(x_flat)\n        return mu, logvar"
  },
  {
    "objectID": "vae_faces.html#vae-model-wrapper-encoder-reparameterization-and-decoder",
    "href": "vae_faces.html#vae-model-wrapper-encoder-reparameterization-and-decoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "VAE Model Wrapper: Encoder, Reparameterization, and Decoder",
    "text": "VAE Model Wrapper: Encoder, Reparameterization, and Decoder\n\nclass VAE(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"\n        z = mu + sigma * epsilon\n        where epsilon ~ N(0, I)\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass:\n        - Encode input to get mu and logvar\n        - Sample z using reparameterization trick\n        - Decode z to reconstruct image\n        \"\"\"\n        mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        x_recon = self.decoder(z)\n        return x_recon, mu, logvar"
  },
  {
    "objectID": "vae_faces.html#vae-loss",
    "href": "vae_faces.html#vae-loss",
    "title": "Face Generation with Convolutional VAE",
    "section": "VAE Loss",
    "text": "VAE Loss\n\ndef vae_loss(x, x_recon, mu, logvar, beta=BETA):\n    \"\"\"\n    VAE loss = beta * reconstruction loss (MSE) + KL divergence\n    \"\"\"\n    # Reconstruction loss (MSE)\n    recon_loss = F.mse_loss(x_recon, x, reduction='mean') * beta\n\n    # KL divergence between q(z|x) and N(0, I)\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    kl_loss = kl_loss / x.size(0)  # average over batch\n\n    return recon_loss + kl_loss, recon_loss, kl_loss"
  },
  {
    "objectID": "vae_faces.html#generating-and-saving-sample-images-from-the-latent-space",
    "href": "vae_faces.html#generating-and-saving-sample-images-from-the-latent-space",
    "title": "Face Generation with Convolutional VAE",
    "section": "Generating and Saving Sample Images from the Latent Space",
    "text": "Generating and Saving Sample Images from the Latent Space\n\nfrom torchvision.utils import save_image\nimport os\n\ndef generate_images(model, epoch, z_dim, num_img=8, path=\"./output\"):\n    \"\"\"\n    Generate and save images from random latent vectors after each epoch.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    model.eval()\n    z = torch.randn(num_img, z_dim).to(next(model.parameters()).device)\n    with torch.no_grad():\n        generated = model.decoder(z)\n    for i in range(num_img):\n        save_image(generated[i], f\"{path}/generated_img_{epoch:03d}_{i}.png\")"
  },
  {
    "objectID": "vae_faces.html#save-checkpoint",
    "href": "vae_faces.html#save-checkpoint",
    "title": "Face Generation with Convolutional VAE",
    "section": "Save Checkpoint",
    "text": "Save Checkpoint\n\ndef save_checkpoint(model, epoch, loss, best_loss, path=\"./checkpoint.pt\"):\n    if loss &lt; best_loss:\n        print(f\"Saving new best model at epoch {epoch} with loss {loss:.4f}\")\n        torch.save(model.state_dict(), path)\n        return loss\n    return best_loss"
  },
  {
    "objectID": "vae_faces.html#training-step",
    "href": "vae_faces.html#training-step",
    "title": "Face Generation with Convolutional VAE",
    "section": "Training Step",
    "text": "Training Step\n\ndef train_step(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n\n    for batch in dataloader:\n        x, _ = batch if isinstance(batch, (tuple, list)) else (batch, None)\n        x = x.to(device)\n\n        optimizer.zero_grad()\n\n        x_recon, mu, logvar = model(x)\n        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        total_recon += recon_loss.item()\n        total_kl += kl_loss.item()\n\n    num_batches = len(dataloader)\n    return {\n        \"loss\": total_loss / num_batches,\n        \"reconstruction_loss\": total_recon / num_batches,\n        \"kl_loss\": total_kl / num_batches\n    }"
  },
  {
    "objectID": "vae_faces.html#validation-step",
    "href": "vae_faces.html#validation-step",
    "title": "Face Generation with Convolutional VAE",
    "section": "Validation Step",
    "text": "Validation Step\n\n@torch.no_grad()\ndef validate_epoch(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n\n    for batch in dataloader:\n        x, _ = batch if isinstance(batch, (tuple, list)) else (batch, None)\n        x = x.to(device)\n        x_recon, mu, logvar = model(x)\n        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mu, logvar)\n        total_loss += loss.item()\n        total_recon += recon_loss.item()\n        total_kl += kl_loss.item()\n\n    num_batches = len(dataloader)\n    return {\n        \"loss\": total_loss / num_batches,\n        \"reconstruction_loss\": total_recon / num_batches,\n        \"kl_loss\": total_kl / num_batches\n    }"
  },
  {
    "objectID": "vae_faces.html#instantiate-model-optimizer-writer-device",
    "href": "vae_faces.html#instantiate-model-optimizer-writer-device",
    "title": "Face Generation with Convolutional VAE",
    "section": "Instantiate model, optimizer, writer, device",
    "text": "Instantiate model, optimizer, writer, device\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nencoder = Encoder(z_dim=Z_DIM)\ndecoder = Decoder(z_dim=Z_DIM)\nvae = VAE(encoder, decoder).to(device)\n\noptimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n\nwriter = SummaryWriter(log_dir=\"./logs\")"
  },
  {
    "objectID": "vae_faces.html#training-the-vae-model",
    "href": "vae_faces.html#training-the-vae-model",
    "title": "Face Generation with Convolutional VAE",
    "section": "Training the VAE Model",
    "text": "Training the VAE Model\n\nbest_loss = float(\"inf\")\n\nfor epoch in range(EPOCHS):\n    train_logs = train_step(vae, train_loader, optimizer, device)\n    val_logs = validate_epoch(vae, val_loader, device)\n\n    print(f\"Epoch {epoch+1:02d} | \"\n          f\"Train Loss: {train_logs['loss']:.4f} | \"\n          f\"Val Loss: {val_logs['loss']:.4f}\")\n\n    # Save best model\n    best_loss = save_checkpoint(vae, epoch, val_logs[\"loss\"], best_loss)\n\n    # Generate and save sample images\n    generate_images(vae, epoch, Z_DIM)\n\n    # Log to TensorBoard\n    writer.add_scalar(\"Loss/train\", train_logs[\"loss\"], epoch)\n    writer.add_scalar(\"Loss/val\", val_logs[\"loss\"], epoch)\n    writer.add_scalar(\"KL/train\", train_logs[\"kl_loss\"], epoch)\n    writer.add_scalar(\"Recon/train\", train_logs[\"reconstruction_loss\"], epoch)\n\nEpoch 01 | Train Loss: 72.4671 | Val Loss: 58.4050\nSaving new best model at epoch 0 with loss 58.4050"
  },
  {
    "objectID": "vae_faces.html#reconstruct-using-the-variational-autoencoder",
    "href": "vae_faces.html#reconstruct-using-the-variational-autoencoder",
    "title": "Face Generation with Convolutional VAE",
    "section": "Reconstruct using the variational autoencoder",
    "text": "Reconstruct using the variational autoencoder\n\n# Select a batch of images from the training set\nexample_images, _ = next(iter(train_loader))  # Ignore labels\nexample_images = example_images[:8].to(device)  # Select first 8 images\n\n# Set model to evaluation mode\nvae.eval()\n\n# Forward pass through the VAE to get reconstructions\nwith torch.no_grad():\n    reconstructions, _, _ = vae(example_images)\n\n# Move tensors to CPU for display\nexample_images = example_images.cpu()\nreconstructions = reconstructions.cpu()\n\n# Display original images\nprint(\"Example real faces\")\ndisplay(example_images)\n\n# Display reconstructed images\nprint(\"Reconstructions\")\ndisplay(reconstructions)\n\nExample real faces\n\n\n\n\n\n\n\n\n\nReconstructions"
  },
  {
    "objectID": "vae_faces.html#generate-new-faces",
    "href": "vae_faces.html#generate-new-faces",
    "title": "Face Generation with Convolutional VAE",
    "section": "Generate New Faces",
    "text": "Generate New Faces\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample latent vectors from standard normal distribution\ngrid_width, grid_height = 10, 3\nnum_samples = grid_width * grid_height\nz_sample = torch.randn(num_samples, Z_DIM).to(device)\n\n# Decode to generate new faces\nvae.eval()\nwith torch.no_grad():\n    generated_faces = vae.decoder(z_sample).cpu()  # Shape: [N, C, H, W]\n\n# Plot grid of generated images\nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(num_samples):\n    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n    ax.axis(\"off\")\n    img = generated_faces[i].permute(1, 2, 0).numpy()  # CHW → HWC\n    ax.imshow(img)\n\nplt.suptitle(\"Generated Faces from Sampled Latent Vectors\", fontsize=16)\nplt.show()"
  },
  {
    "objectID": "vae.html",
    "href": "vae.html",
    "title": "Variational Autoencoders",
    "section": "",
    "text": "Variational Autoencoders (VAEs) combine the power of neural networks with probabilistic inference to model complex data distributions. This blog unpacks the intuition, math, and implementation of VAEs — from KL divergence and the ELBO to PyTorch code that generates to generate new images."
  },
  {
    "objectID": "vae.html#autoencoders-vs-variational-autoencoders",
    "href": "vae.html#autoencoders-vs-variational-autoencoders",
    "title": "Variational Autoencoders",
    "section": "Autoencoders vs Variational Autoencoders",
    "text": "Autoencoders vs Variational Autoencoders\nTraditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:\n\nThey lack generative capabilities — they cannot sample new data effectively\nThe latent space is unstructured, offering little control or interpretation\nThere is no probabilistic modeling, limiting uncertainty estimation\n\nVariational Autoencoders (VAEs) were introduced to overcome these limitations. Rather than encoding inputs into fixed latent vectors, VAEs learn a probabilistic latent space by modeling each input as a distribution — typically a Gaussian with a learned mean \\(\\\\mu\\) and standard deviation \\(\\\\sigma\\). This approach enables the model to sample latent variables \\(z\\) using the reparameterization trick, allowing the entire architecture to remain differentiable and trainable. By doing so, VAEs not only enable reconstruction, but also promote the learning of a continuous, interpretable latent space — a key enabler for generation and interpolation.\nThe diagram below illustrates this process:\n\nSource: Wikimedia Commons, licensed under CC BY-SA 4.0."
  },
  {
    "objectID": "vae.html#probabilistic-framework",
    "href": "vae.html#probabilistic-framework",
    "title": "Variational Autoencoders",
    "section": "Probabilistic Framework",
    "text": "Probabilistic Framework\nMore formally, VAEs assume the data is generated by a two-step process:\n\nSample a latent variable \\(\\mathbf{z} \\sim \\mathcal{N}(0, I)\\)\nGenerate the observation \\(\\mathbf{x}\\) from: \\[\np(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{z}), \\Sigma_\\theta(\\mathbf{z}))\n\\] where \\(\\mu_\\theta\\) and \\(\\Sigma_\\theta\\) are neural networks parameterized by \\(\\theta\\)\n\nHere, \\(\\mathbf{z}\\) acts as a hidden or latent variable, which is unobserved during training. The model thus defines a mixture of infinitely many Gaussians — one for each \\(\\mathbf{z}\\).\nTo compute the likelihood of a data point \\(\\mathbf{x}\\), we must marginalize over all possible latent variables: \\[\n  p(\\mathbf{x}) = \\int p(\\mathbf{x}, \\mathbf{z}) \\, d\\mathbf{z}\n  \\]\nThis integral requires integrating over all possible values of the latent variable \\(\\mathbf{z}\\), which is often high-dimensional and affects the likelihood in a non-linear way through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of techniques like variational inference and ELBO.\n\nComputational Challenge\nThis integral requires integrating over:\n\nAll possible values of \\(\\mathbf{z}\\) (often high-dimensional)\nNon-linear transformations through neural networks\n\nResult: Exact computation is intractable, motivating techniques like variational inference and ELBO (developed next)."
  },
  {
    "objectID": "vae.html#estimating-the-marginal-likelihood",
    "href": "vae.html#estimating-the-marginal-likelihood",
    "title": "Variational Autoencoders",
    "section": "Estimating the Marginal Likelihood",
    "text": "Estimating the Marginal Likelihood\n\nNaive Monte Carlo Estimation\nOne natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:\n\\[\np(x) \\approx \\frac{1}{K} \\sum_{j=1}^K p_\\theta(x, z_j), \\quad z_j \\sim \\text{Uniform}\n\\]\nHowever, this fails in practice. For most values of \\(z\\), the joint probability \\(p_\\theta(x, z)\\) is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely “hits” likely values of \\(z\\).\n\n\nImportance Sampling\nTo address this, we use importance sampling, introducing a proposal distribution \\(q(z)\\):\n\\[\np(x) = \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nThis gives an unbiased estimator of \\(p(x)\\) if \\(q(z)\\) is well-chosen (ideally close to \\(p_\\theta(z|x)\\)). Intuitively, we sample \\(z\\) more frequently in regions where \\(p_\\theta(x, z)\\) is high.\n\n\n\nLog likelihood\nOur goal is to optimize the log-likelihood, and the log of an expectation is not the same as the expectation of the log. That is,\n\\[\n\\log p(x) = log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\neq \\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]\n\\]\nWhile the marginal likelihood p(x) can be estimated unbiasedly using importance sampling, estimating its logarithm \\(p(x)\\) introduces bias due to the concavity of the log function. This is captured by Jensen’s Inequality, which tells us:\n\\[\n\\log \\mathbb{E}_{q(z)} \\left[ \\frac{p_\\theta(x, z)}{q(z)} \\right] \\geq \\underbrace{\\mathbb{E}_{q(z)} \\left[ \\log \\frac{p_\\theta(x, z)}{q(z)} \\right]}_{\\text{ELBO}}\n\\]\nThis means that the expected log of the estimator underestimates the true log-likelihood. The right-hand side provides a tractable surrogate objective known as the Evidence Lower Bound (ELBO), which is a biased lower bound to \\(\\log p(x)\\). Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood.\nIn the next section, we formally derive this bound and explore its components in detail."
  },
  {
    "objectID": "vae.html#why-variational-inference",
    "href": "vae.html#why-variational-inference",
    "title": "Variational Autoencoders",
    "section": "Why Variational Inference?",
    "text": "Why Variational Inference?\nComputing the true posterior distribution \\(p(z \\mid x)\\) is intractable in most cases, because it requires evaluating the marginal likelihood \\(p(x)\\), which involves integrating over all possible values of \\(z\\):\n\\[\np(x) = \\int p(x, z) \\, dz\n\\]\nVariational inference tackles this by introducing a tractable, parameterized distribution \\(q(z)\\) to approximate \\(p(z|x)\\). We aim to make \\(q(z)\\) as close as possible to the true posterior by minimizing the KL divergence:\n\\[\nD_{\\text{KL}}(q(z) \\| p(z|x))\n\\]\nThis turns inference into an optimization problem. A key result is the Evidence Lower Bound (ELBO). See next section."
  },
  {
    "objectID": "vae.html#training-a-vae",
    "href": "vae.html#training-a-vae",
    "title": "Variational Autoencoders",
    "section": "Training a VAE",
    "text": "Training a VAE\n\nELBO Objective\nNow that we’ve introduced the challenge of approximating the intractable posterior using variational inference, we turn our attention to deriving the Evidence Lower Bound (ELBO). This derivation reveals how optimizing a surrogate objective allows us to approximate the true log-likelihood of the data while keeping the approximate posterior close to the prior. The steps below walk through this formulation.\n\nStep 1: KL Divergence Objective\n\\[\\begin{equation}\nD_{KL}(q(z)\\|p(z|x; \\theta)) = \\sum_z q(z) \\log \\frac{q(z)}{p(z|x; \\theta)}\n\\end{equation}\\]\n\n\nStep 2: Apply Bayes’ Rule\nSubstitute \\(p(z|x; \\theta) = \\frac{p(z,x;\\theta)}{p(x;\\theta)}\\): \\[\\begin{equation}\n= \\sum_z q(z) \\log \\left( \\frac{q(z) \\cdot p(x; \\theta)}{p(z, x; \\theta)} \\right)\n\\end{equation}\\]\n\n\nStep 3: Decompose Terms\n\\[\\begin{align}\n&= \\sum_z q(z) \\log q(z) + \\sum_z q(z) \\log p(x; \\theta) \\nonumber \\\\\n&\\quad - \\sum_z q(z) \\log p(z, x; \\theta) \\\\\n&= -H(q) + \\log p(x; \\theta) - \\mathbb{E}_q[\\log p(z,x;\\theta)]\n\\end{align}\\]\n\nNote: The term \\(\\mathcal{H}(q)\\) represents the entropy of the variational distribution \\(q(z|x)\\). Entropy is defined as:\n\\[\n\\mathcal{H}(q) = -\\sum_z q(z) \\log q(z) = -\\mathbb{E}_{q(z)}[\\log q(z)]\n\\]\nEntropy measures the amount of uncertainty or “spread” in a distribution. A high-entropy \\(q(z)\\) places probability mass across a wide region of the latent space, while a low-entropy \\(q(z)\\) is more concentrated. This decomposition is key to understanding the KL divergence term in the ELBO.\n\n\n\nStep 4: Rearrange for ELBO\n\\[\n\\log p(x; \\theta) =\n\\underbrace{\n    \\mathbb{E}_q[\\log p(z, x; \\theta)] + \\mathcal{H}(q)\n}_{\\text{ELBO}}\n+D_{KL}(q(z)\\|p(z|x; \\theta))\n\\]\nThis equation shows that the log-likelihood \\(\\log p(x)\\) can be decomposed into the ELBO and the KL divergence between the approximate posterior and the true posterior. Since the KL divergence is always non-negative, the ELBO serves as a lower bound to the log-likelihood. By maximizing the ELBO, we indirectly minimize the KL divergence, bringing \\(q(z)\\) closer to \\(p(z|x)\\).\n Visualizing how \\(\\log p(x)\\) decomposes into the ELBO and KL divergence.\nSource: deepgenerativemodels.github.io\n\n\nKey Results\n\nEvidence Lower Bound (ELBO): \\[\\begin{equation}\n\\mathcal{L}(\\theta,\\phi) = \\mathbb{E}_{q(z;\\phi)}[\\log p(x,z;\\theta)] + H(q(z;\\phi))\n\\end{equation}\\]\nOptimization: \\[\\begin{equation}\n\\max_{\\theta,\\phi} \\mathcal{L}(\\theta,\\phi) \\Rightarrow\n\\begin{cases}\n\\text{Maximizes data likelihood} \\\\\n\\text{Minimizes } D_{KL}(q\\|p)\n\\end{cases}\n\\end{equation}\\]"
  },
  {
    "objectID": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "href": "vae.html#understanding-the-kl-divergence-term-in-the-vae-loss",
    "title": "Variational Autoencoders",
    "section": "Understanding the KL Divergence Term in the VAE Loss",
    "text": "Understanding the KL Divergence Term in the VAE Loss\nIn a VAE, the KL divergence term penalizes the encoder for producing latent distributions that deviate too far from the standard normal prior. This regularization has several important benefits:\n\nIt ensures that the latent space has a consistent structure, enabling meaningful sampling and interpolation.\nIt helps avoid large gaps between clusters in latent space by encouraging the encoder to distribute representations more uniformly.\nIt pushes the model to use the space around the origin more symmetrically and efficiently.\n\n\nBalancing KL Divergence and Reconstruction\nIn a Variational Autoencoder, the loss balances two goals:\n\nReconstruction — making the output resemble the input\nRegularization — keeping the latent space close to a standard normal distribution\n\nThis is captured by the loss function:\n\\[\n\\mathcal{L}_{\\text{VAE}} = \\text{Reconstruction Loss} + \\beta \\cdot D_{\\text{KL}}(q(z|x) \\,\\|\\, p(z))\n\\]\nThe parameter \\(\\beta\\) controls how strongly we enforce this regularization. Getting its value right is critical.\n\nWhen \\(\\beta\\) is too low:\n\nThe model mostly ignores the KL term, behaving like a plain autoencoder\nThe latent space becomes disorganized or fragmented\nSampling from the prior \\(p(z) = \\mathcal{N}(0, I)\\) results in unrealistic or broken outputs\n\n\n\nWhen \\(\\beta\\) is too high:\n\nThe encoder is forced to keep \\(q(z|x)\\) too close to the prior\nIt encodes less information about the input\nReconstructions become blurry or generic, since the decoder gets little to work with\n\n\nChoosing \\(\\beta\\) carefully is essential for balancing generalization and fidelity.\nA well-tuned \\(\\beta\\) helps the VAE both reconstruct accurately and generate new samples that resemble the training data.\n\n\n\n\nGradient Challenge\nIn variational inference, we approximate the true posterior \\(p(z|x)\\) with a tractable distribution \\(q_\\phi(z)\\). This allows us to optimize the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q(z; \\phi)} \\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nOur goal is to maximize this objective with respect to both \\(\\theta\\) and \\(\\phi\\). While computing the gradient with respect to \\(\\theta\\) is straightforward, optimizing with respect to \\(\\phi\\) presents a challenge.\nThe complication arises because \\(\\phi\\) appears both in the density \\(q_\\phi(z|x)\\) and in the expectation operator. That is:\n\\[\n\\nabla_\\phi \\mathbb{E}_{q(z; \\phi)} \\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nThis gradient is hard to compute directly because we’re sampling from a distribution that depends on the parameters we’re trying to update.\n\n\n\nThe Reparameterization Trick\nTo make this expression differentiable, we reparameterize the random variable \\(z\\) as a deterministic transformation of a parameter-free noise variable \\(\\epsilon\\):\n\\[\n\\epsilon \\sim \\mathcal{N}(0, I), \\quad z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon\n\\]\nThis turns the expectation into:\n\\[\n\\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, I)}\\left[ \\log p(z, x; \\theta) - \\log q(z; \\phi) \\right]\n\\]\nwhere \\(z\\) is now a differentiable function of \\(\\phi\\).\n\n\n\nReparameterization Trick Diagram\n\n\nImage source: Wikipedia (CC BY-SA 4.0)\nThis diagram illustrates how the reparameterization trick enables differentiable sampling:\n\nIn the original formulation, \\(z\\) is sampled directly from a learned distribution, breaking the gradient flow.\nIn the reparameterized formulation, we sample noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\), and compute \\(z = \\mu + \\sigma \\cdot \\epsilon\\), making the sampling path fully differentiable.\n\n\nMonte Carlo Approximation\nWe approximate the expectation using Monte Carlo sampling:\n\\[\n\\mathbb{E}_{\\epsilon}[\\log p_\\theta(x, z) - \\log q_\\phi(z)] \\approx \\frac{1}{K} \\sum_{k=1}^K \\left[\\log p_\\theta(x, z^{(k)}) - \\log q_\\phi(z^{(k)})\\right]\n\\]\nwith:\n\\[\nz^{(k)} = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon^{(k)}, \\quad \\epsilon^{(k)} \\sim \\mathcal{N}(0, I)\n\\]\nThis enables us to compute gradients using backpropagation.\n\n\n\nSummary\n\nVariational inference introduces a gradient challenge because \\(q_\\phi(z)\\) depends on \\(\\phi\\)\nThe reparameterization trick expresses \\(z\\) as a differentiable function of noise and \\(\\phi\\)\nThis allows us to use backpropagation to optimize the ELBO efficiently\n\n\n\n\n\nAmortized Inference\nIn classical variational inference, we introduce a separate set of variational parameters \\(\\phi^i\\) for each datapoint \\(x^i\\) to approximate the true posterior \\(p(z|x^i)\\). However:\n\nOptimizing a separate \\(\\phi^i\\) for every datapoint is computationally expensive and does not scale to large datasets.\n\n\n\nThe Key Idea: Amortization\nInstead of learning and storing a separate \\(\\phi^i\\) for every datapoint, we learn a single parametric function \\(f_\\phi(x)\\) — typically a neural network — that maps each input \\(x\\) to the parameters of the approximate posterior:\n\\[\nq_\\phi(z|x) = \\mathcal{N}\\left(\\mu_\\phi(x), \\sigma^2_\\phi(x)\\right)\n\\]\nHere, \\(\\phi\\) are the shared parameters of the encoder network, and \\(\\mu_\\phi(x), \\sigma_\\phi(x)\\) are its outputs.\nThis is like learning a regression function that predicts the optimal variational parameters for any input \\(x\\).\n\n\n\n\nTraining with Amortized Inference\nOur training objective remains the ELBO:\n\\[\n\\mathcal{L}(x; \\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x, z) - \\log q_\\phi(z|x)\\right]\n\\]\nWe optimize both \\(\\theta\\) (decoder parameters) and \\(\\phi\\) (encoder parameters) using stochastic gradient descent.\n\nAlgorithm:\n\nInitialize \\(\\theta^{(0)}, \\phi^{(0)}\\)\nSample a datapoint \\(x^i\\)\nUse \\(f_\\phi(x^i)\\) to produce \\(\\mu^i, \\sigma^i\\)\nSample \\(z^i = \\mu^i + \\sigma^i \\cdot \\epsilon\\), with \\(\\epsilon \\sim \\mathcal{N}(0, I)\\)\nEstimate the ELBO and compute gradients w.r.t. \\(\\theta, \\phi\\)\nUpdate \\(\\theta, \\phi\\) using gradient descent\nUpdate \\(\\theta\\), \\(\\phi\\) using gradient descent:\n\n\\[\n\\phi \\leftarrow \\phi + \\tilde{\\nabla}_\\phi \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\n\\[\n\\theta \\leftarrow \\theta + \\tilde{\\nabla}_\\theta \\sum_{x \\in \\mathcal{B}} \\text{ELBO}(x; \\theta, \\phi)\n\\]\nwhere \\(\\mathcal{B}\\) is the current minibatch and \\(\\tilde{\\nabla}\\) indicates a stochastic gradient approximation.\n\n\n\n\nSummary\n\nAmortized inference replaces per-datapoint optimization with a single learned mapping \\(f_\\phi(x)\\)\nThis makes variational inference scalable and efficient\nThe model can generalize to unseen inputs by predicting variational parameters on-the-fly\n\n\nNote: Following common practice in the literature, we use \\(\\phi\\) to denote the parameters of the encoder network, even though it now defines a function rather than individual variational parameters."
  },
  {
    "objectID": "vae.html#applications-of-vaes",
    "href": "vae.html#applications-of-vaes",
    "title": "Variational Autoencoders",
    "section": "Applications of VAEs",
    "text": "Applications of VAEs\nVariational Autoencoders are widely used in:\n\nImage Generation: VAEs can generate new images similar to the training data (e.g., MNIST digits)\n\nAnomaly Detection: High reconstruction error flags unusual data points\n\nRepresentation Learning: Latent space captures features for downstream tasks\n\n\n😎 Face Generation with Convolutional VAE\nTo complement the theory, I’ve built a full PyTorch implementation of a Variational Autoencoder trained on the CelebA dataset.\n📘 The notebook walks through:\n\nDefining the encoder, decoder, and reparameterization trick\n\nImplementing the ELBO loss function (reconstruction + KL divergence)\n\nTraining the model on face images\n\nGenerating new faces from random latent vectors\n\n\n📓 View on GitHub"
  },
  {
    "objectID": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections.",
    "href": "vae.html#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections.",
    "title": "Variational Autoencoders",
    "section": "This example is designed to reinforce the theoretical concepts from earlier sections.",
    "text": "This example is designed to reinforce the theoretical concepts from earlier sections."
  },
  {
    "objectID": "vae.html#further-reading",
    "href": "vae.html#further-reading",
    "title": "Variational Autoencoders",
    "section": "Further Reading",
    "text": "Further Reading\nFor readers interested in diving deeper into the theory and applications of variational autoencoders, the following resources are recommended:\n\nTutorial on Variational Autoencoders\nCarl Doersch (2016)\nhttps://arxiv.org/pdf/1606.05908\nAuto-Encoding Variational Bayes\nKingma & Welling (2014) — the original VAE paper\nhttps://arxiv.org/pdf/1312.6114\nThe Challenges of Amortized Inference for Structured Prediction\nCremer, Li, & Duvenaud (2019)\nhttps://arxiv.org/pdf/1906.02691\nDeep Generative Models course notes\nhttps://deepgenerativemodels.github.io/notes/vae/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "vae_mnist.html",
    "href": "vae_mnist.html",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "",
    "text": "This notebook explains a Variational Autoencoder (VAE) trained on the MNIST dataset using PyTorch.\nEach step is annotated with detailed comments to help beginners understand what’s happening.\n!pip install torch torchvision"
  },
  {
    "objectID": "vae_mnist.html#import-required-libraries",
    "href": "vae_mnist.html#import-required-libraries",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "1. Import Required Libraries",
    "text": "1. Import Required Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "href": "vae_mnist.html#load-and-prepare-the-mnist-dataset",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "2. Load and Prepare the MNIST Dataset",
    "text": "2. Load and Prepare the MNIST Dataset\n\n# We transform MNIST images into tensors.\ntransform = transforms.ToTensor()\n\n# Download and load the training data\ntrain_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n\n# DataLoader for batching and shuffling\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
  },
  {
    "objectID": "vae_mnist.html#define-the-vae-model",
    "href": "vae_mnist.html#define-the-vae-model",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "3. Define the VAE Model",
    "text": "3. Define the VAE Model\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):\n        super(VAE, self).__init__()\n        # Encoder layers: input -&gt; hidden -&gt; (mu, logvar)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # outputs mean of q(z|x)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # outputs log-variance of q(z|x)\n\n        # Decoder layers: latent -&gt; hidden -&gt; reconstruction\n        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, input_dim)\n\n    def encode(self, x):\n        # Apply a hidden layer then split into mean and logvar\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        # Apply the reparameterization trick\n        std = torch.exp(0.5 * logvar)      # standard deviation\n        eps = torch.randn_like(std)        # random normal noise\n        return mu + eps * std              # sample z\n\n    def decode(self, z):\n        # Reconstruct input from latent representation\n        h = F.relu(self.fc3(z))\n        return torch.sigmoid(self.fc4(h))  # Output in [0, 1] range for binary MNIST\n\n    def forward(self, x):\n        # Full VAE forward pass\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon_x = self.decode(z)\n        return recon_x, mu, logvar"
  },
  {
    "objectID": "vae_mnist.html#define-the-elbo-loss",
    "href": "vae_mnist.html#define-the-elbo-loss",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "4. Define the ELBO Loss",
    "text": "4. Define the ELBO Loss\n\ndef elbo_loss(recon_x, x, mu, logvar):\n    # Binary cross-entropy for reconstruction\n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n\n    # KL divergence term to regularize q(z|x) against standard normal p(z)\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    # Total loss is negative ELBO\n    return BCE + KLD"
  },
  {
    "objectID": "vae_mnist.html#train-the-vae",
    "href": "vae_mnist.html#train-the-vae",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "5. Train the VAE",
    "text": "5. Train the VAE\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = VAE().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for x, _ in train_loader:\n        x = x.view(-1, 784).to(device)               # Flatten 28x28 images into 784 vectors\n        recon_x, mu, logvar = model(x)               # Forward pass\n        loss = elbo_loss(recon_x, x, mu, logvar)     # Compute loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader.dataset):.2f}\")"
  },
  {
    "objectID": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "href": "vae_mnist.html#visualize-original-and-reconstructed-digits",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "6. Visualize Original and Reconstructed Digits",
    "text": "6. Visualize Original and Reconstructed Digits\n\nmodel.eval()\nwith torch.no_grad():\n    x, _ = next(iter(train_loader))\n    x = x.view(-1, 784).to(device)\n    recon_x, _, _ = model(x)\n\n    # Convert back to image format\n    x = x.view(-1, 1, 28, 28).cpu()\n    recon_x = recon_x.view(-1, 1, 28, 28).cpu()\n\n    fig, axs = plt.subplots(2, 10, figsize=(15, 3))\n    for i in range(10):\n        axs[0, i].imshow(x[i][0], cmap='gray')\n        axs[0, i].axis('off')\n        axs[1, i].imshow(recon_x[i][0], cmap='gray')\n        axs[1, i].axis('off')\n    axs[0, 0].set_ylabel(\"Original\", fontsize=12)\n    axs[1, 0].set_ylabel(\"Reconstruction\", fontsize=12)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "vae_mnist.html#visualize-latent-space",
    "href": "vae_mnist.html#visualize-latent-space",
    "title": "Variational Autoencoder (VAE) on MNIST — Beginner Friendly Walkthrough",
    "section": "7. Visualize Latent Space",
    "text": "7. Visualize Latent Space\n\nimport seaborn as sns\n\nmodel.eval()\nall_z = []\nall_labels = []\n\n# Go through a few batches and collect latent representations\nwith torch.no_grad():\n    for x, y in train_loader:\n        x = x.view(-1, 784).to(device)\n        mu, _ = model.encode(x)  # use the mean as representation\n        all_z.append(mu.cpu())\n        all_labels.append(y)\n\n# Concatenate all batches\nz = torch.cat(all_z, dim=0).numpy()\nlabels = torch.cat(all_labels, dim=0).numpy()\n\n# Plot with seaborn\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=z[:, 0], y=z[:, 1], hue=labels, palette=\"tab10\", s=15)\nplt.title(\"Latent Space Visualization (using μ)\")\nplt.xlabel(\"z[0]\")\nplt.ylabel(\"z[1]\")\nplt.legend(title=\"Digit\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()"
  }
]