<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Evaluating Language Model Outputs: Metrics, Models, and Benchmarks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> <i class="bi bi-house" role="img">
</i> 
<span class="menu-text">Changez Akram</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-generative-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Generative AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-generative-ai">    
        <li>
    <a class="dropdown-item" href="./vae.html">
 <span class="dropdown-text">Variational Autoencoders (VAEs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./flows.html">
 <span class="dropdown-text">Normalizing Flows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ebm.html">
 <span class="dropdown-text">Energy-Based Models (EBMs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./diffusion.html">
 <span class="dropdown-text">Diffusion Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-large-language-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Large Language Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-large-language-models">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/transformers.html">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/post-training.html">
 <span class="dropdown-text">Post Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/nlp-eval.html">
 <span class="dropdown-text">NLP Evaluation</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-agentic-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Agentic AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-agentic-ai">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic-ai.html">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math-review" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math Review</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math-review">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/calculus.html">
 <span class="dropdown-text">Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/probability.html">
 <span class="dropdown-text">Probability</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-use-cases" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Use Cases</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-use-cases">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/banking-use-cases.html">
 <span class="dropdown-text">Banking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/healthcare-use-cases.html">
 <span class="dropdown-text">Healthcare</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction:</a></li>
  <li><a href="#closed-ended-tasks" id="toc-closed-ended-tasks" class="nav-link" data-scroll-target="#closed-ended-tasks"><span class="header-section-number">2</span> Closed-Ended Tasks</a></li>
  <li><a href="#open-ended-text-generation" id="toc-open-ended-text-generation" class="nav-link" data-scroll-target="#open-ended-text-generation"><span class="header-section-number">3</span> Open-Ended Text Generation</a></li>
  <li><a href="#references-further-reading" id="toc-references-further-reading" class="nav-link" data-scroll-target="#references-further-reading"><span class="header-section-number">4</span> References &amp; Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Evaluating Language Model Outputs: Metrics, Models, and Benchmarks</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction:</h2>
<p>NLP evaluation measures how well language models actually work. It answers basic questions: Is this model good enough? Can we trust it with real users? But here’s the challenge—testing language models isn’t like testing regular software. When code breaks, it’s obvious. When a language model fails, it might write something that sounds perfect but is completely wrong. A model might be 92% accurate overall but fail every time it sees sarcasm. This is what makes NLP evaluation so tricky: we’re measuring how well computers understand the messy, complex world of human language.</p>
<p><strong>Why It Matters Now More Than Ever</strong></p>
<p>In 2023 alone, we saw an AI chatbot fail to recognize suicide warning signs (leading to a death), lawyers submit fake AI-generated cases to courts, and Air Canada forced to honor a refund policy its bot invented. As Chip Huyen warns: “The more AI is used, the more opportunity there is for catastrophic failure.” The smarter our models get, the harder they become to evaluate. It’s easy to check a kid’s math homework, but verifying if an AI’s medical advice is accurate requires medical expertise. We need good evaluation to build better models, but we need expertise to do good evaluation.</p>
<p><strong>Two Types of Tasks, Two Different Challenges</strong></p>
<ul>
<li><p><strong>Closed-ended tasks</strong> have clear right answers (Is this email spam? What’s the sentiment?). We can use traditional metrics like accuracy and precision, but even these “simple” tasks suffer from shortcuts, dataset problems, and human labeling errors.</p></li>
<li><p><strong>Open-ended tasks</strong> have no single right answer (Write a summary, translate this text, answer this question). Traditional metrics completely fail here. Word-matching might say “Heck no!” is similar to “Heck yes!” because they share words.</p></li>
</ul>
<p><strong>What’s Ahead</strong></p>
<p>This overview covers how to evaluate both closed and open-ended tasks, why current methods fail and what’s replacing them, major problems like contamination and bias, and practical solutions for real-world applications.</p>
<hr>
</section>
<section id="closed-ended-tasks" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="closed-ended-tasks"><span class="header-section-number">2</span> Closed-Ended Tasks</h2>
<p>Closed-ended tasks are widely used in NLP evaluation because they provide clear right or wrong answers. Since the model’s output is limited to a small set of predefined choices—often fewer than ten—they make it easier to compare models using standard metrics like accuracy, precision, recall, and F1-score. This structure allows for objective evaluation, consistent benchmarking, and easier tracking of progress over time.</p>
<p><strong>Bounded Output Space:</strong> Unlike text generation where models can produce any sequence of tokens, closed-ended tasks constrain outputs to predefined categories or structured formats.<br>
<strong>Objective Evaluation:</strong> Success can be measured automatically using established metrics from classical machine learning—accuracy, precision, recall, and F1-score.<br>
<strong>Reproducible Benchmarks:</strong> Standard datasets enable fair comparison across models and over time.<br>
<strong>Systematic Progress Tracking:</strong> Clear metrics allow the field to monitor advancement and identify when models have genuinely improved.</p>
<p>While this constraint makes evaluation more straightforward than open-ended generation, closed-ended tasks still present significant challenges that can mislead researchers and practitioners. Understanding these problems is important because closed-ended evaluation is often the first test of model quality, affecting research focus and decisions about which models to deploy.</p>
<section id="evaluation-metrics-for-closed-ended-tasks" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="evaluation-metrics-for-closed-ended-tasks"><span class="header-section-number">2.1</span> Evaluation Metrics for Closed-Ended Tasks</h3>
<p>Understanding how to measure performance is fundamental to closed-ended evaluation. Different metrics serve different purposes and can lead to very different conclusions about model quality. Following metrics evaluate performance on individual labeled tasks:</p>
<ul>
<li><strong>Accuracy:</strong> Percentage of correct predictions (simple but can be misleading with imbalanced data).<br>
</li>
<li><strong>Precision:</strong> Of all positive predictions, how many were correct? (important when false alarms are costly).<br>
</li>
<li><strong>Recall:</strong> Of all actual positive cases, how many were found? (important when missing cases is costly).<br>
</li>
<li><strong>F1-Score:</strong> Harmonic mean of precision and recall (balances both concerns).</li>
</ul>
<p>These metrics are typically applied to standardized datasets called <strong>benchmarks</strong>, which we’ll explore in the next section. Like standardized tests for AI, benchmarks enable fair comparison between models from different research groups and track progress over time.</p>
</section>
<section id="types-of-closed-ended-tasks" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="types-of-closed-ended-tasks"><span class="header-section-number">2.2</span> Types of Closed-Ended Tasks</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 18%">
<col style="width: 45%">
<col style="width: 9%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Task Type</strong></th>
<th><strong>Description</strong></th>
<th><strong>Example</strong></th>
<th><strong>Popular Benchmarks</strong></th>
<th><strong>Common Pitfalls</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Sentiment Analysis</strong></td>
<td>Classify emotional tone (positive/negative/neutral)</td>
<td>“Read the book, forget the movie!” → <strong>Negative</strong></td>
<td>SST, IMDB, Yelp</td>
<td>Fails on sarcasm, irony, cultural context</td>
</tr>
<tr class="even">
<td><strong>Textual Entailment</strong></td>
<td>Does sentence B logically follow from sentence A?</td>
<td><em>Premise:</em> “A soccer game with multiple males playing”<br><em>Hypothesis:</em> “Some men are playing sport” → <strong>Entailment</strong></td>
<td>SNLI, MultiNLI, RTE</td>
<td>Shortcut learning (e.g., keyword overlap); see SNLI issues</td>
</tr>
<tr class="odd">
<td><strong>Named Entity Recognition (NER)</strong></td>
<td>Identify and classify proper nouns</td>
<td>“Apple released the iPhone.” → <strong>[Apple–ORG]</strong>, <strong>[iPhone–PRODUCT]</strong></td>
<td>CoNLL-2003</td>
<td>Ambiguity: Apple (fruit vs company)</td>
</tr>
<tr class="even">
<td><strong>Part-of-Speech Tagging</strong></td>
<td>Assign grammatical categories to words</td>
<td>“The quick brown fox” → <strong>[The–DET]</strong>, <strong>[quick–ADJ]</strong>, <strong>[brown–ADJ]</strong>, <strong>[fox–NOUN]</strong></td>
<td>Penn Treebank (PTB)</td>
<td>Often used as foundation for parsing &amp; other tasks</td>
</tr>
<tr class="odd">
<td><strong>Coreference Resolution</strong></td>
<td>Determine pronoun references</td>
<td>“Mark told Pete lies about himself. He should have been more truthful.” → <strong>“He” = Mark</strong></td>
<td>WSC, OntoNotes</td>
<td>Requires deep context or world knowledge</td>
</tr>
<tr class="even">
<td><strong>Question Answering</strong></td>
<td>Extract answers from passage</td>
<td><em>Context:</em> “The ESA was passed in 1973.”<br><em>Q:</em> “When was it passed?” → <strong>1973</strong></td>
<td>SQuAD, SQuAD 2.0</td>
<td>Models memorize patterns or position, not true reasoning</td>
</tr>
</tbody>
</table>
<p>These individual tasks can also be grouped into multi-task benchmarks that evaluate general language understanding across a range of closed-ended challenges. <strong>SuperGLUE</strong> is one of the most prominent such benchmarks.</p>
</section>
<section id="multi-task-benchmark-superglue" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="multi-task-benchmark-superglue"><span class="header-section-number">2.3</span> Multi-Task Benchmark: <strong>SuperGLUE</strong></h3>
<p>While the previous section outlined individual closed-ended tasks, real-world evaluation often demands a unified benchmark that spans multiple task types. <strong>SuperGLUE</strong> is a widely adopted <strong>closed-ended multi-task benchmark</strong> created to evaluate broad general language understanding. It builds on its predecessor (GLUE) with harder tasks, more robust metrics, and an emphasis on reasoning.</p>
<p>SuperGLUE combines a diverse set of tasks—ranging from entailment and coreference resolution to causal reasoning and word sense disambiguation—designed to holistically assess a model’s linguistic and reasoning capabilities across multiple dimensions.</p>
<p><strong>Tasks in SuperGLUE:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 57%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Description</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>BoolQ</strong>, <strong>MultiRC</strong></td>
<td>Reading comprehension</td>
<td>QA / Inference</td>
</tr>
<tr class="even">
<td><strong>CB</strong>, <strong>RTE</strong></td>
<td>Natural language inference</td>
<td>Entailment</td>
</tr>
<tr class="odd">
<td><strong>COPA</strong></td>
<td>Causal reasoning (cause/effect)</td>
<td>Reasoning</td>
</tr>
<tr class="even">
<td><strong>ReCoRD</strong></td>
<td>Reading comprehension with commonsense reasoning</td>
<td>QA / Commonsense</td>
</tr>
<tr class="odd">
<td><strong>WiC</strong></td>
<td>Word meaning in context</td>
<td>Word Sense Disambiguation</td>
</tr>
<tr class="even">
<td><strong>WSC</strong></td>
<td>Coreference resolution</td>
<td>Coreference</td>
</tr>
</tbody>
</table>
<p>Together, these tasks go beyond surface-level prediction—testing abilities like logical reasoning, commonsense application, coreference tracking, and contextual understanding.</p>
<p><strong>Leaderboard Highlights (v2.0):</strong></p>
<p>To measure and compare model performance on SuperGLUE, an official leaderboard tracks results across all tasks using standardized metrics. The <strong>v2.0 leaderboard</strong> showcases most advanced models—ranging from parameter-efficient <strong>Mixture of Experts (MoE)</strong> systems to massive transformer-based architectures—offering a clear snapshot of the state of the art in general language understanding.</p>
<ul>
<li><strong>Top models</strong>: Vega v2, ST-MoE-32B, ERNIE, PaLM 540B, T5<br>
</li>
<li><strong>Metrics used</strong>: Accuracy, F1 score, Exact Match, Gender Parity, etc.</li>
</ul>
<p>The leaderboard emphasizes <strong>balanced generalization</strong>, rewarding models that perform consistently well across diverse task types—not just a few. This makes it a reliable benchmark for tracking progress toward broadly capable language models.</p>
</section>
<section id="domain-rich-multi-task-benchmark-mmlu" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="domain-rich-multi-task-benchmark-mmlu"><span class="header-section-number">2.4</span> Domain-Rich Multi-Task Benchmark: MMLU</h3>
<p>While SuperGLUE focuses on general linguistic reasoning, <strong>Massive Multitask Language Understanding (MMLU)</strong> shifts the spotlight to <strong>domain knowledge</strong>. It has rapidly become the <em>de facto</em> benchmark for evaluating a model’s grasp of academic and professional subjects—effectively acting as a proxy for general intelligence in many LLM leaderboards.</p>
<p><strong>What is MMLU?</strong></p>
<ul>
<li><strong>57 subjects</strong> spanning elementary math, anatomy, law, philosophy, computer science, US history, and more<br>
</li>
<li><strong>Multiple-choice format</strong> (A, B, C, D), with ~100 questions per subject<br>
</li>
<li><strong>Balanced question design</strong> that mimics real academic tests and professional licensing exams<br>
</li>
<li><strong>Closed-book evaluation</strong> testing what the model has internalized from pretraining</li>
</ul>
<p>MMLU has emerged as a <strong>standardized benchmark</strong> for evaluating foundational knowledge across domains. It allows for <strong>direct accuracy-based comparisons</strong> between models of different sizes and architectures. Performance gains have been dramatic—rising from ~<strong>25% (random guessing)</strong> to over <strong>90% accuracy</strong> in just four years.</p>
<blockquote class="blockquote">
<p><strong>Note</strong>: MMLU is often treated as a shortcut for measuring “general intelligence,” but that can be misleading. What it really tests is how well a model can recall facts and recognize patterns—not necessarily how well it can reason or think abstractly. We’ll explore these limitations later.</p>
</blockquote>
<p><strong>MMLU in Modern LLM Leaderboards</strong></p>
<p>Modern LLMs—including GPT-4, Claude 3, Gemini, LLaMA 3, and PaLM—routinely report MMLU scores as a primary metric. As with SuperGLUE, MMLU supports <strong>multi-subject generalization</strong>, but with a stronger emphasis on <strong>world knowledge</strong> rather than linguistic nuance.</p>
<p><strong>SuperGLUE vs MMLU: A Comparison</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 47%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th><strong>SuperGLUE</strong></th>
<th><strong>MMLU</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Focus</strong></td>
<td>Language reasoning</td>
<td>Factual subject knowledge</td>
</tr>
<tr class="even">
<td><strong>Format</strong></td>
<td>Varied NLP tasks</td>
<td>Multiple choice</td>
</tr>
<tr class="odd">
<td><strong>Tasks / Subjects</strong></td>
<td>8 tasks</td>
<td>57 subjects</td>
</tr>
<tr class="even">
<td><strong>Primary Skill Tested</strong></td>
<td>Inference, disambiguation, coreference</td>
<td>Retained domain knowledge</td>
</tr>
<tr class="odd">
<td><strong>Metric</strong></td>
<td>Accuracy, F1, etc.</td>
<td>Accuracy only</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>While both are multi-task benchmarks, they evaluate very different capabilities—SuperGLUE emphasizes reasoning and understanding, whereas MMLU stresses factual recall across disciplines.</p>
</blockquote>
</section>
<section id="challenges-in-closed-ended-evaluation" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="challenges-in-closed-ended-evaluation"><span class="header-section-number">2.5</span> Challenges in Closed-Ended Evaluation</h3>
<ol type="1">
<li><p><strong>Metric Selection:</strong> Different metrics highlight different aspects of model performance. For example: Accuracy can be misleading on imbalanced datasets—it may just reflect majority-class predictions. Precision measures correctness (fewer false positives), while Recall measures completeness (fewer false negatives). Using a single metric in isolation—especially on skewed tasks—can hide a model’s weaknesses.</p></li>
<li><p><strong>Metric Aggregation:</strong> Benchmarks like SuperGLUE combine many tasks, each with its own metric (e.g., F1, accuracy, loss). Simply averaging scores can give an incomplete picture. Some tasks are easier than others, and their metrics scale differently. Without proper weighting or normalization, overall scores may not reflect true performance.</p></li>
<li><p><strong>Label Quality:</strong> Poorly defined or inconsistent labels can introduce noise into both training and evaluation. This reduces reliability and makes it hard to tell if performance differences are meaningful or just due to annotation issues.</p></li>
<li><p><strong>Spurious Correlations:</strong> Models may rely on patterns or keywords rather than real understanding.</p>
<ul>
<li><strong>Example</strong> (SNLI):
<ul>
<li>Premise: “The economy could be still better.”<br>
</li>
<li>Hypothesis: “The economy has <em>never</em> been better.”<br>
</li>
<li>Model might infer contradiction simply due to the word <em>never</em> rather than actual reasoning</li>
</ul></li>
</ul></li>
<li><p><strong>Annotation Artifacts and Dataset Bias</strong>: Some datasets contain structural biases—for example, QA answers often appear at the start of a passage. Models may exploit these shortcuts without real comprehension. Stylistic cues (e.g., sentence length or formality) may also correlate with certain labels, inflating scores without reflecting true understanding.</p></li>
<li><p><strong>Adversarial Robustness</strong>: Small tweaks—like changing a word or rephrasing a sentence—can confuse models even if the meaning stays the same. This shows a lack of generalization. Robust evaluation should test whether models maintain performance under such paraphrased or adversarial inputs.</p></li>
</ol>
<p>This highlights that even well-defined tasks require evaluation beyond raw accuracy to capture true model behavior and ensure robustness.</p>
<hr>
</section>
</section>
<section id="open-ended-text-generation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="open-ended-text-generation"><span class="header-section-number">3</span> Open-Ended Text Generation</h2>
<p>Open-ended generation models, such as large language models (LLMs), can produce diverse free-form outputs like summaries, translations, stories, or answers to instructions. Unlike classification models that generate a fixed label or number, open-ended models return entire sequences of text—often with many valid responses for the same input.</p>
<p>This flexibility makes them powerful but difficult to evaluate. There’s rarely a single “correct” output, and even seemingly unrelated completions may still be valid. For example, the prompt <em>“Tell me something interesting about the moon”</em> could yield many accurate and fluent yet different answers.</p>
<p>Evaluating such responses goes beyond simple reference matching—it requires assessing coherence, fluency, relevance, factual accuracy, style, and semantic alignment. As a result, evaluation of open-ended generation must adopt a more nuanced, multi-faceted approach.</p>
<p>The following sections outline key evaluation methods and challenges.</p>
<section id="content-overlap-metrics" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="content-overlap-metrics"><span class="header-section-number">3.1</span> Content Overlap Metrics</h3>
<p>These methods compute similarity based on surface-level word overlap between the generated text and a human-written reference. They are fast, interpretable, and have long been used in machine translation, summarization, and captioning. However, they often fail to recognize valid paraphrases or semantic equivalence.</p>
<p><strong>Popular metrics:</strong></p>
<section id="bleu-bilingual-evaluation-understudy" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="bleu-bilingual-evaluation-understudy"><span class="header-section-number">3.1.1</span> BLEU (Bilingual Evaluation Understudy)</h4>
<ul>
<li><strong>Focus:</strong> Precision-oriented n-gram overlap</li>
<li><strong>Calculation:</strong> Geometric mean of 1-gram through 4-gram precision, with brevity penalty</li>
<li><strong>Use Case:</strong> Machine Translation — how much of the generated text appears in the reference</li>
<li><strong>Formula:</strong> <span class="math display">\[
\text{BLEU} = \text{BP} \cdot \exp\left( \sum_{n=1}^{N} \frac{1}{N} \cdot \log(p_n) \right)
\]</span> where <span class="math inline">\(p_n\)</span> is n-gram precision and BP is brevity penalty.</li>
</ul>
</section>
<section id="rouge-recall-oriented-understudy-for-gisting-evaluation" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="rouge-recall-oriented-understudy-for-gisting-evaluation"><span class="header-section-number">3.1.2</span> ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</h4>
<ul>
<li><strong>Focus:</strong> Recall-oriented n-gram overlap</li>
<li><strong>Variants:</strong> ROUGE-1: Unigrams, ROUGE-2: Bigrams, ROUGE-L: Longest common subsequence</li>
<li><strong>Use Case:</strong> Summarization — how much of the reference is captured in the generated text</li>
<li><strong>Formula (ROUGE-L F1 score)</strong>:<br>
<span class="math display">\[
\text{ROUGE-L} = \frac{(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}{\text{Recall} + \beta^2 \cdot \text{Precision}}
\]</span> where:
<ul>
<li>Precision = <span class="math inline">\(\frac{LCS}{\text{candidate length}}\)</span></li>
<li>Recall = <span class="math inline">\(\frac{LCS}{\text{reference length}}\)</span></li>
<li>LCS = Longest Common Subsequence</li>
<li><span class="math inline">\(\beta\)</span> balances recall and precision (often <span class="math inline">\(\beta = 1\)</span>)</li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p><strong>Limitation:</strong> N-gram metrics have no concept of meaning. They fail when different words express the same idea.</p>
</blockquote>
<p><strong>Example 1:</strong></p>
<p><strong>Reference</strong>: <code>"Heck yes!"</code></p>
<table class="caption-top table">
<colgroup>
<col style="width: 43%">
<col style="width: 16%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Generated Output</th>
<th>BLEU Score</th>
<th>Semantic Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>"Yes!"</code></td>
<td>67%</td>
<td>Correct</td>
</tr>
<tr class="even">
<td><code>"You know it!"</code></td>
<td>Low</td>
<td>Correct</td>
</tr>
<tr class="odd">
<td><code>"Yup"</code></td>
<td>0%</td>
<td>Correct</td>
</tr>
<tr class="even">
<td><code>"Heck no!"</code></td>
<td>67%</td>
<td><strong>Wrong</strong> (opposite meaning)</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>These metrics reward lexical matches even when the meaning is incorrect.</p>
</blockquote>
<p><strong>Example 2:</strong></p>
<p><strong>Reference</strong>: <code>"The innovative startup secured substantial funding"</code></p>
<ul>
<li><p><strong>Generated A</strong>:<br>
<code>"The creative company obtained significant investment"</code><br>
→ 0% BLEU overlap, <strong>perfect semantic match</strong></p></li>
<li><p><strong>Generated B</strong>:<br>
<code>"The innovative startup funding substantial secured"</code><br>
→ 83% BLEU overlap, <strong>grammatically broken and semantically wrong</strong></p></li>
</ul>
<blockquote class="blockquote">
<p>BLEU would incorrectly score <strong>Generated B</strong> higher than A due to word overlap.</p>
</blockquote>
</section>
</section>
<section id="model-based-metrics" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="model-based-metrics"><span class="header-section-number">3.2</span> Model-Based Metrics</h3>
<p>As NLP systems generate increasingly fluent and diverse outputs, traditional word-overlap metrics like BLEU and ROUGE often fail to capture deeper aspects of quality such as semantic fidelity, naturalness, and paraphrasing. To overcome these limitations, researchers have developed model-based evaluation metrics that harness the semantic understanding of pretrained language models.</p>
<p>Two widely adopted examples are <strong>BERTScore</strong>, which uses contextual embeddings for token-level semantic similarity, and <strong>BLEURT</strong>, which is trained to predict human quality judgments directly.</p>
<section id="bertscore-semantic-matching-with-contextual-embeddings" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="bertscore-semantic-matching-with-contextual-embeddings"><span class="header-section-number">3.2.1</span> BERTScore: Semantic Matching with Contextual Embeddings</h4>
<p>BERTScore measures how semantically close a generated text is to a reference by comparing their contextualized token embeddings from BERT. It operates in three main steps:</p>
<ol type="1">
<li><strong>Token Embedding</strong>: Each token is mapped to a contextual vector using a pre-trained BERT model.<br>
</li>
<li><strong>Similarity Matrix</strong>: Cosine similarity is computed between each candidate and reference token.<br>
</li>
<li><strong>Greedy Matching</strong>:
<ul>
<li><strong>Precision</strong>: Max similarity for each candidate token to reference.<br>
</li>
<li><strong>Recall</strong>: Max similarity for each reference token to candidate.<br>
</li>
<li><strong>F1 Score</strong>: Harmonic mean of the two.</li>
</ul></li>
</ol>
<p><strong>Optional Enhancements</strong></p>
<ul>
<li><strong>IDF Weighting</strong>: Emphasizes rare and informative words.<br>
</li>
<li><strong>Baseline Rescaling</strong>: Normalizes scores for consistency.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/bert-score.png" class="img-fluid figure-img" width="800"></p>
<figcaption><strong>Figure:</strong> BERTScore computes contextual similarity between candidate and reference tokens using BERT embeddings and cosine similarity, followed by precision, recall, and F1 aggregation. <em>(Source: Zhang et al.&nbsp;(2020))</em></figcaption>
</figure>
</div>
<p><strong>Example:</strong><br>
- Reference: “The weather is cold today”<br>
- Candidate: “It is freezing today”</p>
<p>BLEU would assign a low score due to low word overlap. BERTScore correctly identifies that “cold” and “freezing” are semantically similar and aligns “it” with “weather,” producing a high score.</p>
</section>
<section id="bleurt-learned-quality-estimation" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="bleurt-learned-quality-estimation"><span class="header-section-number">3.2.2</span> BLEURT: Learned Quality Estimation</h4>
<p>BLEURT offers a learning-based alternative to similarity-based metrics like BERTScore. Instead of relying on heuristic rules, BLEURT is trained to predict human quality judgments directly using fine-tuned BERT models.</p>
<ol type="1">
<li><strong>Pre-training:</strong> Initialized with BERT and trained on synthetically modified sentence pairs. For example, original sentences and their noisy or paraphrased versions. This helps the model learn how edits, errors, or rewordings affect meaning.</li>
<li><strong>Fine-tuning:</strong> BLEURT is fine-tuned using real sentence pairs labeled by humans in shared tasks. This allows it to learn what humans consider high- or low-quality responses.</li>
<li><strong>(Optional) Application-Specific Fine-tuning:</strong> For custom use cases (e.g., legal, medical, or customer support), BLEURT can be further fine-tuned using domain-specific human feedback, improving alignment with task-specific quality standards.</li>
</ol>
<p>Given a candidate and reference sentence, BLEURT returns a scalar score, typically between -1 and 1, reflecting how closely the candidate aligns with human expectations in meaning, fluency, and grammatical correctness.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/bleurt.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure:</strong> BLEURT learns to predict human ratings by pre-training on synthetic sentence pairs and fine-tuning on labeled examples, optionally adapting to task-specific human feedback. <em>(Source: Sellam et al.&nbsp;(2020))</em></figcaption>
</figure>
</div>
<p><strong>Example:</strong><br>
- Reference: “The weather is cold today”<br>
- Candidate: “It is freezing today”</p>
<p>Despite limited word overlap, BLEURT assigns a high score, recognizing semantic equivalence and fluent expression — something traditional metrics might miss.</p>
</section>
<section id="summary-when-to-use-which" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="summary-when-to-use-which"><span class="header-section-number">3.2.3</span> Summary: When to Use Which?</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 46%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Strengths</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>BERTScore</strong></td>
<td>Fast, interpretable; captures token-level semantics</td>
<td>Relies on heuristic alignment; less fluency-aware</td>
</tr>
<tr class="even">
<td><strong>BLEURT</strong></td>
<td>Trained on human ratings; captures fluency and variation</td>
<td>More computationally intensive; less transparent</td>
</tr>
</tbody>
</table>
<p>Both metrics represent a shift toward evaluation methods that better reflect <strong>semantic correctness</strong>, <strong>paraphrasing</strong>, and <strong>generation quality</strong>. BERTScore excels in <strong>semantic alignment</strong>, while BLEURT better captures <strong>fluency</strong> and <strong>natural language variation</strong>.</p>
</section>
</section>
<section id="the-reference-quality-crisis" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="the-reference-quality-crisis"><span class="header-section-number">3.3</span> The Reference Quality Crisis</h3>
<p>Reference-based evaluation metrics—such as BLEU, ROUGE, BERTScore, and BLEURT—are only as reliable as the references themselves. If the reference is flawed—due to poor quality, weak alignment, or narrow scope—it can mislead the metric and penalize otherwise valid model outputs.</p>
<section id="case-study-cnndaily-mail-summarization" class="level4" data-number="3.3.1">
<h4 data-number="3.3.1" class="anchored" data-anchor-id="case-study-cnndaily-mail-summarization"><span class="header-section-number">3.3.1</span> Case Study: CNN/Daily Mail Summarization</h4>
<p>A widely used benchmark for summarization, the CNN/Daily Mail dataset, illustrates the pitfalls of poor reference quality in real-world evaluation. The dataset was constructed by collecting CNN news articles along with their accompanying bullet-point highlights. These highlights, written by journalists under deadline pressure, were used as “gold standard” summaries to train and evaluate models.</p>
<p>However, closer scrutiny revealed major issues. The highlights were often incomplete, loosely structured, or inconsistent in tone. They sometimes emphasized trivial surface-level details over summary-worthy content, and rarely reflected what real users actually wanted from a summary. This raised serious concerns about their reliability as evaluation references.</p>
<p>Several studies have examined this issue. For example, Fabbri et al.&nbsp;(2021) found that common metrics like ROUGE showed <strong>weak or near-zero correlation with human quality judgments</strong> under such conditions. However, when expert-written or carefully revised references were used, the same metrics exhibited significantly stronger correlation (r ≈ 0.6–0.7), suggesting that the evaluation metric itself wasn’t broken—the references were. Similar conclusions were drawn by Akter et al.&nbsp;(2022), who proposed a semantic-aware alternative and highlighted how poor references in CNN/Daily Mail limit metric reliability.</p>
</section>
<section id="broader-challenges-with-reference-based-evaluation" class="level4" data-number="3.3.2">
<h4 data-number="3.3.2" class="anchored" data-anchor-id="broader-challenges-with-reference-based-evaluation"><span class="header-section-number">3.3.2</span> Broader Challenges with Reference-Based Evaluation</h4>
<p>Beyond CNN/Daily Mail, reference quality issues are widespread across many NLP benchmarks:</p>
<ul>
<li><strong>Sparse or Limited References</strong>: Many benchmarks provide only a single reference output, even though multiple diverse and valid responses may exist.</li>
<li><strong>Stylistic or Tonal Inconsistency</strong>: Reference texts often vary in formality, phrasing, or verbosity, which can skew similarity-based metrics.</li>
<li><strong>Task Misalignment</strong>: Some references emphasize different aspects than what users or tasks actually prioritize.</li>
<li><strong>Noisy or Imperfect Texts</strong>: Crowdsourced or time-constrained references may include grammatical mistakes, ambiguity, or shallow reasoning.</li>
</ul>
<p>These limitations are not just theoretical. They directly influence metric scores and evaluation outcomes in ways that can misrepresent model performance.</p>
</section>
<section id="consequences" class="level4" data-number="3.3.3">
<h4 data-number="3.3.3" class="anchored" data-anchor-id="consequences"><span class="header-section-number">3.3.3</span> Consequences</h4>
<p>These reference quality problems have several downstream consequences:</p>
<ul>
<li>Misleading comparisons between models<br>
</li>
<li>Underestimation of strong models that diverge stylistically from the reference<br>
</li>
<li>Optimization toward flawed targets (“gaming the metric”)<br>
</li>
<li>Motivation to explore <strong>Reference-Free Evaluation</strong>, which avoids dependency on flawed gold standards</li>
</ul>
</section>
</section>
<section id="reference-free-evaluation" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="reference-free-evaluation"><span class="header-section-number">3.4</span> Reference-Free Evaluation</h3>
<p>Instead of comparing model outputs to human-written references, <strong>reference-free evaluation</strong> methods attempt to judge quality directly—<strong>without requiring a gold standard</strong>. This is especially useful in open-ended generation tasks where multiple outputs may be valid, and reference quality is often poor (as discussed in Section 3.3).</p>
<p>Reference-based metrics can penalize valid outputs that differ in wording or structure from the reference. As an alternative, reference-free methods evaluate the generation directly—by learning quality predictors or leveraging language models as judges.</p>
<section id="traditional-reference-free-approaches" class="level4" data-number="3.4.1">
<h4 data-number="3.4.1" class="anchored" data-anchor-id="traditional-reference-free-approaches"><span class="header-section-number">3.4.1</span> Traditional Reference-Free Approaches</h4>
<p>Before the rise of large language models (LLMs), researchers explored simpler reference-free methods by training <strong>regression models</strong> to predict human quality ratings.</p>
<ul>
<li><strong>Input</strong>: Model-generated output only (no reference)</li>
<li><strong>Output</strong>: Predicted quality score, usually trained on large annotated datasets</li>
<li><strong>Limitation</strong>: Required costly human-labeled examples and struggled to generalize across domains or tasks</li>
</ul>
<p>These approaches demonstrated the <em>possibility</em> of reference-free evaluation, but lacked robustness and flexibility.</p>
</section>
<section id="llm-as-judge-a-paradigm-shift" class="level4" data-number="3.4.2">
<h4 data-number="3.4.2" class="anchored" data-anchor-id="llm-as-judge-a-paradigm-shift"><span class="header-section-number">3.4.2</span> LLM-as-Judge: A Paradigm Shift</h4>
<p>Recent advances have shown that <strong>LLMs like GPT-4</strong> can act as <strong>sophisticated, multi-criteria evaluators</strong>—scoring generated outputs with surprisingly high alignment to human judgments.</p>
<p><strong>Methodology</strong></p>
<p>A typical prompt might look like this:</p>
<blockquote class="blockquote">
<p><strong>Original Article</strong>: [article text]<br>
<strong>Generated Summary</strong>: [summary text]</p>
<p><strong>Evaluate on a scale of 1 to 5 based on:</strong><br>
- Accuracy (factual correctness)<br>
- Completeness (coverage of key points)<br>
- Clarity (writing quality and coherence)<br>
- Conciseness (appropriate length and focus)</p>
<p><strong>Score</strong>: ___<br>
<strong>Explanation</strong>: ___</p>
</blockquote>
<p>This format enables LLMs to perform nuanced evaluation—<strong>without needing a gold reference</strong>—and produce both numeric ratings and reasoning.</p>
<p><strong>Advantages of LLM-Based Evaluation</strong></p>
<p>Research has shown that LLM-as-judge methods offer several compelling benefits:</p>
<ul>
<li><strong>Multi-dimensional</strong>: Can evaluate factuality, fluency, style, etc.</li>
<li><strong>Scalable and Fast</strong>: ~100× faster and cheaper than human annotators</li>
<li><strong>Consistent</strong>: Inter-rater agreement higher than among humans (85% vs 67%)</li>
<li><strong>Correlated with Human Judgments</strong>: Pearson correlation of <strong>r = 0.8–0.9</strong> in many tasks</li>
</ul>
<blockquote class="blockquote">
<p><em>GPT-4 often shows higher agreement with humans than humans show with each other.</em></p>
</blockquote>
<p><strong>Popular Benchmarks</strong></p>
<p>Several modern benchmarks now use LLMs as evaluation agents:</p>
<ul>
<li><strong>AlpacaEval</strong>: GPT-4 judges instruction-following responses<br>
</li>
<li><strong>MT-Bench</strong>: Evaluates multi-turn dialogues with GPT-based raters<br>
</li>
<li><strong>Chatbot Arena</strong>: Combines LLM judgments with human preferences at scale</li>
</ul>
<p>These benchmarks have helped validate the use of LLMs for practical, scalable evaluation.</p>
<p><strong>Limitations and Biases of LLM-as-Judge Methods</strong></p>
<p>Despite their promise, LLM-based evaluations are not without flaws. Known issues include:</p>
<ul>
<li><strong>Self-Preference Bias</strong>
<ul>
<li>LLMs (e.g., GPT-4) tend to prefer outputs generated by themselves<br>
</li>
<li><em>Mitigation</em>: Use diverse evaluator models and blind prompts</li>
<li><em>Level of Effort</em>: <em>Moderate</em> — requires model access and prompt engineering adjustments</li>
</ul></li>
<li><strong>Length Bias</strong>
<ul>
<li>Tendency to favor longer outputs (~70% of the time), regardless of quality<br>
</li>
<li><em>Mitigation</em>: Normalize for length or include brevity constraints</li>
<li><em>Level of Effort</em>: <em>Low</em> — can be addressed with prompt tuning and post-processing rules</li>
</ul></li>
<li><strong>Style Bias</strong>
<ul>
<li>Preference for structured or bullet-style outputs over more creative formats<br>
</li>
<li><em>Mitigation</em>: Introduce style calibration or reference multiple formats during scoring<br>
</li>
<li><em>Level of Effort</em>: <em>Moderate to High</em> — requires developing style-sensitive evaluation logic</li>
</ul></li>
<li><strong>Cultural and Demographic Bias</strong>
<ul>
<li>Reflect evaluator training data (e.g., Western academic tone)<br>
</li>
<li><em>Mitigation (long-term goal)</em>: Encourage more inclusive evaluator design and culturally diverse data sources for training and evaluation<br>
</li>
<li><em>Level of Effort</em>: <em>High</em> — requires curating diverse datasets and broader systemic changes to training pipelines</li>
</ul></li>
</ul>
<p>These limitations suggest that while LLM-based evaluation is powerful, <strong>careful prompt design, evaluator diversity, and bias monitoring</strong> are essential.</p>
</section>
</section>
<section id="human-evaluations" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="human-evaluations"><span class="header-section-number">3.5</span> Human Evaluations</h3>
<p>Despite rapid advances in automatic metrics, <strong>human judgment remains the gold standard</strong> for evaluating open-ended tasks. Automated scoring methods often miss subtle details, personal opinions, and user preferences—especially in tasks like summarization, dialogue, or creative generation. Human evaluation remains essential for validating whether model outputs are helpful, coherent, accurate, and aligned with human expectations.</p>
<p><strong>What Human Evaluation Measures</strong></p>
<p>Evaluators typically assess a model’s output along key quality dimensions:</p>
<ul>
<li><strong>Fluency</strong> – Is the text grammatically correct and natural?<br>
</li>
<li><strong>Coherence</strong> – Does the output flow logically and make sense throughout?<br>
</li>
<li><strong>Relevance</strong> – Is the response appropriate to the input and task?<br>
</li>
<li><strong>Factual Accuracy</strong> – Are the claims factually correct?<br>
</li>
<li><strong>Completeness</strong> – Does the output cover all necessary points?<br>
</li>
<li><strong>Originality</strong> – Is the output non-redundant and creative?</li>
</ul>
<p>These dimensions align closely with human expectations in real-world applications like summarization, dialogue, and question answering.</p>
<p><strong>Evaluation Methodologies</strong></p>
<p>There are two primary approaches to scoring: <strong>absolute scoring</strong> and <strong>comparative judgment</strong>.</p>
<p>Absolute scoring methods use Likert scales, where annotators rate outputs numerically across different dimensions—typically on a 1-to-5 or 1-to-7 scale. While this provides granular data, it can introduce subjectivity due to differences in annotators’ interpretations of the scale.</p>
<p>Comparative methods, such as pairwise ranking or best–worst scaling, tend to be more robust. In pairwise ranking, annotators compare two or more outputs and choose the better one. Best–worst scaling takes this further by having annotators identify both the best and worst examples from a set. These approaches reduce variability and are commonly used in benchmark leaderboards and model comparisons.</p>
<section id="the-human-evaluation-crisis" class="level4" data-number="3.5.1">
<h4 data-number="3.5.1" class="anchored" data-anchor-id="the-human-evaluation-crisis"><span class="header-section-number">3.5.1</span> The Human Evaluation Crisis</h4>
<p>While human evaluations are critical, they are often plagued by reproducibility, consistency, and cost challenges.</p>
<p>An analysis of NLP papers from 2015–2020 found that only 5% of human evaluation studies were fully reproducible. The primary culprits were lack of methodological detail and unavailability of materials, which made it difficult for others to verify or replicate the results. Common issues included missing annotator guidelines, inconsistent scoring criteria, and the absence of publicly shared data. This undermines scientific rigor and prevents meaningful comparisons across models and studies.</p>
<p>Even when guidelines are clear, <strong>annotator disagreement remains high</strong>. Studies have shown inter-annotator agreement rates hovering around 60–70%, and for more subjective tasks—like helpfulness or summarization—agreement can drop to as low as 40–50%. For example, in one study on summarization quality, even expert annotators disagreed 30–40% of the time.</p>
<p>Human evaluations are also <strong>expensive and time-consuming</strong>. Annotators typically charge $15–$30 per hour, and evaluating a single output can take 5–15 minutes. Large-scale evaluations, which require hundreds or thousands of samples, quickly become cost-prohibitive.</p>
<p>Moreover, <strong>quality control</strong> is non-trivial. Annotators need training, ongoing calibration, and clear rubrics. Without rigorous oversight, annotators may interpret vague instructions differently or apply inconsistent standards. There’s also a natural <strong>incentive misalignment</strong>: annotators may prioritize speed over quality, while researchers seek careful, nuanced judgments. This tension can lead to superficial or biased evaluations.</p>
</section>
<section id="best-practices-for-human-evaluation" class="level4" data-number="3.5.2">
<h4 data-number="3.5.2" class="anchored" data-anchor-id="best-practices-for-human-evaluation"><span class="header-section-number">3.5.2</span> Best Practices for Human Evaluation</h4>
<p>To mitigate these issues, researchers have proposed several best practices. First, providing <strong>clear instructions</strong>—including detailed rubrics and examples—can significantly reduce ambiguity. Conducting <strong>pilot studies</strong> before full-scale evaluation helps refine the task and catch misunderstandings early.</p>
<p>Using <strong>multiple annotators</strong> (typically 3–5 per example) improves reliability, especially when combined with inter-annotator agreement tracking. Regular <strong>calibration sessions</strong> and <strong>feedback loops</strong> ensure that annotators stay aligned over time and surface discrepancies before they affect results.</p>
<p>When applied carefully, these practices make human evaluation more consistent, interpretable, and fair.</p>
<blockquote class="blockquote">
<p><strong>Note</strong>: Human scores should never be compared across studies, as prompts, instructions, and annotators vary widely.</p>
</blockquote>
</section>
</section>
<section id="evaluation-pitfalls-and-biases" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="evaluation-pitfalls-and-biases"><span class="header-section-number">3.6</span> Evaluation Pitfalls and Biases</h3>
<p>Automatic metrics are only as good as the references they rely on. If reference texts are low-quality, misaligned with the task, or poorly written, evaluation results can be misleading. For example, studies have shown that ROUGE is poorly correlated with human preferences unless expert-written references are used.</p>
<p>Beyond reference quality, evaluation metrics can suffer from spurious correlations—patterns that have nothing to do with true output quality. For instance, length bias leads to a ~70% preference for longer outputs, regardless of informativeness. Similarly, models often benefit from list bias (bullet-style responses), position bias (left-vs-right placement in comparisons), and self-bias, where models like GPT-4 subtly favor outputs they generated themselves.</p>
<p>These pitfalls suggest that even widely used metrics can be unintentionally skewed. Careful design and manual oversight are crucial to avoid false confidence in metric-based scores.</p>
</section>
<section id="broader-challenges" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="broader-challenges"><span class="header-section-number">3.7</span> Broader Challenges</h3>
<ul>
<li><strong>Consistency Issues</strong>: Results can vary significantly based on small changes in prompt style or decoding strategy. MMLU scores can fluctuate by up to 15% due to formatting alone—changing “Answer: A” to “A)” or adding “Please” can shift model rankings. This makes benchmark comparisons unreliable when papers don’t specify exact evaluation setups.</li>
<li><strong>Contamination</strong>: Models sometimes perform well on benchmarks because they were trained on test data, not because they generalize. GPT-4’s performance on Codeforces pre-2021 vs post-2021 illustrates this concern. Recent analysis suggests 10-30% of benchmark improvements may reflect contamination rather than genuine capability gains—models scoring 85% on original MMLU might only achieve 65% on contamination-free variants.</li>
<li><strong>Overfitting</strong>: Benchmarks saturate quickly, reducing their long-term usefulness. The pattern is predictable: introduction (40-60% scores) → optimization (rapid progress) → saturation (85-95%) → gaming (exploiting artifacts). GLUE reached human performance within 2 years and required replacement by SuperGLUE.</li>
<li><strong>Monoculture</strong>: A lack of linguistic and cultural diversity—70% of ACL 2021 papers were English-only—limits the generalizability of results. With 85% of NLP benchmarks focusing on English despite 6B non-English speakers globally, models may fail catastrophically in non-Western contexts.</li>
<li><strong>Single Metric Fallacy</strong>: Over-relying on one metric (e.g., accuracy) oversimplifies model performance and can obscure fairness, bias, and usability trade-offs. A customer service chatbot optimized for accuracy might achieve 95% correct answers but sound robotic, leading to poor user satisfaction despite high benchmark scores.</li>
<li><strong>Evaluation Infrastructure Gaps</strong>: 78% of papers don’t specify exact prompts, 65% don’t report model versions or temperature settings, and only 23% provide reproducible evaluation code. This makes meta-analyses impossible and progress measurement unreliable.</li>
</ul>
<p>These challenges collectively suggest that evaluation metrics provide useful signals but should be interpreted cautiously and combined with multiple assessment approaches.</p>
</section>
<section id="evaluation-tools-and-frameworks" class="level3" data-number="3.8">
<h3 data-number="3.8" class="anchored" data-anchor-id="evaluation-tools-and-frameworks"><span class="header-section-number">3.8</span> Evaluation Tools and Frameworks</h3>
<p><strong>Automated Metric Libraries</strong></p>
<ul>
<li><strong>Hugging Face Evaluate</strong>: 60+ metrics with standardized interface. Best for rapid prototyping and consistent implementation.</li>
<li><strong>NLTK</strong>: Traditional metrics (BLEU, ROUGE, METEOR). Mature and well-tested but slower than modern alternatives.</li>
</ul>
<p><strong>Model-Based Evaluation</strong></p>
<ul>
<li><strong>BERTScore/BLEURT</strong>: Semantic similarity using pretrained models. Use for paraphrase-heavy tasks when reference quality varies.</li>
<li><strong>Comet</strong>: Neural MT evaluation framework with high human correlation (r=0.87).</li>
</ul>
<p><strong>LLM-as-Judge Frameworks</strong></p>
<ul>
<li><strong>G-Eval</strong>: GPT-4-based evaluation with chain-of-thought reasoning. Cost: ~$0.01-0.05 per evaluation.</li>
<li><strong>AlpacaEval</strong>: Standardized instruction-following evaluation with 85% human agreement.</li>
</ul>
<p><strong>Comprehensive Suites</strong></p>
<ul>
<li><strong>HELM (Stanford)</strong>: Holistic evaluation across 16 scenarios, 7 metrics. Best for research organizations.</li>
<li><strong>EleutherAI LM Evaluation Harness</strong>: 200+ tasks, open-source. Ideal for reproducible benchmarking.</li>
</ul>
<p><strong>Selection Guide</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/task-evals.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure:</strong> Choosing evaluation metrics based on task type and requirements. Classification tasks are best evaluated using traditional metrics like accuracy or F1. For text generation tasks, the choice of metric depends on context—BLEU/ROUGE for speed, BERTScore/BLEURT for semantic alignment, human evaluation for high-stakes scenarios, and LLM-as-judge for scalable, high-quality assessment.</figcaption>
</figure>
</div>
</section>
<section id="key-takeaways" class="level3" data-number="3.9">
<h3 data-number="3.9" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">3.9</span> Key Takeaways</h3>
<p>Evaluating open-ended tasks is inherently complex. No single approach—automatic, human, or LLM-based—works universally well across all scenarios.</p>
<p>Traditional metrics like BLEU and ROUGE still serve a role, especially for structured tasks like translation. But for generation tasks involving reasoning, creativity, or nuance, they increasingly fall short. Human evaluations offer deeper insights but remain expensive, slow, and often difficult to reproduce. LLM-based evaluations (like GPT-4 as a judge) are fast, scalable, and correlate better with human preferences than n-gram metrics—yet they can hallucinate scores and inherit training biases.</p>
<p><strong>Practical Recommendations:</strong></p>
<ul>
<li>Start with automated metrics for rapid iteration, then validate with human evaluation</li>
<li>Use multiple metrics rather than relying on a single score</li>
<li>For high-stakes applications, always include domain expert review</li>
<li>Consider the evaluation cost vs.&nbsp;application risk trade-off when choosing methods</li>
</ul>
<p>In practice, the most effective strategy is hybrid: combine multiple metrics, validate with human feedback, and always review outputs manually. Quantitative scores alone can be misleading.</p>
<p>As generative systems grow more powerful and widely deployed, evaluation becomes a critical bottleneck. The future lies in developing evaluation methods that are both scalable and semantically meaningful, while addressing emerging challenges like benchmark contamination and safety alignment. The quality of our evaluation methods will ultimately determine how safely and effectively we can deploy these powerful systems.</p>
</section>
</section>
<section id="references-further-reading" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="references-further-reading"><span class="header-section-number">4</span> References &amp; Further Reading</h2>
<p>[1] Wang, A., et al.&nbsp;(2019). SuperGLUE: A Stickier Benchmark for General‑Purpose Language Understanding Systems. https://arxiv.org/pdf/1905.00537</p>
<p>[2] Hendrycks, D., et al.&nbsp;(2020). Measuring Massive Multitask Language Understanding (MMLU). https://arxiv.org/pdf/2009.03300</p>
<p>[3] Papineni, K., et al.&nbsp;(2002). BLEU: A Method for Automatic Evaluation of Machine Translation. https://aclanthology.org/P02-1040</p>
<p>[4] Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. https://aclanthology.org/W04-1013</p>
<p>[5] Chang, T., et al.&nbsp;(2020). <em>BERTSCORE: Evaluating Text Generation With BERT</em>. https://arxiv.org/pdf/1904.09675</p>
<p>[6] Sellam, T., et al.&nbsp;(2020). <em>BLEURT: Learning Robust Metrics for Text Generation</em>. https://arxiv.org/pdf/2004.04696</p>
<p>[7] Durmus, E., et al.&nbsp;(2022). Spurious Correlations in Reference‑Free Evaluation of Text Generation. https://arxiv.org/abs/2204.09890</p>
<p>[8] Fabbri, A., et al.&nbsp;(2021). SummEval: Re-evaluating Summarization Evaluation. https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation</p>
<p>[9] Akter, M., et al.&nbsp;(2022). Can We Do Better than ROUGE? Sem-nCG for Evaluating Summarization. https://aclanthology.org/2022.findings-acl.122.pdf</p>
<p>[10] Specia, L., et al.&nbsp;(2010). Quality Estimation for Machine Translation Without Human-Labeled Data. https://www.aclweb.org/anthology/W10-1753.pdf</p>
<p>[11] Su, Y., et al.&nbsp;(2023). Reference-Free Evaluation Metrics for Text Generation: A Survey. https://arxiv.org/pdf/2305.04648.pdf</p>
<p>[12] Rei, M., et al.&nbsp;(2020). Comet: A neural framework for MT evaluation. https://arxiv.org/pdf/2009.09025</p>
<p>[13] Zheng, S., et al.&nbsp;(2023). Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. https://arxiv.org/pdf/2306.05685</p>
<p>[14] Fu, Y., et al.&nbsp;(2023). G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment. https://arxiv.org/pdf/2303.16634</p>
<p>[15] Tuan, Y., et al.&nbsp;(2021). Quality Estimation without Human-labeled Data. https://arxiv.org/pdf/2102.04020</p>
<p>[16] Celikyilmaz, A., et al.&nbsp;(2020). Evaluation of Text Generation: A Survey. https://arxiv.org/abs/2006.14799</p>
<p>[17] Krishna, K., et al.&nbsp;(2023). LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization. https://aclanthology.org/2023.eacl-main.121.pdf</p>
<p>[18] Liang, P. et al.&nbsp;(2023). Holistic Evaluation of Language Models. https://arxiv.org/pdf/2211.09110</p>
<p>[19] Zheng, L. et al.&nbsp;(2024). Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. https://arxiv.org/pdf/2306.05685</p>
<p>[20] Ouyang, L. et al.&nbsp;(2022). Training language models to follow instructions with human feedback. https://arxiv.org/pdf/2203.02155</p>
<p>[21] Anthropic. (2022). Constitutional AI: Harmlessness from AI Feedback. https://arxiv.org/pdf/2212.08073</p>
<p>[22] Wei, J. et al.&nbsp;(2023). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. https://arxiv.org/pdf/2201.11903</p>
<p>[23] Huyen, C. (2024). <em>AI Engineering: Building Applications with Foundation Models</em>. O’Reilly Media.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>