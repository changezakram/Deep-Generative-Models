<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>NLP Model Evaluation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> <i class="bi bi-house" role="img">
</i> 
<span class="menu-text">Changez Akram</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-generative-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Generative AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-generative-ai">    
        <li>
    <a class="dropdown-item" href="./vae.html">
 <span class="dropdown-text">Variational Autoencoders (VAEs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./flows.html">
 <span class="dropdown-text">Normalizing Flows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ebm.html">
 <span class="dropdown-text">Energy-Based Models (EBMs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./diffusion.html">
 <span class="dropdown-text">Diffusion Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-large-language-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Large Language Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-large-language-models">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/transformers.html">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/post-training.html">
 <span class="dropdown-text">Post Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/nlp-eval.html">
 <span class="dropdown-text">NLP Evaluation</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-agentic-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Agentic AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-agentic-ai">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic-ai.html">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math-review" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math Review</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math-review">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/calculus.html">
 <span class="dropdown-text">Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/probability.html">
 <span class="dropdown-text">Probability</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-use-cases" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Use Cases</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-use-cases">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/banking-use-cases.html">
 <span class="dropdown-text">Banking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/healthcare-use-cases.html">
 <span class="dropdown-text">Healthcare</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction:</a></li>
  <li><a href="#closed-ended-tasks" id="toc-closed-ended-tasks" class="nav-link" data-scroll-target="#closed-ended-tasks"><span class="header-section-number">2</span> Closed-Ended Tasks</a></li>
  <li><a href="#open-ended-text-generation" id="toc-open-ended-text-generation" class="nav-link" data-scroll-target="#open-ended-text-generation"><span class="header-section-number">3</span> Open-Ended Text Generation</a></li>
  <li><a href="#references-further-reading" id="toc-references-further-reading" class="nav-link" data-scroll-target="#references-further-reading"><span class="header-section-number">4</span> References &amp; Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">NLP Model Evaluation</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction:</h2>
<p><strong>What is NLP Evaluation?</strong></p>
<p>NLP evaluation measures how well language models actually work. It answers basic questions: Is this model good enough? Can we trust it with real users?</p>
<p>But here’s the challenge—testing language models isn’t like testing regular software. When code breaks, it’s obvious. When a language model fails, it might write something that sounds perfect but is completely wrong. A model might be 92% accurate overall but fail every time it sees sarcasm. This is what makes NLP evaluation so tricky: we’re measuring how well computers understand the messy, complex world of human language.</p>
<p><strong>Why It Matters Now More Than Ever</strong></p>
<p>In 2023 alone, we saw an AI chatbot fail to recognize suicide warning signs (leading to a death), lawyers submit fake AI-generated cases to courts, and Air Canada forced to honor a refund policy its bot invented. As Chip Huyen warns: “The more AI is used, the more opportunity there is for catastrophic failure.”</p>
<p><strong>The paradox</strong> — The smarter our models get, the harder they become to evaluate. It’s easy to check a kid’s math homework, but verifying if an AI’s medical advice is accurate requires medical expertise. We need good evaluation to build better models, but we need expertise to do good evaluation.</p>
<p><strong>Two Types of Tasks, Two Different Challenges</strong></p>
<ul>
<li><p><strong>Closed-ended tasks</strong> have clear right answers (Is this email spam? What’s the sentiment?). We can use traditional metrics like accuracy and precision, but even these “simple” tasks suffer from shortcuts, dataset problems, and human labeling errors.</p></li>
<li><p><strong>Open-ended tasks</strong> have no single right answer (Write a summary, translate this text, answer this question). Traditional metrics completely fail here. Word-matching might say “Heck no!” is similar to “Heck yes!” because they share words.</p></li>
</ul>
<hr>
</section>
<section id="closed-ended-tasks" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="closed-ended-tasks"><span class="header-section-number">2</span> Closed-Ended Tasks</h2>
<p>Closed-ended tasks are those with a <strong>limited number of possible answers</strong>, often constrained to a <strong>single correct response</strong>. These tasks are well-defined and bounded, making them ideal for <strong>standardized evaluation</strong>, <strong>automation</strong>, and <strong>benchmarking</strong>. They serve as the foundation for much of classical NLP evaluation and are especially useful for training and validating models in structured environments.</p>
<p>While they are more straightforward than open-ended tasks, closed-ended tasks still involve meaningful challenges in evaluation design, metric selection, and dataset construction. The clarity in output space allows for objective comparison, but can sometimes mask nuanced weaknesses in model behavior.</p>
<section id="common-closed-ended-tasks-benchmarks" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="common-closed-ended-tasks-benchmarks"><span class="header-section-number">2.1</span> Common Closed-Ended Tasks &amp; Benchmarks</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 52%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Task Type</th>
<th>Description</th>
<th>Popular Benchmarks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sentiment Analysis</td>
<td>Classify sentiment as Positive, Negative, Neutral</td>
<td>SST, IMDB, Yelp</td>
</tr>
<tr class="even">
<td>Textual Entailment</td>
<td>Determine if a hypothesis logically follows from a premise</td>
<td>SNLI, RTE</td>
</tr>
<tr class="odd">
<td>Named Entity Recognition</td>
<td>Identify named entities in text</td>
<td>CoNLL-2003</td>
</tr>
<tr class="even">
<td>Part-of-Speech Tagging</td>
<td>Assign parts of speech to each word</td>
<td>PTB</td>
</tr>
<tr class="odd">
<td>Coreference Resolution</td>
<td>Determine if pronouns refer to the same entity</td>
<td>WSC</td>
</tr>
<tr class="even">
<td>Question Answering</td>
<td>Extract a plausible answer from a given passage</td>
<td>SQuAD2</td>
</tr>
</tbody>
</table>
<p><strong>Example: Sentiment Analysis:</strong></p>
<ul>
<li><strong>Text</strong>: “Read the book, forget the movie!”<br>
</li>
<li><strong>Label</strong>: Negative</li>
</ul>
<p><strong>Example: Entailment:</strong></p>
<ul>
<li><strong>Premise</strong>: “A soccer game with multiple males playing.”<br>
</li>
<li><strong>Hypothesis</strong>: “Some men are playing sport.”<br>
</li>
<li><strong>Label</strong>: Entailment</li>
</ul>
</section>
<section id="multi-task-benchmark-superglue" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="multi-task-benchmark-superglue"><span class="header-section-number">2.2</span> Multi-Task Benchmark: <strong>SuperGLUE</strong></h3>
<p>SuperGLUE is a comprehensive <strong>closed-ended multi-task benchmark</strong> designed to evaluate general language understanding. It combines multiple tasks that test various aspects of comprehension and reasoning in language models.</p>
<p><strong>Tasks in SuperGLUE:</strong></p>
<ul>
<li><strong>BoolQ, MultiRC</strong>: Reading comprehension<br>
</li>
<li><strong>CB, RTE</strong>: Natural language inference (entailment)<br>
</li>
<li><strong>COPA</strong>: Causal reasoning (cause and effect)<br>
</li>
<li><strong>ReCoRD</strong>: Reading comprehension with commonsense reasoning<br>
</li>
<li><strong>WiC</strong>: Word meaning in context<br>
</li>
<li><strong>WSC</strong>: Coreference resolution</li>
</ul>
<p><strong>Leaderboard Highlights (v2.0):</strong></p>
<ul>
<li><strong>Top models</strong>: Vega v2, ST-MoE-32B, ERNIE, PaLM 540B, T5<br>
</li>
<li><strong>Metrics used</strong>: Accuracy, F1 score, Exact Match, Gender Parity, etc.</li>
</ul>
</section>
<section id="challenges-in-closed-ended-evaluation" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="challenges-in-closed-ended-evaluation"><span class="header-section-number">2.3</span> Challenges in Closed-Ended Evaluation</h3>
<ol type="1">
<li><strong>Choosing the Right Metric:</strong> Choosing the right evaluation metric is not trivial. Different tasks and datasets require different evaluation metrics. A model may appear strong on one metric but perform poorly on another depending on task setup and class imbalance. For instance, accuracy may seem intuitive but can be misleading on imbalanced datasets. A model might achieve high accuracy simply by predicting the majority class. On the other hand, recall and precision offer complementary perspectives: one focusing on completeness (recall) and the other on correctness (precision). The choice of metric directly impacts how model performance is interpreted, compared, and optimized. In multi-class or skewed-distribution scenarios, using a single metric without understanding its limitations can mask critical weaknesses.</li>
</ol>
<p><strong>Common Evaluation Metrics</strong>:</p>
<ul>
<li><strong>Accuracy</strong>: Proportion of correct predictions; suitable for balanced datasets.</li>
<li><strong>Precision</strong>: Proportion of true positives out of all predicted positives.</li>
<li><strong>Recall</strong>: Proportion of true positives out of all actual positives.</li>
<li><strong>F1-Score</strong>: Harmonic mean of precision and recall; ideal for class imbalance.</li>
<li><strong>ROC AUC</strong>: Measures ability to distinguish between classes across thresholds.</li>
</ul>
<ol start="2" type="1">
<li><p><strong>Aggregating Metrics:</strong> In multi-task or multi-subtask setups (e.g., SuperGLUE), it’s important to combine metrics effectively. Simply averaging can underrepresent harder tasks or those with imbalanced label distributions.</p></li>
<li><p><strong>Label Quality</strong></p></li>
</ol>
<ul>
<li>Are the ground-truth labels reliable and consistent?</li>
<li>Is there ambiguity in annotations due to vague definitions or task subjectivity?</li>
</ul>
<ol start="4" type="1">
<li><p><strong>Spurious Correlations:</strong> Models often latch onto dataset artifacts or heuristics rather than true understanding.</p>
<ul>
<li><strong>Example</strong> (SNLI):
<ul>
<li>Premise: “The economy could be still better.”<br>
</li>
<li>Hypothesis: “The economy has <em>never</em> been better.”<br>
</li>
<li>Model might infer contradiction simply due to the word <em>never</em> rather than actual reasoning</li>
</ul></li>
</ul></li>
</ol>
<p>This reveals how even in closed-ended tasks, evaluation must consider model behavior beyond raw accuracy to ensure generalization and robustness.</p>
<hr>
</section>
</section>
<section id="open-ended-text-generation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="open-ended-text-generation"><span class="header-section-number">3</span> Open-Ended Text Generation</h2>
<p>Open-ended generation models, such as large language models (LLMs), are capable of producing free-form outputs like summaries, translations, stories, or answers to instructions. Unlike traditional models that produce a fixed label or number, these models generate entire sequences of text—often with multiple plausible responses for a single input.</p>
<p>This flexibility makes them powerful but also difficult to evaluate. There is rarely a single “correct” output, and even seemingly unrelated completions may still be valid. For example, a user prompt like <em>“Tell me something interesting about the moon”</em> could lead to many diverse, accurate, and coherent answers.</p>
<p>Evaluating such outputs requires more than checking against a reference—it demands assessing coherence, fluency, relevance, factual accuracy, and semantic alignment.</p>
<p>As a result, evaluation of open-ended generation must go beyond traditional metrics and adopt a more nuanced, multi-faceted approach.</p>
<section id="types-of-evaluation-methods" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="types-of-evaluation-methods"><span class="header-section-number">3.1</span> Types of Evaluation Methods</h3>
<p>There are three broad classes of evaluation methods for text generation:</p>
<ul>
<li><strong>Content Overlap Metrics</strong> – compare model outputs against human-written references based on lexical or n-gram overlap.</li>
<li><strong>Model-Based Metrics</strong> – assess semantic similarity using pretrained embedding models (e.g., BERTScore, BLEURT).</li>
<li><strong>Human Evaluations</strong> – involve direct human judgment of qualities such as fluency, coherence, and factual correctness.</li>
</ul>
<p><strong>Content Overlap Metrics:</strong></p>
<p>These methods compute similarity based on surface-level word overlap between the generated text and a reference.</p>
<ul>
<li>Fast and widely used but limited in capturing meaning.</li>
<li>Common metrics include <strong>BLEU</strong>, <strong>ROUGE</strong>, <strong>METEOR</strong>, <strong>CIDEr</strong>.</li>
<li>Often reported for tasks like summarization and translation, despite known limitations.</li>
</ul>
<blockquote class="blockquote">
<p><em>n-gram metrics have no concept of meaning—they fail when different words express the same idea.</em></p>
</blockquote>
<p><strong>Model-Based Metrics</strong></p>
<p>These metrics leverage learned representations from pre-trained language models to evaluate <strong>semantic similarity</strong>.</p>
<ul>
<li>Compute similarity in embedding space between reference and generated outputs.</li>
<li>More robust to paraphrasing and lexical variation.</li>
</ul>
<p>Two popular types:</p>
<ul>
<li><strong>BERTScore</strong>
<ul>
<li>Uses contextual embeddings from BERT and cosine similarity to compare word pairs.<br>
</li>
<li>Captures semantic relationships between words.<br>
</li>
<li><em>(Zhang et al., 2020)</em></li>
</ul></li>
<li><strong>BLEURT</strong>
<ul>
<li>Fine-tuned regression model based on BERT.<br>
</li>
<li>Outputs a score indicating grammar and meaning similarity with reference text.<br>
</li>
<li><em>(Sellam et al., 2020)</em></li>
</ul></li>
</ul>
<p><strong>Human Evaluations</strong></p>
<p>Human evaluation remains the gold standard—especially for open-ended tasks.</p>
<ul>
<li>Captures subjective qualities like <strong>fluency</strong>, <strong>coherence</strong>, <strong>factual accuracy</strong>, and <strong>harmlessness</strong>.</li>
<li>Can use Likert scales, pairwise comparisons, or ranking approaches.</li>
<li>Important to run multiple annotators and test inter-annotator agreement.</li>
</ul>
<p>Evaluation of factuality is particularly difficult. As the lecture notes emphasize, a model can produce highly fluent and confident outputs that are factually wrong. In tasks like summarization, this means comparing to source documents—not just a reference summary.</p>
<p><strong>Reference Quality Matters</strong></p>
<p>Evaluation scores are only as good as the reference they compare against.</p>
<ul>
<li>A flawed reference can mislead metrics like ROUGE or BLEU.</li>
<li>Expert-written references lead to better correlation with human judgments of faithfulness.</li>
<li>When references are missing, metrics break down—making human evaluation essential.</li>
</ul>
<hr>
<blockquote class="blockquote">
<p>“We need to rethink evaluation for open-ended tasks. The answer space is huge, and automated scores don’t always capture what humans care about.”</p>
</blockquote>
<hr>
</section>
</section>
<section id="references-further-reading" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="references-further-reading"><span class="header-section-number">4</span> References &amp; Further Reading</h2>
<p>[16] Huyen, C. (2024). <em>AI Engineering: Building Applications with Foundation Models</em>. O’Reilly Media.</p>
<p>[17] Alammar, J., &amp; Grootendorst, M. (2023). <em>Hands-On Large Language Models: Language Understanding and Generation</em>. O’Reilly Media.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>