<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>NLP Model Evaluation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> <i class="bi bi-house" role="img">
</i> 
<span class="menu-text">Changez Akram</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-generative-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Generative AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-generative-ai">    
        <li>
    <a class="dropdown-item" href="./vae.html">
 <span class="dropdown-text">Variational Autoencoders (VAEs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./flows.html">
 <span class="dropdown-text">Normalizing Flows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ebm.html">
 <span class="dropdown-text">Energy-Based Models (EBMs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./diffusion.html">
 <span class="dropdown-text">Diffusion Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-large-language-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Large Language Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-large-language-models">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/transformers.html">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/post-training.html">
 <span class="dropdown-text">Post Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/nlp-eval.html">
 <span class="dropdown-text">NLP Evaluation</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-agentic-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Agentic AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-agentic-ai">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic-ai.html">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math-review" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math Review</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math-review">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/calculus.html">
 <span class="dropdown-text">Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/probability.html">
 <span class="dropdown-text">Probability</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-use-cases" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Use Cases</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-use-cases">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/banking-use-cases.html">
 <span class="dropdown-text">Banking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/healthcare-use-cases.html">
 <span class="dropdown-text">Healthcare</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction:</a></li>
  <li><a href="#closed-ended-tasks" id="toc-closed-ended-tasks" class="nav-link" data-scroll-target="#closed-ended-tasks"><span class="header-section-number">2</span> Closed-Ended Tasks</a></li>
  <li><a href="#open-ended-text-generation" id="toc-open-ended-text-generation" class="nav-link" data-scroll-target="#open-ended-text-generation"><span class="header-section-number">3</span> Open-Ended Text Generation</a></li>
  <li><a href="#references-further-reading" id="toc-references-further-reading" class="nav-link" data-scroll-target="#references-further-reading"><span class="header-section-number">4</span> References &amp; Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">NLP Model Evaluation</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction:</h2>
<p><strong>What is NLP Evaluation?</strong></p>
<p>NLP evaluation measures how well language models actually work. It answers basic questions: Is this model good enough? Can we trust it with real users?</p>
<p>But here’s the challenge—testing language models isn’t like testing regular software. When code breaks, it’s obvious. When a language model fails, it might write something that sounds perfect but is completely wrong. A model might be 92% accurate overall but fail every time it sees sarcasm. This is what makes NLP evaluation so tricky: we’re measuring how well computers understand the messy, complex world of human language.</p>
<p><strong>Why It Matters Now More Than Ever</strong></p>
<p>In 2023 alone, we saw an AI chatbot fail to recognize suicide warning signs (leading to a death), lawyers submit fake AI-generated cases to courts, and Air Canada forced to honor a refund policy its bot invented. As Chip Huyen warns: “The more AI is used, the more opportunity there is for catastrophic failure.”</p>
<p><strong>The paradox</strong> — The smarter our models get, the harder they become to evaluate. It’s easy to check a kid’s math homework, but verifying if an AI’s medical advice is accurate requires medical expertise. We need good evaluation to build better models, but we need expertise to do good evaluation.</p>
<p><strong>Two Types of Tasks, Two Different Challenges</strong></p>
<ul>
<li><p><strong>Closed-ended tasks</strong> have clear right answers (Is this email spam? What’s the sentiment?). We can use traditional metrics like accuracy and precision, but even these “simple” tasks suffer from shortcuts, dataset problems, and human labeling errors.</p></li>
<li><p><strong>Open-ended tasks</strong> have no single right answer (Write a summary, translate this text, answer this question). Traditional metrics completely fail here. Word-matching might say “Heck no!” is similar to “Heck yes!” because they share words.</p></li>
</ul>
<p><strong>What’s Ahead</strong></p>
<p>This overview covers how to evaluate both closed and open-ended tasks, why current methods fail and what’s replacing them, major problems like contamination and bias, and practical solutions for real-world applications.</p>
<hr>
</section>
<section id="closed-ended-tasks" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="closed-ended-tasks"><span class="header-section-number">2</span> Closed-Ended Tasks</h2>
<p>Closed-ended tasks are widely used in NLP evaluation because they provide clear right or wrong answers. Since the model’s output is limited to a small set of predefined choices—often fewer than ten—they make it easier to compare models using standard metrics like accuracy, precision, recall, and F1-score. This structure allows for objective evaluation, consistent benchmarking, and easier tracking of progress over time.</p>
<p><strong>Bounded Output Space:</strong> Unlike text generation where models can produce any sequence of tokens, closed-ended tasks constrain outputs to predefined categories or structured formats.<br>
<strong>Objective Evaluation:</strong> Success can be measured automatically using established metrics from classical machine learning—accuracy, precision, recall, and F1-score.<br>
<strong>Reproducible Benchmarks:</strong> Standard datasets enable fair comparison across models and over time.<br>
<strong>Systematic Progress Tracking:</strong> Clear metrics allow the field to monitor advancement and identify when models have genuinely improved.</p>
<p>While this constraint makes evaluation more straightforward than open-ended generation, closed-ended tasks still present significant challenges that can mislead researchers and practitioners. Understanding these problems is important because closed-ended evaluation is often the first test of model quality, affecting research focus and decisions about which models to deploy.</p>
<p><strong>Common metrics</strong> such as accuracy, precision, recall, and F1 score are frequently used to evaluate these tasks.</p>
<ul>
<li><strong>Accuracy:</strong> Percentage of correct predictions (simple but can be misleading with imbalanced data).<br>
</li>
<li><strong>Precision:</strong> Of all positive predictions, how many were correct? (important when false alarms are costly).<br>
</li>
<li><strong>Recall:</strong> Of all actual positive cases, how many were found? (important when missing cases is costly).<br>
</li>
<li><strong>F1-Score:</strong> Harmonic mean of precision and recall (balances both concerns).</li>
</ul>
<p><strong>Benchmark</strong> is a standardized dataset with known correct answers that researchers use to test and compare AI models objectively. Like standardized tests for AI, benchmarks enable fair comparison between models from different research groups and track progress over time.</p>
<section id="types-of-closed-ended-tasks" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="types-of-closed-ended-tasks"><span class="header-section-number">2.1</span> Types of Closed-Ended Tasks</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 18%">
<col style="width: 45%">
<col style="width: 9%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Task Type</strong></th>
<th><strong>Description</strong></th>
<th><strong>Example</strong></th>
<th><strong>Popular Benchmarks</strong></th>
<th><strong>Common Pitfalls</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Sentiment Analysis</strong></td>
<td>Classify emotional tone (positive/negative/neutral)</td>
<td>“Read the book, forget the movie!” → <strong>Negative</strong></td>
<td>SST, IMDB, Yelp</td>
<td>Fails on sarcasm, irony, cultural context</td>
</tr>
<tr class="even">
<td><strong>Textual Entailment</strong></td>
<td>Does sentence B logically follow from sentence A?</td>
<td><em>Premise:</em> “A soccer game with multiple males playing”<br><em>Hypothesis:</em> “Some men are playing sport” → <strong>Entailment</strong></td>
<td>SNLI, MultiNLI, RTE</td>
<td>Shortcut learning (e.g., keyword overlap); see SNLI issues</td>
</tr>
<tr class="odd">
<td><strong>Named Entity Recognition (NER)</strong></td>
<td>Identify and classify proper nouns</td>
<td>“Apple released the iPhone.” → <strong>[Apple–ORG]</strong>, <strong>[iPhone–PRODUCT]</strong></td>
<td>CoNLL-2003</td>
<td>Ambiguity: Apple (fruit vs company)</td>
</tr>
<tr class="even">
<td><strong>Part-of-Speech Tagging</strong></td>
<td>Assign grammatical categories to words</td>
<td>“The quick brown fox” → <strong>[The–DET]</strong>, <strong>[quick–ADJ]</strong>, <strong>[brown–ADJ]</strong>, <strong>[fox–NOUN]</strong></td>
<td>Penn Treebank (PTB)</td>
<td>Often used as foundation for parsing &amp; other tasks</td>
</tr>
<tr class="odd">
<td><strong>Coreference Resolution</strong></td>
<td>Determine pronoun references</td>
<td>“Mark told Pete lies about himself. He should have been more truthful.” → <strong>“He” = Mark</strong></td>
<td>WSC, OntoNotes</td>
<td>Requires deep context or world knowledge</td>
</tr>
<tr class="even">
<td><strong>Question Answering</strong></td>
<td>Extract answers from passage</td>
<td><em>Context:</em> “The ESA was passed in 1973.”<br><em>Q:</em> “When was it passed?” → <strong>1973</strong></td>
<td>SQuAD, SQuAD 2.0</td>
<td>Models memorize patterns or position, not true reasoning</td>
</tr>
</tbody>
</table>
<p>These individual tasks can also be grouped into multi-task benchmarks that evaluate general language understanding across a range of closed-ended challenges. <strong>SuperGLUE</strong> is one of the most prominent such benchmarks.</p>
</section>
<section id="multi-task-benchmark-superglue" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="multi-task-benchmark-superglue"><span class="header-section-number">2.2</span> Multi-Task Benchmark: <strong>SuperGLUE</strong></h3>
<p>While the previous section outlined individual closed-ended tasks, real-world evaluation often demands a unified benchmark that spans multiple task types. <strong>SuperGLUE</strong> is a widely adopted <strong>closed-ended multi-task benchmark</strong> created to evaluate broad general language understanding. It builds on its predecessor (GLUE) with harder tasks, more robust metrics, and an emphasis on reasoning.</p>
<p>SuperGLUE combines a diverse set of tasks—ranging from entailment and coreference resolution to causal reasoning and word sense disambiguation—designed to holistically assess a model’s linguistic and reasoning capabilities across multiple dimensions.</p>
<p><strong>Tasks in SuperGLUE:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 57%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Description</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>BoolQ</strong>, <strong>MultiRC</strong></td>
<td>Reading comprehension</td>
<td>QA / Inference</td>
</tr>
<tr class="even">
<td><strong>CB</strong>, <strong>RTE</strong></td>
<td>Natural language inference</td>
<td>Entailment</td>
</tr>
<tr class="odd">
<td><strong>COPA</strong></td>
<td>Causal reasoning (cause/effect)</td>
<td>Reasoning</td>
</tr>
<tr class="even">
<td><strong>ReCoRD</strong></td>
<td>Reading comprehension with commonsense reasoning</td>
<td>QA / Commonsense</td>
</tr>
<tr class="odd">
<td><strong>WiC</strong></td>
<td>Word meaning in context</td>
<td>Word Sense Disambiguation</td>
</tr>
<tr class="even">
<td><strong>WSC</strong></td>
<td>Coreference resolution</td>
<td>Coreference</td>
</tr>
</tbody>
</table>
<p>Together, these tasks go beyond surface-level prediction—testing abilities like logical reasoning, commonsense application, coreference tracking, and contextual understanding.</p>
<p><strong>Leaderboard Highlights (v2.0):</strong></p>
<p>To measure and compare model performance on SuperGLUE, an official leaderboard tracks results across all tasks using standardized metrics. The <strong>v2.0 leaderboard</strong> showcases most advanced models—ranging from parameter-efficient <strong>Mixture of Experts (MoE)</strong> systems to massive transformer-based architectures—offering a clear snapshot of the state of the art in general language understanding.</p>
<ul>
<li><strong>Top models</strong>: Vega v2, ST-MoE-32B, ERNIE, PaLM 540B, T5<br>
</li>
<li><strong>Metrics used</strong>: Accuracy, F1 score, Exact Match, Gender Parity, etc.</li>
</ul>
<p>The leaderboard emphasizes <strong>balanced generalization</strong>, rewarding models that perform consistently well across diverse task types—not just a few. This makes it a reliable benchmark for tracking progress toward broadly capable language models.</p>
</section>
<section id="domain-rich-multi-task-benchmark-mmlu" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="domain-rich-multi-task-benchmark-mmlu"><span class="header-section-number">2.3</span> Domain-Rich Multi-Task Benchmark: MMLU</h3>
<p>While SuperGLUE focuses on general linguistic reasoning, <strong>Massive Multitask Language Understanding (MMLU)</strong> shifts the spotlight to <strong>domain knowledge</strong>. It has rapidly become the <em>de facto</em> benchmark for evaluating a model’s grasp of academic and professional subjects—effectively acting as a proxy for general intelligence in many LLM leaderboards.</p>
<p><strong>What is MMLU?</strong></p>
<ul>
<li><strong>57 subjects</strong> spanning elementary math, anatomy, law, philosophy, computer science, US history, and more<br>
</li>
<li><strong>Multiple-choice format</strong> (A, B, C, D), with ~100 questions per subject<br>
</li>
<li><strong>Balanced question design</strong> that mimics real academic tests and professional licensing exams<br>
</li>
<li><strong>Closed-book evaluation</strong> (no retrieval), testing what the model has internalized from pretraining</li>
</ul>
<p>MMLU has emerged as a <strong>standardized benchmark</strong> for evaluating foundational knowledge across domains. It allows for <strong>direct accuracy-based comparisons</strong> between models of different sizes and architectures. Performance gains have been dramatic—rising from ~<strong>25% (random guessing)</strong> to over <strong>90% accuracy</strong> in just four years.</p>
<blockquote class="blockquote">
<p><strong>Note</strong>: MMLU is often treated as a shortcut for measuring “general intelligence,” but that can be misleading. What it really tests is how well a model can recall facts and recognize patterns—not necessarily how well it can reason or think abstractly. We’ll explore these limitations later.</p>
</blockquote>
<p><strong>MMLU in Modern LLM Leaderboards</strong></p>
<p>Modern LLMs—including GPT-4, Claude 3, Gemini, LLaMA 3, and PaLM—routinely report MMLU scores as a primary metric. As with SuperGLUE, MMLU supports <strong>multi-subject generalization</strong>, but with a stronger emphasis on <strong>world knowledge</strong> rather than linguistic nuance.</p>
<p><strong>SuperGLUE vs MMLU: A Comparison</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 47%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th><strong>SuperGLUE</strong></th>
<th><strong>MMLU</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Focus</strong></td>
<td>Language reasoning</td>
<td>Factual subject knowledge</td>
</tr>
<tr class="even">
<td><strong>Format</strong></td>
<td>Varied NLP tasks</td>
<td>Multiple choice</td>
</tr>
<tr class="odd">
<td><strong>Tasks / Subjects</strong></td>
<td>8 tasks</td>
<td>57 subjects</td>
</tr>
<tr class="even">
<td><strong>Primary Skill Tested</strong></td>
<td>Inference, disambiguation, coreference</td>
<td>Retained domain knowledge</td>
</tr>
<tr class="odd">
<td><strong>Metric</strong></td>
<td>Accuracy, F1, etc.</td>
<td>Accuracy only</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>While both are multi-task benchmarks, they evaluate very different capabilities—SuperGLUE emphasizes reasoning and understanding, whereas MMLU stresses factual recall across disciplines.</p>
</blockquote>
</section>
<section id="challenges-in-closed-ended-evaluation" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="challenges-in-closed-ended-evaluation"><span class="header-section-number">2.4</span> Challenges in Closed-Ended Evaluation</h3>
<ol type="1">
<li><p><strong>Metric Selection:</strong> Different metrics highlight different aspects of model performance. For example: Accuracy can be misleading on imbalanced datasets—it may just reflect majority-class predictions. Precision measures correctness (fewer false positives), while Recall measures completeness (fewer false negatives). Using a single metric in isolation—especially on skewed tasks—can hide a model’s weaknesses.</p></li>
<li><p><strong>Metric Aggregation:</strong> Benchmarks like SuperGLUE combine many tasks, each with its own metric (e.g., F1, accuracy, loss). Simply averaging scores can give an incomplete picture. Some tasks are easier than others, and their metrics scale differently. Without proper weighting or normalization, overall scores may not reflect true performance.</p></li>
<li><p><strong>Label Quality:</strong> Poorly defined or inconsistent labels can introduce noise into both training and evaluation. This reduces reliability and makes it hard to tell if performance differences are meaningful or just due to annotation issues.</p></li>
<li><p><strong>Spurious Correlations:</strong> Models may rely on patterns or keywords rather than real understanding.</p>
<ul>
<li><strong>Example</strong> (SNLI):
<ul>
<li>Premise: “The economy could be still better.”<br>
</li>
<li>Hypothesis: “The economy has <em>never</em> been better.”<br>
</li>
<li>Model might infer contradiction simply due to the word <em>never</em> rather than actual reasoning</li>
</ul></li>
</ul></li>
<li><p><strong>Annotation Artifacts and Dataset Bias</strong>: Some datasets contain structural biases—for example, QA answers often appear at the start of a passage. Models may exploit these shortcuts without real comprehension. Stylistic cues (e.g., sentence length or formality) may also correlate with certain labels, inflating scores without reflecting true understanding.</p></li>
<li><p><strong>Adversarial Robustness</strong>: Small tweaks—like changing a word or rephrasing a sentence—can confuse models even if the meaning stays the same. This shows a lack of generalization. Robust evaluation should test whether models maintain performance under such paraphrased or adversarial inputs.</p></li>
</ol>
<p>This highlights that even well-defined tasks require evaluation beyond raw accuracy to capture true model behavior and ensure robustness.</p>
<hr>
</section>
</section>
<section id="open-ended-text-generation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="open-ended-text-generation"><span class="header-section-number">3</span> Open-Ended Text Generation</h2>
<p>Open-ended generation models, such as large language models (LLMs), can produce diverse free-form outputs like summaries, translations, stories, or answers to instructions. Unlike classification models that generate a fixed label or number, open-ended models return entire sequences of text—often with many valid responses for the same input.</p>
<p>This flexibility makes them powerful but difficult to evaluate. There’s rarely a single “correct” output, and even seemingly unrelated completions may still be valid. For example, the prompt <em>“Tell me something interesting about the moon”</em> could yield many accurate and fluent yet different answers.</p>
<p>Evaluating such responses goes beyond simple reference matching—it requires assessing coherence, fluency, relevance, factual accuracy, style, and semantic alignment. As a result, evaluation of open-ended generation must adopt a more nuanced, multi-faceted approach.</p>
<p>The following sections outline key evaluation methods and challenges.</p>
<section id="content-overlap-metrics" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="content-overlap-metrics"><span class="header-section-number">3.1</span> Content Overlap Metrics</h3>
<p>These methods compute similarity based on surface-level word overlap between the generated text and a human-written reference. They are fast and widely used but fail to capture meaning or paraphrased content.</p>
<ul>
<li><strong>Common metrics:</strong> <code>BLEU</code>, <code>ROUGE</code>, <code>METEOR</code>, <code>CIDEr</code>.</li>
<li>BLEU emphasizes precision; ROUGE emphasizes recall.</li>
<li>Often used in summarization and translation tasks.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Limitation:</strong> N-gram metrics have no concept of meaning—they fail when different words express the same idea.</p>
</blockquote>
<p><strong>Illustrative Example:</strong></p>
<p>Reference: “Heck yes!”<br>
Generated → BLEU score:</p>
<pre><code>- "Yes!" → 67%
- "You know it!" → low
- "Yup" → 0% (but semantically correct!)
- "Heck no!" → 67% (opposite meaning!)</code></pre>
<p>These metrics penalize valid rephrasings and may reward word overlap even when meaning is incorrect.</p>
</section>
<section id="model-based-metrics" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="model-based-metrics"><span class="header-section-number">3.2</span> Model-Based Metrics</h3>
<p>These metrics use learned representations from pre-trained language models to compute <strong>semantic similarity</strong> in embedding space—making them more robust to lexical variation.</p>
<p><strong>Why they matter:</strong> They go beyond word matching and better capture meaning between generated and reference texts.</p>
<p><strong>Popular approaches:</strong></p>
<ul>
<li><strong>BERTScore</strong>
<ul>
<li>Compares contextual embeddings from BERT using cosine similarity.<br>
</li>
<li>Captures pairwise semantic relations.<br>
</li>
<li><em>Zhang et al., 2020</em></li>
</ul></li>
<li><strong>BLEURT</strong>
<ul>
<li>Fine-tuned BERT on human rating datasets.<br>
</li>
<li>Outputs a single score reflecting grammar, fluency, and semantic accuracy.<br>
</li>
<li><em>Sellam et al., 2020</em></li>
</ul></li>
</ul>
<p>These metrics have become popular in evaluating paraphrased or abstractive generation tasks.</p>
</section>
<section id="human-evaluations" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="human-evaluations"><span class="header-section-number">3.3</span> Human Evaluations</h3>
<p>Human evaluation remains the <strong>gold standard</strong>, especially for open-ended tasks.</p>
<p><strong>What they measure:</strong></p>
<ul>
<li>Fluency</li>
<li>Coherence</li>
<li>Factual Accuracy</li>
<li>Commonsense</li>
<li>Harmlessness</li>
<li>Style and grammar</li>
<li>Redundancy</li>
</ul>
<p><strong>Methods:</strong></p>
<ul>
<li>Likert-scale ratings<br>
</li>
<li>Pairwise comparisons<br>
</li>
<li>Ranking systems<br>
</li>
<li>Multi-annotator voting (for inter-rater reliability)</li>
</ul>
<blockquote class="blockquote">
<p><strong>Note:</strong> Never compare scores across studies—evaluation methods, prompts, and annotators vary.</p>
</blockquote>
<p><strong>Challenges:</strong></p>
<ul>
<li><strong>Expensive</strong> and <strong>time-consuming</strong></li>
<li><strong>Inter-annotator disagreement</strong>: Even simple tasks had only 67% agreement.</li>
<li><strong>Intra-annotator inconsistency</strong></li>
<li><strong>Crowdworker incentives</strong>: Speed over care, favoring longer outputs.</li>
<li><strong>Precision-only</strong>: Humans can’t judge what <em>could</em> have been generated, only what <em>was</em>.</li>
<li><strong>Reproducibility crisis</strong>: Only ~5% of NLP papers between 2015–2020 documented enough to replicate human evals.</li>
</ul>
</section>
<section id="reference-free-evaluation" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="reference-free-evaluation"><span class="header-section-number">3.4</span> Reference-Free Evaluation</h3>
<p>Instead of comparing to human-written references, these approaches judge quality <strong>without a gold standard</strong>.</p>
<p><strong>Approaches:</strong></p>
<ul>
<li><strong>Traditional:</strong> Fine-tune BERT to predict quality scores.</li>
<li><strong>Modern (LLM-based):</strong> Use GPT-4 to evaluate generated outputs.
<ul>
<li>Benchmarks: <code>AlpacaEval</code>, <code>MT-Bench</code>, <code>Chatbot Arena</code></li>
<li>Surprisingly strong alignment with human judgments</li>
<li>Fast and scalable—100× cheaper than human eval</li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p>GPT-4 shows <strong>higher agreement with humans than humans show with each other</strong> due to low variance.</p>
</blockquote>
</section>
<section id="side-by-side-comparison-arena-style" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="side-by-side-comparison-arena-style"><span class="header-section-number">3.5</span> Side-by-Side Comparison (Arena-style)</h3>
<p>Used to compare models directly, especially for chatbots or instruction-following tasks.</p>
<p><strong>Example:</strong></p>
<ul>
<li>Ask the same question to two models.</li>
<li>Let humans (or GPT-4) choose the better output.</li>
<li>Use <strong>Elo ratings</strong> (like chess) to build a leaderboard.</li>
</ul>
<p><strong>Tools:</strong></p>
<ul>
<li><strong>Chatbot Arena</strong>: Crowdsourced, ongoing, 200K+ votes</li>
<li><strong>HELM</strong>, <strong>Open LLM Leaderboard</strong>, <strong>VLLM Eval Leaderboard</strong></li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>User-generated prompts may not be representative.</li>
<li>Early-stage models don’t get enough votes.</li>
<li>Not usable for training or development, only end-product evaluation.</li>
</ul>
</section>
<section id="evaluation-pitfalls-and-biases" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="evaluation-pitfalls-and-biases"><span class="header-section-number">3.6</span> Evaluation Pitfalls and Biases</h3>
<p><strong>Reference Quality Matters:</strong></p>
<ul>
<li>Bad references = bad evaluation.</li>
<li>Studies show ROUGE is uncorrelated with human scores unless references are written by experts.</li>
</ul>
<p><strong>Spurious Correlations:</strong></p>
<ul>
<li><strong>Length bias</strong>: ~70% preference for longer outputs</li>
<li><strong>List bias</strong>: Preference for bullet-style responses</li>
<li><strong>Position bias</strong>: Left vs right positioning in comparison</li>
<li><strong>Self-bias</strong>: GPT-4 mildly favors its own outputs</li>
</ul>
</section>
<section id="broader-challenges" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="broader-challenges"><span class="header-section-number">3.7</span> Broader Challenges</h3>
<ul>
<li><strong>Consistency Issues</strong>: Different prompt styles or decoding methods yield very different results (e.g., MMLU scores varied by 15%).</li>
<li><strong>Contamination</strong>: Models trained on test data (e.g., GPT-4 aces pre-2021 Codeforces, flunks post-2021).</li>
<li><strong>Overfitting</strong>: Datasets saturate quickly; benchmark usefulness decays.</li>
<li><strong>Monoculture</strong>: 70% of ACL 2021 papers were English-only; many ignore bias or efficiency.</li>
<li><strong>Single Metric Fallacy</strong>: Oversimplifies model performance. E.g., accuracy alone misses bias, latency, and fairness tradeoffs.</li>
</ul>
</section>
<section id="key-takeaways" class="level3" data-number="3.8">
<h3 data-number="3.8" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">3.8</span> Key Takeaways</h3>
<ul>
<li>Open-ended evaluation is <strong>complex</strong>—no one-size-fits-all.</li>
<li>Traditional metrics (BLEU, ROUGE) are useful but <strong>insufficient</strong>.</li>
<li>Model-based metrics and human evals are better but expensive or slow.</li>
<li>LLM-based eval is promising—<strong>fast</strong>, <strong>scalable</strong>, <strong>well-correlated</strong>.</li>
<li>Always <strong>manually inspect outputs</strong>—numbers alone can be misleading.</li>
</ul>
</section>
</section>
<section id="references-further-reading" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="references-further-reading"><span class="header-section-number">4</span> References &amp; Further Reading</h2>
<p>[16] Huyen, C. (2024). <em>AI Engineering: Building Applications with Foundation Models</em>. O’Reilly Media.</p>
<p>[17] Alammar, J., &amp; Grootendorst, M. (2023). <em>Hands-On Large Language Models: Language Understanding and Generation</em>. O’Reilly Media.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>