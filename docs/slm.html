<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Future of AI Isn’t Bigger — It’s Smaller</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> <i class="bi bi-house" role="img">
</i> 
<span class="menu-text">Changez Akram</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-generative-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Generative AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-generative-ai">    
        <li>
    <a class="dropdown-item" href="./index.html">
 <span class="dropdown-text">Gen AI Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./vae.html">
 <span class="dropdown-text">Variational Autoencoders (VAEs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./flows.html">
 <span class="dropdown-text">Normalizing Flows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ebm.html">
 <span class="dropdown-text">Energy-Based Models (EBMs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./diffusion.html">
 <span class="dropdown-text">Diffusion Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-large-language-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Large Language Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-large-language-models">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/transformers.html">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/post-training.html">
 <span class="dropdown-text">Post Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/nlp-eval.html">
 <span class="dropdown-text">NLP Evaluation</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-agentic-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Agentic AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-agentic-ai">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic-ai.html">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic-analytics.html">
 <span class="dropdown-text">Agentic Analytics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math-review" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math Review</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math-review">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/calculus.html">
 <span class="dropdown-text">Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/probability.html">
 <span class="dropdown-text">Probability</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-use-cases" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Use Cases</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-use-cases">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/banking-use-cases.html">
 <span class="dropdown-text">Banking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/healthcare-use-cases.html">
 <span class="dropdown-text">Healthcare</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#what-is-a-small-language-model" id="toc-what-is-a-small-language-model" class="nav-link" data-scroll-target="#what-is-a-small-language-model"><span class="header-section-number">2</span> What is a Small Language Model?</a>
  <ul class="collapse">
  <li><a href="#generalist-vs.-specialist" id="toc-generalist-vs.-specialist" class="nav-link" data-scroll-target="#generalist-vs.-specialist"><span class="header-section-number">2.1</span> Generalist vs.&nbsp;Specialist</a></li>
  </ul></li>
  <li><a href="#proven-performance-slms-competing-with-giants" id="toc-proven-performance-slms-competing-with-giants" class="nav-link" data-scroll-target="#proven-performance-slms-competing-with-giants"><span class="header-section-number">3</span> Proven Performance: SLMs Competing with Giants</a></li>
  <li><a href="#the-scalability-problem" id="toc-the-scalability-problem" class="nav-link" data-scroll-target="#the-scalability-problem"><span class="header-section-number">4</span> The Scalability Problem</a>
  <ul class="collapse">
  <li><a href="#the-solution-heterogeneous-architecture" id="toc-the-solution-heterogeneous-architecture" class="nav-link" data-scroll-target="#the-solution-heterogeneous-architecture"><span class="header-section-number">4.1</span> The Solution: Heterogeneous Architecture</a></li>
  <li><a href="#why-agents-specifically-need-slms" id="toc-why-agents-specifically-need-slms" class="nav-link" data-scroll-target="#why-agents-specifically-need-slms"><span class="header-section-number">4.2</span> Why Agents Specifically Need SLMs</a></li>
  </ul></li>
  <li><a href="#how-do-they-work" id="toc-how-do-they-work" class="nav-link" data-scroll-target="#how-do-they-work"><span class="header-section-number">5</span> How Do They Work?</a></li>
  <li><a href="#the-landscape-selecting-a-model" id="toc-the-landscape-selecting-a-model" class="nav-link" data-scroll-target="#the-landscape-selecting-a-model"><span class="header-section-number">6</span> The Landscape: Selecting a Model</a>
  <ul class="collapse">
  <li><a href="#quick-reference-the-top-models-of-2025" id="toc-quick-reference-the-top-models-of-2025" class="nav-link" data-scroll-target="#quick-reference-the-top-models-of-2025"><span class="header-section-number">6.1</span> Quick Reference: The Top Models of 2025</a></li>
  </ul></li>
  <li><a href="#the-business-case-the-escalation-strategy" id="toc-the-business-case-the-escalation-strategy" class="nav-link" data-scroll-target="#the-business-case-the-escalation-strategy"><span class="header-section-number">7</span> The Business Case: The “Escalation” Strategy</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">8</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Future of AI Isn’t Bigger — It’s Smaller</h1>
<p class="subtitle lead">An Executive Introduction to Small Language Models (SLMs)</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>For the last few years, the AI industry has been obsessed with size. We raced from 7 billion parameters to a trillion, chasing the dream of a single “God Model” that could do everything.</p>
<p>But in 2025, the wind has shifted. We are entering the era of Small Language Models (SLMs).</p>
<p>These aren’t just downgraded versions of GPT-4; they are specialized, privacy-focused engines designed for efficiency. This shift is fundamentally reshaping how we build AI architectures, especially for the new wave of Autonomous Agents.</p>
<p>Here is an executive overview of the Small Language Models (SLMs).</p>
<hr>
</section>
<section id="what-is-a-small-language-model" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> What is a Small Language Model?</h1>
<p>A Small Language Model (SLM) is a generative AI model — typically under <strong>10 billion parameters</strong> — optimized for specific tasks rather than broad, general-purpose reasoning.</p>
<section id="generalist-vs.-specialist" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="generalist-vs.-specialist"><span class="header-section-number">2.1</span> Generalist vs.&nbsp;Specialist</h2>
<p>The best way to understand SLMs is to view them as <strong>specialized tools</strong> rather than weaker alternatives.</p>
<ul>
<li><p><strong>The Large Model (The Generalist):</strong> Models like GPT-4 are designed to handle ambiguity. They can reason across disconnected topics and solve complex, vague problems. However, using them for routine data processing is inefficient — it’s like firing up a supercomputer just to run a simple spreadsheet.</p></li>
<li><p><strong>The Small Model (The Specialist):</strong> These models are optimized for execution. While they lack the broad “world knowledge” to discuss 14th-century history, they excel at defined tasks like code generation, summarization, or data formatting. Because they are smaller, they deliver results with lower latency and significantly lower costs.</p></li>
</ul>
<hr>
</section>
</section>
<section id="proven-performance-slms-competing-with-giants" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Proven Performance: SLMs Competing with Giants</h1>
<p>The evidence is clear: modern SLMs are not just “smaller” — they are matching and even exceeding larger models on specific tasks.</p>
<p><strong>Recent benchmarks reveal:</strong></p>
<ul>
<li><p><strong>Microsoft Phi-3 (7B parameters)</strong> achieves language understanding and code generation scores comparable to models with 70 billion parameters while running significantly faster.</p></li>
<li><p><strong>NVIDIA Nemotron-H family (2-9B parameters)</strong> delivers instruction-following accuracy comparable to 30B LLMs at a fraction of the computational cost — requiring an order of magnitude fewer FLOPs.</p></li>
<li><p><strong>Salesforce xLAM-2 (8B parameters)</strong> demonstrates state-of-the-art tool-calling performance, surpassing frontier models like GPT-4o and Claude 3.5 Sonnet on specialized benchmarks.</p></li>
</ul>
<p>DeepSeek-R1-Distill-Qwen (7B parameters) outperforms Claude 3.5 Sonnet on reasoning tasks — a remarkable achievement showing that properly trained small models can exceed much larger proprietary systems.</p>
<blockquote class="blockquote">
<p>The performance gap between small and large models is closing rapidly. With modern training techniques, SLMs can deliver 95%+ of the accuracy at 10% of the cost for well-defined tasks.</p>
</blockquote>
<hr>
</section>
<section id="the-scalability-problem" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The Scalability Problem</h1>
<p>The shift to “Small” is largely driven by the rise of <strong>Agentic AI</strong>. We are currently building AI Agents that use tools to complete actual work. Since these agents operate in a continuous loop—thinking, acting, checking, and repeating—the cost of running them stacks up quickly. If every step in that loop relies on a massive model, operations become prohibitively expensive and sluggish.</p>
<section id="the-solution-heterogeneous-architecture" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="the-solution-heterogeneous-architecture"><span class="header-section-number">4.1</span> The Solution: Heterogeneous Architecture</h2>
<p>NVIDIA suggests that scalable systems need to mix different model sizes to work effectively.</p>
<p>In this “Heterogeneous” approach, a Large Language Model (LLM) acts as the Planner, orchestrating the workflow and handling vague user instructions. Meanwhile, a swarm of SLMs acts as the Executors, performing specific steps like running SQL queries, summarizing text, or formatting output.</p>
<p>This approach can reduce costs by 10x to 30x. It also improves reliability, as small models are fine-tuned for a narrow scope and are less likely to “hallucinate” or get distracted by irrelevant information.</p>
</section>
<section id="why-agents-specifically-need-slms" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="why-agents-specifically-need-slms"><span class="header-section-number">4.2</span> Why Agents Specifically Need SLMs</h2>
<p>Agentic systems present a unique economic challenge. Unlike traditional applications where a user submits a single query, agents operate in continuous loops:</p>
<ol type="1">
<li>Perception — Read the current state</li>
<li>Decision — Choose the next action</li>
<li>Execution — Call a tool or API</li>
<li>Verification — Check if the goal is met</li>
<li>Repeat — Loop until complete</li>
</ol>
<p>A single user request might trigger 20-50 model invocations. When each call goes to a large model, costs compound rapidly.</p>
<blockquote class="blockquote">
<p><strong>Real-World Impact</strong> - NVIDIA’s analysis of popular open-source agents (MetaGPT, Open Operator, Cradle) suggests that 60-70% of LLM queries could be reliably handled by specialized SLMs without any loss in end-user experience.</p>
</blockquote>
<p>Most agentic subtasks are repetitive and narrow:</p>
<ul>
<li>Parsing structured data (JSON, XML)</li>
<li>Formatting tool parameters</li>
<li>Validating outputs</li>
<li>Simple decision logic</li>
</ul>
<p>These don’t require the broad “world knowledge” of a 400B parameter model. A specialized 3B model fine-tuned for the specific task often performs better and costs 30× less.</p>
<hr>
</section>
</section>
<section id="how-do-they-work" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> How Do They Work?</h1>
<p>Engineers use four advanced techniques to shrink these models while keeping them smart enough to be useful.</p>
<p><strong>1. Distillation:</strong> A large “Teacher” model generates millions of examples that a small “Student” model studies. This allows the student to learn the necessary logic (the how) without inheriting the teacher’s massive memory requirements.</p>
<p><strong>2. Pruning:</strong> Think of this as trimming a bonsai tree. Researchers analyze the neural network to identify connections that do not contribute to the output. By removing these “dead branches,” they reduce the file size and computational load without significantly hurting performance.</p>
<p><strong>3. Quantization:</strong> This is the most common compression method. Instead of storing numbers with high precision (like 3.14159…), the model rounds them down to simple integers (like 3). This drastically reduces the computer memory (RAM) needed to run the model, allowing powerful AI to run on phones or laptops.</p>
<p><strong>4. LoRA (The Skill Plugin):</strong> This is a game-changer for developers. Instead of retraining the entire model (which takes weeks), engineers use Low-Rank Adaptation (LoRA). This involves adding a tiny, trainable “layer” on top of the model. It’s like giving the model a specific cheat sheet—for example, one for reading legal contracts—without rewriting its entire brain.</p>
<hr>
</section>
<section id="the-landscape-selecting-a-model" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> The Landscape: Selecting a Model</h1>
<p>According to Microsoft, SLMs generally fall into three categories: Distilled Models (smaller versions of popular architectures), Task-Specific Models (fine-tuned for a single domain), and Edge Natives (built from scratch for mobile devices).</p>
<section id="quick-reference-the-top-models-of-2025" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="quick-reference-the-top-models-of-2025"><span class="header-section-number">6.1</span> Quick Reference: The Top Models of 2025</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Best Use Case</th>
<th style="text-align: left;">Why?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Microsoft Phi-3.5</strong></td>
<td style="text-align: left;"><strong>Logic &amp; Reasoning</strong></td>
<td style="text-align: left;">Excels at math and analytical tasks. Ideal for agents that need to solve logic puzzles.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Qwen 2.5 (Alibaba)</strong></td>
<td style="text-align: left;"><strong>Coding &amp; Multilingual</strong></td>
<td style="text-align: left;">State-of-the-art for its size; powerhouse for coding and non-English languages.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Meta Llama 3.2</strong></td>
<td style="text-align: left;"><strong>Mobile / Edge</strong></td>
<td style="text-align: left;">Optimized for ARM processors (phones/tablets). Great for tool calling and strict instructions.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Google Gemma 2</strong></td>
<td style="text-align: left;"><strong>Creative NLP</strong></td>
<td style="text-align: left;">Built on Gemini technology; strong in conversation and creative writing.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Mistral (Ministral)</strong></td>
<td style="text-align: left;"><strong>Low Latency</strong></td>
<td style="text-align: left;">Designed for extreme speed; ideal for instant-response applications.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>IBM Granite 3.0</strong></td>
<td style="text-align: left;"><strong>Enterprise Coding</strong></td>
<td style="text-align: left;">Trained on business software; excellent for RAG and structured coding tasks.</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="the-business-case-the-escalation-strategy" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> The Business Case: The “Escalation” Strategy</h1>
<p>Smart businesses are moving toward a <strong>Hybrid Escalation</strong> model to balance cost and quality.</p>
<ol type="1">
<li><p><strong>Level 1 (The SLM):</strong> A fast, cheap local model handles 80% of routine user queries (password resets, simple questions).</p></li>
<li><p><strong>Level 2 (The LLM):</strong> If the SLM detects a complex issue or gets confused, it “escalates” the ticket to a smarter cloud-based LLM (like GPT-4).</p></li>
</ol>
<p>This keeps operating costs low while ensuring the user always gets an answer.</p>
<p>Transparency &amp; Privacy Beyond cost, businesses adopt SLMs for Transparency. Large models are “Black Boxes”—it is hard to explain why they made a decision. SLMs are simpler, making it easier to trace their logic.</p>
<p>They also enable On-Device Processing. A phone can summarize notifications locally without sending data to the cloud, ensuring user privacy and working even without an internet connection.</p>
<hr>
</section>
<section id="conclusion" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Conclusion</h1>
<p>We are transitioning from the era of the <strong>“God Model”</strong> to the era of the <strong>Model Swarm</strong>. As research highlights, the future of AI agents relies on <strong>specialization</strong>. By offloading routine tasks to Small Language Models, organizations gain greater speed, improved privacy, stronger reliability, and dramatically lower costs.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>