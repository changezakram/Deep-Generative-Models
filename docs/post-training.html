<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Post-Training for Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> <i class="bi bi-house" role="img">
</i> 
<span class="menu-text">Changez Akram</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-generative-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Generative AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-generative-ai">    
        <li>
    <a class="dropdown-item" href="./vae.html">
 <span class="dropdown-text">Variational Autoencoders (VAEs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./flows.html">
 <span class="dropdown-text">Normalizing Flows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ebm.html">
 <span class="dropdown-text">Energy-Based Models (EBMs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./diffusion.html">
 <span class="dropdown-text">Diffusion Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-large-language-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Large Language Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-large-language-models">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/transformers.html">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/post-training.html">
 <span class="dropdown-text">Post Training</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-agentic-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Agentic AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-agentic-ai">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic-ai.html">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math-review" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math Review</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math-review">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/calculus.html">
 <span class="dropdown-text">Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/probability.html">
 <span class="dropdown-text">Probability</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-use-cases" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Use Cases</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-use-cases">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/banking-use-cases.html">
 <span class="dropdown-text">Banking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/healthcare-use-cases.html">
 <span class="dropdown-text">Healthcare</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#introduction-why-post-training-matters-for-reasoning" id="toc-introduction-why-post-training-matters-for-reasoning" class="nav-link active" data-scroll-target="#introduction-why-post-training-matters-for-reasoning"><span class="header-section-number">1</span> Introduction: Why Post-Training Matters for Reasoning</a></li>
  <li><a href="#prompting" id="toc-prompting" class="nav-link" data-scroll-target="#prompting"><span class="header-section-number">2</span> Prompting</a></li>
  <li><a href="#testtime-scaling-methods" id="toc-testtime-scaling-methods" class="nav-link" data-scroll-target="#testtime-scaling-methods"><span class="header-section-number">3</span> Test‑Time Scaling Methods</a></li>
  <li><a href="#supervised-fine-tuning-sft" id="toc-supervised-fine-tuning-sft" class="nav-link" data-scroll-target="#supervised-fine-tuning-sft"><span class="header-section-number">4</span> Supervised Fine-Tuning (SFT)</a></li>
  <li><a href="#preference-optimization" id="toc-preference-optimization" class="nav-link" data-scroll-target="#preference-optimization"><span class="header-section-number">5</span> Preference Optimization</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6</span> Conclusion</a></li>
  <li><a href="#references-further-reading" id="toc-references-further-reading" class="nav-link" data-scroll-target="#references-further-reading"><span class="header-section-number">7</span> References &amp; Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Post-Training for Foundation Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-why-post-training-matters-for-reasoning" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction-why-post-training-matters-for-reasoning"><span class="header-section-number">1</span> Introduction: Why Post-Training Matters for Reasoning</h2>
<p>Large language models like GPT-4 and Claude have demonstrated impressive capabilities across a wide range of tasks, including writing, summarization, problem solving, and open-ended reasoning. These are examples of <strong>foundation models</strong>—large, general-purpose models trained on broad data at scale, designed to serve as flexible building blocks for many downstream applications. But their raw, pre-trained form is not sufficient for safe, reliable deployment.</p>
<p>The core limitation lies in how LLMs are initially trained. They don’t follow strict rules or formal logic. Instead, they learn from statistical patterns in massive text corpora. This makes them flexible and creative—but also means their reasoning can be inconsistent, misleading, or untrustworthy.</p>
<p>These models are typically optimized to predict the next word in a sentence—not to follow instructions, act ethically, or meet human expectations. This initial phase is called <strong>pre-training</strong>, and it focuses on token-level quality—generating the most likely next word given the prior context. But users care more about the quality of the entire response than just the next token.</p>
<p>This is where <strong>post-training</strong> comes in. It refines the model’s behavior to be helpful, safe, and aligned with human intent. Some compare pre-training to reading a vast amount of text, and post-training to learning how to apply that knowledge in a useful, trustworthy way.</p>
<p>Post-training typically involves two key stages:</p>
<ul>
<li><strong>Supervised Fine-Tuning (SFT)</strong>: teaching the model to follow instructions and carry out tasks</li>
<li><strong>Preference Optimization</strong>: aligning outputs with human feedback using methods like RLHF, DPO, or RLAIF</li>
</ul>
<p>These techniques not only improve factual accuracy and safety, but also enhance the model’s ability to reason. And surprisingly, they require relatively little compute—InstructGPT, for example, used only 2% of its total compute budget for post-training.</p>
<p>While supervised and preference tuning are the core of post-training, it’s also important to consider <strong>prompting techniques</strong>. These methods—such as <strong>zero-shot</strong>, <strong>few-shot</strong>, and <strong>chain-of-thought</strong> prompting—don’t change the model’s weights but can steer its behavior at inference time. Prompting often delivers strong results without additional training, making it an essential part of real-world LLM deployment.</p>
<p>In the sections that follow, we’ll explore both post-training techniques and prompting strategies, and how they work together to transform general-purpose LLMs into dependable, aligned assistants.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/post-training-methods.png" class="img-fluid figure-img" width="800"></p>
<figcaption><strong>Figure:</strong> High-level taxonomy of large language model (LLM) post-training strategies, showing where scaling methods (bottom right) fit alongside reinforcement, tuning, and alignment approaches. <em>(Source: Kumar et al., 2023)</em></figcaption>
</figure>
</div>
<hr>
</section>
<section id="prompting" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="prompting"><span class="header-section-number">2</span> Prompting</h2>
<section id="in-context-learning-zero-shot-and-few-shot" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="in-context-learning-zero-shot-and-few-shot"><span class="header-section-number">2.1</span> In-Context Learning: Zero-Shot and Few-Shot</h3>
<p><strong>Concept:</strong><br>
In-Context Learning (ICL) refers to the ability of large language models (LLMs) to perform tasks by conditioning on examples provided in the input prompt, without any gradient updates or fine-tuning. This capability emerged prominently with GPT-3, as shown in Brown et al.&nbsp;(2020), <em>Language Models are Few-Shot Learners</em>.</p>
<p>Traditionally, models learn behavior during training stages such as pre-training and post-training that involve updating model weights. In contrast, ICL allows a model to learn the desired behavior from the <em>context</em> in which the prompt is given, even if the desired task differs from what it was explicitly trained to do. No weight updates are performed; instead, the model leverages its massive training data and generalization ability.</p>
<p><strong>How It Works</strong></p>
<ul>
<li>The model treats the prompt as a temporary training set, inferring patterns between inputs and outputs.</li>
<li>These patterns are applied to new examples within the same prompt context.</li>
<li>This enables the model to adapt its behavior on-the-fly without persistent changes to its parameters.</li>
</ul>
<p><strong>Key Variants</strong></p>
<p><strong>1. Zero-Shot Prompting:</strong> Ask the model to perform a task using only an instruction or question — without providing any explicit examples of how the task should be done.</p>
<ul>
<li>The model relies solely on its pre-trained knowledge and its understanding of the instruction to produce the output.<br>
</li>
<li>Even without examples, the instruction itself acts as a minimal form of <em>context</em> that the model conditions on.</li>
</ul>
<p>Example: Translate the following sentence into French: The weather is beautiful today.<br>
No translations are shown beforehand — the model generates the French sentence using patterns it learned during training.</p>
<p><strong>2. Few-Shot Prompting:</strong> Provide a small number of (input → output) examples before asking for a new prediction.</p>
<ul>
<li>One example = one “shot” (e.g., 5 examples = 5-shot learning).</li>
<li>More examples generally improve performance, but are limited by the model’s maximum context length.</li>
</ul>
<p>Example: Showing labeled sentiment examples before asking for classification of a new sentence.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/eval_strategies.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure:</strong> Illustration of three In-Context Learning settings — Zero-Shot, One-Shot, and Few-Shot — compared with traditional fine-tuning. In ICL, the model adapts based on examples in the prompt without weight updates, while fine-tuning updates model parameters through repeated training. <em>(Source: Brown et al., 2020, Language Models are Few-Shot Learners)</em></figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/superglue_analysis.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure:</strong> SuperGLUE performance comparison between zero-shot, one-shot, and few-shot prompting, and the effect of model size and number of in-context examples. <strong>Left:</strong> Larger models consistently improve SuperGLUE scores across all prompting types, with few-shot showing the largest gains. <strong>Right:</strong> For GPT-3 (175B parameters), performance improves rapidly with just a few in-context examples, then plateaus. <em>(Source: Brown et al., 2020, Language Models are Few-Shot Learners)</em></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Performance Summary">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Performance Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>Across all prompting types, performance consistently improves as model size increases. Few-shot prompting delivers the largest gains, with GPT-3 (175B) achieving scores close to fine-tuned models without any parameter updates.</p>
</div>
</div>
<p><strong>Advantages</strong></p>
<ul>
<li><strong>Flexibility:</strong> The model can incorporate new information at inference time, enabling it to respond to queries beyond its original training cut-off date.</li>
<li><strong>No Retraining Needed:</strong> Eliminates the need to re-train for minor domain or task changes.</li>
<li><strong>Emergent Performance:</strong> Works surprisingly well in very large models (e.g., GPT-3 with 175B parameters) for diverse tasks like translation, reading comprehension, arithmetic, SAT questions, and commonsense reasoning.</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li><strong>Context Length Constraints</strong>: Limited by the model’s maximum input size, restricting the number of examples that can be provided.</li>
<li><strong>Performance Sensitivity</strong>: Output quality varies depending on prompt wording, ordering, and example selection.</li>
<li><strong>Lack of True Learning</strong>: The model’s weights are not updated; adaptation is temporary and disappears when the context changes.</li>
<li><strong>Weak Multi-Step Reasoning</strong>: Struggles with tasks requiring reasoning across multiple logical or arithmetic steps.</li>
<li><strong>Limited Interpretability</strong>: Produces final answers without an explicit reasoning trail, making verification and debugging difficult.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>ICL effectively turns prompting into a form of <em>temporary fine-tuning</em>, where the “training data” exists only within the prompt window. This makes it a powerful method for continual adaptation without persistent retraining.</p>
</div>
</div>
<p>Some of these reasoning limitations — particularly weak multi-step reasoning — can be addressed with <strong>Chain-of-Thought prompting</strong> (Wei et al., 2022), which guides the model to generate intermediate steps before producing the final answer.</p>
</section>
<section id="addressing-icls-reasoning-limitations-chain-of-thought-prompting" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="addressing-icls-reasoning-limitations-chain-of-thought-prompting"><span class="header-section-number">2.2</span> Addressing ICL’s Reasoning Limitations: Chain-of-Thought Prompting</h3>
<p><strong>Concept:</strong><br>
Chain-of-Thought (CoT) prompting is a technique that augments ICL by adding <strong>natural language reasoning steps</strong> between the input and the final output. Instead of going directly from problem → answer, the model is encouraged to generate a sequence of intermediate steps — a “reasoning chain” — before producing its final answer.</p>
<p><strong>How It Works:</strong></p>
<ul>
<li>The prompt explicitly demonstrates step-by-step reasoning for a few training examples in the context.<br>
</li>
<li>The model learns to emulate this reasoning pattern when solving new tasks in the same prompt.<br>
</li>
<li>The final output is preceded by a textual reasoning trace, making the model’s thought process transparent.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/cot-reasoning.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure:</strong> Comparison of standard prompting and Chain-of-Thought prompting on arithmetic reasoning tasks. CoT provides explicit step-by-step reasoning, enabling the model to correctly solve problems that standard prompting fails. <em>(Source: Wei et al., 2022)</em></figcaption>
</figure>
</div>
<p><strong>Why It Helps:</strong></p>
<p>CoT prompting addresses several ICL weaknesses:</p>
<ul>
<li><strong>Improved Multi-Step Reasoning:</strong> By breaking down problems into smaller sub-problems, the model performs better on tasks requiring arithmetic computation, symbolic reasoning, and commonsense logic.</li>
<li><strong>Interpretability:</strong> Produces intermediate steps that allow humans to verify and debug reasoning.</li>
<li><strong>Error Reduction:</strong> Reduces the likelihood of “shortcut” or guess-based answers by forcing the model to articulate the full solution path.</li>
</ul>
<p><strong>Empirical Evidence (Wei et al., 2022):</strong></p>
<p>The chart below is arranged <strong>by benchmark in rows</strong> (Row 1 = GSM8K, Row 2 = SVAMP, Row 3 = MAWPS) and <strong>by model family in columns</strong> (Column 1 = LaMDA, Column 2 = GPT, Column 3 = PaLM).</p>
<ul>
<li><strong>On the GSM8K math word problem benchmark</strong> (Row 1), CoT prompting with <strong>GPT-3 175B</strong> improved solve rate from <strong>17.7%</strong> (standard prompting) to <strong>57.1%</strong>, a gain of nearly <strong>40 percentage points</strong>.<br>
</li>
<li><strong>On the SVAMP benchmark</strong> (Row 2), CoT prompting with <strong>GPT-3 175B</strong> increased performance from about <strong>44%</strong> to <strong>72%</strong>, surpassing the prior supervised best.<br>
</li>
<li><strong>On the MAWPS benchmark</strong> (Row 3), CoT prompting with <strong>PaLM 540B</strong> boosted accuracy from roughly <strong>78%</strong> to <strong>92%</strong>, matching or exceeding the prior supervised best.</li>
</ul>
<p>Across all benchmarks and model families (LaMDA, GPT, PaLM), <strong>larger models consistently show greater gains</strong> from CoT prompting, with the blue curves sitting above the black curves, especially at the largest scales.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/cot-performance.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure:</strong> Performance comparison between standard prompting and Chain-of-Thought prompting across GSM8K, SVAMP, and MAWPS benchmarks for different model families (LaMDA, GPT, PaLM). CoT delivers large gains, particularly for larger models. <em>(Source: Wei et al., 2022)</em></figcaption>
</figure>
</div>
<p><strong>When It Works Best:</strong></p>
<ul>
<li>Tasks with compositional structure where intermediate reasoning steps are natural.<br>
</li>
<li>Models with sufficient scale (CoT’s benefits appear more strongly in models &gt;100B parameters).<br>
</li>
<li>Prompts that clearly format and separate reasoning from final answers.</li>
</ul>
<p><strong>Advantages</strong></p>
<ul>
<li><strong>Enhanced Multi-Step Reasoning:</strong> Significant gains on benchmarks like GSM8K and MultiArith.</li>
<li><strong>Interpretability:</strong> Step-by-step outputs help in auditing and debugging.</li>
<li><strong>Prompt-Only Method:</strong> No fine-tuning required; works within ICL framework.</li>
<li><strong>Emergent Capability:</strong> Reasoning skills become apparent in large-scale models.</li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li><strong>Model Size Dependency:</strong> Large performance gains appear mainly in models &gt;100B parameters; smaller models often produce incoherent chains.</li>
<li><strong>Increased Output Length:</strong> More tokens may affect latency and cost.</li>
<li><strong>Limited Benefit for Simple Tasks:</strong> Minimal improvement for factual recall or single-step reasoning.</li>
<li><strong>Error Propagation:</strong> Early mistakes in reasoning can cascade to incorrect final answers.</li>
<li><strong>Hallucinated Logic:</strong> Reasoning may be coherent but factually wrong.</li>
</ul>
<p>Beyond prompt engineering, another way to boost model performance without changing its weights is through <strong>Test-Time Scaling (TTS)</strong>.<br>
While both prompting and TTS work without retraining the model, they target <em>different levers</em> for improvement:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 24%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Prompting</th>
<th>Test-Time Scaling (TTS)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>When applied</strong></td>
<td>Before inference</td>
<td>During inference</td>
</tr>
<tr class="even">
<td><strong>What changes</strong></td>
<td>The <em>input prompt</em> to guide the model’s output</td>
<td>The <em>inference process</em> — computation allocation, search, sampling</td>
</tr>
<tr class="odd">
<td><strong>Core mechanism</strong></td>
<td>Better instructions, examples, or formatting</td>
<td>Iterative refinement, search-based decoding, adaptive sampling</td>
</tr>
<tr class="even">
<td><strong>Goal</strong></td>
<td>Steer the model toward a better answer</td>
<td>Improve accuracy/reasoning by giving the model more “thinking” time or attempts</td>
</tr>
<tr class="odd">
<td><strong>Analogy</strong></td>
<td>Asking a student a well-phrased question</td>
<td>Giving the student more scratch paper and extra tries</td>
</tr>
</tbody>
</table>
<p>This distinction is important: <strong>prompting</strong> focuses on <em>how you ask</em>, while <strong>TTS</strong> focuses on <em>how the model thinks and searches for answers once asked</em>.</p>
<hr>
</section>
</section>
<section id="testtime-scaling-methods" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="testtime-scaling-methods"><span class="header-section-number">3</span> Test‑Time Scaling Methods</h2>
<p><strong>Concept:</strong><br>
Test‑time scaling (TTS) is the practice of improving model performance and reasoning <strong>during inference</strong> without modifying the model’s parameters. Unlike pre-training, which enhances capabilities through large‑scale training, TTS dynamically allocates computational resources—such as iterative refinement, search‑based decoding, or adaptive sampling—based on task complexity.</p>
<p>TTS can match the performance of a model up to 14× larger on easy to intermediate tasks, while using up to 4× lower compute cost<sup>[1]</sup>. This makes it highly effective when inference budgets are limited or when the base model already has the necessary core competence. However, for tasks requiring fundamentally new capabilities—such as novel reasoning—<strong>pre-training remains superior</strong>, as larger pre-trained models inherently encode deeper reasoning ability.</p>
<p>In practice, TTS and pre-training are <strong>complementary</strong>: pre-training builds broad, general‑purpose competence, while TTS offers flexible, on‑demand optimization at inference time. Many deployments use a <strong>hybrid approach</strong>, combining a pre-trained base model with TTS to achieve cost‑efficient scaling and adaptability.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Improves performance without retraining or altering model parameters<br>
</li>
<li>Can match or exceed much larger models on certain tasks at a fraction of the compute cost<br>
</li>
<li>Flexible allocation of compute based on input complexity<br>
</li>
<li>Enables on‑demand scaling in resource‑constrained settings<br>
</li>
<li>Works well as part of a hybrid strategy with pre-training</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Gains are often smaller on tasks requiring novel skills not present in the base model<br>
</li>
<li>Additional inference compute can increase latency and operational costs<br>
</li>
<li>Requires effective task‑complexity estimation to allocate resources efficiently<br>
</li>
<li>May yield diminishing returns if scaling is excessive for a given task</li>
</ul>
<p>The diagram below organizes TTS techniques into categories, showing typical integration flows between methods.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/test-time-scaling.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure:</strong> An overview of Test-Time Scaling methods: parallel scaling, sequential scaling, and search-based methods. It also illustrates typical integration flows — for example, Chain-of-Thought Prompting can feed into Tree-of-Thoughts or Self-Consistency Decoding. <em>(Source: Kumar et al., 2023)</em></figcaption>
</figure>
</div>
<p>While many approaches exist for test‑time scaling, a few have emerged as especially influential in practical LLM applications. The sections below explore some of these methods in greater detail, including their mechanics, strengths, and trade‑offs.</p>
<section id="best-of-n-search-rejection-sampling" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="best-of-n-search-rejection-sampling"><span class="header-section-number">3.1</span> Best-of-N Search (Rejection Sampling)</h3>
<p><strong>Concept:</strong><br>
Best-of-N (BoN) search enhances model performance at inference by generating <em>N</em> candidate outputs (often via sampling) and selecting the one that scores highest according to a chosen criterion — such as a reward model, likelihood score, or rule-based evaluator. This approach systematically explores multiple solution paths and prunes all but the top-rated result. Compared to Beam Search, BoN treats each candidate independently, which can increase diversity but may also be more computationally expensive.</p>
<p>In reinforcement learning contexts, BoN can be integrated with a learned reward model to pick the best candidate from multiple rollouts, yielding strong performance in reasoning-heavy tasks. It is a popular choice in applications like question answering, code generation, and reasoning benchmarks.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Can significantly boost task performance, especially with a robust reward model</li>
<li>Flexible — works with both rule-based and learned scoring functions</li>
<li>Simple to implement; requires only control over <em>N</em> and the selection criterion</li>
<li>Competitive with post-training methods like RLHF and DPO when paired with strong evaluators</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Higher computational cost as <em>N</em> increases<br>
</li>
<li>Performance depends heavily on the quality of the scoring function or reward model<br>
</li>
<li>May select lower-probability solutions if scoring is imperfect (reward hacking risk)<br>
</li>
<li>Instability can occur if the <em>N</em> parameter is too large or too small for the task</li>
</ul>
</section>
<section id="self-consistency-decoding" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="self-consistency-decoding"><span class="header-section-number">3.2</span> Self-Consistency Decoding</h3>
<p><strong>Concept:</strong><br>
Self-Consistency is a decoding strategy designed to improve reasoning by aggregating answers from multiple reasoning paths. Instead of following a single chain of thought, the model samples diverse reasoning chains (using techniques like prompt engineering to encourage diversity, temperature sampling, or stochastic decoding) and then outputs the final answer that is most consistent across them.</p>
<p>The underlying intuition is that if a complex question has a unique correct answer, different valid reasoning paths should converge on the same result. This <em>majority vote</em> or <em>highest probability after marginalization</em> approach reduces the likelihood of errors from flawed single reasoning chains and is especially effective in reasoning tasks.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Improves correctness in complex reasoning scenarios<br>
</li>
<li>Works well for arithmetic, commonsense reasoning, and multi-step problem-solving<br>
</li>
<li>Reduces reliance on any single, potentially flawed reasoning path<br>
</li>
<li>Can be combined with Chain-of-Thought prompting for greater gains</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Higher inference cost due to multiple reasoning path generations<br>
</li>
<li>Gains diminish for simpler tasks where a single reasoning path is sufficient<br>
</li>
<li>Effectiveness depends on diversity and quality of sampled reasoning paths</li>
</ul>
<p>The following table summarizes key test‑time scaling methods described in <em>LLM Post‑Training: A Deep Dive into Reasoning</em> by Komal Kumar, Tajamul Ashraf et al.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 43%">
<col style="width: 18%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Goal &amp; Common Use Cases</th>
<th>Benefits</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Beam Search</strong></td>
<td>Maintain top‑N highest‑probability reasoning paths at each step; used in structured reasoning, planning, and Tree‑of‑Thought search.</td>
<td>Improves coherence and accuracy by systematically exploring multiple reasoning chains.</td>
<td>Computationally expensive; beam width must be tuned; may still miss rare but correct paths.</td>
</tr>
<tr class="even">
<td><strong>Best‑of‑N (Rejection Sampling)</strong></td>
<td>Generate N candidate outputs (via sampling) and select the best using a criterion (e.g., reward model, model likelihood).</td>
<td>Improves answer quality for easier tasks; straightforward to implement; flexible budget.</td>
<td>High cost if N is large; requires effective selection criteria; may miss diversity benefits.</td>
</tr>
<tr class="odd">
<td><strong>Compute‑Optimal Scaling (COS)</strong></td>
<td>Dynamically allocate compute based on task difficulty; use more search for hard cases, less for easy ones.</td>
<td>Balances accuracy and efficiency; can achieve large efficiency gains over uniform sampling.</td>
<td>Requires reliable difficulty estimation; complex to implement in production.</td>
</tr>
<tr class="even">
<td><strong>Self‑Consistency Decoding</strong></td>
<td>Sample multiple reasoning chains, then select the most common final answer.</td>
<td>Improves multi‑step reasoning accuracy; simple and model‑agnostic.</td>
<td>Higher inference cost; relies on majority vote, which may fail if most outputs are wrong.</td>
</tr>
<tr class="odd">
<td><strong>Tree of Thoughts (ToT)</strong></td>
<td>Expand multiple reasoning paths as a search tree, evaluating and pruning branches.</td>
<td>Enhances complex problem solving and planning; allows backtracking.</td>
<td>High computational cost; requires good heuristics to prune effectively.</td>
</tr>
<tr class="even">
<td><strong>Graph of Thoughts (GoT)</strong></td>
<td>Represent reasoning steps as a graph with flexible dependencies; enables merging and dynamic exploration.</td>
<td>More adaptable than ToT; reduces redundant computation; efficient for complex reasoning.</td>
<td>Implementation complexity; less studied than ToT.</td>
</tr>
<tr class="odd">
<td><strong>Confidence‑Based Sampling</strong></td>
<td>Select expansions or answers based on model confidence scores.</td>
<td>Reduces wasted exploration on low‑probability paths; can improve efficiency.</td>
<td>Confidence scores may not correlate with correctness; risk of overconfidence.</td>
</tr>
<tr class="even">
<td><strong>Search‑Augmented Verification</strong></td>
<td>Use external verifiers to evaluate and rank candidate answers or reasoning steps.</td>
<td>Increases correctness in binary decision tasks; modular and flexible.</td>
<td>Dependent on verifier quality; additional inference steps add latency.</td>
</tr>
<tr class="odd">
<td><strong>Self‑Improvement via Refinements</strong></td>
<td>Model iteratively critiques and revises its own answers until acceptable.</td>
<td>Can improve accuracy across varied tasks; useful for open‑ended reasoning.</td>
<td>Risk of over‑editing or drifting from initial intent; higher inference cost.</td>
</tr>
<tr class="even">
<td><strong>Monte Carlo Tree Search (MCTS)</strong></td>
<td>Explore possible reasoning paths via random simulations, guided by heuristics and rewards.</td>
<td>Effective for planning and reasoning under uncertainty; supports large search spaces.</td>
<td>High computational cost; requires effective reward design.</td>
</tr>
<tr class="odd">
<td><strong>Chain‑of‑Action‑Thought Reasoning</strong></td>
<td>Integrate action planning with reasoning steps for complex tasks.</td>
<td>Handles multi‑agent, tool‑augmented reasoning; improves adaptability.</td>
<td>Complex to implement; requires specialized training or prompting.</td>
</tr>
</tbody>
</table>
<p>While prompting and test-time scaling adjust model behavior without altering weights, supervised fine-tuning directly updates model parameters to improve performance on specific tasks.</p>
</section>
</section>
<section id="supervised-fine-tuning-sft" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="supervised-fine-tuning-sft"><span class="header-section-number">4</span> Supervised Fine-Tuning (SFT)</h2>
<p><strong>Concept:</strong></p>
<p>Supervised fine-tuning is a supervised post-training technique that trains LLMs on curated (instruction, response) pairs. It adapts a pre-trained foundation model to follow natural language instructions across a broad range of tasks — even ones it hasn’t seen before. In doing so, it adjusts the model’s internal weights to better handle the target task or domain. Instruction finetuning can also be applied to specific domains or tasks, such as sentiment analysis, question answering, or medical diagnosis, by updating model parameters on high-quality datasets.</p>
<p>While instruction fine-tuning improves task performance and alignment, it poses challenges including <strong>overfitting</strong>, <strong>high computational costs</strong>, and <strong>sensitivity to dataset biases</strong>. Parameter-efficient variants such as <strong>LoRA</strong> and <strong>adapters</strong> address these issues by updating only a small subset of parameters, reducing compute and storage requirements. However, increased specialization may reduce generalization to out-of-domain tasks, creating a trade-off between specificity and versatility.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Improves zero- and few-shot task generalization<br>
</li>
<li>Increases consistency and alignment across varied prompts<br>
</li>
<li>Produces more helpful, structured, and controllable responses<br>
</li>
<li>Can be tailored for domain-specific performance gains<br>
</li>
<li>Parameter-efficient methods (e.g., LoRA, adapters) enable faster, cheaper adaptation</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Requires significant effort to create high-quality, diverse instruction datasets<br>
</li>
<li>Evaluating open-ended instructions can be subjective<br>
</li>
<li>Human-generated data may introduce bias, inconsistency, or noise<br>
</li>
<li>Risk of overfitting to narrow domains<br>
</li>
<li>May reduce generalization to out-of-domain scenarios</li>
</ul>
<p>While many approaches exist for supervised fine‑tuning, <strong>Instruction Fine‑Tuning</strong> and <strong>Domain‑Specific Fine‑Tuning</strong> are among the most prevalent in modern LLM post‑training pipelines. The sections below explore these methods in greater detail.</p>
<section id="instruction-fine-tuning" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="instruction-fine-tuning"><span class="header-section-number">4.1</span> Instruction Fine-Tuning</h3>
<p><strong>Concept:</strong><br>
Instruction Fine‑Tuning is the most widely used form of supervised fine‑tuning in LLM post‑training. It involves training a pre‑trained model on curated datasets of instruction–response pairs covering a wide variety of tasks, such as summarization, question answering, classification, and creative writing. The goal is to make the model follow natural language instructions reliably, even for tasks it hasn’t seen before.<br>
This process adjusts the model’s internal weights so it produces outputs that align with the intent of the instruction rather than simply predicting the next word. Instruction fine‑tuning is often the <strong>first post‑training step</strong> in modern LLM pipelines and serves as a foundation for subsequent preference optimization stages like RLHF or DPO.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Improves generalization to unseen tasks in zero‑shot and few‑shot settings.<br>
</li>
<li>Produces more helpful, controllable, and structured outputs.<br>
</li>
<li>Increases consistency across varied prompt styles.<br>
</li>
<li>Provides a strong baseline for further preference optimization.</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Requires large, high‑quality, and diverse instruction datasets.<br>
</li>
<li>Open‑ended instructions can be difficult to evaluate objectively.<br>
</li>
<li>Human‑generated data may introduce bias, inconsistency, or noise.<br>
</li>
<li>Can overfit to dataset style if training data lacks diversity.</li>
</ul>
</section>
<section id="domain-specific-fine-tuning" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="domain-specific-fine-tuning"><span class="header-section-number">4.2</span> Domain-Specific Fine-Tuning</h3>
<p><strong>Concept:</strong></p>
<p>Domain‑Specific Fine‑Tuning adapts a general‑purpose LLM to excel in a specialized field, such as biomedicine, finance, legal, climate science, or software engineering. The process uses curated domain‑specific datasets to teach the model the terminology, style, and knowledge relevant to that field.<br>
By focusing on specialized corpora, the model can deliver more accurate, relevant, and trustworthy outputs for domain‑specific use cases, making it valuable for enterprise and industry applications.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Improves accuracy, factual grounding, and relevance in the target domain.<br>
</li>
<li>Enhances user trust for high‑stakes applications.<br>
</li>
<li>Supports compliance with domain‑specific standards or regulations.<br>
</li>
<li>Can reduce hallucinations by anchoring the model in verified domain content.</li>
</ul>
<p><strong>Limitations:</strong><br>
- Requires high‑quality, domain‑specific datasets that can be costly or difficult to obtain.<br>
- Risk of reduced generalization to tasks outside the target domain.<br>
- May inherit biases or gaps present in domain data.<br>
- Can lead to over‑specialization if fine‑tuning data is too narrow.</p>
<p>The following table summarizes supervised fine-tuning methods described in <em>LLM Post-Training: A Deep Dive into Reasoning</em> by Komal Kumar, Tajamul Ashraf et al.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 37%">
<col style="width: 15%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Fine-tuning Type</th>
<th>Goal &amp; Common Use Cases</th>
<th>Benefits</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Instruction Fine-Tuning</strong></td>
<td>Train LLMs to follow diverse instructions (e.g., summarization, classification, QA, creative writing). Enables zero-/few-shot generalization across tasks.</td>
<td>Improves generalization and alignment; makes outputs more helpful and controllable.</td>
<td>Requires large, curated datasets; open-ended tasks are harder to evaluate; may reflect human bias.</td>
</tr>
<tr class="even">
<td><strong>Dialogue (Multi-turn) Fine-Tuning</strong></td>
<td>Enable coherent, context-aware multi-turn conversations for chatbots and digital assistants.</td>
<td>Improves coherence, context tracking, and conversational experience.</td>
<td>Can overfit to chattiness; needs large, high-quality multi-turn dialogue datasets.</td>
</tr>
<tr class="odd">
<td><strong>Chain-of-Thought (CoT) Reasoning Fine-Tuning</strong></td>
<td>Encourage step-by-step reasoning in math, logic puzzles, multi-hop QA.</td>
<td>Improves reasoning interpretability and multi-step accuracy.</td>
<td>Requires structured reasoning traces; limited to reasoning-style tasks.</td>
</tr>
<tr class="even">
<td><strong>Domain-Specific Fine-Tuning</strong></td>
<td>Adapt models for specialized fields (e.g., biomedicine, finance, legal, climate, code).</td>
<td>Improves accuracy and relevance in domain-specific applications.</td>
<td>Needs high-quality, domain-specific corpora; risk of reduced generality.</td>
</tr>
<tr class="odd">
<td><strong>Distillation-Based Fine-Tuning</strong></td>
<td>Transfer capabilities from a large “teacher” model to a smaller “student” model.</td>
<td>Produces smaller, faster models with high performance; reduces compute cost.</td>
<td>May lose nuance or performance compared to teacher; quality depends on teacher data.</td>
</tr>
<tr class="even">
<td><strong>Preference/Alignment SFT</strong></td>
<td>Train models on labeled or ranked preference data before RLHF or DPO stages.</td>
<td>Improves alignment with human values; reduces harmful or irrelevant outputs.</td>
<td>Limited by scope and quality of preference data; definitions of “desirable” can vary.</td>
</tr>
<tr class="odd">
<td><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong></td>
<td>Efficiently adapt models without updating all weights (e.g., LoRA, adapters, prefix tuning).</td>
<td>Resource-efficient; enables adaptation on limited hardware.</td>
<td>May underperform full fine-tuning; sensitive to hyperparameter choices.</td>
</tr>
</tbody>
</table>
<p>Once the model has been trained to follow instructions, the next step is often to align it more closely with human preferences through preference optimization.</p>
</section>
</section>
<section id="preference-optimization" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="preference-optimization"><span class="header-section-number">5</span> Preference Optimization</h2>
<p><strong>Concept:</strong></p>
<p>Preference optimization is the process of aligning a model’s outputs with human preferences using feedback signals, rather than solely optimizing for task performance. In large language models (LLMs), this often involves training the model to produce responses that are not only factually correct but also safe, contextually relevant, and consistent with user expectations.<br>
Unlike conventional reinforcement learning, which operates in small, well‑defined action spaces with clear objectives, preference optimization in LLMs must navigate a <strong>vast vocabulary action space</strong>, delayed and subjective rewards, and multiple, sometimes conflicting, objectives. Feedback is often based on human or AI‑generated preference comparisons and can involve both outcome‑based metrics (e.g., correctness) and process‑based evaluations (e.g., reasoning quality).<br>
The overarching goal is to make models behave according to human preferences — an ambitious aim given that universal human values do not exist, and perceptions of “desirable” behavior vary widely across cultures, political views, and personal beliefs.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Aligns model outputs with user expectations and ethical guidelines.<br>
</li>
<li>Improves perceived helpfulness and safety in real‑world use.<br>
</li>
<li>Can reduce harmful, offensive, or irrelevant outputs.<br>
</li>
<li>Allows optimization for nuanced objectives (e.g., tone, reasoning quality, factuality).<br>
</li>
<li>Supports personalization to match specific user or organizational preferences.</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Subjectivity of preferences — what’s “helpful” or “appropriate” can vary across cultures, contexts, and individuals.<br>
</li>
<li>Controversial topics — responses risk alienating some users regardless of stance; overly cautious models may seem bland or evasive.<br>
</li>
<li>Technical complexity — high‑dimensional action space, delayed rewards, and balancing multiple objectives make optimization challenging.<br>
</li>
<li>Bias and fairness risks — preferences used in training may embed societal biases.<br>
</li>
<li>Deployment trade‑offs — excessive filtering can reduce engagement, while insufficient filtering can create reputational and safety risks.</li>
</ul>
<p>While many approaches exist for preference optimization, <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> and <strong>Direct Preference Optimization (DPO)</strong> are two of the most prominent in modern LLM alignment pipelines. The sections below explore these methods in greater detail.</p>
<section id="reinforcement-learning-from-human-feedback-rlhf" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="reinforcement-learning-from-human-feedback-rlhf"><span class="header-section-number">5.1</span> Reinforcement Learning from Human Feedback (RLHF)</h3>
<p><strong>Concept:</strong><br>
RLHF aligns LLMs with human preferences by collecting feedback from human annotators and using it to guide reinforcement learning. The process typically involves:<br>
1. Supervised Fine‑Tuning (SFT) on high‑quality instruction‑response pairs to create a baseline model.<br>
2. Preference data collection — human annotators rank multiple model outputs for the same prompt.<br>
3. Reward model training — a separate model learns to predict these rankings.<br>
4. Reinforcement learning optimization — the LLM is fine‑tuned using algorithms like PPO to maximize the reward model score while staying close to the baseline model.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Produces highly aligned and safe responses.<br>
</li>
<li>Can optimize for complex, nuanced objectives beyond accuracy.<br>
</li>
<li>Allows fine‑grained control through reward model design.</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Very expensive and labor‑intensive to gather preference data.<br>
</li>
<li>Quality depends heavily on the consistency and skill of annotators.<br>
</li>
<li>Reward models can be exploited (reward hacking).<br>
</li>
<li>Sensitive to bias in the collected preferences.</li>
</ul>
</section>
<section id="direct-preference-optimization-dpo" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="direct-preference-optimization-dpo"><span class="header-section-number">5.2</span> Direct Preference Optimization (DPO)</h3>
<p><strong>Concept:</strong><br>
DPO is a simpler alternative to RLHF that removes the need for a separate reward model and online reinforcement learning. Instead of predicting absolute reward scores, DPO learns directly from preference pairs (chosen vs.&nbsp;rejected outputs) by optimizing the log‑likelihood ratio to make preferred responses more probable. This approach “bakes in” the user’s preferences directly into the model parameters, avoiding the complexity of the RL loop.</p>
<p><strong>Advantages:</strong></p>
<ul>
<li>Simpler and more stable than RLHF.<br>
</li>
<li>No need for online sampling or PPO training.<br>
</li>
<li>Scales well to large datasets.<br>
</li>
<li>Increasingly popular in open‑source models like LLaMA and OpenChat.</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>No per‑step credit assignment — treats whole responses as a unit.<br>
</li>
<li>May underperform RLHF for multi‑step reasoning tasks.<br>
</li>
<li>Dependent on high‑quality preference pairs; poor data reduces effectiveness.</li>
</ul>
<p>The following table summarizes supervised Preference Optimization methods described in <em>LLM Post-Training: A Deep Dive into Reasoning</em> by Komal Kumar, Tajamul Ashraf et al.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 43%">
<col style="width: 18%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Goal &amp; Common Use Cases</th>
<th>Benefits</th>
<th>Limitations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>RLHF (Reinforcement Learning from Human Feedback)</strong></td>
<td>Align model outputs with human expectations using preference comparisons from human annotators to train a reward model, then optimize with RL (e.g., PPO).</td>
<td>Produces helpful, safe, and human-aligned responses; can optimize for nuanced objectives; widely adopted in practice.</td>
<td>Expensive and time‑consuming to collect human preference data; reward models can be overfit or gamed; dependent on noisy human judgments.</td>
</tr>
<tr class="even">
<td><strong>RLAIF (Reinforcement Learning from AI Feedback)</strong></td>
<td>Replace human annotation with AI‑generated feedback to create preference labels for training the reward model.</td>
<td>Reduces cost and time; scalable to large datasets; avoids bottleneck of human labeling.</td>
<td>Quality depends on feedback model; risk of propagating biases or errors from the AI judge; less diversity than human feedback.</td>
</tr>
<tr class="odd">
<td><strong>DPO (Direct Preference Optimization)</strong></td>
<td>Learn directly from preference pairs without training a separate reward model or running PPO, by optimizing likelihood ratios to favor preferred responses.</td>
<td>Simpler and more stable than RLHF; no online sampling; scalable; increasingly popular in open‑source LLMs.</td>
<td>Lacks per‑step credit assignment; may underperform RLHF for complex reasoning tasks; dependent on high‑quality preference data.</td>
</tr>
<tr class="even">
<td><strong>OREO (Online Reasoning Optimization)</strong></td>
<td>RL method to improve multi‑step reasoning by refining policies based on reasoning‑step evaluations rather than just final answers.</td>
<td>Fine‑grained feedback at reasoning step level; boosts reasoning accuracy and interpretability.</td>
<td>Computationally intensive; domain‑specific; requires curated reasoning traces.</td>
</tr>
<tr class="odd">
<td><strong>GRPO (Group Relative Policy Optimization)</strong></td>
<td>RL variant that scores multiple outputs for the same query relative to each other, eliminating the need for a critic model.</td>
<td>Reduces memory usage; stabilizes training; enables fine‑grained rewards for complex reasoning tasks.</td>
<td>Requires large groups of candidate responses; effectiveness depends on diversity and quality of generated outputs.</td>
</tr>
<tr class="even">
<td><strong>Pure RL‑Based LLM Refinement</strong></td>
<td>Multi‑stage RL pipelines (e.g., DeepSeek-R1) that refine models without or with minimal SFT, often incorporating distillation and curated reasoning traces.</td>
<td>Can achieve high performance without large SFT datasets; distillation improves efficiency; robust reasoning capabilities.</td>
<td>Complex to implement; computationally expensive; requires large curated datasets for stability and quality.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="conclusion" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">6</span> Conclusion</h2>
<p>Post‑training for foundation models is not a single method, but a toolbox of strategies that work best in combination. Prompting and test‑time scaling methods can produce immediate gains without changing model weights, making them ideal for rapid experimentation and deployment. Supervised fine‑tuning adapts models for broad instruction‑following or specialized domains, while preference optimization aligns model behavior with human values and expectations.</p>
<p>In practice, high‑performing LLM systems often blend these techniques: using prompting for quick control, test‑time scaling for reasoning‑heavy queries, supervised fine‑tuning for skill building, and preference optimization for safety and alignment. As the field evolves, we can expect tighter integration between these approaches, along with emerging trends like agentic AI, retrieval‑augmented generation, and continuous self‑improvement loops.</p>
<p>The key takeaway: effective post‑training is about matching the method to the goal. Understanding the trade‑offs of each category enables practitioners to build LLM solutions that are not only more capable, but also safer, more reliable, and better aligned with human intent.</p>
</section>
<section id="references-further-reading" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="references-further-reading"><span class="header-section-number">7</span> References &amp; Further Reading</h2>
<p>[1] Kumar, K., Ashraf, T., Thawakar, O., et al.&nbsp;(2024). <em>LLM Post-Training: A Deep Dive into Reasoning Large Language Models</em>. https://arxiv.org/abs/2502.21321</p>
<p>[2] Brown, T., Mann, B., Ryder, N., et al.&nbsp;(2020). <em>Language Models are Few-Shot Learners (GPT-3)</em>. https://arxiv.org/abs/2005.14165</p>
<p>[3] Wei, J., Wang, X., Schuurmans, D., et al.&nbsp;(2022). <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em>. https://arxiv.org/abs/2201.11903</p>
<p>[4] Ouyang, L., Wu, J., Jiang, X., et al.&nbsp;(2022). <em>Training Language Models to Follow Instructions with Human Feedback (InstructGPT)</em>. https://arxiv.org/abs/2203.02155</p>
<p>[5] Chung, H. W., Hou, L., Longpre, S., et al.&nbsp;(2022). <em>Scaling Instruction-Finetuned Language Models (Flan)</em>. https://arxiv.org/abs/2210.11416</p>
<p>[6] Rafailov, R., Sharma, A., Mitchell, E., et al.&nbsp;(2023). <em>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</em>. https://arxiv.org/abs/2305.18290</p>
<p>[7] Wei, J., Wang, X., Schuurmans, D., et al.&nbsp;(2022). <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em>. https://arxiv.org/abs/2201.11903</p>
<p>[8] Wang, X., Wei, J., Schuurmans, D., et al.&nbsp;(2022). <em>Self-Consistency Improves Chain of Thought Reasoning in Language Models</em>. https://arxiv.org/abs/2203.11171</p>
<p>[9] Yao, S., Yu, D., Zhao, J., et al.&nbsp;(2023). <em>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</em>. https://arxiv.org/abs/2305.10601</p>
<p>[10] Houlsby, N., Giurgiu, A., Jastrzebski, S., et al.&nbsp;(2019). <em>Parameter-Efficient Transfer Learning for NLP</em>. https://arxiv.org/abs/1902.00751</p>
<p>[11] Radford, A., Wu, J., Child, R., et al.&nbsp;(2019). <em>Language Models are Unsupervised Multitask Learners</em>. https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</p>
<p>[12] Kojima, T., Gu, S. S., Reid, M., et al.&nbsp;(2022). <em>Large Language Models are Zero-Shot Reasoners</em>. https://arxiv.org/abs/2205.11916</p>
<p>[13] Zhou, X., Santurkar, S., Bau, D., et al.&nbsp;(2022). <em>Large Language Models Are Human-Level Prompt Engineers</em>. https://arxiv.org/abs/2211.01910</p>
<p>[14] Huyen, C. (2024). <em>AI Engineering: Building Applications with Foundation Models</em>. O’Reilly Media.</p>
<p>[15] Alammar, J., &amp; Grootendorst, M. (2023). <em>Hands-On Large Language Models: Language Understanding and Generation</em>. O’Reilly Media.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>