<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ebm – Deep Generative Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a14e3238c51140e99ccc48519b6ed9ce.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Deep Generative Models</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In the world of generative models, techniques like VAEs, GANs, and normalizing flows have each carved out their niche—but all of them come with specific constraints. <strong>Energy-Based Models (EBMs)</strong> offer a powerful alternative that’s <strong>architecturally flexible</strong>, <strong>conceptually elegant</strong>, and <strong>growing in popularity</strong> in modern deep learning research.</p>
<hr>
</section>
<section id="limitations-of-mainstream-generative-models" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-mainstream-generative-models">Limitations of Mainstream Generative Models</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 30%">
<col style="width: 51%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>VAEs</strong></td>
<td>Probabilistic framework, tractable ELBO</td>
<td>Model architecture restrictions; blurry samples</td>
</tr>
<tr class="even">
<td><strong>Normalizing Flows</strong></td>
<td>Exact likelihood, invertibility</td>
<td>Restricted to invertible architectures; expensive Jacobian computation</td>
</tr>
<tr class="odd">
<td><strong>Autoregressive Models</strong></td>
<td>Exact likelihood</td>
<td>Slow sampling; autoregressive dependency limits parallelism</td>
</tr>
<tr class="even">
<td><strong>GANs</strong></td>
<td>High-quality samples; flexible</td>
<td>No likelihood; unstable training; mode collapse</td>
</tr>
</tbody>
</table>
<p>These models attempt to approximate the true data distribution <span class="math inline">\(P_{\text{data}}\)</span> by selecting a model <span class="math inline">\(P_\theta\)</span> from a constrained family, often limited by the need for tractable likelihoods, invertible mappings, or adversarial training stability.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>To understand how Energy-Based Models overcome many of these limitations, we first need to revisit a fundamental question in generative modeling: <strong>what makes a valid probability distribution?</strong></p>
<p>The next section walks through the mathematical foundation that underpins EBMs — and introduces a clever trick that allows them to sidestep many of the constraints faced by traditional generative models.</p>
</div>
</div>
</div>
<hr>
</section>
<section id="math-review" class="level2">
<h2 class="anchored" data-anchor-id="math-review">Math Review</h2>
<section id="understanding-the-probability-foundation-behind-ebms" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-probability-foundation-behind-ebms">Understanding the Probability Foundation Behind EBMs</h3>
<p>In generative modeling, a valid probability distribution <span class="math inline">\(p(x)\)</span> must satisfy:</p>
<ul>
<li><p><strong>Non-negativity</strong>:<br>
<span class="math display">\[
p(x) \geq 0
\]</span></p></li>
<li><p><strong>Normalization</strong>:<br>
<span class="math display">\[
\int p(x)\, dx = 1
\]</span></p></li>
</ul>
<p>While it’s easy to define a function that satisfies <span class="math inline">\(p(x) \geq 0\)</span> (e.g., using exponentials), ensuring that it also sums to 1 — i.e., <span class="math inline">\(\int p(x)\, dx = 1\)</span> — is much more difficult, especially for flexible functions like neural networks.</p>
<hr>
</section>
<section id="why-do-we-introduce-gx" class="level3">
<h3 class="anchored" data-anchor-id="why-do-we-introduce-gx">Why do we introduce <span class="math inline">\(g(x)\)</span>?</h3>
<p>Instead of modeling <span class="math inline">\(p(x)\)</span> directly, we define a non-negative function <span class="math inline">\(g(x) \geq 0\)</span> and turn it into a probability distribution by normalizing:</p>
<p><span class="math display">\[
p_\theta(x) = \frac{g_\theta(x)}{Z(\theta)}, \quad \text{where} \quad Z(\theta) = \int g_\theta(x)\, dx
\]</span></p>
<p>This trick simplifies the problem by separating the two requirements:</p>
<ul>
<li><span class="math inline">\(g_\theta(x)\)</span> ensures non-negativity<br>
</li>
<li><span class="math inline">\(Z(\theta)\)</span> enforces normalization</li>
</ul>
<p>This normalization constant <span class="math inline">\(Z(\theta)\)</span> is also known as the <strong>partition function</strong>.</p>
<hr>
</section>
<section id="intuition" class="level3">
<h3 class="anchored" data-anchor-id="intuition">Intuition</h3>
<p>Think of <span class="math inline">\(g_\theta(x)\)</span> as a scoring function:</p>
<ul>
<li>Higher <span class="math inline">\(g_\theta(x)\)</span> means more likely<br>
</li>
<li>Dividing by <span class="math inline">\(Z(\theta)\)</span> rescales these scores to form a valid probability distribution</li>
</ul>
<hr>
</section>
<section id="from-scores-to-probabilities-in-ebms" class="level3">
<h3 class="anchored" data-anchor-id="from-scores-to-probabilities-in-ebms">From Scores to Probabilities in EBMs</h3>
<p>Energy-Based Models follow the same idea we’ve established earlier: define a <strong>scoring function</strong> <span class="math inline">\(f_\theta(x)\)</span> that assigns high values to likely data points, and then convert these scores into probabilities using an exponential transformation and normalization.<br>
This allows us to build flexible probabilistic models without needing tractable likelihoods or invertible mappings.</p>
<p>We use an <strong>exponential function</strong> because:</p>
<ul>
<li>It guarantees non-negativity: <span class="math inline">\(\exp(f_\theta(x)) \geq 0\)</span></li>
<li>It allows us to interpret <span class="math inline">\(f_\theta(x)\)</span> as an unnormalized log-probability</li>
<li>It connects naturally to many well-known distributions (e.g., exponential family, Boltzmann distribution)</li>
</ul>
<p>We define:</p>
<p><span class="math display">\[
g_\theta(x) = \exp(f_\theta(x)) \quad \Rightarrow \quad p_\theta(x) = \frac{e^{f_\theta(x)}}{Z(\theta)}
\]</span></p>
<p>To align with the physics intuition that lower energy = higher probability, we define the <strong>energy function</strong> as:</p>
<p><span class="math display">\[
E_\theta(x) = -f_\theta(x)
\]</span></p>
<p>This gives us the classic EBM form:</p>
<p><span class="math display">\[
p_\theta(x) = \frac{e^{-E_\theta(x)}}{Z(\theta)}
\]</span></p>
<p>This formulation gives EBMs the freedom to use any differentiable function for <span class="math inline">\(f_\theta(x)\)</span>, and only requires that we can compute or approximate its gradients.</p>
<hr>
</section>
<section id="practical-applications" class="level3">
<h3 class="anchored" data-anchor-id="practical-applications">Practical Applications</h3>
<p>Energy-Based Models (EBMs) offer unique benefits in scenarios where traditional models struggle. Below are two practical applications that demonstrate how EBMs shine in real-world settings.</p>
<section id="when-you-dont-need-the-partition-function" class="level4">
<h4 class="anchored" data-anchor-id="when-you-dont-need-the-partition-function">1. When You Don’t Need the Partition Function</h4>
<p>In general, evaluating the full probability <span class="math inline">\(p_\theta(x)\)</span> requires computing the partition function <span class="math inline">\(Z(\theta)\)</span>:</p>
<p><span class="math display">\[
p_\theta(x) = \frac{1}{Z(\theta)} \exp(f_\theta(x))
\]</span></p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Key Insight</strong><br>
In some applications, we don’t need the exact probability — we only need to compare scores.<br>
This allows EBMs to be useful even when the partition function is intractable.</p>
</div>
</div>
</div>
<p>When comparing two samples <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span>, we can compute the ratio of their probabilities:</p>
<p><span class="math display">\[
\frac{p_\theta(x)}{p_\theta(x')} = \exp(f_\theta(x) - f_\theta(x'))
\]</span></p>
<p>This lets us determine which input is more likely — without ever computing <span class="math inline">\(Z(\theta)\)</span> — a powerful advantage of EBMs.</p>
<p><strong>Practical Applications</strong>:</p>
<ul>
<li><strong>Anomaly detection</strong>: Identify inputs with unusually low likelihood.</li>
<li><strong>Denoising</strong>: Prefer cleaner versions of corrupted data by comparing likelihoods.</li>
<li><strong>Object Recognition</strong>: Given an input image <span class="math inline">\(x\)</span>, assign the most likely label <span class="math inline">\(y\)</span> by minimizing energy <span class="math inline">\(E(y, x)\)</span>.</li>
<li><strong>Sequence Labeling</strong>: For each input token <span class="math inline">\(x\)</span>, select the tag <span class="math inline">\(y\)</span> (e.g., part-of-speech) that minimizes <span class="math inline">\(E(y, x)\)</span>.</li>
<li><strong>Image Restoration</strong>: Given a noisy image <span class="math inline">\(x\)</span>, predict a clean version <span class="math inline">\(y\)</span> by selecting the output with lowest energy <span class="math inline">\(E(y, x)\)</span></li>
</ul>
</section>
<section id="product-of-experts-compositional-generation" class="level4">
<h4 class="anchored" data-anchor-id="product-of-experts-compositional-generation">2. Product of Experts (Compositional Generation)</h4>
<p>In some cases, we want to combine multiple expert models that each score different attributes of an input <span class="math inline">\(\mathbf{x}\)</span> — for example, age, gender, or hairstyle. This is where Energy-Based Models (EBMs) shine through <strong>Product of Experts (PoE)</strong>.</p>
<p>Suppose you have three trained expert models <span class="math inline">\(f_{\theta_1}(x)\)</span>, <span class="math inline">\(f_{\theta_2}(x)\)</span>, and <span class="math inline">\(f_{\theta_3}(x)\)</span>. A tempting idea is to combine their scores additively and exponentiate:</p>
<p><span class="math display">\[
\exp\left(f_{\theta_1}(x) + f_{\theta_2}(x) + f_{\theta_3}(x)\right)
\]</span></p>
<p>To make this a valid probability distribution, we normalize:</p>
<p><span class="math display">\[
p_{\theta_1, \theta_2, \theta_3}(x) = \frac{1}{Z(\theta_1, \theta_2, \theta_3)} \exp\left(f_{\theta_1}(x) + f_{\theta_2}(x) + f_{\theta_3}(x)\right)
\]</span></p>
<p>This behaves like a logical <strong>AND</strong>: if any expert assigns low score, the overall likelihood drops. This contrasts with mixture models (like Mixture of Gaussians), which behave more like <strong>OR</strong>.</p>
</section>
<section id="example" class="level4">
<h4 class="anchored" data-anchor-id="example">Example</h4>
<p>In the figure below (Du et al., 2020), EBMs were used to model attributes such as “young”, “female”, “smiling”, and “wavy hair”. Combining them via Product of Experts allowed for flexible generation of images with <strong>multiple attributes</strong>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/product_of_experts_faces.png" class="img-fluid figure-img"></p>
<figcaption>Source: Du et al., 2020. <em>Compositional Visual Generation with Energy Based Models</em></figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="key-benefits-of-ebms" class="level3">
<h3 class="anchored" data-anchor-id="key-benefits-of-ebms">Key Benefits of EBMs</h3>
<p><strong>Very flexible model architectures</strong><br>
No need for invertibility, autoregressive factorization, or adversarial design.</p>
<p><strong>Stable training</strong><br>
Compared to GANs, EBMs can be more robust and easier to optimize.</p>
<p><strong>High sample quality</strong><br>
Capable of modeling complex, multi-modal data distributions.</p>
<p><strong>Flexible composition</strong><br>
Energies can be combined to support multi-task objectives or structured learning.</p>
<hr>
</section>
<section id="limitations-of-ebms" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-ebms">Limitations of EBMs</h3>
<p>Despite their strengths, EBMs come with notable challenges:</p>
<p><strong>Hard Sampling</strong></p>
<ul>
<li>No direct way to draw samples from <span class="math inline">\(p_\theta(x)\)</span><br>
</li>
<li>Requires iterative MCMC methods (e.g., Langevin dynamics, Metropolis-Hastings)<br>
</li>
<li>Sampling cost is high and scales poorly in high dimensions</li>
</ul>
<p><strong>Hard Likelihood Evaluation and Learning</strong></p>
<ul>
<li>Partition function <span class="math inline">\(Z(\theta)\)</span> is intractable<br>
</li>
<li>Cannot evaluate <span class="math inline">\(\log p_\theta(x)\)</span> directly<br>
</li>
<li>Learning requires <strong>pushing down energy of incorrect samples</strong>, not just increasing energy of training data</li>
</ul>
<p><strong>No Feature Learning (by default)</strong></p>
<ul>
<li>EBMs don’t learn latent features by default unless explicitly structured (e.g., RBMs)</li>
</ul>
<hr>
</section>
</section>
<section id="training-and-inference-in-ebms" class="level2">
<h2 class="anchored" data-anchor-id="training-and-inference-in-ebms">Training and Inference in EBMs</h2>
<p>The goal in training Energy-Based Models (EBMs) is to assign higher scores (i.e., lower energy) to correct examples, and lower scores (higher energy) to incorrect ones. This corresponds to <strong>maximizing the (unnormalized) likelihood</strong> of the training data:</p>
<p><span class="math display">\[
p_\theta(x_{\text{train}}) = \frac{\exp(f_\theta(x_{\text{train}}))}{Z(\theta)}
\]</span></p>
<p>This expression tells us that increasing the score for <span class="math inline">\(x_{\text{train}}\)</span> is not enough — we must also decrease scores for other <span class="math inline">\(x\)</span> to reduce <span class="math inline">\(Z(\theta)\)</span> and make the probability higher <strong>relatively</strong>.</p>
<hr>
<section id="log-likelihood-and-its-gradient" class="level3">
<h3 class="anchored" data-anchor-id="log-likelihood-and-its-gradient">Log-Likelihood and Its Gradient</h3>
<p>We start by writing the log-likelihood:</p>
<p><span class="math display">\[
\log p_\theta(x_{\text{train}}) = f_\theta(x_{\text{train}}) - \log Z(\theta)
\]</span></p>
<p>Taking the gradient with respect to <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\nabla_\theta \log p_\theta(x_{\text{train}}) = \nabla_\theta f_\theta(x_{\text{train}}) - \nabla_\theta \log Z(\theta)
\]</span></p>
<p>To compute this, we need the gradient of the log partition function:</p>
<p><span class="math display">\[
Z(\theta) = \int \exp(f_\theta(x))\, dx
\]</span></p>
<p>Applying the chain rule:</p>
<p><span class="math display">\[
\nabla_\theta \log Z(\theta)
= \frac{1}{Z(\theta)} \int \exp(f_\theta(x)) \nabla_\theta f_\theta(x)\, dx
= \mathbb{E}_{x \sim p_\theta} \left[ \nabla_\theta f_\theta(x) \right]
\]</span></p>
<p>Substitute this back:</p>
<p><span class="math display">\[
\nabla_\theta \log p_\theta(x_{\text{train}})
= \nabla_\theta f_\theta(x_{\text{train}}) - \mathbb{E}_{x \sim p_\theta} \left[ \nabla_\theta f_\theta(x) \right]
\]</span></p>
<p>The first term is straightforward — it’s the gradient of the model’s score on the training point.</p>
<p>But the second term, the <strong>expectation over model samples</strong>, is difficult. It requires drawing samples from <span class="math inline">\(p_\theta(x)\)</span>, which in turn depends on the intractable normalization constant <span class="math inline">\(Z(\theta)\)</span>.</p>
<p>To deal with this, we use <strong>sampling-based approximations</strong> like Contrastive Divergence.</p>
<hr>
</section>
<section id="contrastive-divergence" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-divergence">Contrastive Divergence</h3>
<p>We approximate the expectation:</p>
<p><span class="math display">\[
\mathbb{E}_{x \sim p_\theta} \left[ \nabla_\theta f_\theta(x) \right] \approx \nabla_\theta f_\theta(x_{\text{sample}})
\]</span></p>
<p>This gives:</p>
<p><span class="math display">\[
\nabla_\theta \log p_\theta(x_{\text{train}}) \approx \nabla_\theta f_\theta(x_{\text{train}}) - \nabla_\theta f_\theta(x_{\text{sample}})
= \nabla_\theta \left( f_\theta(x_{\text{train}}) - f_\theta(x_{\text{sample}}) \right)
\]</span></p>
<p><strong>Contrastive Divergence Algorithm:</strong></p>
<ol type="1">
<li>Sample <span class="math inline">\(x_{\text{sample}} \sim p_\theta\)</span> (typically via MCMC)</li>
<li>Take a gradient step on:</li>
</ol>
<p><span class="math display">\[
\nabla_\theta \left( f_\theta(x_{\text{train}}) - f_\theta(x_{\text{sample}}) \right)
\]</span></p>
<p>This encourages the model to increase the score of the training sample and decrease the score of samples it currently believes are likely.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>EBM Training Recap</strong><br>
- Want: High scores (low energy) for real data<br>
- Avoid: High scores for incorrect data<br>
- Can’t compute exact gradient due to <span class="math inline">\(Z(\theta)\)</span><br>
- So: Approximate using Monte Carlo sample <span class="math inline">\(\sim p_\theta(x)\)</span></p>
</div>
</div>
</div>
<hr>
</section>
<section id="intuition-recap" class="level3">
<h3 class="anchored" data-anchor-id="intuition-recap">Intuition Recap</h3>
<ul>
<li><strong>Pull up</strong> the training sample: <span class="math inline">\(\nabla_\theta f_\theta(x_{\text{train}})\)</span></li>
<li><strong>Push down</strong> samples from the model: <span class="math inline">\(\nabla_\theta f_\theta(x_{\text{sample}})\)</span></li>
<li>The model improves by sharpening its belief in real data and correcting mistaken high-scoring areas of the space.</li>
</ul>
<p><img src="images/ebm_training_flow.png" class="img-fluid" alt="Visualizing EBM training"> <em>During training, EBMs increase the score of correct samples and decrease the score of incorrect ones.</em></p>
<p class="caption" style="font-size: 0.85em; color: #666; margin-top: -0.5em;">
<em>Source: course material from <a href="https://deepgenerativemodels.github.io/assets/slides/cs236_lecture11.pdf" target="_blank">CS236: Deep Generative Models</a></em>
</p>
<hr>
</section>
<section id="sampling-from-energy-based-models" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-energy-based-models">Sampling from Energy-Based Models</h3>
<p>Recall that EBMs define a probability distribution as:</p>
<p><span class="math display">\[
p_\theta(x) = \frac{1}{Z(\theta)} \exp(f_\theta(x))
\]</span></p>
<p>Unlike autoregressive or flow models, there is <strong>no direct way to sample</strong> from <span class="math inline">\(p_\theta(x)\)</span> because we cannot easily compute how likely each possible sample is. That’s because the normalization term <span class="math inline">\(Z(\theta)\)</span> is intractable.</p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Key Insight</strong><br>
We can still compare two samples <span class="math inline">\(x\)</span> and <span class="math inline">\(x'\)</span> without needing <span class="math inline">\(Z(\theta)\)</span>:</p>
<p><span class="math display">\[
\frac{p_\theta(x)}{p_\theta(x')} = \exp(f_\theta(x) - f_\theta(x'))
\]</span></p>
<p>This property is useful for tasks like ranking, anomaly detection, and denoising.</p>
</div>
</div>
</div>
<p>While we can’t sample from <span class="math inline">\(p_\theta(x)\)</span> directly due to the intractable <span class="math inline">\(Z(\theta)\)</span>, we can still generate approximate samples using <strong>Markov Chain Monte Carlo (MCMC)</strong> methods.</p>
<hr>
</section>
<section id="metropolis-hastings-mh-mcmc" class="level3">
<h3 class="anchored" data-anchor-id="metropolis-hastings-mh-mcmc">1. Metropolis-Hastings (MH) MCMC</h3>
<p>To sample from <span class="math inline">\(p_\theta(x)\)</span>, we use an iterative approach like MCMC:</p>
<ol type="1">
<li><strong>Initialize</strong> <span class="math inline">\(x^0\)</span> randomly<br>
</li>
<li><strong>Propose</strong> a new sample: <span class="math inline">\(x' = x^t + \text{noise}\)</span><br>
</li>
<li><strong>Accept or reject</strong> based on scores:
<ul>
<li>If <span class="math inline">\(f_\theta(x') &gt; f_\theta(x^t)\)</span>, set <span class="math inline">\(x^{t+1} = x'\)</span></li>
<li>Else set <span class="math inline">\(x^{t+1} = x'\)</span> with probability <span class="math inline">\(\exp(f_\theta(x') - f_\theta(x^t))\)</span></li>
<li>Otherwise, set <span class="math inline">\(x^{t+1} = x^t\)</span></li>
</ul></li>
<li><strong>Repeat</strong> this process until the chain converges</li>
</ol>
<p><strong>Pros:</strong><br>
- General-purpose<br>
- Guaranteed to converge to <span class="math inline">\(p_\theta(x)\)</span> under mild conditions</p>
<p><strong>Cons:</strong><br>
- Can take a very long time to convergence - Sensitive to proposal distribution<br>
- Computationally expensive in high dimensions</p>
</section>
<section id="unadjusted-langevin-mcmc-ula" class="level3">
<h3 class="anchored" data-anchor-id="unadjusted-langevin-mcmc-ula">2. Unadjusted Langevin MCMC (ULA)</h3>
<p>To sample from <span class="math inline">\(p_\theta(x)\)</span>, Unadjusted Langevin MCMC uses gradient information to guide proposals:</p>
<ol type="1">
<li><strong>Initialize</strong> <span class="math inline">\(x^0 \sim \pi(x)\)</span><br>
</li>
<li><strong>Repeat</strong> for <span class="math inline">\(t = 0, 1, 2, \dots, T - 1\)</span>:
<ul>
<li>Sample <span class="math inline">\(z^t \sim \mathcal{N}(0, I)\)</span><br>
</li>
<li>Update: <span class="math inline">\(x^{t+1} = x^t + \epsilon \nabla_x \log p_\theta(x^t) + \sqrt{2\epsilon} z^t\)</span></li>
</ul></li>
</ol>
<p>For EBMs, since <span class="math inline">\(\nabla_x \log p_\theta(x) = \nabla_x f_\theta(x)\)</span> the update becomes:</p>
<p><span class="math display">\[
x^{t+1} = x^t + \epsilon \nabla_x f_\theta(x^t) + \sqrt{2\epsilon} z^t
\]</span></p>
<p><strong>Pros:</strong><br>
- Uses gradient to improve proposal<br>
- Often faster mixing than random-walk methods</p>
<p><strong>Cons:</strong><br>
- Still requires many steps for good convergence<br>
- Sensitive to step size <span class="math inline">\(\epsilon\)</span></p>
</section>
<section id="adjusted-langevin-mcmc-ala" class="level3">
<h3 class="anchored" data-anchor-id="adjusted-langevin-mcmc-ala">3. Adjusted Langevin MCMC (ALA)</h3>
<p>To sample from <span class="math inline">\(p_\theta(x)\)</span>, Adjusted Langevin MCMC applies a step after each Langevin update to ensure samples follow the correct stationary distribution.</p>
<p>This makes it a corrected version of ULA with proper stationary distribution.</p>
<ol type="1">
<li> <span class="math inline">\(x^0 \sim \pi(x)\)</span><br>
</li>
<li> for <span class="math inline">\(t = 0, 1, 2, \dots, T - 1\)</span>:
<ul>
<li>Sample <span class="math inline">\(z^t \sim \mathcal{N}(0, I)\)</span><br>
</li>
<li>Propose: <span class="math inline">\(x' = x^t + \epsilon \nabla_x \log p_\theta(x^t) + \sqrt{2\epsilon} z^t\)</span></li>
<li>Forward proposal: <span class="math inline">\(q(x^{t+1} \mid x^t) = \mathcal{N}\left(x^{t+1} \mid x^t + \epsilon \nabla_x \log p_\theta(x^t),\ 2\epsilon I\right)\)</span></li>
<li>Reverse proposal: <span class="math inline">\(q(x^t \mid x^{t+1}) = \mathcal{N}\left(x^t \mid x^{t+1} + \epsilon \nabla_x \log p_\theta(x^{t+1}),\ 2\epsilon I\right)\)</span></li>
<li>Accept <span class="math inline">\(x\)</span> with probability <span class="math inline">\(\alpha = \min\left(1, \frac{p_\theta(x') \cdot q(x^t \mid x')}{p_\theta(x^t) \cdot q(x' \mid x^t)}\right)\)</span>
<ul>
<li>If accepted: <span class="math inline">\(x^{t+1} = x'\)</span><br>
</li>
<li>Otherwise: <span class="math inline">\(x^{t+1} = x^t\)</span></li>
</ul></li>
</ul></li>
</ol>
<p>For EBMs, since</p>
<p><span class="math display">\[
\nabla_x \log p_\theta(x) = \nabla_x f_\theta(x)
\]</span> and <span class="math display">\[  
q(x' \mid x^t) = \mathcal{N}\left(x' \mid x^t + \epsilon \nabla_x f_\theta(x^t), 2\epsilon I\right)
\]</span></p>
<p>the proposal becomes:</p>
<p><span class="math display">\[
x' = x^t + \epsilon \nabla_x f_\theta(x^t) + \sqrt{2\epsilon} z^t
\]</span></p>
<hr>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>All these methods aim to sample from <span class="math inline">\(p_\theta(x)\)</span>, but differ in how they explore the space:</p>
<ul>
<li><strong>MH</strong> is simple but inefficient.</li>
<li><strong>ULA</strong> is gradient-guided but approximate.</li>
<li><strong>Adjusted Langevin</strong> corrects ULA with MH.</li>
</ul>
<p>Sampling is a core challenge in EBMs — especially because we need to sample during <strong>every training step</strong> when using contrastive divergence.</p>
<hr>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">📚 References</h2>
<p>[1] Atcold, Y. (2020). <em>NYU Deep Learning Spring 2020 – Week 07: Energy-Based Models</em>. Retrieved from <a href="https://atcold.github.io/NYU-DLSP20/en/week07/07-1/">https://atcold.github.io/NYU-DLSP20/en/week07/07-1/</a></p>
<p>[2] LeCun, Y., Hinton, G., &amp; Bengio, Y. (2021). <em>A Path Towards Autonomous Machine Intelligence</em>. arXiv. Retrieved from <a href="https://arxiv.org/pdf/2101.03288">https://arxiv.org/pdf/2101.03288</a></p>
<p>[3] MIT. (2022). <em>Energy-Based Models – MIT Class Project</em>. Retrieved from <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">https://energy-based-model.github.io/Energy-based-Model-MIT/</a></p>
<p>[4] University of Amsterdam. (2021). <em>Deep Energy Models – UvA DL Notebooks</em>. Retrieved from <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html</a></p>
<p>[5] MIT. (2022). <em>Compositional Generation and Inference with Energy-Based Models</em>. Retrieved from <a href="https://energy-based-model.github.io/compositional-generation-inference/">https://energy-based-model.github.io/compositional-generation-inference/</a></p>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>