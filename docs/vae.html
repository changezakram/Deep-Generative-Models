<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Variational Autoencoders</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> <i class="bi bi-house" role="img">
</i> 
<span class="menu-text">Changez Akram</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-generative-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Generative AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-generative-ai">    
        <li>
    <a class="dropdown-item" href="./index.html">
 <span class="dropdown-text">Gen AI Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./vae.html">
 <span class="dropdown-text">Variational Autoencoders (VAEs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./flows.html">
 <span class="dropdown-text">Normalizing Flows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ebm.html">
 <span class="dropdown-text">Energy-Based Models (EBMs)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./diffusion.html">
 <span class="dropdown-text">Diffusion Models</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-large-language-models" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Large Language Models</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-large-language-models">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/transformers.html">
 <span class="dropdown-text">Transformers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/post-training.html">
 <span class="dropdown-text">Post Training</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/nlp-eval.html">
 <span class="dropdown-text">NLP Evaluation</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-agentic-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Agentic AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-agentic-ai">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic-ai.html">
 <span class="dropdown-text">Introduction</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic-analytics.html">
 <span class="dropdown-text">Agentic Analytics</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/slm.html">
 <span class="dropdown-text">Small Language Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/agentic-ai/agentic_ai_safety.html">
 <span class="dropdown-text">Building Safe &amp; Secure Agentic AI</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-ai-strategy" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">AI Strategy</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-ai-strategy">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/ai_first_bank.html">
 <span class="dropdown-text">Building the AI-First Bank</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/banking-use-cases.html">
 <span class="dropdown-text">Gen AI in Banking</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/Deep-Generative-Models/gen-ai-use-cases/healthcare-use-cases.html">
 <span class="dropdown-text">Gen AI in Healthcare</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-math-review" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Math Review</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-math-review">    
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/calculus.html">
 <span class="dropdown-text">Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://changezakram.github.io/math-review/probability.html">
 <span class="dropdown-text">Probability</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#autoencoders-vs-variational-autoencoders" id="toc-autoencoders-vs-variational-autoencoders" class="nav-link active" data-scroll-target="#autoencoders-vs-variational-autoencoders"><span class="header-section-number">1</span> Autoencoders vs Variational Autoencoders</a></li>
  <li><a href="#probabilistic-framework" id="toc-probabilistic-framework" class="nav-link" data-scroll-target="#probabilistic-framework"><span class="header-section-number">2</span> Probabilistic Framework</a></li>
  <li><a href="#estimating-the-marginal-likelihood" id="toc-estimating-the-marginal-likelihood" class="nav-link" data-scroll-target="#estimating-the-marginal-likelihood"><span class="header-section-number">3</span> Estimating the Marginal Likelihood</a></li>
  <li><a href="#why-variational-inference" id="toc-why-variational-inference" class="nav-link" data-scroll-target="#why-variational-inference"><span class="header-section-number">4</span> Why Variational Inference?</a></li>
  <li><a href="#training-a-vae" id="toc-training-a-vae" class="nav-link" data-scroll-target="#training-a-vae"><span class="header-section-number">5</span> Training a VAE</a></li>
  <li><a href="#understanding-the-kl-divergence-term-in-the-vae-loss" id="toc-understanding-the-kl-divergence-term-in-the-vae-loss" class="nav-link" data-scroll-target="#understanding-the-kl-divergence-term-in-the-vae-loss"><span class="header-section-number">6</span> Understanding the KL Divergence Term in the VAE Loss</a></li>
  <li><a href="#applications-of-vaes" id="toc-applications-of-vaes" class="nav-link" data-scroll-target="#applications-of-vaes"><span class="header-section-number">7</span> Applications of VAEs</a></li>
  <li><a href="#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections." id="toc-this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections." class="nav-link" data-scroll-target="#this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections."><span class="header-section-number">8</span> This example is designed to reinforce the theoretical concepts from earlier sections.</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">9</span> Further Reading</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Variational Autoencoders</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Variational Autoencoders (VAEs) combine the power of neural networks with probabilistic inference to model complex data distributions. This blog unpacks the intuition, math, and implementation of VAEs ‚Äî from KL divergence and the ELBO to PyTorch code that generates to generate new images.</p>
<section id="autoencoders-vs-variational-autoencoders" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="autoencoders-vs-variational-autoencoders"><span class="header-section-number">1</span> Autoencoders vs Variational Autoencoders</h2>
<p>Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:</p>
<ul>
<li>They lack <strong>generative capabilities</strong> ‚Äî they cannot sample new data effectively</li>
<li>The <strong>latent space is unstructured</strong>, offering little control or interpretation</li>
<li>There is no <strong>probabilistic modeling</strong>, limiting uncertainty estimation</li>
</ul>
<p>Variational Autoencoders (VAEs) were introduced to overcome these limitations. Rather than encoding inputs into fixed latent vectors, VAEs learn a <strong>probabilistic latent space</strong> by modeling each input as a distribution ‚Äî typically a Gaussian with a learned mean <span class="math inline">\(\\mu\)</span> and standard deviation <span class="math inline">\(\\sigma\)</span>. This approach enables the model to sample latent variables <span class="math inline">\(z\)</span> using the <strong>reparameterization trick</strong>, allowing the entire architecture to remain differentiable and trainable. By doing so, VAEs not only enable reconstruction, but also promote the learning of a <strong>continuous, interpretable latent space</strong> ‚Äî a key enabler for generation and interpolation.</p>
<p>The diagram below illustrates this process:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Reparameterized_Variational_Autoencoder.png/960px-Reparameterized_Variational_Autoencoder.png" class="img-fluid"></p>
<p><em>Source: <a href="https://commons.wikimedia.org/wiki/File:Reparameterized_Variational_Autoencoder.png">Wikimedia Commons</a>, licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.</em></p>
</section>
<section id="probabilistic-framework" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="probabilistic-framework"><span class="header-section-number">2</span> Probabilistic Framework</h2>
<p>More formally, VAEs assume the data is generated by a two-step process:</p>
<ol type="1">
<li>Sample a latent variable <span class="math inline">\(\mathbf{z} \sim \mathcal{N}(0, I)\)</span></li>
<li>Generate the observation <span class="math inline">\(\mathbf{x}\)</span> from: <span class="math display">\[
p(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mu_\theta(\mathbf{z}), \Sigma_\theta(\mathbf{z}))
\]</span> where <span class="math inline">\(\mu_\theta\)</span> and <span class="math inline">\(\Sigma_\theta\)</span> are neural networks parameterized by <span class="math inline">\(\theta\)</span></li>
</ol>
<p>Here, <span class="math inline">\(\mathbf{z}\)</span> acts as a hidden or latent variable, which is <strong>unobserved during training</strong>. The model thus defines a mixture of infinitely many Gaussians ‚Äî one for each <span class="math inline">\(\mathbf{z}\)</span>.</p>
<p>To compute the likelihood of a data point <span class="math inline">\(\mathbf{x}\)</span>, we must marginalize over all possible latent variables: <span class="math display">\[
  p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) \, d\mathbf{z}
  \]</span></p>
<p>This integral requires integrating over all possible values of the latent variable <span class="math inline">\(\mathbf{z}\)</span>, which is often high-dimensional and affects the likelihood in a non-linear way through neural networks. Because of this, computing the marginal likelihood exactly is computationally intractable. This motivates the use of techniques like variational inference and ELBO.</p>
<section id="computational-challenge" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="computational-challenge"><span class="header-section-number">2.1</span> Computational Challenge</h3>
<p>This integral requires integrating over:</p>
<ul>
<li>All possible values of <span class="math inline">\(\mathbf{z}\)</span> (often high-dimensional)</li>
<li>Non-linear transformations through neural networks</li>
</ul>
<p><strong>Result:</strong> Exact computation is intractable, motivating techniques like variational inference and ELBO (developed next).</p>
<hr>
</section>
</section>
<section id="estimating-the-marginal-likelihood" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="estimating-the-marginal-likelihood"><span class="header-section-number">3</span> Estimating the Marginal Likelihood</h2>
<section id="naive-monte-carlo-estimation" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="naive-monte-carlo-estimation"><span class="header-section-number">3.1</span> Naive Monte Carlo Estimation</h3>
<p>One natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:</p>
<p><span class="math display">\[
p(x) \approx \frac{1}{K} \sum_{j=1}^K p_\theta(x, z_j), \quad z_j \sim \text{Uniform}
\]</span></p>
<p>However, this fails in practice. For most values of <span class="math inline">\(z\)</span>, the joint probability <span class="math inline">\(p_\theta(x, z)\)</span> is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has high variance and rarely ‚Äúhits‚Äù likely values of <span class="math inline">\(z\)</span>.</p>
</section>
<section id="importance-sampling" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="importance-sampling"><span class="header-section-number">3.2</span> Importance Sampling</h3>
<p>To address this, we use <strong>importance sampling</strong>, introducing a proposal distribution <span class="math inline">\(q(z)\)</span>:</p>
<p><span class="math display">\[
p(x) = \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right]
\]</span></p>
<p>This gives an <strong>unbiased estimator</strong> of <span class="math inline">\(p(x)\)</span> if <span class="math inline">\(q(z)\)</span> is well-chosen (ideally close to <span class="math inline">\(p_\theta(z|x)\)</span>). Intuitively, we sample <span class="math inline">\(z\)</span> more frequently in regions where <span class="math inline">\(p_\theta(x, z)\)</span> is high.</p>
<hr>
</section>
<section id="log-likelihood" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="log-likelihood"><span class="header-section-number">3.3</span> Log likelihood</h3>
<p>Our goal is to optimize the <strong>log-likelihood</strong>, and the log of an expectation is not the same as the expectation of the log. That is,</p>
<p><span class="math display">\[
\log p(x) = log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \neq \mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]
\]</span></p>
<p>While the marginal likelihood p(x) can be estimated unbiasedly using importance sampling, estimating its logarithm <span class="math inline">\(p(x)\)</span> introduces bias due to the concavity of the log function. This is captured by <strong>Jensen‚Äôs Inequality</strong>, which tells us:</p>
<p><span class="math display">\[
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \geq \underbrace{\mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]}_{\text{ELBO}}
\]</span></p>
<p>This means that the <strong>expected log of the estimator underestimates the true log-likelihood</strong>. The right-hand side provides a tractable surrogate objective known as the <strong>Evidence Lower Bound (ELBO)</strong>, which is a biased lower bound to <span class="math inline">\(\log p(x)\)</span>. Optimizing the ELBO allows us to indirectly maximize the intractable log-likelihood.</p>
<p>In the next section, we formally derive this bound and explore its components in detail.</p>
<hr>
</section>
</section>
<section id="why-variational-inference" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="why-variational-inference"><span class="header-section-number">4</span> Why Variational Inference?</h2>
<p>Computing the true posterior distribution <span class="math inline">\(p(z \mid x)\)</span> is intractable in most cases, because it requires evaluating the marginal likelihood <span class="math inline">\(p(x)\)</span>, which involves integrating over all possible values of <span class="math inline">\(z\)</span>:</p>
<p><span class="math display">\[
p(x) = \int p(x, z) \, dz
\]</span></p>
<p>Variational inference tackles this by introducing a tractable, parameterized distribution <span class="math inline">\(q(z)\)</span> to approximate <span class="math inline">\(p(z|x)\)</span>. We aim to make <span class="math inline">\(q(z)\)</span> as close as possible to the true posterior by minimizing the KL divergence:</p>
<p><span class="math display">\[
D_{\text{KL}}(q(z) \| p(z|x))
\]</span></p>
<p>This turns inference into an optimization problem. A key result is the Evidence Lower Bound (ELBO). See next section.</p>
<hr>
</section>
<section id="training-a-vae" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="training-a-vae"><span class="header-section-number">5</span> Training a VAE</h2>
<section id="elbo-objective" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="elbo-objective"><span class="header-section-number">5.1</span> ELBO Objective</h3>
<p>Now that we‚Äôve introduced the challenge of approximating the intractable posterior using variational inference, we turn our attention to deriving the Evidence Lower Bound (ELBO). This derivation reveals how optimizing a surrogate objective allows us to approximate the true log-likelihood of the data while keeping the approximate posterior close to the prior. The steps below walk through this formulation.</p>
<section id="kl-divergence-objective" class="level4" data-number="5.1.1">
<h4 data-number="5.1.1" class="anchored" data-anchor-id="kl-divergence-objective"><span class="header-section-number">5.1.1</span> KL Divergence Objective</h4>
<p><span class="math display">\[\begin{equation}
D_{KL}(q(z)\|p(z|x; \theta)) = \sum_z q(z) \log \frac{q(z)}{p(z|x; \theta)}
\end{equation}\]</span></p>
</section>
<section id="apply-bayes-rule" class="level4" data-number="5.1.2">
<h4 data-number="5.1.2" class="anchored" data-anchor-id="apply-bayes-rule"><span class="header-section-number">5.1.2</span> Apply Bayes‚Äô Rule</h4>
<p>Substitute <span class="math inline">\(p(z|x; \theta) = \frac{p(z,x;\theta)}{p(x;\theta)}\)</span>: <span class="math display">\[\begin{equation}
= \sum_z q(z) \log \left( \frac{q(z) \cdot p(x; \theta)}{p(z, x; \theta)} \right)
\end{equation}\]</span></p>
</section>
<section id="decompose-terms" class="level4" data-number="5.1.3">
<h4 data-number="5.1.3" class="anchored" data-anchor-id="decompose-terms"><span class="header-section-number">5.1.3</span> Decompose Terms</h4>
<p><span class="math display">\[\begin{align}
&amp;= \sum_z q(z) \log q(z) + \sum_z q(z) \log p(x; \theta) \nonumber \\
&amp;\quad - \sum_z q(z) \log p(z, x; \theta) \\
&amp;= -H(q) + \log p(x; \theta) - \mathbb{E}_q[\log p(z,x;\theta)]
\end{align}\]</span></p>
<blockquote class="blockquote">
<p><strong>Note:</strong> The term <span class="math inline">\(\mathcal{H}(q)\)</span> represents the <strong>entropy</strong> of the variational distribution <span class="math inline">\(q(z|x)\)</span>. Entropy is defined as:</p>
<p><span class="math display">\[
\mathcal{H}(q) = -\sum_z q(z) \log q(z) = -\mathbb{E}_{q(z)}[\log q(z)]
\]</span></p>
<p>Entropy measures the amount of uncertainty or ‚Äúspread‚Äù in a distribution. A high-entropy <span class="math inline">\(q(z)\)</span> places probability mass across a wide region of the latent space, while a low-entropy <span class="math inline">\(q(z)\)</span> is more concentrated. This decomposition is key to understanding the KL divergence term in the ELBO.</p>
</blockquote>
</section>
<section id="rearrange-for-elbo" class="level4" data-number="5.1.4">
<h4 data-number="5.1.4" class="anchored" data-anchor-id="rearrange-for-elbo"><span class="header-section-number">5.1.4</span> Rearrange for ELBO</h4>
<p><span class="math display">\[
\log p(x; \theta) =
\underbrace{
    \mathbb{E}_q[\log p(z, x; \theta)] + \mathcal{H}(q)
}_{\text{ELBO}}
+D_{KL}(q(z)\|p(z|x; \theta))
\]</span></p>
<p>This equation shows that the log-likelihood <span class="math inline">\(\log p(x)\)</span> can be decomposed into the ELBO and the KL divergence between the approximate posterior and the true posterior. Since the KL divergence is always non-negative, the ELBO serves as a lower bound to the log-likelihood. By maximizing the ELBO, we indirectly minimize the KL divergence, bringing <span class="math inline">\(q(z)\)</span> closer to <span class="math inline">\(p(z|x)\)</span>.</p>
<p><img src="https://deepgenerativemodels.github.io/notes/vae/klgap.png" class="img-fluid"> <em>Visualizing how <span class="math inline">\(\log p(x)\)</span> decomposes into the ELBO and KL divergence.</em><br>
Source: <a href="https://deepgenerativemodels.github.io/notes/vae/">deepgenerativemodels.github.io</a></p>
</section>
<section id="key-results" class="level4" data-number="5.1.5">
<h4 data-number="5.1.5" class="anchored" data-anchor-id="key-results"><span class="header-section-number">5.1.5</span> Key Results</h4>
<ol type="1">
<li><p><strong>Evidence Lower Bound (ELBO)</strong>: <span class="math display">\[\begin{equation}
\mathcal{L}(\theta,\phi) = \mathbb{E}_{q(z;\phi)}[\log p(x,z;\theta)] + H(q(z;\phi))
\end{equation}\]</span></p></li>
<li><p><strong>Optimization</strong>: <span class="math display">\[\begin{equation}
\max_{\theta,\phi} \mathcal{L}(\theta,\phi) \Rightarrow
\begin{cases}
\text{Maximizes data likelihood} \\
\text{Minimizes } D_{KL}(q\|p)
\end{cases}
\end{equation}\]</span></p></li>
</ol>
</section>
</section>
</section>
<section id="understanding-the-kl-divergence-term-in-the-vae-loss" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="understanding-the-kl-divergence-term-in-the-vae-loss"><span class="header-section-number">6</span> Understanding the KL Divergence Term in the VAE Loss</h2>
<p>In a VAE, the KL divergence term <strong>penalizes the encoder</strong> for producing latent distributions that deviate too far from the standard normal prior. This regularization has several important benefits:</p>
<ul>
<li>It ensures that the latent space has a <strong>consistent structure</strong>, enabling meaningful sampling and interpolation.</li>
<li>It helps avoid <strong>large gaps between clusters</strong> in latent space by encouraging the encoder to distribute representations more uniformly.</li>
<li>It pushes the model to use the space around the origin more symmetrically and efficiently.</li>
</ul>
<section id="balancing-kl-divergence-and-reconstruction" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="balancing-kl-divergence-and-reconstruction"><span class="header-section-number">6.1</span> Balancing KL Divergence and Reconstruction</h3>
<p>In a Variational Autoencoder, the loss balances two goals:</p>
<ol type="1">
<li><strong>Reconstruction</strong> ‚Äî making the output resemble the input</li>
<li><strong>Regularization</strong> ‚Äî keeping the latent space close to a standard normal distribution</li>
</ol>
<p>This is captured by the loss function:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{VAE}} = \text{Reconstruction Loss} + \beta \cdot D_{\text{KL}}(q(z|x) \,\|\, p(z))
\]</span></p>
<p>The parameter <span class="math inline">\(\beta\)</span> controls how strongly we enforce this regularization. Getting its value right is critical.</p>
<section id="when-beta-is-too-low" class="level4" data-number="6.1.1">
<h4 data-number="6.1.1" class="anchored" data-anchor-id="when-beta-is-too-low"><span class="header-section-number">6.1.1</span> When <span class="math inline">\(\beta\)</span> is too low:</h4>
<ul>
<li>The model mostly ignores the KL term, behaving like a plain autoencoder</li>
<li>The latent space becomes disorganized or fragmented</li>
<li>Sampling from the prior <span class="math inline">\(p(z) = \mathcal{N}(0, I)\)</span> results in <strong>unrealistic or broken outputs</strong></li>
</ul>
</section>
<section id="when-beta-is-too-high" class="level4" data-number="6.1.2">
<h4 data-number="6.1.2" class="anchored" data-anchor-id="when-beta-is-too-high"><span class="header-section-number">6.1.2</span> When <span class="math inline">\(\beta\)</span> is too high:</h4>
<ul>
<li>The encoder is forced to keep <span class="math inline">\(q(z|x)\)</span> too close to the prior</li>
<li>It encodes less information about the input</li>
<li>Reconstructions become <strong>blurry or generic</strong>, since the decoder gets little to work with</li>
</ul>
<blockquote class="blockquote">
<p><strong>Choosing <span class="math inline">\(\beta\)</span> carefully is essential for balancing generalization and fidelity.</strong><br>
A well-tuned <span class="math inline">\(\beta\)</span> helps the VAE both reconstruct accurately and generate new samples that resemble the training data.</p>
</blockquote>
</section>
</section>
<section id="gradient-challenge" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="gradient-challenge"><span class="header-section-number">6.2</span> Gradient Challenge</h3>
<p>In variational inference, we approximate the true posterior <span class="math inline">\(p(z|x)\)</span> with a tractable distribution <span class="math inline">\(q_\phi(z)\)</span>. This allows us to optimize the ELBO:</p>
<p><span class="math display">\[
\mathcal{L}(x; \theta, \phi) = \mathbb{E}_{q(z; \phi)} \left[ \log p(z, x; \theta) - \log q(z; \phi) \right]
\]</span></p>
<p>Our goal is to maximize this objective with respect to both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span>. While computing the gradient with respect to <span class="math inline">\(\theta\)</span> is straightforward, optimizing with respect to <span class="math inline">\(\phi\)</span> presents a challenge.</p>
<p>The complication arises because <span class="math inline">\(\phi\)</span> appears both in the density <span class="math inline">\(q_\phi(z|x)\)</span> and in the expectation operator. That is:</p>
<p><span class="math display">\[
\nabla_\phi \mathbb{E}_{q(z; \phi)} \left[ \log p(z, x; \theta) - \log q(z; \phi) \right]
\]</span></p>
<p>This gradient is hard to compute directly because we‚Äôre sampling from a distribution that depends on the parameters we‚Äôre trying to update.</p>
<hr>
</section>
<section id="the-reparameterization-trick" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="the-reparameterization-trick"><span class="header-section-number">6.3</span> The Reparameterization Trick</h3>
<p>To make this expression differentiable, we <strong>reparameterize</strong> the random variable <span class="math inline">\(z\)</span> as a deterministic transformation of a parameter-free noise variable <span class="math inline">\(\epsilon\)</span>:</p>
<p><span class="math display">\[
\epsilon \sim \mathcal{N}(0, I), \quad z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon
\]</span></p>
<p>This turns the expectation into:</p>
<p><span class="math display">\[
\mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)}\left[ \log p(z, x; \theta) - \log q(z; \phi) \right]
\]</span></p>
<p>where <span class="math inline">\(z\)</span> is now a differentiable function of <span class="math inline">\(\phi\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Reparameterization_Trick.png/600px-Reparameterization_Trick.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Reparameterization Trick Diagram</figcaption>
</figure>
</div>
<p><em>Image source: <a href="https://en.wikipedia.org/wiki/File:Reparameterization_Trick.png">Wikipedia (CC BY-SA 4.0)</a></em></p>
<p>This diagram illustrates how the reparameterization trick enables <strong>differentiable sampling</strong>:</p>
<ul>
<li>In the <strong>original formulation</strong>, <span class="math inline">\(z\)</span> is sampled directly from a learned distribution, breaking the gradient flow.</li>
<li>In the <strong>reparameterized formulation</strong>, we sample noise <span class="math inline">\(\epsilon \sim \mathcal{N}(0, I)\)</span>, and compute <span class="math inline">\(z = \mu + \sigma \cdot \epsilon\)</span>, making the sampling path fully differentiable.</li>
</ul>
<section id="monte-carlo-approximation" class="level4" data-number="6.3.1">
<h4 data-number="6.3.1" class="anchored" data-anchor-id="monte-carlo-approximation"><span class="header-section-number">6.3.1</span> Monte Carlo Approximation</h4>
<p>We approximate the expectation using Monte Carlo sampling:</p>
<p><span class="math display">\[
\mathbb{E}_{\epsilon}[\log p_\theta(x, z) - \log q_\phi(z)] \approx \frac{1}{K} \sum_{k=1}^K \left[\log p_\theta(x, z^{(k)}) - \log q_\phi(z^{(k)})\right]
\]</span></p>
<p>with:</p>
<p><span class="math display">\[
z^{(k)} = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon^{(k)}, \quad \epsilon^{(k)} \sim \mathcal{N}(0, I)
\]</span></p>
<p>This enables us to compute gradients using backpropagation.</p>
<hr>
</section>
<section id="summary" class="level4" data-number="6.3.2">
<h4 data-number="6.3.2" class="anchored" data-anchor-id="summary"><span class="header-section-number">6.3.2</span> Summary</h4>
<ul>
<li>Variational inference introduces a gradient challenge because <span class="math inline">\(q_\phi(z)\)</span> depends on <span class="math inline">\(\phi\)</span></li>
<li>The reparameterization trick expresses <span class="math inline">\(z\)</span> as a differentiable function of noise and <span class="math inline">\(\phi\)</span></li>
<li>This allows us to use backpropagation to optimize the ELBO efficiently</li>
</ul>
<hr>
</section>
</section>
<section id="amortized-inference" class="level3" data-number="6.4">
<h3 data-number="6.4" class="anchored" data-anchor-id="amortized-inference"><span class="header-section-number">6.4</span> Amortized Inference</h3>
<p>In classical variational inference, we introduce a separate set of variational parameters <span class="math inline">\(\phi^i\)</span> for each datapoint <span class="math inline">\(x^i\)</span> to approximate the true posterior <span class="math inline">\(p(z|x^i)\)</span>. However:</p>
<blockquote class="blockquote">
<p>Optimizing a separate <span class="math inline">\(\phi^i\)</span> for every datapoint is computationally expensive and does not scale to large datasets.</p>
</blockquote>
<hr>
<section id="the-key-idea-amortization" class="level4" data-number="6.4.1">
<h4 data-number="6.4.1" class="anchored" data-anchor-id="the-key-idea-amortization"><span class="header-section-number">6.4.1</span> The Key Idea: Amortization</h4>
<p>Instead of learning and storing a separate <span class="math inline">\(\phi^i\)</span> for every datapoint, we learn a <strong>single parametric function</strong> <span class="math inline">\(f_\phi(x)\)</span> ‚Äî typically a neural network ‚Äî that maps each input <span class="math inline">\(x\)</span> to the parameters of the approximate posterior:</p>
<p><span class="math display">\[
q_\phi(z|x) = \mathcal{N}\left(\mu_\phi(x), \sigma^2_\phi(x)\right)
\]</span></p>
<p>Here, <span class="math inline">\(\phi\)</span> are the shared parameters of the encoder network, and <span class="math inline">\(\mu_\phi(x), \sigma_\phi(x)\)</span> are its outputs.</p>
<p>This is like learning a regression function that predicts the optimal variational parameters for any input <span class="math inline">\(x\)</span>.</p>
<hr>
</section>
</section>
<section id="training-with-amortized-inference" class="level3" data-number="6.5">
<h3 data-number="6.5" class="anchored" data-anchor-id="training-with-amortized-inference"><span class="header-section-number">6.5</span> Training with Amortized Inference</h3>
<p>Our training objective remains the ELBO:</p>
<p><span class="math display">\[
\mathcal{L}(x; \theta, \phi) = \mathbb{E}_{q_\phi(z|x)}\left[\log p_\theta(x, z) - \log q_\phi(z|x)\right]
\]</span></p>
<p>We optimize both <span class="math inline">\(\theta\)</span> (decoder parameters) and <span class="math inline">\(\phi\)</span> (encoder parameters) using stochastic gradient descent.</p>
<section id="algorithm" class="level4" data-number="6.5.1">
<h4 data-number="6.5.1" class="anchored" data-anchor-id="algorithm"><span class="header-section-number">6.5.1</span> Algorithm:</h4>
<ol type="1">
<li><p><strong>Initialize</strong> <span class="math inline">\(\theta^{(0)}, \phi^{(0)}\)</span></p></li>
<li><p>Sample a datapoint <span class="math inline">\(x^i\)</span></p></li>
<li><p>Use <span class="math inline">\(f_\phi(x^i)\)</span> to produce <span class="math inline">\(\mu^i, \sigma^i\)</span></p></li>
<li><p>Sample <span class="math inline">\(z^i = \mu^i + \sigma^i \cdot \epsilon\)</span>, with <span class="math inline">\(\epsilon \sim \mathcal{N}(0, I)\)</span></p></li>
<li><p>Estimate the ELBO and compute gradients w.r.t. <span class="math inline">\(\theta, \phi\)</span></p></li>
<li><p>Update <span class="math inline">\(\theta, \phi\)</span> using gradient descent</p></li>
<li><p>Update <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\phi\)</span> using gradient descent:</p></li>
</ol>
<p><span class="math display">\[
\phi \leftarrow \phi + \tilde{\nabla}_\phi \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi)
\]</span></p>
<p><span class="math display">\[
\theta \leftarrow \theta + \tilde{\nabla}_\theta \sum_{x \in \mathcal{B}} \text{ELBO}(x; \theta, \phi)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{B}\)</span> is the current minibatch and <span class="math inline">\(\tilde{\nabla}\)</span> indicates a stochastic gradient approximation.</p>
<hr>
</section>
</section>
<section id="summary-1" class="level3" data-number="6.6">
<h3 data-number="6.6" class="anchored" data-anchor-id="summary-1"><span class="header-section-number">6.6</span> Summary</h3>
<ul>
<li>Amortized inference replaces per-datapoint optimization with a single learned mapping <span class="math inline">\(f_\phi(x)\)</span></li>
<li>This makes variational inference scalable and efficient</li>
<li>The model can generalize to unseen inputs by predicting variational parameters on-the-fly</li>
</ul>
<blockquote class="blockquote">
<p><strong>Note:</strong> Following common practice in the literature, we use <span class="math inline">\(\phi\)</span> to denote the parameters of the encoder network, even though it now defines a function rather than individual variational parameters.</p>
</blockquote>
<hr>
</section>
</section>
<section id="applications-of-vaes" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="applications-of-vaes"><span class="header-section-number">7</span> Applications of VAEs</h2>
<p>Variational Autoencoders are widely used in:</p>
<ul>
<li><strong>Image Generation</strong>: VAEs can generate new images similar to the training data (e.g., MNIST digits)<br>
</li>
<li><strong>Anomaly Detection</strong>: High reconstruction error flags unusual data points<br>
</li>
<li><strong>Representation Learning</strong>: Latent space captures features for downstream tasks</li>
</ul>
<section id="face-generation-with-convolutional-vae" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="face-generation-with-convolutional-vae"><span class="header-section-number">7.1</span> üòé Face Generation with Convolutional VAE</h3>
<p>To complement the theory, I‚Äôve built a full PyTorch implementation of a Variational Autoencoder trained on the CelebA dataset.</p>
<p>üìò The notebook walks through:</p>
<ul>
<li>Defining the encoder, decoder, and reparameterization trick<br>
</li>
<li>Implementing the ELBO loss function (reconstruction + KL divergence)<br>
</li>
<li>Training the model on face images<br>
</li>
<li>Generating new faces from random latent vectors</li>
</ul>
<p><a href="https://colab.research.google.com/github/changezakram/Deep-Generative-Models/blob/main/vae_faces.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open In Colab"></a><br>
üìì <a href="https://github.com/changezakram/Deep-Generative-Models/blob/main/vae_faces.ipynb">View on GitHub</a></p>
</section>
</section>
<section id="this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections." class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="this-example-is-designed-to-reinforce-the-theoretical-concepts-from-earlier-sections."><span class="header-section-number">8</span> This example is designed to reinforce the theoretical concepts from earlier sections.</h2>
</section>
<section id="further-reading" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">9</span> Further Reading</h2>
<p>For readers interested in diving deeper into the theory and applications of variational autoencoders, the following resources are recommended:</p>
<ul>
<li><p><strong>Tutorial on Variational Autoencoders</strong><br>
<em>Carl Doersch</em> (2016)<br>
<a href="https://arxiv.org/pdf/1606.05908">https://arxiv.org/pdf/1606.05908</a></p></li>
<li><p><strong>Auto-Encoding Variational Bayes</strong><br>
<em>Kingma &amp; Welling</em> (2014) ‚Äî the original VAE paper<br>
<a href="https://arxiv.org/pdf/1312.6114">https://arxiv.org/pdf/1312.6114</a></p></li>
<li><p><strong>The Challenges of Amortized Inference for Structured Prediction</strong><br>
<em>Cremer, Li, &amp; Duvenaud</em> (2019)<br>
<a href="https://arxiv.org/pdf/1906.02691">https://arxiv.org/pdf/1906.02691</a></p></li>
<li><p><strong>Deep Generative Models course notes</strong><br>
<a href="https://deepgenerativemodels.github.io/notes/vae/">https://deepgenerativemodels.github.io/notes/vae/</a></p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>