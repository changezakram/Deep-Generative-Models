<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Normalizing Flow Models – Deep Generative Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a14e3238c51140e99ccc48519b6ed9ce.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="styles.css">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Deep Generative Models</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://changezakram.github.io/"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Normalizing Flow Models</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In generative modeling, the objective is to learn a probability distribution over data that allows us to both <strong>generate new examples</strong> and <strong>evaluate the likelihood</strong> of observed ones. For a model to be practically useful, it must support <strong>efficient sampling</strong> and enable <strong>exact or tractable likelihood computation</strong> during training.</p>
<p><strong>A Variational Autoencoder (VAE)</strong> is a type of generative model that introduces latent variables <span class="math inline">\(z\)</span>, allowing the model to learn compact, structured representations of the data. VAEs are designed to support both sampling and likelihood estimation. However, computing the true marginal likelihood <span class="math inline">\(p(x)\)</span> is often intractable. To address this, VAEs use <strong>variational inference</strong> to approximate the posterior <span class="math inline">\(p(z \mid x)\)</span> and optimize a surrogate objective known as the <strong>Evidence Lower Bound (ELBO)</strong>. This is made possible by the <strong>reparameterization trick</strong>, which enables gradients to flow through stochastic latent variables during training.</p>
<p><strong>Normalizing flows</strong> address the limitations of VAEs by providing a way to perform exact inference and likelihood computation. They model complex data distributions using a sequence of invertible transformations applied to a simple base distribution. In this setup, a data point <span class="math inline">\(x\)</span> is generated by applying a function <span class="math inline">\(x = f(z)\)</span> to a latent variable <span class="math inline">\(z\)</span> sampled from a simple prior (e.g., a standard Gaussian). The transformation is invertible, so <span class="math inline">\(z\)</span> can be exactly recovered as <span class="math inline">\(z = f^{-1}(x)\)</span>. This structure enables direct access to both the data likelihood and latent variables using the change-of-variables formula.</p>
<p>This structure offers several advantages. First, <strong>each <span class="math inline">\(x\)</span> maps to a unique <span class="math inline">\(z\)</span></strong>, eliminating the need to marginalize over latent variables as in VAEs. Second, the <strong>change-of-variables formula</strong> enables <strong>exact computation of the likelihood</strong>, rather than approximations. Third, <strong>sampling is straightforward</strong>: draw <span class="math inline">\(z \sim p_Z(z)\)</span> from the base distribution and apply the transformation <span class="math inline">\(x = f(z)\)</span>.</p>
<p>Despite these strengths, normalizing flows have limitations. Unlike VAEs, which can learn <strong>lower-dimensional latent representations</strong>, flows require the latent and data spaces to have <strong>equal dimensionality</strong> to preserve invertibility. This means flow-based models <strong>do not perform dimensionality reduction</strong>, which can be a disadvantage in tasks where compact representations are important.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/vae_vs_flow.png" class="img-fluid figure-img"></p>
<figcaption>Comparison of VAE and Flow-based Models</figcaption>
</figure>
</div>
<p><em>VAEs compress data into a lower-dimensional latent space using an encoder, then reconstruct it with a decoder. Flow-based models use a single invertible transformation that keeps the same dimensionality between input and latent space. This enables exact inference and likelihood computation.</em></p>
<p>To understand how normalizing flows enable exact likelihood computation, we first need to explore a fundamental mathematical concept: the change-of-variable formula. This principle lies at the heart of flow models, allowing us to transform probability densities through invertible functions. We’ll begin with the 1D case and build up to the multivariate formulation.</p>
</section>
<section id="math-review" class="level2">
<h2 class="anchored" data-anchor-id="math-review">Math Review</h2>
<p>This section builds the mathematical foundation for understanding flow models, starting with change-of-variable and extending to multivariate transformations and Jacobians.</p>
<section id="change-of-variables-in-1d" class="level3">
<h3 class="anchored" data-anchor-id="change-of-variables-in-1d">Change of Variables in 1D</h3>
<p>Suppose we have a <strong>random variable</strong> <span class="math inline">\(z\)</span> with a known distribution <span class="math inline">\(p_Z(z)\)</span>, and we define a new variable:</p>
<p><span class="math display">\[
x = f(z)
\]</span></p>
<p>where <span class="math inline">\(f\)</span> is a <strong>monotonic, differentiable</strong> function with an inverse:</p>
<p><span class="math display">\[
z = f^{-1}(x) = h(x)
\]</span></p>
<p>Our goal is to compute the probability density function (PDF) of <span class="math inline">\(x\)</span>, denoted <span class="math inline">\(p_X(x)\)</span>, in terms of the known PDF <span class="math inline">\(p_Z(z)\)</span>.</p>
<section id="step-1-cumulative-distribution-function-cdf" class="level4">
<h4 class="anchored" data-anchor-id="step-1-cumulative-distribution-function-cdf">Step 1: Cumulative Distribution Function (CDF)</h4>
<p>We begin with the cumulative distribution function of <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
F_X(x) = P(X \leq x) = P(f(Z) \leq x)
\]</span></p>
<p>Since <span class="math inline">\(f\)</span> is monotonic and invertible, this becomes:</p>
<p><span class="math display">\[
P(f(Z) \leq x) = P(Z \leq f^{-1}(x)) = F_Z(h(x))
\]</span></p>
</section>
<section id="step-2-deriving-the-pdf-via-chain-rule" class="level4">
<h4 class="anchored" data-anchor-id="step-2-deriving-the-pdf-via-chain-rule">Step 2: Deriving the PDF via Chain Rule</h4>
<p>To obtain the PDF, we differentiate the CDF:</p>
<p><span class="math display">\[
p_X(x) = \frac{d}{dx} F_X(x) = \frac{d}{dx} F_Z(h(x))
\]</span></p>
<p>Applying the chain rule:</p>
<p><span class="math display">\[
p_X(x) = F_Z'(h(x)) \cdot h'(x) = p_Z(h(x)) \cdot h'(x)
\]</span></p>
</section>
<section id="step-3-rewrite-in-terms-of-z" class="level4">
<h4 class="anchored" data-anchor-id="step-3-rewrite-in-terms-of-z">Step 3: Rewrite in Terms of <span class="math inline">\(z\)</span></h4>
<p>From the previous step:</p>
<p><span class="math display">\[
p_X(x) = p_Z(h(x)) \cdot h'(x)
\]</span></p>
<p>Since <span class="math inline">\(z = h(x)\)</span>, we can rewrite:</p>
<p><span class="math display">\[
p_X(x) = p_Z(z) \cdot h'(x)
\]</span></p>
<p>Now, using the <strong>inverse function theorem</strong>, we express <span class="math inline">\(h'(x)\)</span> as:</p>
<p><span class="math display">\[
h'(x) = \frac{d}{dx} f^{-1}(x) = \frac{1}{f'(z)}
\]</span></p>
<p>So the final expression becomes:</p>
<p><span class="math display">\[
p_X(x) = p_Z(z) \cdot \left| \frac{1}{f'(z)} \right|
\]</span></p>
<p>The <strong>absolute value</strong> ensures the density remains non-negative, as required for any valid probability distribution.</p>
<p>This is the fundamental concept normalizing flows use to model complex distributions by transforming simple ones.</p>
</section>
</section>
<section id="geometry-determinants-and-volume-changes" class="level3">
<h3 class="anchored" data-anchor-id="geometry-determinants-and-volume-changes">Geometry: Determinants and Volume Changes</h3>
<p>To further understand the multivariate change-of-variable formula, it’s helpful to first explore how linear transformations affect <strong>volume</strong> in high-dimensional spaces.</p>
<p>Let <span class="math inline">\(\mathbf{Z}\)</span> be a random vector uniformly distributed in the unit cube <span class="math inline">\([0,1]^n\)</span>, and let <span class="math inline">\(\mathbf{X} = A\mathbf{Z}\)</span>, where <span class="math inline">\(A\)</span> is a square, invertible matrix. Geometrically, the matrix <span class="math inline">\(A\)</span> maps the unit hypercube to a <strong>parallelogram</strong> in 2D or a <strong>parallelotope</strong> in higher dimensions.</p>
<p>The <strong>determinant</strong> of a square matrix tells us how the transformation <strong>scales volume</strong>. For instance, if the determinant of a <span class="math inline">\(2 \times 2\)</span> matrix is 3, applying that matrix will stretch the area of a region by a factor of 3. A negative determinant indicates a <strong>reflection</strong>, meaning the transformation also flips the orientation. When measuring volume, we care about the <strong>absolute value</strong> of the determinant.</p>
<p>The volume of the resulting parallelotope is given by:</p>
<p><span class="math display">\[
\text{Volume} = |\det(A)|
\]</span></p>
<p>This expression tells us how much the transformation <span class="math inline">\(A\)</span> scales space. For example, if <span class="math inline">\(|\det(A)| = 2\)</span>, the transformation doubles the volume.</p>
<p>To make this idea concrete, consider the illustration below. The left figure shows a uniform distribution over the unit square <span class="math inline">\([0, 1]^2\)</span>. When we apply the linear transformation <span class="math inline">\(A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</span>, each point in the square is mapped to a new location, stretching the square into a parallelogram. The area of this parallelogram — and hence the volume scaling — is given by the <strong>absolute value of the determinant</strong> <span class="math inline">\(|\det(A)| = |ad - bc|\)</span>.</p>
<div style="text-align: center;">
<img src="images/determinant_area.png" alt="Linear transformation of a unit square into a parallelogram" width="500">
<p style="color: gray; font-size: 0.95em;">
<em> A linear transformation maps a unit square to a parallelogram. </em>
</p>
</div>
<p>This geometric intuition becomes essential when we apply the same logic to probability densities. The area of the parallelogram equals the absolute value of the determinant, |det(A)|, indicating how the transformation scales area.</p>
</section>
<section id="determinants-and-probability-density" class="level3">
<h3 class="anchored" data-anchor-id="determinants-and-probability-density">Determinants and Probability Density</h3>
<p>Previously, we saw how a linear transformation scales volume. Now we apply the same idea to probability densities — since density is defined per unit volume, scaling the volume also affects the density.</p>
<p>To transform the density from <span class="math inline">\(\mathbf{Z}\)</span> to <span class="math inline">\(\mathbf{X}\)</span>, we use the <strong>change-of-variable formula</strong>. Since <span class="math inline">\(\mathbf{X} = A\mathbf{Z}\)</span>, the inverse transformation is <span class="math inline">\(\mathbf{Z} = A^{-1} \mathbf{X}\)</span>. This tells us how to evaluate the density at <span class="math inline">\(\mathbf{x}\)</span> by “pulling it back” through the inverse mapping. Applying the multivariate change-of-variable rule:</p>
<p><span class="math display">\[
p_X(\mathbf{x}) = p_Z(W \mathbf{x}) \cdot \left| \det(W) \right| \quad \text{where } W = A^{-1}
\]</span></p>
<p>This is directly analogous to the 1D change-of-variable rule:</p>
<p><span class="math display">\[
p_X(x) = p_Z(h(x)) \cdot |h'(x)|
\]</span></p>
<p>but now in multiple dimensions using the determinant of the <strong>inverse transformation</strong>.</p>
<p>To make this more concrete, here’s a simple 2D example demonstrating how linear transformations affect probability density.</p>
<p>Let <span class="math inline">\(\mathbf{Z}\)</span> be a random vector uniformly distributed over the unit square <span class="math inline">\([0, 1]^2\)</span>. Suppose we apply the transformation <span class="math inline">\(\mathbf{X} = A\mathbf{Z}\)</span>, where</p>
<p><span class="math display">\[
A = \begin{bmatrix}
2 &amp; 0 \\
0 &amp; 1
\end{bmatrix}
\quad \text{so that} \quad
W = A^{-1} =
\begin{bmatrix}
\frac{1}{2} &amp; 0 \\
0 &amp; 1
\end{bmatrix}
\]</span></p>
<p>This transformation stretches the square horizontally, doubling its width while keeping the height unchanged. As a result, the area is doubled:<br>
<span class="math display">\[
|\det(A)| = 2 \quad \text{and} \quad |\det(W)| = \frac{1}{2}
\]</span> Since the same total probability must be spread over a larger area, the density decreases, meaning the probability per unit area is reduced due to the increased area over which the same total probability is distributed.</p>
<p>Now, let’s say <span class="math inline">\(p_Z(z) = 1\)</span> inside the unit square (a uniform distribution). To compute <span class="math inline">\(p_X(\mathbf{x})\)</span> at a point <span class="math inline">\(\mathbf{x}\)</span> in the transformed space, we use:</p>
<p><span class="math display">\[
p_X(\mathbf{x}) = p_Z(W\mathbf{x}) \cdot |\det(W)| = 1 \cdot \frac{1}{2} = \frac{1}{2}
\]</span></p>
<p>So, the transformed density is halved — the same total probability (which must remain 1) is now spread over an area that is twice as large.</p>
</section>
<section id="generalizing-to-nonlinear-transformations" class="level3">
<h3 class="anchored" data-anchor-id="generalizing-to-nonlinear-transformations">Generalizing to Nonlinear Transformations</h3>
<p>For <strong>nonlinear</strong> transformations <span class="math inline">\(\mathbf{x} = f(\mathbf{z})\)</span>, the idea is similar. But instead of a constant matrix <span class="math inline">\(A\)</span>, we now consider the <strong>Jacobian matrix</strong> of the function <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
J_f(\mathbf{z}) = \frac{\partial f}{\partial \mathbf{z}}
\]</span></p>
<p>The Jacobian matrix generalizes derivatives to multivariable functions, capturing how a transformation scales and rotates space locally through all partial derivatives. Its determinant tells us how much the transformation stretches or compresses space — acting as a local volume scaling factor.</p>
</section>
<section id="multivariate-change-of-variable" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-change-of-variable">Multivariate Change-of-Variable</h3>
<p>Given an invertible transformation <span class="math inline">\(\mathbf{x} = f(\mathbf{z})\)</span>, the probability density transforms as:</p>
<p><span class="math display">\[
p_X(\mathbf{x}) = p_Z(f^{-1}(\mathbf{x})) \cdot \left| \det \left( \frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}} \right) \right|
\]</span></p>
<p>Alternatively, in the <strong>forward form</strong> (often used during training):</p>
<p><span class="math display">\[
p_X(\mathbf{x}) = p_Z(\mathbf{z}) \cdot \left| \det \left( \frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} \right) \right|^{-1}
\]</span></p>
<p>This generalizes the 1D rule and enables us to compute <strong>exact likelihoods</strong> for complex distributions as long as the transformation is invertible and differentiable. This formula is pivotal in machine learning, where transformations of probability distributions are common — such as in the implementation of normalizing flows for generative modeling.</p>
</section>
</section>
<section id="flow-model" class="level2">
<h2 class="anchored" data-anchor-id="flow-model">Flow Model</h2>
<p>A normalizing flow model defines a <strong>one-to-one and reversible transformation</strong> between observed variables <span class="math inline">\(\mathbf{x}\)</span> and latent variables <span class="math inline">\(\mathbf{z}\)</span>. This transformation is given by an invertible, differentiable function <span class="math inline">\(f_\theta\)</span>, parameterized by <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\mathbf{x} = f_\theta(\mathbf{z}) \quad \text{and} \quad \mathbf{z} = f_\theta^{-1}(\mathbf{x})
\]</span></p>
<div style="text-align: center;">
<img src="images/Flow_Model.png" alt="Flow model showing forward and inverse transformations" style="display: block; margin: auto;">
<p style="color: gray; font-size: 0.95em;">
Flow model showing forward and inverse transformations
</p>
</div>
<p><em>Figure: A flow-based model uses a forward transformation <span class="math inline">\(f_\theta\)</span> to map from latent variables (<span class="math inline">\(\mathbf{z}\)</span>) to data (<span class="math inline">\(\mathbf{x}\)</span>), and an inverse transformation <span class="math inline">\(f_\theta^{-1}\)</span> to compute likelihoods. Adapted from class notes (XCS236, Stanford).</em></p>
<p>Because the transformation is invertible, we can apply the <strong>change-of-variable formula</strong> to compute the exact probability of <span class="math inline">\(\mathbf{x}\)</span>:</p>
<p><span class="math display">\[
p_X(\mathbf{x}; \theta) = p_Z(f_\theta^{-1}(\mathbf{x})) \cdot \left| \det \left( \frac{\partial f_\theta^{-1}(\mathbf{x})}{\partial \mathbf{x}} \right) \right|
\]</span></p>
<p>This makes it possible to evaluate <strong>exact likelihoods</strong> and learn the model via <strong>maximum likelihood estimation (MLE)</strong>.</p>
<blockquote class="blockquote">
<p><strong>Note:</strong> Both <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{z}\)</span> must be continuous and have the same dimensionality since the transformation must be invertible.</p>
</blockquote>
<section id="model-architecture-a-sequence-of-invertible-transformations" class="level3">
<h3 class="anchored" data-anchor-id="model-architecture-a-sequence-of-invertible-transformations">Model Architecture: A Sequence of Invertible Transformations</h3>
<p>The term <em>flow</em> refers to the fact that we can compose multiple invertible functions to form a more expressive transformation:</p>
<p><span class="math display">\[
\mathbf{z}_m = f_\theta^{(m)} \circ f_\theta^{(m-1)} \circ \cdots \circ f_\theta^{(1)}(\mathbf{z}_0)
\]</span></p>
<p>In this setup:</p>
<ul>
<li><span class="math inline">\(\mathbf{z}_0 \sim p_Z\)</span> is sampled from a simple base distribution (e.g., standard Gaussian)</li>
<li><span class="math inline">\(\mathbf{x} = \mathbf{z}_M\)</span> is the final transformed variable</li>
<li>The full transformation <span class="math inline">\(f_\theta\)</span> is the composition of <span class="math inline">\(M\)</span> sequential invertible functions. Each function slightly reshapes the distribution, and together they produce a highly expressive mapping from a simple base distribution to a complex one.</li>
</ul>
<p>The visuals below illustrate this idea from two angles. The first diagram illustrates the structure of a normalizing flow as a composition of invertible steps, while the second shows how this architecture reshapes simple distributions into complex ones through repeated transformations.</p>
<div style="text-align: center;">
<img src="https://upload.wikimedia.org/wikipedia/commons/e/e0/Normalizing-flow.svg" width="700" alt="Normalizing Flow Illustration">
<p style="color: gray; font-size: 0.95em;">
<em> Adapted from Wikipedia: Mapping simple distributions to complex ones via invertible transformations. </em>
</p>
</div>
<div style="text-align: center;">
<img src="images/planar_flow_transformations.png" alt="Planar Flow Transformations" width="600">
<p style="color: gray; font-size: 0.95em;">
<em> Adapted from class notes (XCS236, Stanford), originally based on <i>Rezende &amp; Mohamed, 2016</i>. </em>
</p>
</div>
<p>The density of <span class="math inline">\(\mathbf{x}\)</span> is given by the change-of-variable formula:</p>
<p><span class="math display">\[
p_X(\mathbf{x}; \theta) = p_Z(f_\theta^{-1}(\mathbf{x})) \cdot \prod_{m=1}^M \left| \det \left( \frac{\partial (f_\theta^{(m)})^{-1}(\mathbf{z}_m)}{\partial \mathbf{z}_m} \right) \right|
\]</span></p>
<p>This approach allows the model to approximate highly complex distributions using simple building blocks.</p>
</section>
</section>
<section id="learning-and-inference" class="level2">
<h2 class="anchored" data-anchor-id="learning-and-inference">Learning and Inference</h2>
<p>Training a flow-based model is done by maximizing the log-likelihood over the dataset <span class="math inline">\(\mathcal{D}\)</span>:</p>
<p><span class="math display">\[
\max_\theta \log p_X(\mathcal{D}; \theta) = \sum_{\mathbf{x} \in \mathcal{D}} \log p_Z(f_\theta^{-1}(\mathbf{x})) + \log \left| \det \left( \frac{\partial f_\theta^{-1}(\mathbf{x})}{\partial \mathbf{x}} \right) \right|
\]</span></p>
<p><strong>Key advantages of normalizing flows:</strong></p>
<ul>
<li><strong>Exact likelihoods</strong>: No approximation needed — just apply the change-of-variable rule</li>
<li><strong>Efficient sampling</strong>: Generate new data by drawing <span class="math inline">\(\mathbf{z} \sim p_Z\)</span> and computing <span class="math inline">\(\mathbf{x} = f_\theta(\mathbf{z})\)</span></li>
<li><strong>Latent inference</strong>: Invert <span class="math inline">\(f_\theta\)</span> to compute latent codes <span class="math inline">\(\mathbf{z} = f_\theta^{-1}(\mathbf{x})\)</span>, without needing a separate encoder</li>
</ul>
<section id="computational-considerations" class="level3">
<h3 class="anchored" data-anchor-id="computational-considerations">Computational Considerations</h3>
<p>One challenge in training normalizing flow models is that computing the <strong>exact likelihood</strong> requires evaluating the <strong>determinant of the Jacobian matrix</strong> of the transformation:</p>
<ul>
<li>For a transformation <span class="math inline">\(f : \mathbb{R}^n \to \mathbb{R}^n\)</span>, the Jacobian is an <span class="math inline">\(n \times n\)</span> matrix.</li>
<li>Computing its determinant has a cost of <span class="math inline">\(\mathcal{O}(n^3)\)</span>, which is <strong>computationally expensive</strong> during training — especially in high dimensions.</li>
</ul>
<section id="key-insight" class="level4">
<h4 class="anchored" data-anchor-id="key-insight">Key Insight</h4>
<p>To make normalizing flows scalable, we design transformations where the <strong>Jacobian has a special structure</strong> that makes the determinant <strong>easy to compute</strong>.</p>
<p>For example: - If the Jacobian is a <strong>triangular matrix</strong>, the determinant is just the <strong>product of the diagonal entries</strong>, which can be computed in <span class="math inline">\(\mathcal{O}(n)\)</span> time. - This works because in a triangular matrix, all the off-diagonal elements are zero — so the determinant simplifies significantly.</p>
<p>In practice, flow models like <strong>RealNVP</strong> and <strong>MAF</strong> are designed so that each output dimension <span class="math inline">\(x_i\)</span> depends only on some subset of the input dimensions <span class="math inline">\(z_{\leq i}\)</span> (for lower triangular structure) or <span class="math inline">\(z_{\geq i}\)</span> (for upper triangular structure). This results in a Jacobian of the form:</p>
<p><span class="math display">\[
J = \frac{\partial \mathbf{f}}{\partial \mathbf{z}} =
\begin{pmatrix}
\frac{\partial f_1}{\partial z_1} &amp; 0 &amp; \cdots &amp; 0 \\
\ast &amp; \frac{\partial f_2}{\partial z_2} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\ast &amp; \ast &amp; \cdots &amp; \frac{\partial f_n}{\partial z_n}
\end{pmatrix}
\]</span></p>
<p>Because of this triangular structure, computing the determinant becomes as simple as multiplying the diagonal terms:</p>
<p><span class="math display">\[
\det(J) = \prod_{i=1}^{n} \frac{\partial f_i}{\partial z_i}
\]</span></p>
<p>This is why many modern flow models rely on <strong>coupling layers</strong> or <strong>autoregressive masking</strong>: they preserve invertibility and enable efficient, exact likelihood computation.</p>
</section>
</section>
</section>
<section id="popular-flow-models" class="level2">
<h2 class="anchored" data-anchor-id="popular-flow-models">Popular Flow Models</h2>
<p>To implement scalable and expressive normalizing flows, researchers have designed several model families based on clever transformation strategies—particularly <strong>coupling layers</strong> and <strong>autoregressive masking</strong>.</p>
<p>In this section, we’ll introduce few of these architectures:</p>
<ul>
<li><strong>NICE</strong>: Introduces additive coupling layers for tractable Jacobians.</li>
<li><strong>Real-NVP</strong>: Extends NICE with scaling and better expressivity.</li>
<li><strong>Inverse Autoregressive Flow (IAF)</strong>: Uses autoregressive masking for high flexibility.</li>
<li><strong>Masked Autoregressive Flow (MAF)</strong>: Reverses the computation flow of IAF for efficient density evaluation.</li>
</ul>
<p>We’ll walk through the core design behind each and how they achieve invertibility and efficient learning.</p>
<section id="nice-nonlinear-independent-components-estimation" class="level3">
<h3 class="anchored" data-anchor-id="nice-nonlinear-independent-components-estimation">NICE: Nonlinear Independent Components Estimation</h3>
<p>The NICE model introduces two key components to construct invertible transformations:</p>
<ul>
<li><strong>Additive Coupling Layers</strong>: These preserve volume and maintain a simple Jacobian structure.</li>
<li><strong>Rescaling Layers</strong>: These adjust volume and allow the model to capture more expressive distributions.</li>
</ul>
<section id="additive-coupling-layer" class="level4">
<h4 class="anchored" data-anchor-id="additive-coupling-layer">Additive Coupling Layer</h4>
<p>To make the transformation invertible and computationally efficient, NICE splits the input vector into two parts. One part is kept unchanged, while the other part is modified using a function of the unchanged part. This way, we can easily reverse the process because we always know what was kept intact.</p>
<p>Let’s partition the input <span class="math inline">\(\mathbf{z} \in \mathbb{R}^n\)</span> into two subsets: <span class="math inline">\(\mathbf{z}_{1:d}\)</span> and <span class="math inline">\(\mathbf{z}_{d+1:n}\)</span> for some <span class="math inline">\(1 \leq d &lt; n\)</span>.</p>
<ul>
<li><strong>Forward Mapping</strong> <span class="math inline">\(\mathbf{z} \mapsto \mathbf{x}\)</span>:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\mathbf{x}_{1:d} &amp;= \mathbf{z}_{1:d} \quad \text{(identity transformation)} \\
\mathbf{x}_{d+1:n} &amp;= \mathbf{z}_{d+1:n} + m_\theta(\mathbf{z}_{1:d})
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(m_\theta(\cdot)\)</span> is a neural network with parameters <span class="math inline">\(\theta\)</span>, <span class="math inline">\(d\)</span> input units, and <span class="math inline">\(n - d\)</span> output units.</p>
<ul>
<li><strong>Inverse Mapping</strong> <span class="math inline">\(\mathbf{x} \mapsto \mathbf{z}\)</span>:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{1:d} &amp;= \mathbf{x}_{1:d} \quad \text{(identity transformation)} \\
\mathbf{z}_{d+1:n} &amp;= \mathbf{x}_{d+1:n} - m_\theta(\mathbf{x}_{1:d})
\end{aligned}
\]</span></p>
<ul>
<li><strong>Jacobian</strong> of the forward mapping:</li>
</ul>
<p>The Jacobian matrix captures how much the transformation stretches or compresses space. Because part of the input is unchanged and the other part is only shifted (not scaled), the determinant of the Jacobian is 1 — meaning the transformation preserves volume.</p>
<p><span class="math display">\[
J = \frac{\partial \mathbf{x}}{\partial \mathbf{z}} =
\begin{pmatrix}
I_d &amp; 0 \\
\frac{\partial m_\theta}{\partial \mathbf{z}_{1:d}} &amp; I_{n-d}
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
\det(J) = 1
\]</span></p>
<p>Hence, additive coupling is a <strong>volume-preserving transformation</strong>.</p>
</section>
<section id="rescaling-layer" class="level4">
<h4 class="anchored" data-anchor-id="rescaling-layer">Rescaling Layer</h4>
<p>To enable volume changes, NICE appends a final rescaling layer:</p>
<ul>
<li><strong>Forward Mapping</strong>:</li>
</ul>
<p><span class="math display">\[
x_i = s_i z_i \quad \text{with} \quad s_i &gt; 0
\]</span></p>
<ul>
<li><strong>Inverse Mapping</strong>:</li>
</ul>
<p><span class="math display">\[
z_i = \frac{x_i}{s_i}
\]</span></p>
<ul>
<li><strong>Jacobian</strong>:</li>
</ul>
<p><span class="math display">\[
J = \text{diag}(\mathbf{s})
\quad \Rightarrow \quad
\det(J) = \prod_{i=1}^n s_i
\]</span></p>
</section>
</section>
<section id="real-nvp-non-volume-preserving-extension-of-nice" class="level3">
<h3 class="anchored" data-anchor-id="real-nvp-non-volume-preserving-extension-of-nice">Real-NVP: Non-Volume Preserving Extension of NICE</h3>
<p>Real-NVP (Dinh et al., 2017) extends NICE by introducing a <strong>scaling function</strong> that allows the model to <strong>change volume</strong>, enabling more expressive transformations.</p>
<p>We again partition the input <span class="math inline">\(\mathbf{z} \in \mathbb{R}^n\)</span> into two subsets: <span class="math inline">\(\mathbf{z}_{1:d}\)</span> and <span class="math inline">\(\mathbf{z}_{d+1:n}\)</span>.</p>
<ul>
<li><strong>Forward Mapping</strong> <span class="math inline">\(\mathbf{z} \mapsto \mathbf{x}\)</span>:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\mathbf{x}_{1:d} &amp;= \mathbf{z}_{1:d} \quad \text{(identity transformation)} \\\\
\mathbf{x}_{d+1:n} &amp;= \mathbf{z}_{d+1:n} \odot \exp(\alpha_\theta(\mathbf{z}_{1:d})) + \mu_\theta(\mathbf{z}_{1:d})
\end{aligned}
\]</span></p>
<p>This equation modifies the second half of the input by scaling and shifting it, based on the first half. The use of the exponential function ensures that scaling is always positive, which is important for invertibility.</p>
<ul>
<li><strong>Inverse Mapping</strong> <span class="math inline">\(\mathbf{x} \mapsto \mathbf{z}\)</span>:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\mathbf{z}_{1:d} &amp;= \mathbf{x}_{1:d} \quad \text{(identity transformation)} \\\\
\mathbf{z}_{d+1:n} &amp;= \left( \mathbf{x}_{d+1:n} - \mu_\theta(\mathbf{x}_{1:d}) \right) \odot \exp(-\alpha_\theta(\mathbf{x}_{1:d}))
\end{aligned}
\]</span></p>
<ul>
<li><strong>Jacobian of Forward Mapping</strong>:</li>
</ul>
<p><span class="math display">\[
J = \frac{\partial \mathbf{x}}{\partial \mathbf{z}} =
\begin{pmatrix}
I_d &amp; 0 \\\\
\frac{\partial \mathbf{x}_{d+1:n}}{\partial \mathbf{z}_{1:d}} &amp; \operatorname{diag}\left(\exp(\alpha_\theta(\mathbf{z}_{1:d}))\right)
\end{pmatrix}
\]</span></p>
<ul>
<li><strong>Determinant of the Jacobian</strong>:</li>
</ul>
<p><span class="math display">\[
\det(J) = \prod_{i=d+1}^{n} \exp\left( \alpha_\theta(\mathbf{z}_{1:d})_i \right)
= \exp\left( \sum_{i=d+1}^{n} \alpha_\theta(\mathbf{z}_{1:d})_i \right)
\]</span></p>
<p>This makes Real-NVP a <strong>non-volume preserving transformation</strong>, allowing for both expansion and contraction of volume.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
</colgroup>
<tbody>
<tr class="odd">
<td>## 🔄 Real-NVP: Non-Volume Preserving Extension of NICE</td>
</tr>
<tr class="even">
<td>Real-NVP (Dinh et al., 2017) extends NICE by introducing a <strong>scaling function</strong> that allows the model to change volume, enabling more expressive transformations. This is achieved using <strong>affine coupling layers</strong> that apply learned scaling and translation functions to part of the input while keeping the rest unchanged.</td>
</tr>
<tr class="odd">
<td>### 🔀 Coupling Layer Transformation</td>
</tr>
<tr class="even">
<td>We again partition the input ( ^n ) into two disjoint subsets: ( <em>{1:d} ) and ( </em>{d+1:n} ). The transformation is defined as follows:</td>
</tr>
<tr class="odd">
<td><span class="math display">\[
\mathbf{x}_{1:d} = \mathbf{z}_{1:d} \quad \text{(identity transformation)}
\]</span></td>
</tr>
<tr class="even">
<td><span class="math display">\[
\mathbf{x}_{d+1:n} = \mathbf{z}_{d+1:n} \odot \exp(s(\mathbf{z}_{1:d})) + t(\mathbf{z}_{1:d})
\]</span></td>
</tr>
<tr class="odd">
<td>This equation modifies the second half of the input by <strong>scaling</strong> and <strong>shifting</strong> it based on the first half. The exponential ensures scaling is always positive (important for invertibility), while the identity path allows efficient inversion.</td>
</tr>
<tr class="even">
<td>### 📐 Efficient Jacobian</td>
</tr>
<tr class="odd">
<td>Because the first half ( <em>{1:d} ) is copied directly and only ( </em>{d+1:n} ) is transformed element-wise, the Jacobian of this transformation has a triangular structure:</td>
</tr>
<tr class="even">
<td><span class="math display">\[
\frac{\partial \mathbf{x}}{\partial \mathbf{z}} =
\begin{pmatrix}
I_d &amp; 0 \\
\frac{\partial \mathbf{x}_{d+1:n}}{\partial \mathbf{z}_{1:d}} &amp; \text{diag} \left( \exp(s(\mathbf{z}_{1:d})) \right)
\end{pmatrix}
\]</span></td>
</tr>
<tr class="odd">
<td>The lower-right block is a diagonal matrix because each element of ( _{d+1:n} ) is scaled independently. This makes the <strong>Jacobian determinant</strong> simply the product of diagonal entries:</td>
</tr>
<tr class="even">
<td><span class="math display">\[
\det(J) = \prod_{i=d+1}^{n} \exp(s(\mathbf{z}_{1:d})_i) = \exp \left( \sum_{i=d+1}^{n} s(\mathbf{z}_{1:d})_i \right)
\]</span></td>
</tr>
<tr class="odd">
<td>&gt; 💡 <strong>Why this matters:</strong> The design ensures that the transformation is invertible <strong>and</strong> the log-determinant is <strong>easy to compute</strong> — both are essential for tractable likelihood estimation.</td>
</tr>
<tr class="even">
<td>### 🔁 Inverse Mapping</td>
</tr>
<tr class="odd">
<td>To recover ( ) from ( ), the process is symmetric:</td>
</tr>
<tr class="even">
<td><span class="math display">\[
\mathbf{z}_{1:d} = \mathbf{x}_{1:d} \quad \text{(identity transformation)}
\]</span></td>
</tr>
<tr class="odd">
<td><span class="math display">\[
\mathbf{z}_{d+1:n} = \left( \mathbf{x}_{d+1:n} - t(\mathbf{x}_{1:d}) \right) \odot \exp(-s(\mathbf{x}_{1:d}))
\]</span></td>
</tr>
<tr class="even">
<td>The same coupling structure is reused, with subtraction and elementwise division instead of addition and multiplication.</td>
</tr>
</tbody>
</table>
</section>
<section id="updating-all-dimensions-stacking-coupling-layers" class="level3">
<h3 class="anchored" data-anchor-id="updating-all-dimensions-stacking-coupling-layers">🔁 Updating All Dimensions: Stacking Coupling Layers</h3>
<p>Each coupling layer only transforms part of the input. To ensure that <strong>every dimension is eventually updated</strong>, RealNVP stacks multiple coupling layers and <strong>alternates the masking pattern</strong> between them:</p>
<ul>
<li>In one layer, the first half is fixed, and the second half is transformed.</li>
<li>In the next layer, the roles are reversed.</li>
</ul>
<p>This alternating structure ensures: - All input dimensions are updated across layers - The full transformation remains invertible - The total log-determinant is the <strong>sum of the log-determinants</strong> of each layer</p>
<blockquote class="blockquote">
<p>This stacking strategy is what gives RealNVP its power — simple, local transformations that combine to model globally complex distributions.</p>
</blockquote>
<hr>
</section>
<section id="realnvp-in-action-two-moons" class="level3">
<h3 class="anchored" data-anchor-id="realnvp-in-action-two-moons">🧪 RealNVP in Action (Two Moons)</h3>
<p>The following plots illustrate how RealNVP transforms data in practice:</p>
<div style="text-align: center;">
<img src="images/realnvp_result.png" alt="RealNVP Two Moons Visualization" width="800">
<p style="color: gray; font-size: 0.95em;">
<em> Top-left: Original two-moons data ((X))<br>
Top-right: Encoded latent space ((Z))<br>
Bottom-left: Latent samples from base distribution<br>
Bottom-right: Generated samples mapped back to (X) space<br>
</em>
</p>
</div>
<hr>
<div style="text-align: center;">
<img src="images/coupling-layer-v2.png" alt="Affine Coupling Layer in RealNVP" width="700">
<p style="color: gray; font-size: 0.95em;">
<em> Visualization of a single affine coupling layer in RealNVP. The identity path and affine transform structure allow exact inversion and efficient computation. </em>
</p>
</div>
<hr>
</section>
</section>
<section id="try-it-yourself-flow-model-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="try-it-yourself-flow-model-in-pytorch">🧪 Try It Yourself: Flow Model in Pytorch</h2>
<p>You can explore a minimal PyTorch implementation of a normalizing flow model:</p>
<ul>
<li>📘 <a href="https://github.com/changezakram/Deep-Generative-Models/blob/main/normalizing_flow_pytorch.ipynb"><strong>View Notebook on GitHub</strong></a></li>
<li>🚀 <a href="https://colab.research.google.com/github/changezakram/Deep-Generative-Models/blob/main/normalizing_flow_pytorch.ipynb"><strong>Run in Google Colab</strong></a></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">📚 References</h2>
<ul>
<li><strong>Stanford CS236 Notes.</strong> <em>Normalizing Flows</em>. <a href="https://deepgenerativemodels.github.io/notes/flow/">deepgenerativemodels.github.io</a></li>
<li><strong>Rezende, D. J., &amp; Mohamed, S.</strong> (2015). <em>Variational Inference with Normalizing Flows</em>. <a href="https://arxiv.org/abs/1505.05770">arXiv:1505.05770</a><br>
</li>
<li><strong>Wikipedia.</strong> <em>Normalizing Flow</em>. <a href="https://en.wikipedia.org/wiki/Flow-based_generative_model">Link</a></li>
</ul>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">🔍 Further Reading</h2>
<ul>
<li><strong>Papamakarios et al.</strong> (2019). <em>Normalizing Flows for Probabilistic Modeling and Inference</em>. <a href="https://arxiv.org/abs/1912.02762">arXiv:1912.02762</a><br>
</li>
<li><strong>Lilian Weng.</strong> (2018). <em>Flow-based Models</em>. <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Blog</a><br>
</li>
<li><strong>Eric Jang.</strong> (2018). <em>Normalizing Flows Tutorial – Part 1</em>. <a href="https://blog.evjang.com/2018/01/nf1.html">Blog</a><br>
</li>
<li><strong>Eric Jang.</strong> (2018). <em>Normalizing Flows Tutorial – Part 2</em>. <a href="https://blog.evjang.com/2018/01/nf2.html">Blog</a><br>
</li>
<li><strong>UT Austin Calculus Notes.</strong> <em>Jacobian and Change of Variables</em>. <a href="https://web.ma.utexas.edu/users/m408s/m408d/CurrentWeb/LM15-10-2.php">Web</a></li>
<li><strong>Kobyzev, Prince, &amp; Brubaker.</strong> (2019). <em>Normalizing Flows: An Introduction and Review of Current Methods</em>. <a href="https://arxiv.org/pdf/1908.09257">arXiv PDF</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>