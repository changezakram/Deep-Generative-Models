---
title: "Deep Generative Models"
---

## Overview

This site explores the foundations and frontiers of deep generative models — from core mathematical principles to the architectures powering today’s most advanced AI systems.

It’s designed to document concepts clearly, walk through implementations step by step, and connect theory with real-world relevance. Topics range from early probabilistic frameworks to cutting-edge diffusion models driving breakthroughs in image, text, and multimodal generation.

> Learn by building · Backed by math · Inspired by real-world use cases



## Key Model Families

<details>
<summary><strong> Variational Autoencoders (VAEs)</strong></summary>

- **Core Idea**: Encode input to a latent space and reconstruct it while optimizing a lower bound on likelihood (ELBO).
- **Likelihood**: Approximate (variational lower bound).
- **Sampling**: Fast — sample latent vector \( z \sim p(z) \) and decode.
- **Use Cases**: Representation learning, image generation.
- [Go to VAE write-up →](vae.qmd)

</details>

<details>
<summary><strong> Generative Adversarial Networks (GANs)</strong></summary>

- **Core Idea**: A generator and discriminator compete in a minimax game to produce realistic samples.
- **Likelihood**: None (implicit model).
- **Sampling**: Fast — sample latent vector and pass through generator.
- **Use Cases**: High-quality image generation, style transfer.
- [Go to GAN write-up →]

</details>

<details>
<summary><strong> Autoregressive Models</strong></summary>

- **Core Idea**: Factor the joint distribution as a product of conditional probabilities.
- **Likelihood**: Exact.
- **Sampling**: Slow — token-by-token generation.
- **Use Cases**: Language modeling, code generation, time series.
- [Go to Autoregressive Models →]

</details>

<details>
<summary><strong> Normalizing Flows</strong></summary>

- **Core Idea**: Learn invertible transformations of latent variables using the change-of-variable formula.
- **Likelihood**: Exact and tractable.
- **Sampling**: Fast — sample from base distribution and invert.
- **Use Cases**: Density estimation, latent space modeling.
- [Go to Flow Models →](flows.qmd)

</details>

<details>
<summary><strong> Energy-Based Models (EBMs)</strong></summary>

- **Core Idea**: Define an energy function over inputs; lower energy = higher probability.
- **Likelihood**: Unnormalized (intractable partition function).
- **Sampling**: Very slow — requires MCMC or Langevin dynamics.
- **Use Cases**: Uncertainty modeling, compositional generation.
- [Go to EBMs →](ebm.qmd)

</details>

<details>
<summary><strong> Diffusion Models</strong></summary>

- **Core Idea**: Learn to reverse a gradual noise process via denoising.
- **Likelihood**: Approximate (via variational bound).
- **Sampling**: Slow — requires hundreds of reverse steps.
- **Use Cases**: High-resolution image and audio generation.
- [Go to Diffusion Models →]

## Industry Use Cases

### 1. [Banking](gen-ai-use-cases/banking-use-cases.html)  
6 use cases spanning customer service, operational efficiency, fraud detection, business intelligence, and marketing.  
Includes JPMorgan, Mastercard, Wells Fargo, and Morgan Stanley.

### 2. [Healthcare](gen-ai-use-cases/healthcare-use-cases.html)  
5 use cases spanning clinical scribing, imaging, triage, education, and drug discovery.  
Based on real-world deployments by Nuance (Microsoft), Mayo Clinic, DeepMind, and Insilico.