---
title: "Deep Generative Models"
---

## Evolution of AI Capabilities

AI is changing fast, and each stage brings new strengths, limits, and oversight needs. The table below offers a strategic comparison of how Traditional AI, Generative AI, and Agentic AI differ in behavior and best use.


<div style="overflow-x:auto;">

| Capability            | **Traditional AI**<br><small>Rule-driven tools</small> | **Generative AI**<br><small>Smart assistants</small> | **Agentic AI**<br><small>Autonomous actors</small> |
|-----------------------|---------------------------------------------|--------------------------------------------|--------------------------------------------|
| **What it does**      | Follows hard-coded logic and rules          | Understands prompts and generates output   | Plans, reasons, and acts across steps       |
| **How it behaves**    | Rigid, predictable                          | Creative but guided                        | Goal-seeking and adaptive                   |
| **Human involvement** | Fully manual setup and supervision          | Needs context and oversight                | Can operate independently (with guardrails) |
| **Strengths**         | Reliable on structured tasks                | Great at summarizing, drafting, generating | Handles multistep workflows                 |
| **Best used for**     | Repetitive decisions and automation         | Insight generation and copiloting          | Full process orchestration                  |

</div>


## Why Generative AI Matters

Generative AI models can create entirely new content â€” such as text, images, simulations, or code â€” by learning from existing data. Unlike traditional AI systems that classify, tag, or score inputs, generative models **produce** new outputs based on patterns they've learned.

Since 2022, the pace of innovation in generative AI has accelerated rapidly. Foundation models like ChatGPT, Claude, and Gemini are being applied across industries â€” from writing assistance and image generation to fraud detection and synthetic data creation.

> ðŸ’¡ **Business impact:** According to McKinsey, generative AI could add up to **$4.4 trillion in annual global economic value**, with significant implications for productivity, personalization, and decision support.

However, generative AI also introduces new challenges. Outputs can be wrong, biased, or easily misused â€” especially in high-stakes domains like banking, healthcare, and law. Ensuring responsible use requires robust governance, human oversight, and ethical deployment frameworks.


## Generative AI Models

Generative AI models aim to learn the underlying distribution of real-world data, typically denoted as $p_{\text{data}}(x)$. Their goal is to approximate this distribution as closely as possible.


- The distribution learned by the model is denoted as $p_\theta(x)$  
- We generate new data by **sampling** from this learned distribution  
- In practice, generative models are trained to **maximize the expected log-likelihood** of $p_\theta(x)$, or equivalently, **minimize the divergence** between $p_\theta(x)$ and $p_{\text{data}}(x)$


> ðŸ“Œ **Note**:  
> - Models like **Autoregressive models** and **Normalizing Flows** directly maximize log-likelihood.  
> - **VAEs** maximize a variational lower bound (ELBO) on log-likelihood.  
> - **GANs** minimize the **Jensen-Shannon divergence** through adversarial training.  
> - **Diffusion models** and **EBMs** use score matching or other divergence-minimizing techniques.

This site explores how different model families approach this goal â€” from Variational Autoencoders (VAEs) to Diffusion Models â€” grounded in theory and real-world use.

## Key Gen AI Model Families

<details>
<summary><strong> Variational Autoencoders (VAEs)</strong></summary>

- **Core Idea**: Encode input to a latent space and reconstruct it while optimizing a lower bound on likelihood (ELBO).
- **Likelihood**: Approximate (variational lower bound).
- **Sampling**: Fast â€” sample latent vector and decode.
- **Use Cases**: Representation learning, image generation.
- **Example Models**: Î²-VAE, Conditional VAE
- [Go to VAE write-up â†’](vae.qmd)

</details>

<details>
<summary><strong> Generative Adversarial Networks (GANs)</strong></summary>

- **Core Idea**: A generator and discriminator compete in a minimax game to produce realistic samples.
- **Likelihood**: None (implicit model).
- **Sampling**: Fast â€” sample latent vector and pass through generator.
- **Use Cases**: High-quality image generation, style transfer.
- **Example Models**: StyleGAN, CycleGAN
- [Go to GAN write-up â†’]

</details>

<details>
<summary><strong> Autoregressive Models</strong></summary>

- **Core Idea**: Factor the joint distribution as a product of conditional probabilities.
- **Likelihood**: Exact.
- **Sampling**: Slow â€” token-by-token generation.
- **Use Cases**: Language modeling, code generation, time series.
- **Example Models**: GPT, PixelRNN
- [Go to Autoregressive Models â†’]

</details>

<details>
<summary><strong> Normalizing Flows</strong></summary>

- **Core Idea**: Learn invertible transformations of latent variables using the change-of-variable formula.
- **Likelihood**: Exact and tractable.
- **Sampling**: Fast â€” sample from base distribution and invert.
- **Use Cases**: Density estimation, latent space modeling.
- **Example Models**: RealNVP, Glow
- [Go to Flow Models â†’](flows.qmd)

</details>

<details>
<summary><strong> Energy-Based Models (EBMs)</strong></summary>

- **Core Idea**: Define an energy function over inputs; lower energy = higher probability.
- **Likelihood**: Unnormalized (intractable partition function).
- **Sampling**: Very slow â€” requires MCMC or Langevin dynamics.
- **Use Cases**: Uncertainty modeling, compositional generation.
- **Example Models**: Score-based EBMs
- [Go to EBMs â†’](ebm.qmd)

</details>

<details>
<summary><strong> Diffusion Models</strong></summary>

- **Core Idea**: Learn to reverse a gradual noise process via denoising.
- **Likelihood**: Approximate (via variational bound).
- **Sampling**: Slow â€” requires hundreds of reverse steps.
- **Use Cases**: High-resolution image and audio generation.
- **Example Models**: Stable Diffusion
- [Go to Diffusion Models â†’](diffusion.qmd)

</details>


## Industry Use Cases

<details>
<summary><strong>Banking</strong></summary>

- **Customer Service** â€” Autoregressive Models (e.g., GPT)  
- **Fraud Detection** â€” Energy-Based Models  
- **Operational Efficiency** â€” RAG for document automation and summarization  
- **Business Intelligence** â€” VAE for anomaly detection in transaction patterns 
- **Marketing** â€” GANs for personalized message generation and segmentation  

Includes deployments by JPMorgan, Mastercard, Wells Fargo, and Morgan Stanley.  
[View full Banking use cases â†’](gen-ai-use-cases/banking-use-cases.html)

</details>

<details>
<summary><strong>Healthcare</strong></summary>

- **Clinical Documentation** â€” Autoregressive Models (e.g., GPT-4), RAG (Nuance DAX)  
- **Medical Imaging** â€” Diffusion Models for image enhancement  
- **Diagnostics & Triage** â€” VAEs for uncertainty-aware diagnosis  
- **Patient Education** â€” Conversational AI built on large language models  
- **Drug Discovery** â€” Normalizing Flows for molecular sampling and VAEs for molecule generation  

Highlights work by Nuance (Microsoft), Mayo Clinic, DeepMind, and Insilico Medicine.  
[View full Healthcare use cases â†’](gen-ai-use-cases/healthcare-use-cases.html)

</details>
