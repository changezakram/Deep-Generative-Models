---
title: "Deep Generative Models"
---

## Evolution of AI Capabilities

AI is changing fast, and each stage brings new strengths, limits, and oversight needs. The table below offers a strategic comparison of how Traditional AI, Generative AI, and Agentic AI differ in behavior and best use.


<div style="overflow-x:auto;">

| Capability            | **Traditional AI**<br><small>Rule-driven tools</small> | **Generative AI**<br><small>Smart assistants</small> | **Agentic AI**<br><small>Autonomous actors</small> |
|-----------------------|---------------------------------------------|--------------------------------------------|--------------------------------------------|
| **What it does**      | Follows hard-coded logic and rules          | Understands prompts and generates output   | Plans, reasons, and acts across steps       |
| **How it behaves**    | Rigid, predictable                          | Creative but guided                        | Goal-seeking and adaptive                   |
| **Human involvement** | Fully manual setup and supervision          | Needs context and oversight                | Can operate independently (with guardrails) |
| **Strengths**         | Reliable on structured tasks                | Great at summarizing, drafting, generating | Handles multistep workflows                 |
| **Best used for**     | Repetitive decisions and automation         | Insight generation and copiloting          | Full process orchestration                  |

</div>


## Why Generative AI Matters

Generative AI models can create entirely new content — such as text, images, simulations, or code — by learning from existing data. Unlike traditional AI systems that classify or tag inputs, generative models **produce** new outputs based on patterns they’ve learned.

Since 2022, innovation in this space has accelerated rapidly. Foundation models like ChatGPT, Claude, and Gemini are being deployed across industries — from writing assistance and image generation to fraud detection and synthetic data creation.

> **Business impact:** McKinsey estimates that generative AI could contribute up to **$4.4 trillion in annual global economic value**, with wide-ranging implications for productivity, personalization, and decision support.

At the same time, generative AI introduces new risks. Outputs may appear convincing but be inaccurate, biased, or open to misuse. This is especially critical in high-stakes domains such as banking, healthcare, and law.  
Ensuring responsible use requires strong governance, human oversight, and ethical deployment practices.


## How Generative Models Work

Generative AI models aim to learn the underlying distribution of real-world data, typically denoted as $p_{\text{data}}(x)$. Their goal is to approximate this distribution as closely as possible.


- The distribution learned by the model is denoted as $p_\theta(x)$  
- We generate new data by **sampling** from this learned distribution  
- In practice, generative models are trained to **maximize the expected log-likelihood** of $p_\theta(x)$, or equivalently, **minimize the divergence** between $p_\theta(x)$ and $p_{\text{data}}(x)$


> **Note**:  
> - Models like **Autoregressive models** and **Normalizing Flows** directly maximize log-likelihood.  
> - **VAEs** maximize a variational lower bound (ELBO) on log-likelihood.  
> - **GANs** minimize the **Jensen-Shannon divergence** through adversarial training.  
> - **Diffusion models** and **EBMs** use score matching or other divergence-minimizing techniques.

This site explores how different model families approach this goal — from Variational Autoencoders (VAEs) to Diffusion Models — grounded in theory and real-world use.

## Key Gen AI Model Families

<details>
<summary><strong> Variational Autoencoders (VAEs)</strong></summary>

- **Core Idea**: Encode input to a latent space and reconstruct it while optimizing a lower bound on likelihood (ELBO).
- **Likelihood**: Approximate (variational lower bound).
- **Sampling**: Fast — sample latent vector and decode.
- **Use Cases**: Representation learning, image generation.
- **Example Models**: β-VAE, Conditional VAE
- [Go to VAE write-up →](vae.qmd)

</details>

<details>
<summary><strong> Generative Adversarial Networks (GANs)</strong></summary>

- **Core Idea**: A generator and discriminator compete in a minimax game to produce realistic samples.
- **Likelihood**: None (implicit model).
- **Sampling**: Fast — sample latent vector and pass through generator.
- **Use Cases**: High-quality image generation, style transfer.
- **Example Models**: StyleGAN, CycleGAN
- [Go to GAN write-up →]

</details>

<details>
<summary><strong> Autoregressive Models</strong></summary>

- **Core Idea**: Factor the joint distribution as a product of conditional probabilities.
- **Likelihood**: Exact.
- **Sampling**: Slow — token-by-token generation.
- **Use Cases**: Language modeling, code generation, time series.
- **Example Models**: GPT, PixelRNN
- [Go to Autoregressive Models →]

</details>

<details>
<summary><strong> Normalizing Flows</strong></summary>

- **Core Idea**: Learn invertible transformations of latent variables using the change-of-variable formula.
- **Likelihood**: Exact and tractable.
- **Sampling**: Fast — sample from base distribution and invert.
- **Use Cases**: Density estimation, latent space modeling.
- **Example Models**: RealNVP, Glow
- [Go to Flow Models →](flows.qmd)

</details>

<details>
<summary><strong> Energy-Based Models (EBMs)</strong></summary>

- **Core Idea**: Define an energy function over inputs; lower energy = higher probability.
- **Likelihood**: Unnormalized (intractable partition function).
- **Sampling**: Very slow — requires MCMC or Langevin dynamics.
- **Use Cases**: Uncertainty modeling, compositional generation.
- **Example Models**: Score-based EBMs
- [Go to EBMs →](ebm.qmd)

</details>

<details>
<summary><strong> Diffusion Models</strong></summary>

- **Core Idea**: Learn to reverse a gradual noise process via denoising.
- **Likelihood**: Approximate (via variational bound).
- **Sampling**: Slow — requires hundreds of reverse steps.
- **Use Cases**: High-resolution image and audio generation.
- **Example Models**: Stable Diffusion
- [Go to Diffusion Models →](diffusion.qmd)

</details>



## Use-Case Framing & Prioritization

Before diving into individual use cases, it’s critical to assess *where* and *how* Generative AI can drive real value. Not every idea is equally feasible, safe, or impactful. A structured framing ensures that AI initiatives align with business priorities and responsible innovation.

We recommend evaluating use cases across three key dimensions:

| **Evaluation Dimension**     | **Description**                                                                 |
|-----------------------------|----------------------------------------------------------------------------------|
| **Business Value**           | Impact on revenue, cost, efficiency, risk reduction, or customer experience.    |
| **Feasibility**              | Availability of data, model readiness, integration complexity, compute needs.   |
| **Governance Sensitivity**  | Oversight required for regulatory compliance, explainability, and risk of misuse.|

This prioritization helps teams focus on use cases that are not only promising but also **implementable** and **sustainable**, especially in regulated domains like banking and healthcare.

---

## Industry Use Cases

### Banking

| **Use Case**             | **Model Type**                   | **Example Application**                                               |
|--------------------------|----------------------------------|------------------------------------------------------------------------|
| **Customer Service**     | Autoregressive (e.g., GPT)       | Virtual agents for handling account queries and FAQs                  |
| **Fraud Detection**      | Energy-Based Models              | Real-time anomaly detection in transaction behavior                   |
| **Operational Efficiency** | RAG (Retrieval-Augmented Gen)  | Automating call summaries and back-office document workflows          |
| **Business Intelligence** | VAEs (Variational Autoencoders) | Detecting anomalies in product or branch-level KPIs                   |
| **Marketing & Personalization** | GANs                    | Generating personalized offers based on behavioral segmentation       |

> Includes deployments by JPMorgan, Mastercard, Wells Fargo, and Morgan Stanley. [View more details on Banking use cases →](gen-ai-use-cases/banking-use-cases.html)

---

### Healthcare

| **Use Case**             | **Model Type**                            | **Example Application**                                                   |
|--------------------------|-------------------------------------------|----------------------------------------------------------------------------|
| **Clinical Documentation** | GPT-4, RAG (Nuance DAX)                 | Auto-generating visit summaries and physician notes                        |
| **Medical Imaging**      | Diffusion Models                          | Enhancing and reconstructing radiological images                          |
| **Diagnostics & Triage** | VAEs                                      | Supporting diagnosis with uncertainty-aware modeling                      |
| **Patient Education**    | Conversational AI                         | Explaining test results in accessible, human-like language                |
| **Drug Discovery**       | Normalizing Flows, VAEs                   | Accelerating compound generation and molecular simulation                 |

> Highlights include work by Nuance (Microsoft), Mayo Clinic, DeepMind, and Insilico Medicine. [View more details on Healthcare use cases →](gen-ai-use-cases/healthcare-use-cases.html)


