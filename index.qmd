---
title: "Deep Generative Models"
---

## Autoencoders vs Variational Autoencoders

Traditional autoencoders learn to compress data into a lower-dimensional representation (latent space) and reconstruct it. However, they fall short in several areas:

- They lack **generative capabilities** â€” they cannot sample new data effectively
- The **latent space is unstructured**, offering little control or interpretation
- There is no **probabilistic modeling**, limiting uncertainty estimation

Variational Autoencoders (VAEs) address these limitations by introducing a probabilistic framework. They aim not just to reconstruct data but to learn a structured, continuous, and interpretable latent space useful for generation.

### Probabilistic Framework

More formally, VAEs assume the data is generated by a two-step process:

1. Sample a latent variable $\mathbf{z} \sim \mathcal{N}(0, I)$
2. Generate the observation $\mathbf{x}$ from:
   $$
   p(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mu_\theta(\mathbf{z}), \Sigma_\theta(\mathbf{z}))
   $$
   where $\mu_\theta$ and $\Sigma_\theta$ are neural networks parameterized by $\theta$

### Key Properties

- $\mathbf{z}$ is a **hidden variable**, unobserved during training
- Defines a *mixture of infinitely many Gaussians*
- The marginal likelihood requires integration:
  $$
  p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{z}) \, d\mathbf{z}
  $$

### Computational Challenges

This integral requires integrating over:
- All possible values of $\mathbf{z}$ (often high-dimensional)
- Non-linear transformations through neural networks

**Result:** Exact computation is intractable, motivating variational inference techniques like ELBO (developed next).

## Estimation Techniques

### Naive Monte Carlo Estimation

One natural idea is to approximate the integral using samples from a simple distribution like the uniform distribution:

$$
p(x) \approx \frac{1}{K} \sum_{j=1}^K p_\theta(x, z_j), \quad z_j \sim \text{Uniform}
$$

However, this fails in practice. For most values of $z$, the joint probability $p_\theta(x, z)$ is very low. Only a small region of the latent space contributes significantly to the integral. Since uniform sampling does not concentrate around these regions, the estimator has:

- **High variance** (sparse meaningful samples)
- **Low hit rate** (rarely samples likely $z$ values)

### Importance Sampling

To address this, we use **importance sampling**, introducing a proposal distribution $q(z)$:

$$
p(x) = \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right]
$$

**Key Properties:**
- Unbiased estimator when $\text{supp}(q) \supseteq \text{supp}(p_\theta)$
- Optimal $q(z) \approx p_\theta(z|x)$ (the true posterior)
- More efficient sampling in high-probability regions

#### The Log-Likelihood Challenge

For optimization, we need the log-likelihood, but:

$$
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \neq \mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]
$$

**Jensen's Inequality** reveals the relationship:

$$
\log \mathbb{E}_{q(z)} \left[ \frac{p_\theta(x, z)}{q(z)} \right] \geq \underbrace{\mathbb{E}_{q(z)} \left[ \log \frac{p_\theta(x, z)}{q(z)} \right]}_{\text{ELBO}}
$$

This gives us the **Evidence Lower Bound (ELBO)**, which is lower bounds the true log-likelihood.

