---
title: "Deep Generative Models"
---

## Overview

Generative machine learning models aim to learn the underlying distribution of real-world data, typically denoted as $p_{\text{data}}(x)$. Their goal is to approximate this distribution as closely as possible.


- The distribution learned by the model is denoted as $p_\theta(x)$  
- We generate new data by **sampling** from this learned distribution  
- In practice, generative models are trained to **maximize the expected log-likelihood** of $p_\theta(x)$, or equivalently, **minimize the divergence** between $p_\theta(x)$ and $p_{\text{data}}(x)$


## Evolution of AI Capabilities

Although AI agents are exciting, it‚Äôs important to bring the right tool for the job.

| Capability              | **Traditional AI**<br><small>Rigid but reliable implementer</small> | **Gen AI**<br><small>Supercharged copilot</small> | **Agentic AI**<br><small>Fully autonomous operator</small> |
|-------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|
| **Execution Style**     | Executes predefined workflows<br>Hard to scale  | Understands & generates info<br>Lacks execution | Learns, reasons, and acts<br>Executes workflows |
| **Adaptability**        | ‚ùå Doesn‚Äôt adapt<br>Requires structured inputs   | ‚ö†Ô∏è Context-aware but reactive                   | ‚úÖ Adapts in real-time<br>Minimal oversight      |
| **Cognitive Ability**   | ‚ùå Doesn‚Äôt learn or reason                       | ‚úÖ Smart but needs direction                     | ‚úÖ Makes decisions without predefined rules      |
| **Interaction**         | Rule-based, efficient but inflexible            | Needs human-in-the-loop                         | Fewer human touchpoints                         |
| **Best For**            | <strong>Basic repetitive tasks</strong><br>Structured processes | <strong>Human augmentation</strong><br>Knowledge generation | <strong>End-to-end automation</strong><br>Decision-making & orchestration |

> üìå **Note**:  
> - Models like **Autoregressive models** and **Normalizing Flows** directly maximize log-likelihood.  
> - **VAEs** maximize a variational lower bound (ELBO) on log-likelihood.  
> - **GANs** minimize the **Jensen-Shannon divergence** through adversarial training.  
> - **Diffusion models** and **EBMs** use score matching or other divergence-minimizing techniques.

This site explores how different model families approach this goal ‚Äî from Variational Autoencoders (VAEs) to Diffusion Models ‚Äî grounded in theory and real-world use.

## Key Gen AI Model Families

<details>
<summary><strong> Variational Autoencoders (VAEs)</strong></summary>

- **Core Idea**: Encode input to a latent space and reconstruct it while optimizing a lower bound on likelihood (ELBO).
- **Likelihood**: Approximate (variational lower bound).
- **Sampling**: Fast ‚Äî sample latent vector and decode.
- **Use Cases**: Representation learning, image generation.
- **Example Models**: Œ≤-VAE, Conditional VAE
- [Go to VAE write-up ‚Üí](vae.qmd)

</details>

<details>
<summary><strong> Generative Adversarial Networks (GANs)</strong></summary>

- **Core Idea**: A generator and discriminator compete in a minimax game to produce realistic samples.
- **Likelihood**: None (implicit model).
- **Sampling**: Fast ‚Äî sample latent vector and pass through generator.
- **Use Cases**: High-quality image generation, style transfer.
- **Example Models**: StyleGAN, CycleGAN
- [Go to GAN write-up ‚Üí]

</details>

<details>
<summary><strong> Autoregressive Models</strong></summary>

- **Core Idea**: Factor the joint distribution as a product of conditional probabilities.
- **Likelihood**: Exact.
- **Sampling**: Slow ‚Äî token-by-token generation.
- **Use Cases**: Language modeling, code generation, time series.
- **Example Models**: GPT, PixelRNN
- [Go to Autoregressive Models ‚Üí]

</details>

<details>
<summary><strong> Normalizing Flows</strong></summary>

- **Core Idea**: Learn invertible transformations of latent variables using the change-of-variable formula.
- **Likelihood**: Exact and tractable.
- **Sampling**: Fast ‚Äî sample from base distribution and invert.
- **Use Cases**: Density estimation, latent space modeling.
- **Example Models**: RealNVP, Glow
- [Go to Flow Models ‚Üí](flows.qmd)

</details>

<details>
<summary><strong> Energy-Based Models (EBMs)</strong></summary>

- **Core Idea**: Define an energy function over inputs; lower energy = higher probability.
- **Likelihood**: Unnormalized (intractable partition function).
- **Sampling**: Very slow ‚Äî requires MCMC or Langevin dynamics.
- **Use Cases**: Uncertainty modeling, compositional generation.
- **Example Models**: Score-based EBMs
- [Go to EBMs ‚Üí](ebm.qmd)

</details>

<details>
<summary><strong> Diffusion Models</strong></summary>

- **Core Idea**: Learn to reverse a gradual noise process via denoising.
- **Likelihood**: Approximate (via variational bound).
- **Sampling**: Slow ‚Äî requires hundreds of reverse steps.
- **Use Cases**: High-resolution image and audio generation.
- **Example Models**: Stable Diffusion
- [Go to Diffusion Models ‚Üí](diffusion.qmd)

</details>


## Industry Use Cases

<details>
<summary><strong>Banking</strong></summary>

- **Customer Service** ‚Äî Autoregressive Models (e.g., GPT)  
- **Fraud Detection** ‚Äî Energy-Based Models  
- **Operational Efficiency** ‚Äî RAG for document automation and summarization  
- **Business Intelligence** ‚Äî VAE for anomaly detection in transaction patterns 
- **Marketing** ‚Äî GANs for personalized message generation and segmentation  

Includes deployments by JPMorgan, Mastercard, Wells Fargo, and Morgan Stanley.  
[View full Banking use cases ‚Üí](gen-ai-use-cases/banking-use-cases.html)

</details>

<details>
<summary><strong>Healthcare</strong></summary>

- **Clinical Documentation** ‚Äî Autoregressive Models (e.g., GPT-4), RAG (Nuance DAX)  
- **Medical Imaging** ‚Äî Diffusion Models for image enhancement  
- **Diagnostics & Triage** ‚Äî VAEs for uncertainty-aware diagnosis  
- **Patient Education** ‚Äî Conversational AI built on large language models  
- **Drug Discovery** ‚Äî Normalizing Flows for molecular sampling and VAEs for molecule generation  

Highlights work by Nuance (Microsoft), Mayo Clinic, DeepMind, and Insilico Medicine.  
[View full Healthcare use cases ‚Üí](gen-ai-use-cases/healthcare-use-cases.html)

</details>
