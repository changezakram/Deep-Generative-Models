---
title: "Energy-Based Models: A Flexible Approach to Generative Modeling"
format:
  html:
    toc: true
    code-fold: true
    smooth-scroll: true
    theme: cosmo
    math: mathjax
---

## Introduction

In the world of generative models, techniques like VAEs, GANs, and normalizing flows have each carved out their niche—but all of them come with specific constraints. **Energy-Based Models (EBMs)** offer a powerful alternative that’s **architecturally flexible**, **conceptually elegant**, and **growing in popularity** in modern deep learning research.

---

## Limitations of Mainstream Generative Models

| Model                   | Pros                                    | Cons                                                                 |
|------------------------|-----------------------------------------|----------------------------------------------------------------------|
| **VAEs**               | Probabilistic framework, tractable ELBO | Model architecture restrictions; blurry samples                     |
| **Normalizing Flows**  | Exact likelihood, invertibility          | Restricted to invertible architectures; expensive Jacobian computation |
| **Autoregressive Models** | Exact likelihood                    | Slow sampling; autoregressive dependency limits parallelism         |
| **GANs**               | High-quality samples; flexible           | No likelihood; unstable training; mode collapse                     |

These models attempt to approximate the true data distribution $P_{\text{data}}$ by selecting a model $P_\theta$ from a constrained family, often limited by the need for tractable likelihoods, invertible mappings, or adversarial training stability.

---

## 📐 Understanding the Probability Foundation Behind EBMs

In generative modeling, a valid probability distribution \( p(x) \) must satisfy:

- Non-negativity:  
  \[
  p(x) \geq 0
  \]

- Normalization:  
  \[
  \int p(x)\, dx = 1
  \]

Directly defining a function that satisfies both is difficult, especially when using flexible models like neural networks.

---

### 💡 Why do we introduce \( g(x) \)?

Instead of modeling \( p(x) \) directly, we define a non-negative function \( g(x) \geq 0 \) and turn it into a probability distribution by normalizing:

\[
p(x) = \frac{g(x)}{Z}, \quad \text{where} \quad Z = \int g(x)\, dx
\]

This trick simplifies the problem by separating the two requirements:

- \( g(x) \) ensures non-negativity  
- \( Z \) enforces normalization

---

### 🧠 Intuition

Think of \( g(x) \) as a scoring function:

- Higher \( g(x) \) means more likely
- Dividing by \( Z \) rescales these scores to form a valid probability distribution

This gives us flexibility to use expressive models for \( g(x) \) — for example:

\[
g(x) = \exp(f(x)) \quad \Rightarrow \quad p(x) = \frac{e^{f(x)}}{Z}
\]

---

### 🔗 Connecting to Energy-Based Models

In EBMs, we define:

\[
g_\theta(x) = \exp(f_\theta(x)) \quad \Rightarrow \quad p_\theta(x) = \frac{e^{f_\theta(x)}}{Z(\theta)}
\]

Letting \( E_\theta(x) = -f_\theta(x) \), we get:

\[
p_\theta(x) = \frac{e^{-E_\theta(x)}}{Z(\theta)}
\]

This formulation is powerful because it allows maximum flexibility in defining \( f_\theta(x) \), and only requires that we can compute or approximate the gradients of the energy function.
---

## Enter Energy-Based Models (EBMs)

Energy-Based Models drop the assumption of tractable density. Instead of directly modeling the probability distribution $P(x)$, EBMs define an **energy function** $E_\theta(x)$ that assigns lower energy to more likely data points:

$$
p_\theta(x) = \frac{e^{-E_\theta(x)}}{Z_\theta}
$$

Here, $$Z_\theta = \int e^{-E_\theta(x)} dx$$ is the **partition function**, which normalizes the probability distribution. This implicit formulation gives EBMs the flexibility to use **any neural architecture** for $E_\theta$, without needing invertibility, autoregressive factorization, or adversarial setup.

---


## Key Benefits of EBMs

- **Very flexible model architectures**: No need for invertibility, factorization, or adversarial design.
- **Stable training**: Compared to GANs, EBMs can be more robust.
- **Relatively high sample quality**: Can model multi-modal data well.
- **Flexible composition**: Combine energies for multi-task objectives.

---

## Limitations of EBMs

Despite their elegance, EBMs pose serious challenges:

**Hard Sampling**

- No direct way to draw samples from $p_\theta(x)$
- Requires iterative MCMC methods
- Sampling cost is high and scales poorly in high dimensions

**Hard Likelihood Evaluation and Learning**

- Partition function $Z(\theta)$ is intractable
- Evaluating $\log p_\theta(x)$ is not feasible
- Need to push down energy of incorrect configurations to make learning effective

**No Feature Learning (by default)**

- EBMs do not learn latent features unless explicitly modeled (e.g., RBMs)

---

## Learning and Inference in Energy-Based Models

We optimize the likelihood:

$$
\log p_\theta(x_{\text{train}}) = f_\theta(x_{\text{train}}) - \log Z(\theta)
$$

Gradient:

$$
\nabla_\theta f_\theta(x_{\text{train}}) - \mathbb{E}_{x \sim p_\theta}[\nabla_\theta f_\theta(x)]
$$

Since $$Z(\theta)$$ is intractable, this expectation is approximated using **sampling**.

---

## Contrastive Divergence

Contrastive Divergence approximates the gradient using:

$$
\nabla_\theta f_\theta(x_{\text{train}}) - \nabla_\theta f_\theta(x_{\text{sample}})
$$

Where $$x_{\text{sample}} \sim p_\theta$$ (via MCMC). The goal is to increase $f$ on training data and decrease it elsewhere.

---

## Sampling from Energy-Based Models

We can't sample directly from EBMs, but we can use **MCMC** methods.

### Metropolis-Hastings

1. Propose $$x' = x + \text{noise}$$  
2. Accept:
   - Always if $$f(x') \geq f(x)$$  
   - Otherwise with probability $$\exp(f(x') - f(x))$$  

Converges to $$p_\theta(x)$$ as $$T \to \infty$$, but is slow in high dimensions.

---

### Langevin Dynamics

Uses gradients:

$$
x^{t+1} = x^t + \epsilon \nabla_x f_\theta(x^t) + \sqrt{2\epsilon} z^t, \quad z^t \sim \mathcal{N}(0, I)
$$

Works well when $$\nabla_x f_\theta(x)$$ is tractable (which it is in EBMs).

---

### Challenges of MCMC Sampling

- Convergence is **slow**, especially in high dimensions
- Requires multiple steps **for every training iteration**
- Can dominate training time in contrastive divergence

> Sampling is the key bottleneck in training EBMs. Effective MCMC strategies are critical to making them practical in real-world settings.


## 📚 References

[1] Atcold, Y. (2020). *NYU Deep Learning Spring 2020 – Week 07: Energy-Based Models*. Retrieved from [https://atcold.github.io/NYU-DLSP20/en/week07/07-1/](https://atcold.github.io/NYU-DLSP20/en/week07/07-1/)

[2] LeCun, Y., Hinton, G., & Bengio, Y. (2021). *A Path Towards Autonomous Machine Intelligence*. arXiv. Retrieved from [https://arxiv.org/pdf/2101.03288](https://arxiv.org/pdf/2101.03288)

[3] MIT. (2022). *Energy-Based Models – MIT Class Project*. Retrieved from [https://energy-based-model.github.io/Energy-based-Model-MIT/](https://energy-based-model.github.io/Energy-based-Model-MIT/)

[4] University of Amsterdam. (2021). *Deep Energy Models – UvA DL Notebooks*. Retrieved from [https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html)

[5] MIT. (2022). *Compositional Generation and Inference with Energy-Based Models*. Retrieved from [https://energy-based-model.github.io/compositional-generation-inference/](https://energy-based-model.github.io/compositional-generation-inference/)