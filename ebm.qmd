---
title: "Energy-Based Models"
format: html
---

## Introduction

In the world of generative models, techniques like VAEs, GANs, and normalizing flows have each carved out their niche—but all of them come with specific constraints. **Energy-Based Models (EBMs)** offer a powerful alternative that’s **architecturally flexible**, **conceptually elegant**, and **growing in popularity** in modern deep learning research.

---

## Limitations of Mainstream Generative Models

| Model                   | Pros                                    | Cons                                                                 |
|------------------------|-----------------------------------------|----------------------------------------------------------------------|
| **VAEs**               | Probabilistic framework, tractable ELBO | Model architecture restrictions; blurry samples                     |
| **Normalizing Flows**  | Exact likelihood, invertibility          | Restricted to invertible architectures; expensive Jacobian computation |
| **Autoregressive Models** | Exact likelihood                    | Slow sampling; autoregressive dependency limits parallelism         |
| **GANs**               | High-quality samples; flexible           | No likelihood; unstable training; mode collapse                     |

These models attempt to approximate the true data distribution $P_{\text{data}}$ by selecting a model $P_\theta$ from a constrained family, often limited by the need for tractable likelihoods, invertible mappings, or adversarial training stability.

::: {.callout appearance="simple"}
To understand how Energy-Based Models overcome many of these limitations, we first need to revisit a fundamental question in generative modeling: **what makes a valid probability distribution?**

The next section walks through the mathematical foundation that underpins EBMs — and introduces a clever trick that allows them to sidestep many of the constraints faced by traditional generative models.
:::

---

## Math Review

### Understanding the Probability Foundation Behind EBMs

In generative modeling, a valid probability distribution $p(x)$ must satisfy:

- **Non-negativity**:  
  $$
  p(x) \geq 0
  $$

- **Normalization**:  
  $$
  \int p(x)\, dx = 1
  $$

While it’s easy to define a function that satisfies $p(x) \geq 0$ (e.g., using exponentials), ensuring that it also sums to 1 — i.e., $\int p(x)\, dx = 1$ — is much more difficult, especially for flexible functions like neural networks.

---

### Why do we introduce $g(x)$?

Instead of modeling $p(x)$ directly, we define a non-negative function $g(x) \geq 0$ and turn it into a probability distribution by normalizing:

$$
p_\theta(x) = \frac{g_\theta(x)}{Z(\theta)}, \quad \text{where} \quad Z(\theta) = \int g_\theta(x)\, dx
$$

This trick simplifies the problem by separating the two requirements:

- $g_\theta(x)$ ensures non-negativity  
- $Z(\theta)$ enforces normalization

This normalization constant $Z(\theta)$ is also known as the **partition function**.

---

### Intuition

Think of $g_\theta(x)$ as a scoring function:

- Higher $g_\theta(x)$ means more likely  
- Dividing by $Z(\theta)$ rescales these scores to form a valid probability distribution

---

### From Scores to Probabilities in EBMs

Energy-Based Models follow the same idea we've established earlier: define a **scoring function** $f_\theta(x)$ that assigns high values to likely data points, and then convert these scores into probabilities using an exponential transformation and normalization.  
This allows us to build flexible probabilistic models without needing tractable likelihoods or invertible mappings.

We use an **exponential function** because:

- It guarantees non-negativity: $\exp(f_\theta(x)) \geq 0$
- It allows us to interpret $f_\theta(x)$ as an unnormalized log-probability
- It connects naturally to many well-known distributions (e.g., exponential family, Boltzmann distribution)

We define:

$$
g_\theta(x) = \exp(f_\theta(x)) \quad \Rightarrow \quad p_\theta(x) = \frac{e^{f_\theta(x)}}{Z(\theta)}
$$

To align with the physics intuition that lower energy = higher probability, we define the **energy function** as:

$$
E_\theta(x) = -f_\theta(x)
$$

This gives us the classic EBM form:

$$
p_\theta(x) = \frac{e^{-E_\theta(x)}}{Z(\theta)}
$$

This formulation gives EBMs the freedom to use any differentiable function for $f_\theta(x)$, and only requires that we can compute or approximate its gradients.

--- 

### Applications of EBMs Without Computing $Z(\theta)$

In general, evaluating the full probability $p_\theta(x)$ requires computing the partition function $Z(\theta)$:

$$
p_\theta(x) = \frac{1}{Z(\theta)} \exp(f_\theta(x))
$$

::: {.callout-tip appearance="simple"}
**Key Insight**  
In some applications, we don’t need the exact probability — we only need to compare scores.  
This allows EBMs to be useful even when the partition function is intractable.
:::

When comparing two samples $x$ and $x'$, the ratio:

$$
\frac{p_\theta(x)}{p_\theta(x')} = \exp(f_\theta(x) - f_\theta(x'))
$$

does **not** involve $Z(\theta)$.

This means we can easily compare which input is more likely — a powerful property of EBMs.

**Practical Applications**:

- **Anomaly detection**: Identify inputs with unusually low likelihood.
- **Denoising**: Prefer cleaner versions of corrupted data by comparing likelihoods.

---

### Key Benefits of EBMs

**Very flexible model architectures**  
No need for invertibility, autoregressive factorization, or adversarial design.

**Stable training**  
Compared to GANs, EBMs can be more robust and easier to optimize.

**High sample quality**  
Capable of modeling complex, multi-modal data distributions.

**Flexible composition**  
Energies can be combined to support multi-task objectives or structured learning.

---

### Limitations of EBMs

Despite their strengths, EBMs come with notable challenges:

**Hard Sampling**

- No direct way to draw samples from $p_\theta(x)$  
- Requires iterative MCMC methods (e.g., Langevin dynamics, Metropolis-Hastings)  
- Sampling cost is high and scales poorly in high dimensions

**Hard Likelihood Evaluation and Learning**

- Partition function $Z(\theta)$ is intractable to compute  
- Cannot evaluate $\log p_\theta(x)$ directly  
- Learning requires **pushing down energy of incorrect samples**, not just increasing energy of training data

**No Feature Learning (by default)**

- EBMs do not learn latent representations unless explicitly modeled (e.g., using RBMs or structured latent energy functions)

---

## Training and Inference in EBMs

The training objective for Energy-Based Models (EBMs) is to assign **higher scores** (i.e., lower energy) to correct examples and **lower scores** (higher energy) to incorrect or unlikely ones.

Our goal is to **maximize the (unnormalized) likelihood** of training data:

$$
\text{maximize} \quad \frac{\exp(f_\theta(x_\text{train}))}{Z(\theta)}
$$

This means we want to **increase the numerator** (make the correct sample more likely) and **decrease the denominator** (reduce the overall mass by pushing down other samples).

---

### Why It's Not That Simple

Because the model is not normalized, simply increasing $f_\theta(x_\text{train})$ does **not** guarantee that $x_\text{train}$ becomes more likely **relative to the rest**.

We must also consider the effect on the partition function:

$$
Z(\theta) = \int \exp(f_\theta(x))\, dx
$$

To make $x_\text{train}$ truly more likely, we must **pull up** $f_\theta(x_\text{train})$ while also **pushing down** $f_\theta(x)$ for other (incorrect) samples.

---

### Gradient of the Log-Likelihood

We aim to **maximize** the log-likelihood of training data:

$$
\log p_\theta(x_\text{train}) = f_\theta(x_\text{train}) - \log Z(\theta)
$$

Taking the gradient with respect to \( \theta \):

$$
\nabla_\theta \log p_\theta(x_\text{train}) = \nabla_\theta f_\theta(x_\text{train}) - \nabla_\theta \log Z(\theta)
$$

So to compute this, we need to understand the gradient of the log partition function.

---

### Gradient of the Partition Function

We start with:

$$
Z(\theta) = \int \exp(f_\theta(x))\, dx
$$

Then use the chain rule:

$$
\nabla_\theta \log Z(\theta) 
= \frac{1}{Z(\theta)} \nabla_\theta Z(\theta) 
= \frac{1}{Z(\theta)} \int \exp(f_\theta(x)) \nabla_\theta f_\theta(x)\, dx
$$

This is a **weighted average** of gradients \( \nabla_\theta f_\theta(x) \), where each weight is proportional to how likely \( x \) is under the model — specifically weighted by \( \exp(f_\theta(x)) \).

Since:

$$
p_\theta(x) = \frac{\exp(f_\theta(x))}{Z(\theta)}
$$

we get the simplified form:

$$
\nabla_\theta \log Z(\theta) = \mathbb{E}_{x \sim p_\theta}[\nabla_\theta f_\theta(x)]
$$

Substituting this into the earlier log-likelihood gradient:

$$
\nabla_\theta \log p_\theta(x_\text{train}) = \nabla_\theta f_\theta(x_\text{train}) - \mathbb{E}_{x \sim p_\theta}[\nabla_\theta f_\theta(x)]
$$

---

### Intuition

- **Pull up the training example**: The first term increases the score of the correct data point.
- **Push down other samples**: The second term is an average over everything the model currently thinks is likely.
- **Net effect**: The model learns by increasing the score of the correct data and decreasing the score of points it mistakenly believes are likely.

---

### Contrastive Divergence

In the previous section, we saw that the gradient of the log-likelihood is:

$$
\nabla_\theta \log p_\theta(x_{\text{train}}) = \nabla_\theta f_\theta(x_{\text{train}}) - \mathbb{E}_{x \sim p_\theta} \left[ \nabla_\theta f_\theta(x) \right]
$$

The first term pulls up the score of the training example.  
The second term pushes down the average score of the points that the model believes are likely.

---

### Challenge: The Expectation is Intractable

The expectation \( \mathbb{E}_{x \sim p_\theta} [ \nabla_\theta f_\theta(x) ] \) involves **sampling from the model distribution** \( p_\theta(x) \), which requires computing or approximating the partition function \( Z(\theta) \). This is computationally expensive and often infeasible in practice.

---

### Solution: Use a Monte Carlo Approximation

We approximate the expectation using a **Monte Carlo sample** drawn from the model:

$$
\mathbb{E}_{x \sim p_\theta} \left[ \nabla_\theta f_\theta(x) \right] \approx \nabla_\theta f_\theta(x_{\text{sample}})
$$

Substituting this into the log-likelihood gradient:

$$
\nabla_\theta \log p_\theta(x_{\text{train}}) \approx \nabla_\theta f_\theta(x_{\text{train}}) - \nabla_\theta f_\theta(x_{\text{sample}})
= \nabla_\theta \left( f_\theta(x_{\text{train}}) - f_\theta(x_{\text{sample}}) \right)
$$

---

### Contrastive Divergence Algorithm

**1.** Sample \( x_{\text{sample}} \sim p_\theta \) (typically via MCMC)  
**2.** Take a gradient step on:

$$
\nabla_\theta \left( f_\theta(x_{\text{train}}) - f_\theta(x_{\text{sample}}) \right)
$$

This update encourages the model to **increase** the score of the training data while **decreasing** the score of the samples it currently believes are likely.

---

## 📚 References

[1] Atcold, Y. (2020). *NYU Deep Learning Spring 2020 – Week 07: Energy-Based Models*. Retrieved from [https://atcold.github.io/NYU-DLSP20/en/week07/07-1/](https://atcold.github.io/NYU-DLSP20/en/week07/07-1/)

[2] LeCun, Y., Hinton, G., & Bengio, Y. (2021). *A Path Towards Autonomous Machine Intelligence*. arXiv. Retrieved from [https://arxiv.org/pdf/2101.03288](https://arxiv.org/pdf/2101.03288)

[3] MIT. (2022). *Energy-Based Models – MIT Class Project*. Retrieved from [https://energy-based-model.github.io/Energy-based-Model-MIT/](https://energy-based-model.github.io/Energy-based-Model-MIT/)

[4] University of Amsterdam. (2021). *Deep Energy Models – UvA DL Notebooks*. Retrieved from [https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html)

[5] MIT. (2022). *Compositional Generation and Inference with Energy-Based Models*. Retrieved from [https://energy-based-model.github.io/compositional-generation-inference/](https://energy-based-model.github.io/compositional-generation-inference/)