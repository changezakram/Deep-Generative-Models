---
title: "Diffusion Models"  
---

## Introduction 

Diffusion models are a class of generative models that create data (like images) by reversing a gradual noising process. First, they take real data and add random noise to it step by step, until it becomes pure noise. Then, they train a neural network to undo this process, transforming noise back into realistic data. This method has enabled state-of-the-art results in image generation, powering tools like DALL-E 2 and Stable Diffusion. Unlike older methods, diffusion models are stable to train and can produce highly detailed, diverse outputs.

## Math Review

### What is the Forward Diffusion Process

The forward diffusion process gradually turns a data sample (such as an image) into pure noise by adding a little bit of random noise at each step. This process is a Markov chain, meaning each step depends only on the previous one.

#### Start with a Data Sample

Begin with a data point $x_0$, sampled from dataset (such as a real image). The goal is to slowly corrupt $x_0$ by adding noise over many steps, until it becomes pure noise.  
We’ll later see that it’s also possible to sample $x_t$ directly from $x_0$, without simulating every step.

#### Add Noise Recursively

At each time step $t$, the process is defined as:
$$
q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) I\right)
$$

Where:

- $\alpha_t = 1 - \beta_t$, with $\beta_t$ a small positive number controlling the noise level at step $t$  
- $I$ is the identity matrix, so noise is added independently to each component.

::: {.callout-note appearance="simple"}
**Intuition:** Each step shrinks the previous value a bit and adds some Gaussian noise. As the process repeats, the sample becomes more and more like random noise.
:::

#### The Markov Chain

The full sequence is:

$$
x_0 \rightarrow x_1 \rightarrow x_2 \rightarrow \ldots \rightarrow x_T
$$

The joint probability of the sequence is:

$$
q(x_{1:T} \mid x_0) = \prod_{t=1}^{T} q(x_t \mid x_{t-1})
$$

::: {.callout-note}
**Shortcut Insight:** While the forward process defines a full Markov chain from $x_0$ to $x_T$, we’ll soon see that it’s also possible to sample any $x_t$ directly from $x_0$ using a closed-form Gaussian — without simulating each intermediate step.
:::


The joint probability of the sequence is:
$$
q(x_{1:T} \mid x_0) = \prod_{t=1}^T q(x_t \mid x_{t-1})
$$

This means we can sample the whole chain by repeatedly applying the noise step.

#### Deriving the Marginal Distribution $q(x_t \mid x_0)$

\textbf{Key Question:} How do we get the formula that lets us sample $x_t$ directly from $x_0$ (without simulating all the intermediate steps)?

\textbf{a. Unrolling the Recursion}

Let’s see how $x_t$ is built up from $x_0$:

For $t = 1$:
$$
x_1 = \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_1, \qquad \epsilon_1 \sim \mathcal{N}(0, I)
$$

For $t = 2$:
$$
x_2 = \sqrt{\alpha_2} x_1 + \sqrt{1 - \alpha_2} \epsilon_2
$$
Substitute $x_1$:
$$
x_2 = \sqrt{\alpha_2} \left( \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_1 \right) + \sqrt{1 - \alpha_2} \epsilon_2
$$
$$
= \sqrt{\alpha_2 \alpha_1} x_0 + \sqrt{\alpha_2 (1 - \alpha_1)} \epsilon_1 + \sqrt{1 - \alpha_2} \epsilon_2
$$

For general $t$, recursively expanding gives:
$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sum_{i=1}^t \left( \sqrt{ \left( \prod_{j=i+1}^t \alpha_j \right) (1 - \alpha_i) } \, \epsilon_i \right)
$$
where $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$.

Each $\epsilon_i$ is independent Gaussian noise. The sum of independent Gaussians (each scaled by a constant) is still a Gaussian, with variance equal to the sum of the variances:
$$
\text{Total variance} = \sum_{i=1}^t \left( \prod_{j=i+1}^t \alpha_j \right) (1 - \alpha_i)
$$
This sum simplifies to:
$$
1 - \bar{\alpha}_t
$$

This can be proved by induction or by telescoping the sum.

All the little bits of noise added at each step combine into one big Gaussian noise term, with variance $1 - \bar{\alpha}_t$.

#### The Final Marginal Distribution

So, we can sample $x_t$ directly from $x_0$ using:
$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I)
$$

This lets us sample $x_t$ directly from $x_0$, without recursively computing all previous steps $x_1, x_2, \dots, x_{t-1}$.

This means:
$$
q(x_t \mid x_0) = \mathcal{N}\left(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I\right)
$$

After $t$ steps, sample is a faded version of the original plus one big chunk of noise.

#### What Happens as $t$ Gets Large?

As $t$ increases, $\bar{\alpha}_t$ shrinks toward zero. Eventually, $x_t$ becomes pure noise:
$$
x_T \sim \mathcal{N}(0, I)
$$

#### Recap: Forward Diffusion Steps

| **Step** | **Formula** | **Explanation** |
|---------|-------------|-----------------|
| 1 | $x_0$ | Original data sample |
| 2 | $q(x_t \mid x_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} x_{t-1}, (1-\alpha_t) I)$ | Add noise at each step |
| 3 | $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon$ | Directly sample $x_t$ from $x_0$ using noise $\epsilon$ |
| 4 | $q(x_t \mid x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t) I)$ | Marginal distribution at step $t$ |
| 5 | $x_T \sim \mathcal{N}(0, I)$ | After many steps, pure noise |


#### Key Takeaways

- The forward diffusion process is just repeatedly adding noise to your data.
- Thanks to properties of Gaussian noise, you can describe the result as the original data scaled down plus one cumulative chunk of Gaussian noise.   
- After enough steps, the data becomes indistinguishable from random noise.

