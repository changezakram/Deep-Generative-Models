---
title: "Diffusion Models"  
---

## Introduction

Diffusion models are a powerful class of generative models that learn to create data—such as images—by reversing a gradual noising process. During training, real data is progressively corrupted by adding small amounts of Gaussian noise over many steps until it becomes nearly indistinguishable from pure noise. A neural network is then trained to learn the reverse process: transforming noise back into realistic samples, one step at a time.

This approach has enabled state-of-the-art results in image generation, powering tools like **DALL·E 2**, **Imagen**, and **Stable Diffusion**. One of the key advantages of diffusion models lies in their training stability and output quality, especially when compared to earlier generative approaches:

- **GANs** generate sharp images but rely on adversarial training, which can be unstable and prone to mode collapse.
- **VAEs** are more stable but often produce blurry outputs due to their reliance on Gaussian assumptions and variational approximations.
- **Normalizing Flows** provide exact log-likelihoods and stable training but require invertible architectures, which limit model expressiveness.
- **Diffusion models** avoid adversarial dynamics and use a simple denoising objective. This makes them easier to train and capable of producing highly detailed and diverse samples.

This combination of **theoretical simplicity**, **training robustness**, and **high-quality outputs** has made diffusion models one of the most effective generative modeling techniques in use today.


## Math Review

### Forward Diffusion Process

The forward diffusion process gradually turns a data sample (such as an image) into pure noise by adding a little bit of random noise at each step. This process is a Markov chain, meaning each step depends only on the previous one.

#### Start with a Data Sample

Begin with a data point $x_0$, sampled from dataset (such as a real image). The goal is to slowly corrupt $x_0$ by adding noise over many steps, until it becomes indistinguishable from random Gaussian noise.  
We’ll later see that it’s also possible to sample $x_t$ directly from $x_0$, without simulating every step.

#### Add Noise Recursively

At each time step $t$, the process is defined as:
$$
q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) I\right)
$$

Where:

- $\alpha_t = 1 - \beta_t$, where $\beta_t$ a small positive number controlling the noise level at step $t$, while $\alpha_t$ emphasizes the **amount of original signal retained**.
- $I$ is the identity matrix, so noise is added independently to each component.


::: {.callout-note appearance="simple"}
**Intuition:** At each step, we shrink the signal and add new Gaussian noise. Over many steps, the image becomes blurrier and more like random noise.
:::

> **Why keep $\beta_t$ small?**  
> Keeping $\beta_t$ small ensures that noise is added gradually. This allows the model to retain structure across steps and converge slowly to pure noise. Large values of $\beta_t$ would destroy the signal too quickly, making it harder for the reverse model to reconstruct the data. The design of the forward process balances signal decay (via $\sqrt{\alpha_t}$) and noise growth (via $\sqrt{1 - \alpha_t}$) to ensure a smooth, learnable transition.


#### The Markov Chain

The full sequence is:

$$
x_0 \rightarrow x_1 \rightarrow x_2 \rightarrow \ldots \rightarrow x_T
$$

The joint probability of the sequence is:

$$
q(x_{1:T} \mid x_0) = \prod_{t=1}^{T} q(x_t \mid x_{t-1})
$$

This means we can sample the whole chain by repeatedly applying the noise step.

::: {.callout-note appearance="simple"}
**Insight:** While the forward process defines a full Markov chain from $x_0$ to $x_T$, we’ll soon see that it’s also possible to sample any $x_t$ directly from $x_0$ using a closed-form Gaussian — without simulating each intermediate step.
:::

#### Deriving the Marginal Distribution $q(x_t \mid x_0)$

\textbf{Key Question:} How do we get the formula that lets us sample $x_t$ directly from $x_0$ (without simulating all the intermediate steps)?

\textbf{a. Unrolling the Recursion}

Let’s see how $x_t$ is built up from $x_0$:

For $t = 1$:
$$
x_1 = \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_1, \qquad \epsilon_1 \sim \mathcal{N}(0, I)
$$

For $t = 2$:
$$
x_2 = \sqrt{\alpha_2} x_1 + \sqrt{1 - \alpha_2} \epsilon_2
$$
Substitute $x_1$:
$$
x_2 = \sqrt{\alpha_2} \left( \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_1 \right) + \sqrt{1 - \alpha_2} \epsilon_2
$$
$$
= \sqrt{\alpha_2 \alpha_1} x_0 + \sqrt{\alpha_2 (1 - \alpha_1)} \epsilon_1 + \sqrt{1 - \alpha_2} \epsilon_2
$$

For general $t$, recursively expanding gives:
$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sum_{i=1}^t \left( \sqrt{ \left( \prod_{j=i+1}^t \alpha_j \right) (1 - \alpha_i) } \, \epsilon_i \right)
$$
where $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$.

Each $\epsilon_i$ is independent Gaussian noise. The sum of independent Gaussians (each scaled by a constant) is still a Gaussian, with variance equal to the sum of the variances:
$$
\text{Total variance} = \sum_{i=1}^t \left( \prod_{j=i+1}^t \alpha_j \right) (1 - \alpha_i)
$$
This sum simplifies to:
$$
1 - \bar{\alpha}_t
$$

This can be proved by induction or by telescoping the sum.

All the little bits of noise added at each step combine into one big Gaussian noise term, with variance $1 - \bar{\alpha}_t$.

#### The Final Marginal Distribution

So, we can sample $x_t$ directly from $x_0$ using:
$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I)
$$

This lets us sample $x_t$ directly from $x_0$, without recursively computing all previous steps $x_1, x_2, \dots, x_{t-1}$.

This means:
$$
q(x_t \mid x_0) = \mathcal{N}\left(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I\right)
$$

As $t$ increases, $\bar{\alpha}_t$ shrinks toward zero. Eventually, $x_t$ becomes pure noise:

$$
x_T \sim \mathcal{N}(0, I)
$$

#### Recap: Forward Diffusion Steps

| **Step** | **Formula** | **Explanation** |
|---------|-------------|-----------------|
| 1 | $x_0$ | Original data sample |
| 2 | $q(x_t \mid x_{t-1}) = \mathcal{N}(\sqrt{\alpha_t} x_{t-1}, (1-\alpha_t) I)$ | Add noise at each step |
| 3 | $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon$ | Directly sample $x_t$ from $x_0$ using noise $\epsilon$ |
| 4 | $q(x_t \mid x_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t) I)$ | Marginal distribution at step $t$ |
| 5 | $x_T \sim \mathcal{N}(0, I)$ | After many steps, pure noise |


#### Key Takeaways

- The forward diffusion process is just repeatedly adding noise to your data.
- Thanks to properties of Gaussian noise, you can describe the result as the original data scaled down plus one cumulative chunk of Gaussian noise.   
- After enough steps, the data becomes indistinguishable from random noise.

---



### Reverse Diffusion Process

Let’s break down the reverse diffusion process step by step. This is the **generative phase** of diffusion models, where we learn to turn pure noise back into data. For clarity, we’ll use the same notation as in the forward process:

- **Forward process**: Gradually adds noise to data via $q(x_t \mid x_{t-1})$
- **Reverse process**: Gradually removes noise via $p_\theta(x_{t-1} \mid x_t)$, learned by a neural network


**The Goal of the Reverse Process**

**Objective**: Given a noisy sample $x_t$, we want to estimate the conditional distribution $q(x_{t-1} \mid x_t)$. However, this is **intractable** because it would require knowing the true data distribution.

Instead, we train a neural network to approximate it:
$$
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

Here, $\mu_\theta(x_t, t)$ is the predicted mean and $\Sigma_\theta(x_t, t)$ is the predicted covariance (often diagonal) of the reverse Gaussian distribution. 

In practice, many diffusion models do not directly predict $\mu_\theta$ or $x_0$, but instead predict the noise $\epsilon$ added in the forward process. This makes the objective simpler and more effective, as we'll see in the next section.


**Key Insight from the Forward Process**

If the noise added in the forward process is small (i.e., $\beta_t \ll 1$), then the reverse conditional $q(x_{t-1} \mid x_t)$ is also Gaussian:
$$
q(x_{t-1} \mid x_t) \approx \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t), \tilde{\beta}_t I)
$$

This approximation works because the forward process adds Gaussian noise in small increments at each step. The Markov chain formed by these small Gaussian transitions ensures that local conditionals (like $q(x_{t-1} \mid x_t)$) remain Gaussian under mild assumptions.


::: {.callout-note title="Glossary of Symbols"}
- **$\alpha_t$**: Variance-preserving noise coefficient at step $t$
- **$\bar{\alpha}_t$**: Cumulative product of $\alpha_t$, i.e., $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$
- **$\beta_t$**: Variance of the noise added at step $t$, typically $\beta_t = 1 - \alpha_t$
- **$x_0$**: Original clean data sample (e.g., image)
- **$x_t$**: Noisy version of $x_0$ at timestep $t$
- **$\epsilon$**: Standard Gaussian noise sampled from $\mathcal{N}(0, I)$
- **$\tilde{\mu}_t$**: Mean of the reverse process distribution at time $t$
- **$\tilde{\beta}_t$**: Variance of the reverse process distribution at time $t$
:::

#### Deriving $q(x_{t-1} \mid x_t, x_0)$ Using Bayes’ Rule

We can’t directly evaluate $q(x_{t-1} \mid x_t)$, but we can derive the **posterior** $q(x_{t-1} \mid x_t, x_0)$ using Bayes’ rule:

$$
q(x_{t-1} \mid x_t, x_0) = \frac{q(x_t \mid x_{t-1}, x_0) \cdot q(x_{t-1} \mid x_0)}{q(x_t \mid x_0)}
$$

From the forward process, we know:

- $q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1},\, \beta_t I)$  
- $q(x_{t-1} \mid x_0) = \mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} x_0,\, (1 - \bar{\alpha}_{t-1}) I)$  
- $q(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0,\, (1 - \bar{\alpha}_t) I)$

To derive a usable form of the posterior, we substitute the **Gaussian densities** into Bayes’ rule. The multivariate normal density is:

$$
\mathcal{N}(x \mid \mu, \Sigma) \propto \exp\left( -\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu) \right)
$$

Since all covariances here are multiples of the identity matrix, $\Sigma = \sigma^2 I$, the formula simplifies to:

$$
\mathcal{N}(x \mid \mu, \sigma^2 I) \propto \exp\left( -\frac{1}{2\sigma^2} \|x - \mu\|^2 \right)
$$

::: {.callout-note title="Understanding the squared norm"}
The expression $\|x - \mu\|^2$ is the squared distance between two vectors. In 1D, it’s just $(x - \mu)^2$, but in higher dimensions, it becomes:

$$
\|x - \mu\|^2 = \sum_{i=1}^d (x_i - \mu_i)^2
$$

This term appears in the exponent of the Gaussian and represents how far the sample is from the center (mean), scaled by the variance.
:::

Applying this to the forward process terms:

- $q(x_t \mid x_{t-1}) \propto \exp\left( -\frac{1}{2\beta_t} \| x_t - \sqrt{\alpha_t} x_{t-1} \|^2 \right)$  
- $q(x_{t-1} \mid x_0) \propto \exp\left( -\frac{1}{2(1 - \bar{\alpha}_{t-1})} \| x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0 \|^2 \right)$

We can ignore $q(x_t \mid x_0)$ in the denominator, since it is independent of $x_{t-1}$ and will be absorbed into a proportionality constant.

Putting these together:

$$
q(x_{t-1} \mid x_t, x_0) \propto \exp\left(
-\frac{1}{2} \left[
\frac{ \|x_t - \sqrt{\alpha_t} x_{t-1} \|^2 }{\beta_t} +
\frac{ \| x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0 \|^2 }{1 - \bar{\alpha}_{t-1}}
\right]
\right)
$$


::: {.callout-note title="Why does the product of Gaussians give another Gaussian?"}
When we multiply two Gaussian distributions over the same variable, the result is also a Gaussian.

Here, we are multiplying two Gaussians in $x_{t-1}$:  
- One centered at $\sqrt{\alpha_t} x_t$  
- One centered at $\sqrt{\bar{\alpha}_{t-1}} x_0$

The product is another Gaussian in $x_{t-1}$, with a new mean that is a **weighted average** of both.  
We’ll derive this explicitly by completing the square in the exponent.
:::

> Although we won’t use this posterior directly during sampling, this closed-form expression is essential for defining the ELBO used in training. It gives us a precise target that the reverse model attempts to approximate.

We now complete the square to put the expression into standard Gaussian form.

---

#### Complete the square

To simplify the exponent into the standard form of a Gaussian, we apply the **general formula for completing the square**:

$$
a x^2 - 2 b x = a \left( x - \frac{b}{a} \right)^2 - \frac{b^2}{a}
$$

This identity rewrites a quadratic expression as a **perfect square**, which is essential for expressing a Gaussian in the form:

$$
\exp\left( -\frac{1}{2 \sigma^2} \| x - \mu \|^2 \right)
$$


**Expand the expression**

From earlier, we arrived at this expression for the exponent of the posterior:

$$
-\frac{1}{2} \left[
\frac{(x_t - \sqrt{\alpha_t} x_{t-1})^2}{\beta_t} +
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0)^2}{1 - \bar{\alpha}_{t-1}}
\right]
$$

We expand both terms:

**First term:**

$$
\frac{(x_t - \sqrt{\alpha_t} x_{t-1})^2}{\beta_t}
= \frac{x_t^2 - 2 \sqrt{\alpha_t} x_t x_{t-1} + \alpha_t x_{t-1}^2}{\beta_t}
$$

**Second term:**

$$
\frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0)^2}{1 - \bar{\alpha}_{t-1}}
= \frac{x_{t-1}^2 - 2 \sqrt{\bar{\alpha}_{t-1}} x_{t-1} x_0 + \bar{\alpha}_{t-1} x_0^2}{1 - \bar{\alpha}_{t-1}}
$$


**Group like terms**

Now we collect all the terms involving $x_{t-1}$:

- **Coefficient of $x_{t-1}$:**

$$
a = \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}
$$

- **Coefficient of $x_{t-1}$:**

The full linear term is:

$$
-2 \left(
\frac{ \sqrt{\alpha_t} x_t }{ \beta_t } + \frac{ \sqrt{\bar{\alpha}_{t-1}} x_0 }{ 1 - \bar{\alpha}_{t-1} }
\right)
$$

So we define:

$$
b = \frac{ \sqrt{\alpha_t} x_t }{ \beta_t } + \frac{ \sqrt{\bar{\alpha}_{t-1}} x_0 }{ 1 - \bar{\alpha}_{t-1} }
$$

Remaining terms (like $x_t^2$ and $x_0^2$) are independent of $x_{t-1}$ and can be absorbed into a constant.


Using the identity:

$$
a x^2 - 2 b x = a \left( x - \frac{b}{a} \right)^2 - \frac{b^2}{a}
$$

We rewrite the exponent as:

$$
-\frac{1}{2} \left[
a \left( x_{t-1} - \frac{b}{a} \right)^2 - \frac{b^2}{a}
\right]
$$

The constant term $\frac{b^2}{a}$ can be ignored under proportionality, giving:

$$
q(x_{t-1} \mid x_t, x_0) \propto \exp\left(
- \frac{1}{2 \tilde{\beta}_t} \| x_{t-1} - \tilde{\mu}_t \|^2
\right)
$$


::: {.callout-note title="How does this become a Gaussian?"}
Once we complete the square, the exponent becomes:

$$
-\frac{1}{2} a \left( x_{t-1} - \frac{b}{a} \right)^2
$$

This matches the standard form of a Gaussian:

$$
\exp\left( -\frac{1}{2\sigma^2} \|x - \mu\|^2 \right)
$$

So we identify:

- Mean: $\tilde{\mu}_t = \frac{b}{a}$
- Variance: $\tilde{\beta}_t = \frac{1}{a}$

This gives us the compact expression:

$$
q(x_{t-1} \mid x_t, x_0) \propto \exp\left( -\frac{1}{2 \tilde{\beta}_t} \| x_{t-1} - \tilde{\mu}_t \|^2 \right)
$$
:::

#### Final expressions

Now we can directly read off the mean and variance of the Gaussian:

**Mean:**

$$
\tilde{\mu}_t =
\frac{
\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t +
\sqrt{\bar{\alpha}_{t-1}} \beta_t x_0
}{
1 - \bar{\alpha}_t
}
$$

**Variance:**

$$
\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t
$$

So the full form of the posterior is:

$$
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(x_{t-1};\, \tilde{\mu}_t,\, \tilde{\beta}_t I)
$$

We’ve now expressed the posterior $q(x_{t-1} \mid x_t, x_0)$ in standard Gaussian form, setting up the core training objective.

---


#### Parameterizing the Reverse Process

During training, we can compute this posterior exactly since $x_0$ is known. But at sampling time, we don’t know $x_0$, so we must express everything in terms of $x_t$ and predicted noise $\epsilon$.

Since we don’t know $x_0$ during the reverse process, we algebraically express it in terms of $x_t$ and $\epsilon$ using the forward noising equation:

$$
x_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \, \epsilon}{\sqrt{\bar{\alpha}_t}}
$$

Substituting this into $\tilde{\mu}_t$, we get:

$$
\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \, \epsilon \right)
$$

where $\epsilon$ is the noise added to $x_0$ to get $x_t$.


#### Training the Neural Network

The reverse process is implemented as a neural network (often a U-Net), which is trained to **predict the noise** $\epsilon$ from $x_t$ at each step. The forward process is fixed and non-learned.

Let $\epsilon_\theta(x_t, t)$ be the network's prediction. Then the training objective is:
$$
\mathcal{L}_{\text{simple}} = \mathbb{E}_{t, x_0, \epsilon \sim \mathcal{N}(0, I)} 
\left[ \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2 \right]
$$

::: {.callout-note appearance="simple"}
**Intuition:** The model learns to "denoise" each $x_t$ by predicting the noise that was added to create it.
:::


#### Sampling (Generating New Data)

To generate a sample:

1. Sample noise: $x_T \sim \mathcal{N}(0, I)$  
2. For each $t = T, T-1, ..., 1$:
   - Predict noise: $\epsilon_\theta(x_t, t)$
   - Compute $x_{t-1}$ as:
$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t)\right) + \sigma_t z
$$
Where:
- $z \sim \mathcal{N}(0, I)$  
- $\sigma_t^2 = \beta_t$


#### Simplifications in DDPM

In the **Denoising Diffusion Probabilistic Models (DDPM)** paper:

- The variance is fixed: $\Sigma_\theta = \beta_t I$
- The stochastic term $z$ is removed for **deterministic** sampling

This simplifies the sampling equation to:
$$
x_{t-1} = \mu_\theta(x_t, t)
$$


#### In Summary

- The forward process is a fixed schedule of adding Gaussian noise.
- The reverse process is **learned** by a neural network that predicts noise $\epsilon$ at each step.
- Sampling starts from pure noise and applies the learned denoising steps iteratively.

---


### Learning
**What is the Goal?**
The ultimate goal in diffusion models is to train the neural network so that it can reverse the noising process. In other words, we want the network to learn how to turn random noise back into realistic data (like images).
But how do we actually train the network? We need a loss function—a way to measure how good or bad the network’s predictions are, so we can improve it.


#### Detailed Explanation of the ELBO in Diffusion Models

Let’s break down the **Evidence Lower Bound (ELBO)** used in diffusion models, step by step. This is the core mathematical foundation for training diffusion models, and understanding it will clarify how the neural network learns to reverse the noising process.


##### What is the ELBO?

The ELBO is a **lower bound** on the log-likelihood of the data. Maximizing the ELBO is equivalent to maximizing the likelihood that the model can generate the training data. For diffusion models, the ELBO ensures that the reverse process (denoising) aligns with the forward process (noising).


#### Deriving the ELBO for Diffusion Models

**Goal:**  
We want to maximize the log-likelihood of the data:

$$
\log p_\theta(x_0)
$$

where $x_0$ is a clean data sample (e.g., an image).

**Problem:**  
Computing $\log p_\theta(x_0)$ directly is **intractable** because it involves integrating over all possible noisy intermediate states $x_{1:T}$.

**Solution:**  
Use **Jensen’s Inequality** to derive a lower bound (the ELBO) that we can optimize instead.


##### Full Derivation (Step-by-Step)

**Step 1: Start with the log-likelihood**

$$
\log p_\theta(x_0) = \log \int p_\theta(x_{0:T}) \, dx_{1:T}
$$

**Step 2: Introduce the forward process $q(x_{1:T} \mid x_0)$**

Multiply and divide by the fixed forward process:

$$
\log p_\theta(x_0) = \log \int \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} q(x_{1:T} \mid x_0) \, dx_{1:T}
$$

**Step 3: Rewrite as an expectation**

$$
\log p_\theta(x_0) = \log \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} \right]
$$

**Step 4: Apply Jensen’s Inequality**

$$
\log p_\theta(x_0) \geq \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \log \frac{p_\theta(x_{0:T})}{q(x_{1:T} \mid x_0)} \right] = \text{ELBO}
$$

**Step 5: Expand $p_\theta(x_{0:T})$ and $q(x_{1:T} \mid x_0)$**

Forward process:
$$
q(x_{1:T} \mid x_0) = \prod_{t=1}^T q(x_t \mid x_{t-1})
$$

Reverse process:
$$
p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} \mid x_t)
$$

Substitute into the ELBO:

$$
\text{ELBO} = \mathbb{E}_{q(x_{1:T} \mid x_0)} \left[ \log p(x_T) + \sum_{t=1}^T \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})} \right]
$$

**Step 6: Decompose the ELBO**

$$
\text{ELBO} = \mathbb{E}_{q} \left[ \log p(x_T) \right] + \sum_{t=2}^T \mathbb{E}_q \left[ \log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})} \right] + \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) \right]
$$


##### Interpreting Each Term

**Reconstruction Term:**  
- $\mathbb{E}_q[\log p_\theta(x_0 \mid x_1)]$ measures how well the model can reconstruct $x_0$ from the first noisy sample $x_1$.

**Prior Matching Term:**  
- $\mathbb{E}_q[\log p(x_T)]$ encourages the final state $x_T$ to match the prior $\mathcal{N}(0, I)$.

**Consistency Terms:**  
- $\sum \mathbb{E}_q[\log \frac{p_\theta(x_{t-1} \mid x_t)}{q(x_t \mid x_{t-1})}]$ ensures each denoising step approximates the forward noising step.


##### Practical Training Simplification (DDPM)

In DDPM:

- **$p(x_T) = \mathcal{N}(0, I)$** is fixed and known.  
- The reconstruction term is small and often ignored.  
- The KL terms are approximated via **noise prediction**:

$$
\mathcal{L}_{\text{simple}} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
$$

Where:
- $\epsilon$ is the actual noise  
- $\epsilon_\theta$ is the predicted noise by the neural network

**Why?** If the network can predict the noise $\epsilon$, it can denoise $x_t$ and reverse the diffusion.


##### Connection to VAEs

| Aspect              | VAEs                                   | Diffusion Models                                 |
|---------------------|----------------------------------------|--------------------------------------------------|
| Forward process     | Learned encoder $q_\phi(z \mid x)$      | Fixed noising process $q(x_t \mid x_{t-1})$      |
| Reverse process     | Learned decoder $p_\theta(x \mid z)$     | Learned denoising network $p_\theta(x_{t-1} \mid x_t)$ |
| Training objective  | Optimize ELBO over latent variables     | Optimize ELBO via noise prediction loss          |

---

##### Takeaways

- The ELBO is a tractable lower bound on $\log p_\theta(x_0)$.  
- It aligns the reverse (learned) process with the forward (fixed) noising process.  
- In practice, training reduces to minimizing the difference between predicted and true noise.

---











## Back to the Diffusion (Detailed Explanation)

### Recap: What Is Diffusion in This Context?

Diffusion models work by gradually adding noise to data (like images) in small steps until the data becomes nearly pure noise. This is called the **forward diffusion process**. The challenge lies in reversing this process: starting from noise and recovering realistic data. This is known as the **reverse diffusion process**.


### The Reverse Diffusion Process: Step by Step

#### 1. The Goal

The reverse diffusion process aims to start from random noise and, through a sequence of learned steps, remove the noise and reconstruct a realistic data sample (e.g., an image).

#### 2. Why Is This Hard?

- The forward process is simple and analytically defined.
- The reverse process requires "undoing" the added noise, which is complex and intractable for real data.
- To solve this, we use a neural network to learn how to denoise the sample step-by-step.

#### 3. How Is the Reverse Process Modeled?

- The reverse process is represented as a Markov chain of conditional Gaussians:

$$
p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} \mid x_t)
$$

- Here, $p(x_T)$ is a standard normal distribution.
- Each $p_\theta(x_{t-1} \mid x_t)$ is a Gaussian with parameters predicted by a neural network.

#### 4. Why Use Gaussians?

- If the forward noise schedule uses small variances $\beta_t$, the reverse conditional distributions can be well-approximated by Gaussians.
- This makes training and sampling tractable using standard probabilistic tools.

#### 5. What Does the Neural Network Actually Do?

- Given a noisy input $x_t$ and the time step $t$, the neural network predicts:
  - The mean $\mu_\theta(x_t, t)$ of the reverse Gaussian
  - The added noise $\epsilon$
  - Or another related signal, depending on how the model is parameterized

#### 6. The Sampling (Generation) Process

To generate a new sample:

1. Sample noise: $x_T \sim \mathcal{N}(0, I)$  
2. For $t = T, T-1, \dots, 1$:
   - Predict $\mu_\theta(x_t, t)$ and optionally $\Sigma_\theta(x_t, t)$
   - Sample:

$$
x_{t-1} = \mu_\theta(x_t, t) + \Sigma_\theta^{1/2}(x_t, t) \cdot z, \quad z \sim \mathcal{N}(0, I)
$$

3. The final output $x_0$ should resemble realistic data.

#### 7. Discrete Data and the Final Step

- Real images have discrete pixel values (e.g., 0 to 255).
- The final step often uses a discrete decoder (like a softmax over 256 bins) to convert continuous predictions into discrete outputs.

---

### Key Mathematical Details (With Explanations)

- **Forward process:**

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \, \epsilon
$$

where $\epsilon \sim \mathcal{N}(0, I)$, and $\bar{\alpha}_t$ is the cumulative product of $\alpha_t = 1 - \beta_t$.

- **Reverse process:**

$$
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

- **Sampling equation:**

$$
x_{t-1} = \mu_\theta(x_t, t) + \sigma_t z, \quad z \sim \mathcal{N}(0, I)
$$

- **Final decoding:** Produces discrete pixel values from continuous outputs.


### Why Is This Important?

- Connects the forward and reverse processes to the actual generative algorithm.
- Explains how the neural network learns to remove noise step-by-step.
- Clarifies why Gaussian assumptions and probabilistic modeling matter.

---

### Summary

- The reverse diffusion process generates new data by progressively denoising a random noise sample.
- It’s guided by a neural network trained to predict the direction of denoising at each step.
- Each reverse step is modeled as a Gaussian, and the final output is discretized to match real data.
- This process allows diffusion models to create realistic images, audio, or other types of structured data.
