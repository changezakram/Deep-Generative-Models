---
title: "Transformers — A Deep Dive"
format: html
---

## Introduction

Transformers are a neural network architecture designed for processing sequential data. Introduced in the 2017 paper *Attention is All You Need*, transformers replaced recurrent and convolutional architectures in many NLP tasks. They leverage a novel mechanism called self-attention, which enables better parallelization and more effective handling of long-range dependencies.

![Figure: High-level schematic of the Transformer architecture (Vaswani et al., 2017, Attention Is All You Need).](images/transformer-architecture.png)


## Limitations of Recurrent Neural Networks (RNNs)

Before the advent of transformers, Recurrent Neural Networks (RNNs) were the dominant architecture for sequence modeling tasks. Their ability to process sequences of variable length and maintain a memory of past inputs made them widely adopted. In an RNN, the hidden state $h_t$ at time $t$ is computed based on the current input $x_t$ and the previous hidden state $h_{t-1}$.  
This allows RNNs to process sequences by carrying information forward over time.

The hidden state at time t can be computed recursively as:

$$
h_t = \tanh(W_h h_{t-1} + W_x x_t + b)
$$

where:

- $W_h$ — weight matrix for the hidden state
- $W_x$ — weight matrix for the input
- $b$ — bias term
- $\tanh$ — activation function

The figure below shows a single RNN cell (left) and the unrolled RNN over multiple time steps (right).

![Figure: RNN unrolled over time, showing the recurrence of hidden states and outputs across time steps.](images/rnn.png)

However, RNNs suffer from:

- **Sequential computation**: Cannot fully parallelize processing of sequences.
- **Vanishing/exploding gradients**: Gradients through many time steps can vanish or blow up, making it difficult to learn long-term dependencies.

---

## Architecture Overview

At a high level:

- **Encoder–Decoder** (original transformer) — used in translation.
- **Encoder-only** (e.g., BERT) — for classification, masked language modeling.
- **Decoder-only** (e.g., GPT) — for autoregressive text generation.

At the heart of these architectures is the *self-attention mechanism*, which enables each token to attend to all others in the sequence — a key innovation that we’ll explain shortly.

We’ll break down the encoder and decoder below.


### Key Variables

The following table summarizes the notations, shapes, and meanings of the variables that will appear throughout the Encoder and Attention sections.

![Notation Table](images/notations.png)

---

## Encoder

The **encoder** consists of a stack of identical layers, each containing:

- Multi-Head Self-Attention  
- Feedforward Network  
- Residual Connections + Layer Norm  
- Positional Encoding

---

### Core Concepts

#### Input Representation

Before the attention mechanism can work, each input token is mapped to a continuous vector representation and enriched with positional information.

**Token Embedding**

We start with a sequence of token indices:  
$$
T \in \{1, \dots, V\}^n
$$
where $n$ is the sequence length and $V$ is the vocabulary size.

These indices are mapped to continuous vectors using a learnable embedding matrix $E \in \mathbb{R}^{V \times d_e}$:  
$$
X_\text{tokens} = E[T] \in \mathbb{R}^{n \times d_e}
$$



**Positional Encoding**

The self-attention mechanism processes the input sequence as a set of vectors without any notion of order.  
However, the meaning of a sentence depends on the order of its words.  

> *“The cat chased the dog” ≠ “The dog chased the cat”*

Without positional information, self-attention would treat these sentences as identical.  
To address this, we add a **position vector** $p_i$ to each token embedding $x_i$, producing a position-aware representation:  

$$
\tilde{x}_i = x_i + p_i
$$

where:

- $x_i$ — embedding of the token at position $i$
- $p_i$ — positional encoding vector
- $\tilde{x}_i$ — position-aware embedding passed to the model

The figure below illustrates this process:

![Figure: Token and positional embeddings are summed element-wise to produce the final input embeddings fed into the transformer encoder.](images/embedding.png)


Following are two approaches to Positional Encoding:

**(A) Sinusoidal Positional Encoding**

In the original Transformer paper, $p_i$ is defined using sinusoidal functions of varying frequencies:  

$$
p_i =
\begin{pmatrix}
\sin\big(i / 10000^{\frac{2j}{d_e}}\big) \\
\cos\big(i / 10000^{\frac{2j}{d_e}}\big)
\end{pmatrix}
$$

- **Pros:**
  - Does not introduce additional parameters.
  - Periodicity allows some generalization to longer sequences.
- **Cons:**
  - Not learnable — fixed at initialization.
  - Limited extrapolation in practice.

**(B) Learned Positional Encoding**

An alternative is to treat $p_i$ as a learnable parameter:  
We define a matrix $P \in \mathbb{R}^{d_e \times n}$, where each $p_i$ is a column of $P$.


$$
P = [p_1, p_2, \dots, p_n], \quad P \in \mathbb{R}^{d_e \times n}
$$

- **Pros:**
  - Fully learnable; each position can adapt to the data.
  - Most modern systems (e.g., GPT) use this.
- **Cons:**
  - Cannot extrapolate to sequences longer than seen in training.


In practice, most architectures today use **learned positional encodings** because of their flexibility and performance.


**Summary**

- Positional encoding ensures the model is aware of token order.  
- Sinusoidal encodings are fixed & periodic.  
- Learned encodings are flexible & widely used today.  

---

#### Attention Mechanism

At the heart of the transformer is the scaled dot-product attention mechanism, which allows the model to weigh the relevance of each token in the sequence when processing a given token. This enables the model to capture relationships between tokens regardless of their distance in the sequence.

**What is Attention?**

Each token is projected into three vectors:   

- **Query ($Q$):** represents the token we’re focusing on.
- **Key ($K$):** represents the tokens we compare against.
- **Value ($V$):** represents the information we retrieve if the key is relevant.

For a given query $Q$, the attention weights over all keys $K$ are computed by taking the dot product of $Q$ with $K$, scaling, and passing through a softmax:  

$$
\text{Attention}(Q, K, V) =
\text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) V
$$

where:     

- $Q$ is the matrix of query vectors, shape $[n \times d_k]$
- $K$ is the matrix of key vectors, shape $[n \times d_k]$
- $V$ is the matrix of value vectors, shape $[n \times d_v]$
- $d_k$ is the dimension of the keys (used for scaling)


**Step-by-step Computation**

The figure below illustrates the computation flow of scaled dot-product attention, including the dimensions of each variable at every stage:

![Figure: Attention — step-by-step computation, with variables, and operations.](images/attention-diagram.png)


**Breakdown of the Steps**

1: Input Representation:
$$
X = \text{Embedding}(X_{\text{tokens}}) + \text{PositionalEncoding}
$$  

2: Linear Projections: 
$$
Q = X W_q, \quad K = X W_k, \quad V = X W_v
$$

3: Compute Similarity Scores:
$$
Q K^T
$$

4: Scale the Scores:  
$$
\frac{Q K^T}{\sqrt{d_k}}
$$

5: Softmax to get Attention Weights:
$$
\text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right)
$$

6: Weighted Sum: 
$$
\text{Attention Weights} \cdot V
$$


**Why is Attention Powerful?**

- Captures long-range dependencies.  
- Learns which tokens are most relevant to each other.   
- Fully parallelizable since it operates on the entire sequence at once.  

---

#### Multi-Head Attention

While a single attention head can focus on certain aspects of the input sequence, it may miss other patterns. The multi-head attention (MHA) mechanism allows the model to attend to information from multiple representation subspaces at different positions simultaneously. This helps the transformer to capture more nuanced patterns in the input sequence.

The figure below shows how multiple independent attention heads are computed in parallel, concatenated, and linearly transformed to produce the final output of the multi-head attention layer.

![Figure: Multi-Head Attention — four parallel heads, concatenated and projected by $W_o$ to form the final output.](images/multi-head-attention.png)



**What is Multi-Head Attention?**

Instead of computing a single set of $Q$, $K$, $V$, the model projects the input into $h$ different sets of $Q$, $K$, $V$, called *heads*. Each head performs scaled dot-product attention independently, and their outputs are concatenated and linearly transformed.


For head $i$:
$$
\text{head}_i = \text{Attention}(Q_i, K_i, V_i) =
\text{softmax} \left( \frac{Q_i K_i^T}{\sqrt{d_k}} \right) V_i
$$

where:

- $Q_i = X W_q^{(i)}$
- $K_i = X W_k^{(i)}$
- $V_i = X W_v^{(i)}$

Here, $W_q^{(i)}, W_k^{(i)}, W_v^{(i)}$ are separate learnable weights for each head.


**Combining the Heads**

The outputs of all $h$ heads are concatenated along the feature dimension and projected back into $d_e$ dimensions:
$$
\text{MultiHead}(Q, K, V) =
\text{Concat}(\text{head}_1, \dots, \text{head}_h) W_o
$$

where $W_o$ is a learnable weight matrix of shape $[h \cdot d_v \times d_e]$.

**Why Multi-Head Attention?**

- Allows the model to jointly attend to information from different representation subspaces.  
- Provides richer and more diverse attention patterns.  
- Empirically improves performance compared to a single head.



---

#### Feedforward Network

After the self-attention layer, the transformer applies a **position-wise feedforward network (FFN)** to each token embedding independently.  
This introduces **nonlinearities** into the model and allows it to transform the attended information further.

**Why is this needed?**
- Self-attention by itself is a linear operation — it just computes weighted sums of the value vectors.
- Stacking more self-attention layers without nonlinearity simply re-averages the values, limiting expressiveness.
- To address this, each output vector of the self-attention layer is passed through a **multi-layer perceptron (MLP)**.

**Computation**
For each token output vector $output_i$ from the self-attention:
$$
m_i = MLP(output_i) = W_2 \cdot ReLU(W_1 \cdot output_i + b_1) + b_2
$$

where:

- $W_1$, $W_2$ — learnable weight matrices.
- $b_1$, $b_2$ — learnable biases.
- $ReLU$ — non-linear activation function applied element-wise.

This is done independently for each position.

**Summary:**

- Adds nonlinearity and expressiveness.  
- Processes each token independently after attention.  
- Helps the model learn complex transformations beyond weighted averages.

---

#### Residual Connections & Layer Norm



---

## Decoder

The **decoder** is used for sequence generation.  
It also consists of stacked layers with three components:

- **Masked Multi-Head Self-Attention** — only attends to past tokens.  
- **Encoder–Decoder Attention** — attends to the encoder output.  
- **Feedforward + normalization + residuals**

---

### Attention Mask for Decoder


---

### Simplified Decoder Layer


---

## Training Tips

- Large batches & data.
- Learning rate warm-up.
- Label smoothing.
- Gradient clipping.

---

## Variants and Evolutions

| Model   | Type           | Use Case                   |
|---------|----------------|-----------------------------|
| **BERT**| Encoder-only   | Classification, QA         |
| **GPT** | Decoder-only   | Text generation            |
| **T5**  | Encoder–Decoder| Translation, summarization |
| **ViT** | Encoder-only   | Image classification       |

---

## Applications

- NLP: translation, summarization, question answering.
- Vision: Vision Transformers (ViTs).
- Multimodal: CLIP (image-text), Flamingo.
- Other: Protein folding (AlphaFold).

---

## Strengths and Limitations

**Pros**:

- Captures long dependencies.
- Parallelizable.
- State-of-the-art results.

**Cons**:

- Requires huge compute resources.
- Data-hungry.
- Less interpretable.

---

## References & Further Reading

- *Attention is All You Need* — [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)  
- BERT — [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)  
- GPT-3 — [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)  
- Vision Transformer — [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)


