{"title":"Deep Generative Models","markdown":{"yaml":{"title":"Deep Generative Models"},"headingText":"Evolution of AI Capabilities","containsRefs":false,"markdown":"\n\n\nAI is changing fast, and each stage brings new strengths, limits, and oversight needs. The table below offers a strategic comparison of how Traditional AI, Generative AI, and Agentic AI differ in behavior and best use.\n\n\n<div style=\"overflow-x:auto;\">\n\n| Capability            | **Traditional AI**<br><small>Rule-driven tools</small> | **Generative AI**<br><small>Smart assistants</small> | **Agentic AI**<br><small>Autonomous actors</small> |\n|-----------------------|---------------------------------------------|--------------------------------------------|--------------------------------------------|\n| **What it does**      | Follows hard-coded logic and rules          | Understands prompts and generates output   | Plans, reasons, and acts across steps       |\n| **How it behaves**    | Rigid, predictable                          | Creative but guided                        | Goal-seeking and adaptive                   |\n| **Human involvement** | Fully manual setup and supervision          | Needs context and oversight                | Can operate independently (with guardrails) |\n| **Strengths**         | Reliable on structured tasks                | Great at summarizing, drafting, generating | Handles multistep workflows                 |\n| **Best used for**     | Repetitive decisions and automation         | Insight generation and copiloting          | Full process orchestration                  |\n\n</div>\n\n\n## Why Generative AI Matters\n\nGenerative AI models can create entirely new content â€” such as text, images, simulations, or code â€” by learning from existing data. Unlike traditional AI systems that classify or tag inputs, generative models **produce** new outputs based on patterns theyâ€™ve learned.\n\nSince 2022, innovation in this space has accelerated rapidly. Foundation models like ChatGPT, Claude, and Gemini are being deployed across industries â€” from writing assistance and image generation to fraud detection and synthetic data creation.\n\n> ðŸ’¡ **Business impact:** McKinsey estimates that generative AI could contribute up to **$4.4 trillion in annual global economic value**, with wide-ranging implications for productivity, personalization, and decision support.\n\nAt the same time, generative AI introduces new risks. Outputs may appear convincing but be inaccurate, biased, or open to misuse. This is especially critical in high-stakes domains such as banking, healthcare, and law.  \nEnsuring responsible use requires strong governance, human oversight, and ethical deployment practices.\n\n\n## How Generative Models Work\n\nGenerative AI models aim to learn the underlying distribution of real-world data, typically denoted as $p_{\\text{data}}(x)$. Their goal is to approximate this distribution as closely as possible.\n\n\n- The distribution learned by the model is denoted as $p_\\theta(x)$  \n- We generate new data by **sampling** from this learned distribution  \n- In practice, generative models are trained to **maximize the expected log-likelihood** of $p_\\theta(x)$, or equivalently, **minimize the divergence** between $p_\\theta(x)$ and $p_{\\text{data}}(x)$\n\n\n> ðŸ“Œ **Note**:  \n> - Models like **Autoregressive models** and **Normalizing Flows** directly maximize log-likelihood.  \n> - **VAEs** maximize a variational lower bound (ELBO) on log-likelihood.  \n> - **GANs** minimize the **Jensen-Shannon divergence** through adversarial training.  \n> - **Diffusion models** and **EBMs** use score matching or other divergence-minimizing techniques.\n\nThis site explores how different model families approach this goal â€” from Variational Autoencoders (VAEs) to Diffusion Models â€” grounded in theory and real-world use.\n\n## Key Gen AI Model Families\n\n<details>\n<summary><strong> Variational Autoencoders (VAEs)</strong></summary>\n\n- **Core Idea**: Encode input to a latent space and reconstruct it while optimizing a lower bound on likelihood (ELBO).\n- **Likelihood**: Approximate (variational lower bound).\n- **Sampling**: Fast â€” sample latent vector and decode.\n- **Use Cases**: Representation learning, image generation.\n- **Example Models**: Î²-VAE, Conditional VAE\n- [Go to VAE write-up â†’](vae.qmd)\n\n</details>\n\n<details>\n<summary><strong> Generative Adversarial Networks (GANs)</strong></summary>\n\n- **Core Idea**: A generator and discriminator compete in a minimax game to produce realistic samples.\n- **Likelihood**: None (implicit model).\n- **Sampling**: Fast â€” sample latent vector and pass through generator.\n- **Use Cases**: High-quality image generation, style transfer.\n- **Example Models**: StyleGAN, CycleGAN\n- [Go to GAN write-up â†’]\n\n</details>\n\n<details>\n<summary><strong> Autoregressive Models</strong></summary>\n\n- **Core Idea**: Factor the joint distribution as a product of conditional probabilities.\n- **Likelihood**: Exact.\n- **Sampling**: Slow â€” token-by-token generation.\n- **Use Cases**: Language modeling, code generation, time series.\n- **Example Models**: GPT, PixelRNN\n- [Go to Autoregressive Models â†’]\n\n</details>\n\n<details>\n<summary><strong> Normalizing Flows</strong></summary>\n\n- **Core Idea**: Learn invertible transformations of latent variables using the change-of-variable formula.\n- **Likelihood**: Exact and tractable.\n- **Sampling**: Fast â€” sample from base distribution and invert.\n- **Use Cases**: Density estimation, latent space modeling.\n- **Example Models**: RealNVP, Glow\n- [Go to Flow Models â†’](flows.qmd)\n\n</details>\n\n<details>\n<summary><strong> Energy-Based Models (EBMs)</strong></summary>\n\n- **Core Idea**: Define an energy function over inputs; lower energy = higher probability.\n- **Likelihood**: Unnormalized (intractable partition function).\n- **Sampling**: Very slow â€” requires MCMC or Langevin dynamics.\n- **Use Cases**: Uncertainty modeling, compositional generation.\n- **Example Models**: Score-based EBMs\n- [Go to EBMs â†’](ebm.qmd)\n\n</details>\n\n<details>\n<summary><strong> Diffusion Models</strong></summary>\n\n- **Core Idea**: Learn to reverse a gradual noise process via denoising.\n- **Likelihood**: Approximate (via variational bound).\n- **Sampling**: Slow â€” requires hundreds of reverse steps.\n- **Use Cases**: High-resolution image and audio generation.\n- **Example Models**: Stable Diffusion\n- [Go to Diffusion Models â†’](diffusion.qmd)\n\n</details>\n\n\n## Industry Use Cases\n\n<details>\n<summary><strong>Banking</strong></summary>\n\n- **Customer Service** â€” Autoregressive Models (e.g., GPT)  \n- **Fraud Detection** â€” Energy-Based Models  \n- **Operational Efficiency** â€” RAG for document automation and summarization  \n- **Business Intelligence** â€” VAE for anomaly detection in transaction patterns \n- **Marketing** â€” GANs for personalized message generation and segmentation  \n\nIncludes deployments by JPMorgan, Mastercard, Wells Fargo, and Morgan Stanley.  \n[View full Banking use cases â†’](gen-ai-use-cases/banking-use-cases.html)\n\n</details>\n\n<details>\n<summary><strong>Healthcare</strong></summary>\n\n- **Clinical Documentation** â€” Autoregressive Models (e.g., GPT-4), RAG (Nuance DAX)  \n- **Medical Imaging** â€” Diffusion Models for image enhancement  \n- **Diagnostics & Triage** â€” VAEs for uncertainty-aware diagnosis  \n- **Patient Education** â€” Conversational AI built on large language models  \n- **Drug Discovery** â€” Normalizing Flows for molecular sampling and VAEs for molecule generation  \n\nHighlights work by Nuance (Microsoft), Mayo Clinic, DeepMind, and Insilico Medicine.  \n[View full Healthcare use cases â†’](gen-ai-use-cases/healthcare-use-cases.html)\n\n</details>\n","srcMarkdownNoYaml":"\n\n## Evolution of AI Capabilities\n\nAI is changing fast, and each stage brings new strengths, limits, and oversight needs. The table below offers a strategic comparison of how Traditional AI, Generative AI, and Agentic AI differ in behavior and best use.\n\n\n<div style=\"overflow-x:auto;\">\n\n| Capability            | **Traditional AI**<br><small>Rule-driven tools</small> | **Generative AI**<br><small>Smart assistants</small> | **Agentic AI**<br><small>Autonomous actors</small> |\n|-----------------------|---------------------------------------------|--------------------------------------------|--------------------------------------------|\n| **What it does**      | Follows hard-coded logic and rules          | Understands prompts and generates output   | Plans, reasons, and acts across steps       |\n| **How it behaves**    | Rigid, predictable                          | Creative but guided                        | Goal-seeking and adaptive                   |\n| **Human involvement** | Fully manual setup and supervision          | Needs context and oversight                | Can operate independently (with guardrails) |\n| **Strengths**         | Reliable on structured tasks                | Great at summarizing, drafting, generating | Handles multistep workflows                 |\n| **Best used for**     | Repetitive decisions and automation         | Insight generation and copiloting          | Full process orchestration                  |\n\n</div>\n\n\n## Why Generative AI Matters\n\nGenerative AI models can create entirely new content â€” such as text, images, simulations, or code â€” by learning from existing data. Unlike traditional AI systems that classify or tag inputs, generative models **produce** new outputs based on patterns theyâ€™ve learned.\n\nSince 2022, innovation in this space has accelerated rapidly. Foundation models like ChatGPT, Claude, and Gemini are being deployed across industries â€” from writing assistance and image generation to fraud detection and synthetic data creation.\n\n> ðŸ’¡ **Business impact:** McKinsey estimates that generative AI could contribute up to **$4.4 trillion in annual global economic value**, with wide-ranging implications for productivity, personalization, and decision support.\n\nAt the same time, generative AI introduces new risks. Outputs may appear convincing but be inaccurate, biased, or open to misuse. This is especially critical in high-stakes domains such as banking, healthcare, and law.  \nEnsuring responsible use requires strong governance, human oversight, and ethical deployment practices.\n\n\n## How Generative Models Work\n\nGenerative AI models aim to learn the underlying distribution of real-world data, typically denoted as $p_{\\text{data}}(x)$. Their goal is to approximate this distribution as closely as possible.\n\n\n- The distribution learned by the model is denoted as $p_\\theta(x)$  \n- We generate new data by **sampling** from this learned distribution  \n- In practice, generative models are trained to **maximize the expected log-likelihood** of $p_\\theta(x)$, or equivalently, **minimize the divergence** between $p_\\theta(x)$ and $p_{\\text{data}}(x)$\n\n\n> ðŸ“Œ **Note**:  \n> - Models like **Autoregressive models** and **Normalizing Flows** directly maximize log-likelihood.  \n> - **VAEs** maximize a variational lower bound (ELBO) on log-likelihood.  \n> - **GANs** minimize the **Jensen-Shannon divergence** through adversarial training.  \n> - **Diffusion models** and **EBMs** use score matching or other divergence-minimizing techniques.\n\nThis site explores how different model families approach this goal â€” from Variational Autoencoders (VAEs) to Diffusion Models â€” grounded in theory and real-world use.\n\n## Key Gen AI Model Families\n\n<details>\n<summary><strong> Variational Autoencoders (VAEs)</strong></summary>\n\n- **Core Idea**: Encode input to a latent space and reconstruct it while optimizing a lower bound on likelihood (ELBO).\n- **Likelihood**: Approximate (variational lower bound).\n- **Sampling**: Fast â€” sample latent vector and decode.\n- **Use Cases**: Representation learning, image generation.\n- **Example Models**: Î²-VAE, Conditional VAE\n- [Go to VAE write-up â†’](vae.qmd)\n\n</details>\n\n<details>\n<summary><strong> Generative Adversarial Networks (GANs)</strong></summary>\n\n- **Core Idea**: A generator and discriminator compete in a minimax game to produce realistic samples.\n- **Likelihood**: None (implicit model).\n- **Sampling**: Fast â€” sample latent vector and pass through generator.\n- **Use Cases**: High-quality image generation, style transfer.\n- **Example Models**: StyleGAN, CycleGAN\n- [Go to GAN write-up â†’]\n\n</details>\n\n<details>\n<summary><strong> Autoregressive Models</strong></summary>\n\n- **Core Idea**: Factor the joint distribution as a product of conditional probabilities.\n- **Likelihood**: Exact.\n- **Sampling**: Slow â€” token-by-token generation.\n- **Use Cases**: Language modeling, code generation, time series.\n- **Example Models**: GPT, PixelRNN\n- [Go to Autoregressive Models â†’]\n\n</details>\n\n<details>\n<summary><strong> Normalizing Flows</strong></summary>\n\n- **Core Idea**: Learn invertible transformations of latent variables using the change-of-variable formula.\n- **Likelihood**: Exact and tractable.\n- **Sampling**: Fast â€” sample from base distribution and invert.\n- **Use Cases**: Density estimation, latent space modeling.\n- **Example Models**: RealNVP, Glow\n- [Go to Flow Models â†’](flows.qmd)\n\n</details>\n\n<details>\n<summary><strong> Energy-Based Models (EBMs)</strong></summary>\n\n- **Core Idea**: Define an energy function over inputs; lower energy = higher probability.\n- **Likelihood**: Unnormalized (intractable partition function).\n- **Sampling**: Very slow â€” requires MCMC or Langevin dynamics.\n- **Use Cases**: Uncertainty modeling, compositional generation.\n- **Example Models**: Score-based EBMs\n- [Go to EBMs â†’](ebm.qmd)\n\n</details>\n\n<details>\n<summary><strong> Diffusion Models</strong></summary>\n\n- **Core Idea**: Learn to reverse a gradual noise process via denoising.\n- **Likelihood**: Approximate (via variational bound).\n- **Sampling**: Slow â€” requires hundreds of reverse steps.\n- **Use Cases**: High-resolution image and audio generation.\n- **Example Models**: Stable Diffusion\n- [Go to Diffusion Models â†’](diffusion.qmd)\n\n</details>\n\n\n## Industry Use Cases\n\n<details>\n<summary><strong>Banking</strong></summary>\n\n- **Customer Service** â€” Autoregressive Models (e.g., GPT)  \n- **Fraud Detection** â€” Energy-Based Models  \n- **Operational Efficiency** â€” RAG for document automation and summarization  \n- **Business Intelligence** â€” VAE for anomaly detection in transaction patterns \n- **Marketing** â€” GANs for personalized message generation and segmentation  \n\nIncludes deployments by JPMorgan, Mastercard, Wells Fargo, and Morgan Stanley.  \n[View full Banking use cases â†’](gen-ai-use-cases/banking-use-cases.html)\n\n</details>\n\n<details>\n<summary><strong>Healthcare</strong></summary>\n\n- **Clinical Documentation** â€” Autoregressive Models (e.g., GPT-4), RAG (Nuance DAX)  \n- **Medical Imaging** â€” Diffusion Models for image enhancement  \n- **Diagnostics & Triage** â€” VAEs for uncertainty-aware diagnosis  \n- **Patient Education** â€” Conversational AI built on large language models  \n- **Drug Discovery** â€” Normalizing Flows for molecular sampling and VAEs for molecule generation  \n\nHighlights work by Nuance (Microsoft), Mayo Clinic, DeepMind, and Insilico Medicine.  \n[View full Healthcare use cases â†’](gen-ai-use-cases/healthcare-use-cases.html)\n\n</details>\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":2,"number-sections":true,"html-math-method":"mathjax","output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","resources":["images/**"],"theme":"cosmo","title":"Deep Generative Models"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}