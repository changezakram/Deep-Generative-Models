---
title: "Language Model Evaluation"
format: html
---

## Introduction:

NLP evaluation measures how well language models actually work. It answers basic questions: Is this model good enough? Can we trust it with real users? But here's the challenge—testing language models isn't like testing regular software. When code breaks, it's obvious. When a language model fails, it might write something that sounds perfect but is completely wrong. A model might be 92% accurate overall but fail every time it sees sarcasm. This is what makes NLP evaluation so tricky: we're measuring how well computers understand the messy, complex world of human language.

**Why It Matters Now More Than Ever**

In 2023 alone, we saw an AI chatbot fail to recognize suicide warning signs (leading to a death), lawyers submit fake AI-generated cases to courts, and Air Canada forced to honor a refund policy its bot invented. As Chip Huyen warns: "The more AI is used, the more opportunity there is for catastrophic failure." The smarter our models get, the harder they become to evaluate. It's easy to check a kid's math homework, but verifying if an AI's medical advice is accurate requires medical expertise. We need good evaluation to build better models, but we need expertise to do good evaluation.

**Two Types of Tasks, Two Different Challenges**

- **Closed-ended tasks** have clear right answers (Is this email spam? What's the sentiment?). We can use traditional metrics like accuracy and precision, but even these "simple" tasks suffer from shortcuts, dataset problems, and human labeling errors.

- **Open-ended tasks** have no single right answer (Write a summary, translate this text, answer this question). Traditional metrics completely fail here. Word-matching might say "Heck no!" is similar to "Heck yes!" because they share words.

**What's Ahead**

This overview covers how to evaluate both closed and open-ended tasks, why current methods fail and what's replacing them, major problems like contamination and bias, and practical solutions for real-world applications.

---

## Closed-Ended Tasks

Closed-ended tasks are widely used in NLP evaluation because they provide clear right or wrong answers. Since the model's output is limited to a small set of predefined choices—often fewer than ten—they make it easier to compare models using standard metrics like accuracy, precision, recall, and F1-score. This structure allows for objective evaluation, consistent benchmarking, and easier tracking of progress over time.

**Bounded Output Space:** Unlike text generation where models can produce any sequence of tokens, closed-ended tasks constrain outputs to predefined categories or structured formats.    
**Objective Evaluation:** Success can be measured automatically using established metrics from classical machine learning—accuracy, precision, recall, and F1-score.    
**Reproducible Benchmarks:** Standard datasets enable fair comparison across models and over time.    
**Systematic Progress Tracking:** Clear metrics allow the field to monitor advancement and identify when models have genuinely improved.

While this constraint makes evaluation more straightforward than open-ended generation, closed-ended tasks still present significant challenges that can mislead researchers and practitioners. Understanding these problems is important because closed-ended evaluation is often the first test of model quality, affecting research focus and decisions about which models to deploy.

### Evaluation Metrics for Closed-Ended Tasks

Understanding how to measure performance is fundamental to closed-ended evaluation. Different metrics serve different purposes and can lead to very different conclusions about model quality. Following metrics evaluate performance on individual labeled tasks:

- **Accuracy:** Percentage of correct predictions (simple but can be misleading with imbalanced data).    
- **Precision:** Of all positive predictions, how many were correct? (important when false alarms are costly).    
- **Recall:** Of all actual positive cases, how many were found? (important when missing cases is costly).    
- **F1-Score:** Harmonic mean of precision and recall (balances both concerns).    

These metrics are typically applied to standardized datasets called **benchmarks**, which we'll explore in the next section. Like standardized tests for AI, benchmarks enable fair comparison between models from different research groups and track progress over time.

### Types of Closed-Ended Tasks


| **Task Type**             | **Description**                                           | **Example**                                                                                                                                     | **Popular Benchmarks**     | **Common Pitfalls**                                        |
|---------------------------|-----------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------|-------------------------------------------------------------|
| **Sentiment Analysis**    | Classify emotional tone (positive/negative/neutral)       | "Read the book, forget the movie!" → **Negative**                                                                                               | SST, IMDB, Yelp             | Fails on sarcasm, irony, cultural context                  |
| **Textual Entailment**    | Does sentence B logically follow from sentence A?         | *Premise:* "A soccer game with multiple males playing"<br>*Hypothesis:* "Some men are playing sport" → **Entailment**                            | SNLI, MultiNLI, RTE         | Shortcut learning (e.g., keyword overlap); see SNLI issues |
| **Named Entity Recognition (NER)** | Identify and classify proper nouns                     | "Apple released the iPhone." → **[Apple–ORG]**, **[iPhone–PRODUCT]**                                                                             | CoNLL-2003                  | Ambiguity: Apple (fruit vs company)                        |
| **Part-of-Speech Tagging**| Assign grammatical categories to words                    | "The quick brown fox" → **[The–DET]**, **[quick–ADJ]**, **[brown–ADJ]**, **[fox–NOUN]**                                                           | Penn Treebank (PTB)         | Often used as foundation for parsing & other tasks         |
| **Coreference Resolution**| Determine pronoun references                              | "Mark told Pete lies about himself. He should have been more truthful." → **"He" = Mark**                                                        | WSC, OntoNotes              | Requires deep context or world knowledge                   |
| **Question Answering**    | Extract answers from passage                              | *Context:* "The ESA was passed in 1973."<br>*Q:* "When was it passed?" → **1973**                                                                | SQuAD, SQuAD 2.0            | Models memorize patterns or position, not true reasoning   |

These individual tasks can also be grouped into multi-task benchmarks that evaluate general language understanding across a range of closed-ended challenges. **SuperGLUE** is one of the most prominent such benchmarks.

### Multi-Task Benchmark: **SuperGLUE**

While the previous section outlined individual closed-ended tasks, real-world evaluation often demands a unified benchmark that spans multiple task types. **SuperGLUE** is a widely adopted **closed-ended multi-task benchmark** created to evaluate broad general language understanding. It builds on its predecessor (GLUE) with harder tasks, more robust metrics, and an emphasis on reasoning.

SuperGLUE combines a diverse set of tasks—ranging from entailment and coreference resolution to causal reasoning and word sense disambiguation—designed to holistically assess a model’s linguistic and reasoning capabilities across multiple dimensions.

**Tasks in SuperGLUE:**

| Task          | Description                                               | Type                        |
|---------------|-----------------------------------------------------------|-----------------------------|
| **BoolQ**, **MultiRC** | Reading comprehension                              | QA / Inference              |
| **CB**, **RTE**        | Natural language inference                         | Entailment                  |
| **COPA**              | Causal reasoning (cause/effect)                    | Reasoning                   |
| **ReCoRD**            | Reading comprehension with commonsense reasoning  | QA / Commonsense            |
| **WiC**               | Word meaning in context                            | Word Sense Disambiguation   |
| **WSC**               | Coreference resolution                             | Coreference                 |

Together, these tasks go beyond surface-level prediction—testing abilities like logical reasoning, commonsense application, coreference tracking, and contextual understanding.

**Leaderboard Highlights (v2.0):**

To measure and compare model performance on SuperGLUE, an official leaderboard tracks results across all tasks using standardized metrics. The **v2.0 leaderboard** showcases most advanced models—ranging from parameter-efficient **Mixture of Experts (MoE)** systems to massive transformer-based architectures—offering a clear snapshot of the state of the art in general language understanding.

- **Top models**: Vega v2, ST-MoE-32B, ERNIE, PaLM 540B, T5  
- **Metrics used**: Accuracy, F1 score, Exact Match, Gender Parity, etc.

The leaderboard emphasizes **balanced generalization**, rewarding models that perform consistently well across diverse task types—not just a few. This makes it a reliable benchmark for tracking progress toward broadly capable language models.


### Domain-Rich Multi-Task Benchmark: MMLU

While SuperGLUE focuses on general linguistic reasoning, **Massive Multitask Language Understanding (MMLU)** shifts the spotlight to **domain knowledge**. It has rapidly become the *de facto* benchmark for evaluating a model’s grasp of academic and professional subjects—effectively acting as a proxy for general intelligence in many LLM leaderboards.

**What is MMLU?**

- **57 subjects** spanning elementary math, anatomy, law, philosophy, computer science, US history, and more  
- **Multiple-choice format** (A, B, C, D), with ~100 questions per subject  
- **Balanced question design** that mimics real academic tests and professional licensing exams  
- **Closed-book evaluation** testing what the model has internalized from pretraining  

MMLU has emerged as a **standardized benchmark** for evaluating foundational knowledge across domains. It allows for **direct accuracy-based comparisons** between models of different sizes and architectures. Performance gains have been dramatic—rising from ~**25% (random guessing)** to over **90% accuracy** in just four years.

> **Note**: MMLU is often treated as a shortcut for measuring "general intelligence," but that can be misleading. What it really tests is how well a model can recall facts and recognize patterns—not necessarily how well it can reason or think abstractly. We’ll explore these limitations later.

**MMLU in Modern LLM Leaderboards**

Modern LLMs—including GPT-4, Claude 3, Gemini, LLaMA 3, and PaLM—routinely report MMLU scores as a primary metric. As with SuperGLUE, MMLU supports **multi-subject generalization**, but with a stronger emphasis on **world knowledge** rather than linguistic nuance.


**SuperGLUE vs MMLU: A Comparison**

| Aspect | **SuperGLUE** | **MMLU** |
|--------|----------------|----------|
| **Focus** | Language reasoning | Factual subject knowledge |
| **Format** | Varied NLP tasks | Multiple choice |
| **Tasks / Subjects** | 8 tasks | 57 subjects |
| **Primary Skill Tested** | Inference, disambiguation, coreference | Retained domain knowledge |
| **Metric** | Accuracy, F1, etc. | Accuracy only |

> While both are multi-task benchmarks, they evaluate very different capabilities—SuperGLUE emphasizes reasoning and understanding, whereas MMLU stresses factual recall across disciplines.


### Challenges in Closed-Ended Evaluation

1. **Metric Selection:** Different metrics highlight different aspects of model performance. For example: Accuracy can be misleading on imbalanced datasets—it may just reflect majority-class predictions. Precision measures correctness (fewer false positives), while Recall measures completeness (fewer false negatives). Using a single metric in isolation—especially on skewed tasks—can hide a model’s weaknesses.

2. **Metric Aggregation:** Benchmarks like SuperGLUE combine many tasks, each with its own metric (e.g., F1, accuracy, loss). Simply averaging scores can give an incomplete picture. Some tasks are easier than others, and their metrics scale differently. Without proper weighting or normalization, overall scores may not reflect true performance.

3. **Label Quality:** Poorly defined or inconsistent labels can introduce noise into both training and evaluation. This reduces reliability and makes it hard to tell if performance differences are meaningful or just due to annotation issues.

4. **Spurious Correlations:** Models may rely on patterns or keywords rather than real understanding.

   - **Example** (SNLI):  
     - Premise: “The economy could be still better.”  
     - Hypothesis: “The economy has *never* been better.”  
     - Model might infer contradiction simply due to the word *never* rather than actual reasoning

5. **Annotation Artifacts and Dataset Bias**: Some datasets contain structural biases—for example, QA answers often appear at the start of a passage. Models may exploit these shortcuts without real comprehension. Stylistic cues (e.g., sentence length or formality) may also correlate with certain labels, inflating scores without reflecting true understanding.

6. **Adversarial Robustness**: Small tweaks—like changing a word or rephrasing a sentence—can confuse models even if the meaning stays the same. This shows a lack of generalization. Robust evaluation should test whether models maintain performance under such paraphrased or adversarial inputs.

This highlights that even well-defined tasks require evaluation beyond raw accuracy to capture true model behavior and ensure robustness.

---


## Open-Ended Text Generation

Open-ended generation models, such as large language models (LLMs), can produce diverse free-form outputs like summaries, translations, stories, or answers to instructions. Unlike classification models that generate a fixed label or number, open-ended models return entire sequences of text—often with many valid responses for the same input.

This flexibility makes them powerful but difficult to evaluate. There's rarely a single “correct” output, and even seemingly unrelated completions may still be valid. For example, the prompt _“Tell me something interesting about the moon”_ could yield many accurate and fluent yet different answers.

Evaluating such responses goes beyond simple reference matching—it requires assessing coherence, fluency, relevance, factual accuracy, style, and semantic alignment. As a result, evaluation of open-ended generation must adopt a more nuanced, multi-faceted approach.

The following sections outline key evaluation methods and challenges.


### Content Overlap Metrics

These methods compute similarity based on surface-level word overlap between the generated text and a human-written reference. They are fast, interpretable, and have long been used in machine translation, summarization, and captioning. However, they often fail to recognize valid paraphrases or semantic equivalence.

**Popular metrics:**   
   
#### BLEU (Bilingual Evaluation Understudy)

- **Focus:** Precision-oriented n-gram overlap
- **Calculation:** Geometric mean of 1-gram through 4-gram precision, with brevity penalty
- **Use Case:** Machine Translation — how much of the generated text appears in the reference
- **Formula:**
$$
\text{BLEU} = \text{BP} \cdot \exp\left( \sum_{n=1}^{N} \frac{1}{N} \cdot \log(p_n) \right)
$$
where $p_n$ is n-gram precision and BP is brevity penalty.


#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

- **Focus:** Recall-oriented n-gram overlap
- **Variants:** ROUGE-1: Unigrams, ROUGE-2: Bigrams, ROUGE-L: Longest common subsequence
- **Use Case:** Summarization — how much of the reference is captured in the generated text
- **Formula (ROUGE-L F1 score)**:  
  $$
  \text{ROUGE-L} = \frac{(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}{\text{Recall} + \beta^2 \cdot \text{Precision}}
  $$
  where:    
  - Precision = $\frac{LCS}{\text{candidate length}}$
  - Recall = $\frac{LCS}{\text{reference length}}$
  - LCS = Longest Common Subsequence
  - $\beta$ balances recall and precision (often $\beta = 1$)

>**Limitation:** N-gram metrics have no concept of meaning. They fail when different words express the same idea.

**Example 1:**

**Reference**: `"Heck yes!"`

| Generated Output              | BLEU Score | Semantic Meaning            |
|-------------------------------|------------|-----------------------------|
| `"Yes!"`                      | 67%        | Correct                     |
| `"You know it!"`              | Low        | Correct                     |
| `"Yup"`                       | 0%         | Correct                     |
| `"Heck no!"`                  | 67%        | **Wrong** (opposite meaning)|

> These metrics reward lexical matches even when the meaning is incorrect.


**Example 2:**

**Reference**: `"The innovative startup secured substantial funding"`

- **Generated A**:  
  `"The creative company obtained significant investment"`  
  → 0% BLEU overlap, **perfect semantic match**

- **Generated B**:  
  `"The innovative startup funding substantial secured"`  
  → 83% BLEU overlap, **grammatically broken and semantically wrong**

> BLEU would incorrectly score **Generated B** higher than A due to word overlap.





### Model-Based Metrics

As NLP systems generate increasingly fluent and diverse outputs, traditional word-overlap metrics like BLEU and ROUGE often fail to capture deeper aspects of quality such as semantic fidelity, naturalness, and paraphrasing. To overcome these limitations, researchers have developed model-based evaluation metrics that harness the semantic understanding of pretrained language models.

Two widely adopted examples are **BERTScore**, which uses contextual embeddings for token-level semantic similarity, and **BLEURT**, which is trained to predict human quality judgments directly.


#### BERTScore: Semantic Matching with Contextual Embeddings 

BERTScore measures how semantically close a generated text is to a reference by comparing their contextualized token embeddings from BERT. It operates in three main steps:

1. **Token Embedding**: Each token is mapped to a contextual vector using a pre-trained BERT model.  
2. **Similarity Matrix**: Cosine similarity is computed between each candidate and reference token.  
3. **Greedy Matching**:
   - **Precision**: Max similarity for each candidate token to reference.  
   - **Recall**: Max similarity for each reference token to candidate.  
   - **F1 Score**: Harmonic mean of the two.

**Optional Enhancements**

- **IDF Weighting**: Emphasizes rare and informative words.  
- **Baseline Rescaling**: Normalizes scores for consistency.

![**Figure:** BERTScore computes contextual similarity between candidate and reference tokens using BERT embeddings and cosine similarity, followed by precision, recall, and F1 aggregation. *(Source: Zhang et al. (2020))*](images/bert-score.png){fig-align="center" width="800"}


**Example:**  
- Reference: “The weather is cold today”  
- Candidate: “It is freezing today”  

BLEU would assign a low score due to low word overlap. BERTScore correctly identifies that “cold” and “freezing” are semantically similar and aligns “it” with “weather,” producing a high score.


#### BLEURT: Learned Quality Estimation

BLEURT offers a learning-based alternative to similarity-based metrics like BERTScore. Instead of relying on heuristic rules, BLEURT is trained to predict human quality judgments directly using fine-tuned BERT models.

1. **Pre-training:** Initialized with BERT and trained on synthetically modified sentence pairs. For example, original sentences and their noisy or paraphrased versions. This helps the model learn how edits, errors, or rewordings affect meaning. 
2. **Fine-tuning:** BLEURT is fine-tuned using real sentence pairs labeled by humans in shared tasks. This allows it to learn what humans consider high- or low-quality responses. 
3. **(Optional) Application-Specific Fine-tuning:** For custom use cases (e.g., legal, medical, or customer support), BLEURT can be further fine-tuned using domain-specific human feedback, improving alignment with task-specific quality standards.

Given a candidate and reference sentence, BLEURT returns a scalar score, typically between -1 and 1, reflecting how closely the candidate aligns with human expectations in meaning, fluency, and grammatical correctness.

![**Figure:** BLEURT learns to predict human ratings by pre-training on synthetic sentence pairs and fine-tuning on labeled examples, optionally adapting to task-specific human feedback. *(Source: Sellam et al. (2020))*](images/bleurt.png)

**Example:**         
- Reference: “The weather is cold today”           
- Candidate: “It is freezing today”

Despite limited word overlap, BLEURT assigns a high score, recognizing semantic equivalence and fluent expression — something traditional metrics might miss.


#### Summary: When to Use Which?

| Metric     | Strengths                                         | Limitations                                   |
|------------|---------------------------------------------------|-----------------------------------------------|
| **BERTScore** | Fast, interpretable; captures token-level semantics | Relies on heuristic alignment; less fluency-aware |
| **BLEURT**    | Trained on human ratings; captures fluency and variation | More computationally intensive; less transparent |

Both metrics represent a shift toward evaluation methods that better reflect **semantic correctness**, **paraphrasing**, and **generation quality**. BERTScore excels in **semantic alignment**, while BLEURT better captures **fluency** and **natural language variation**.


### Human Evaluations

Human evaluation remains the **gold standard**, especially for open-ended tasks.

**What they measure:**

- Fluency
- Coherence
- Factual Accuracy
- Commonsense
- Harmlessness
- Style and grammar
- Redundancy

**Methods:**

- Likert-scale ratings  
- Pairwise comparisons  
- Ranking systems  
- Multi-annotator voting (for inter-rater reliability)

> **Note:** Never compare scores across studies—evaluation methods, prompts, and annotators vary.

**Challenges:**

- **Expensive** and **time-consuming**
- **Inter-annotator disagreement**: Even simple tasks had only 67% agreement.
- **Intra-annotator inconsistency**
- **Crowdworker incentives**: Speed over care, favoring longer outputs.
- **Precision-only**: Humans can't judge what _could_ have been generated, only what _was_.
- **Reproducibility crisis**: Only ~5% of NLP papers between 2015–2020 documented enough to replicate human evals.


### Reference-Free Evaluation

Instead of comparing to human-written references, these approaches judge quality **without a gold standard**.

**Approaches:**

- **Traditional:** Fine-tune BERT to predict quality scores.
- **Modern (LLM-based):** Use GPT-4 to evaluate generated outputs.
  - Benchmarks: `AlpacaEval`, `MT-Bench`, `Chatbot Arena`
  - Surprisingly strong alignment with human judgments
  - Fast and scalable—100× cheaper than human eval

> GPT-4 shows **higher agreement with humans than humans show with each other** due to low variance.


### Side-by-Side Comparison (Arena-style)

Used to compare models directly, especially for chatbots or instruction-following tasks.

**Example:**

- Ask the same question to two models.
- Let humans (or GPT-4) choose the better output.
- Use **Elo ratings** (like chess) to build a leaderboard.

**Tools:**

- **Chatbot Arena**: Crowdsourced, ongoing, 200K+ votes
- **HELM**, **Open LLM Leaderboard**, **VLLM Eval Leaderboard**

**Limitations:**

- User-generated prompts may not be representative.
- Early-stage models don't get enough votes.
- Not usable for training or development, only end-product evaluation.


### Evaluation Pitfalls and Biases

**Reference Quality Matters:**

- Bad references = bad evaluation.
- Studies show ROUGE is uncorrelated with human scores unless references are written by experts.

**Spurious Correlations:**

- **Length bias**: ~70% preference for longer outputs
- **List bias**: Preference for bullet-style responses
- **Position bias**: Left vs right positioning in comparison
- **Self-bias**: GPT-4 mildly favors its own outputs


### Broader Challenges

- **Consistency Issues**: Different prompt styles or decoding methods yield very different results (e.g., MMLU scores varied by 15%).
- **Contamination**: Models trained on test data (e.g., GPT-4 aces pre-2021 Codeforces, flunks post-2021).
- **Overfitting**: Datasets saturate quickly; benchmark usefulness decays.
- **Monoculture**: 70% of ACL 2021 papers were English-only; many ignore bias or efficiency.
- **Single Metric Fallacy**: Oversimplifies model performance. E.g., accuracy alone misses bias, latency, and fairness tradeoffs.


### Key Takeaways

- Open-ended evaluation is **complex**—no one-size-fits-all.
- Traditional metrics (BLEU, ROUGE) are useful but **insufficient**.
- Model-based metrics and human evals are better but expensive or slow.
- LLM-based eval is promising—**fast**, **scalable**, **well-correlated**.
- Always **manually inspect outputs**—numbers alone can be misleading.




## References & Further Reading

[1] Wang, A., et al. (2019). SuperGLUE: A Stickier Benchmark for General‑Purpose Language Understanding Systems. https://arxiv.org/pdf/1905.00537

[2] Hendrycks, D., et al. (2020). Measuring Massive Multitask Language Understanding (MMLU). https://arxiv.org/pdf/2009.03300

[3] Papineni, K., et al. (2002). BLEU: A Method for Automatic Evaluation of Machine Translation. https://aclanthology.org/P02-1040

[4] Lin, C.-Y. (2004). ROUGE: A Package for Automatic Evaluation of Summaries. https://aclanthology.org/W04-1013

[5] Chang, T., et al. (2020). *BERTSCORE: Evaluating Text Generation With BERT*. https://arxiv.org/pdf/1904.09675

[6] Sellam, T., et al. (2020). *BLEURT: Learning Robust Metrics for Text Generation*. https://arxiv.org/pdf/2004.04696


[16] Huyen, C. (2024). *AI Engineering: Building Applications with Foundation Models*. O’Reilly Media.  

[17] Alammar, J., & Grootendorst, M. (2023). *Hands-On Large Language Models: Language Understanding and Generation*. O’Reilly Media. 


 