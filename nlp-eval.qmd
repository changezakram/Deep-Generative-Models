---
title: "NLP Model Evaluation"
format: html
---

## Introduction:

Evaluating NLP models presents distinct challenges for both **closed-ended** and **open-ended** tasks, and these have become more complex with the rise of foundation models.

- **Closed-ended tasks** (e.g., classification, named entity recognition) have clearly defined outputs and ground-truth labels. While traditional ML evaluation methods like accuracy, precision, recall, and F1-score are applicable here, they still face issues such as task ambiguity, annotation bias, and benchmark saturation.
- **Open-ended tasks** (e.g., summarization, question answering, code generation, dialogue) are more difficult to evaluate because there is often no single correct output. The outputs are probabilistic and highly variable, which makes defining and applying evaluation metrics far more complex.

Assessing the output of more intelligent models‚Äîespecially on open-ended tasks‚Äîis particularly difficult. While it‚Äôs easy to spot a math error in a first-grader‚Äôs homework, judging whether an AI-generated summary is factually accurate may require reading and understanding the entire source text.

Traditional ML evaluation assumes relatively stable output spaces and ground truth labels. However, foundation models produce diverse and nuanced outputs that challenge this assumption. As a result, even closed-ended tasks must now contend with emerging evaluation issues such as:

- **Evaluation scalability**: Automated metrics work well, but can overlook nuance.
- **Annotation bias**: Even in classification tasks, human-created datasets often have spurious correlations.
- **Benchmark saturation**: As models improve, they quickly saturate existing benchmarks, giving a false sense of progress.
- **Undersupported evaluation infrastructure**: The *AI Engineering* book notes that evaluation lags behind other areas like model training, optimization, and deployment tooling.

Despite these challenges, closed-ended evaluation remains a valuable starting point. Its well-constrained answer space allows use of established metrics like accuracy, precision, recall, and F1-score to measure progress systematically.

---

## Closed-Ended Tasks

- Limited number of potential answers
- Often only one or a few correct answers
- Enables automatic and scalable evaluation, making them well-suited for benchmarking

### Common Closed-Ended Tasks & Benchmarks

| Task Type                    | Description                                             | Popular Benchmarks             |
|-----------------------------|---------------------------------------------------------|--------------------------------|
| **Sentiment Analysis**       | Classify sentiment as Positive, Negative, Neutral       | SST, IMDB, Yelp                |
| **Textual Entailment**       | Determine if a hypothesis logically follows from a premise | SNLI, RTE                    |
| **Named Entity Recognition** | Identify named entities in text                         | CoNLL-2003                     |
| **Part-of-Speech Tagging**   | Assign parts of speech to each word                     | PTB                            |
| **Coreference Resolution**   | Determine if pronouns refer to the same entity          | WSC                            |
| **Question Answering**       | Extract a plausible answer from a given passage         | SQuAD2                         |

**Example: Sentiment Analysis:**  

- **Text**: ‚ÄúRead the book, forget the movie!‚Äù  
- **Label**: Negative

**Example: Entailment:**  

- **Premise**: ‚ÄúA soccer game with multiple males playing.‚Äù  
- **Hypothesis**: ‚ÄúSome men are playing sport.‚Äù  
- **Label**: Entailment


### Multi-Task Benchmark: **SuperGLUE**

SuperGLUE is a widely used **closed-ended multi-task benchmark** designed to test **general language understanding**. It evaluates performance across diverse NLP tasks.

**Tasks in SuperGLUE:**

- **BoolQ, MultiRC**: Reading comprehension  
- **CB, RTE**: Natural language inference (entailment)  
- **COPA**: Causal reasoning (cause and effect)  
- **ReCoRD**: Reading comprehension with commonsense reasoning  
- **WiC**: Word meaning in context  
- **WSC**: Coreference resolution

**Leaderboard Highlights (v2.0):**

- **Top models**: Vega v2, ST-MoE-32B, ERNIE, PaLM 540B, T5  
- **Metrics used**: Accuracy, F1 score, Exact Match, Gender Parity, etc.


### Challenges in Closed-Ended Evaluation

1. **Choosing the Right Metric**  

Choosing the right evaluation metric is not trivial. Different tasks and datasets require different performance measures. For instance, accuracy may seem intuitive but can be misleading on imbalanced datasets. A model might achieve high accuracy simply by predicting the majority class. On the other hand, recall and precision offer complementary perspectives: one focusing on completeness (recall) and the other on correctness (precision). The choice of metric directly impacts how model performance is interpreted, compared, and optimized. In multi-class or skewed-distribution scenarios, using a single metric without understanding its limitations can mask critical weaknesses.

::: {.callout-tip title="Common Evaluation Metrics"}

- **Accuracy**: The proportion of correct predictions out of all predictions. Suitable for balanced datasets.
- **Precision**: The proportion of true positives out of all predicted positives. High precision means fewer false positives.
- **Recall**: The proportion of true positives out of all actual positives. High recall means fewer false negatives.
- **F1-Score**: The harmonic mean of precision and recall. Useful when there is an uneven class distribution.
- **ROC AUC**: Measures the ability of the model to distinguish between classes across thresholds. Higher AUC indicates better performance.

:::

2. **Aggregating Metrics** 
 
   - How to combine across multiple tasks or sub-tasks

3. **Label Quality**  

   - Are the gold-standard labels always reliable?  
   - Are there ambiguities in annotation?

4. **Spurious Correlations**  

   - Models might exploit dataset artifacts rather than genuinely learning the task  
   - **Example** (SNLI):  
     - Premise: ‚ÄúThe economy could be still better.‚Äù  
     - Hypothesis: ‚ÄúThe economy has *never* been better.‚Äù  
     - Model might infer contradiction simply due to the word *never* rather than actual reasoning

---


## Open-Ended Text Generation

Open-ended generation models, such as large language models (LLMs), are capable of producing free-form outputs like summaries, translations, stories, or answers to instructions. Unlike traditional models that produce a fixed label or number, these models generate entire sequences of text‚Äîoften with multiple plausible responses for a single input.

This flexibility makes them powerful but also difficult to evaluate. There is rarely a single "correct" output, and even seemingly unrelated completions may still be valid. For example, a user prompt like *"Tell me something interesting about the moon"* could lead to many diverse, accurate, and coherent answers. Evaluating such outputs requires more than checking against a reference‚Äîit demands assessing coherence, fluency, relevance, factual accuracy, and semantic alignment.

As a result, evaluation of open-ended generation must go beyond traditional metrics and adopt a more nuanced, multi-faceted approach.


### Types of Evaluation Methods

There are three broad classes of evaluation methods for text generation:

- **Content Overlap Metrics** ‚Äì compare model outputs against human-written references based on lexical or n-gram overlap.
- **Model-Based Metrics** ‚Äì assess semantic similarity using pretrained embedding models (e.g., BERTScore, BLEURT).
- **Human Evaluations** ‚Äì involve direct human judgment of qualities such as fluency, coherence, and factual correctness.

**Content Overlap Metrics:**

These methods compute similarity based on surface-level word overlap between the generated text and a reference.

- Fast and widely used but limited in capturing meaning.
- Common metrics include **BLEU**, **ROUGE**, **METEOR**, **CIDEr**.
- Often reported for tasks like summarization and translation, despite known limitations.

> üîç *n-gram metrics have no concept of meaning‚Äîthey fail when different words express the same idea.*


**Model-Based Metrics**

These metrics leverage learned representations from pre-trained language models to evaluate semantic similarity:

- Compute similarity in embedding space between reference and generated outputs.
- More robust to paraphrasing and lexical variation.

Two popular types:

**BERTScore**
 
- Uses contextual embeddings from BERT and cosine similarity to compare word pairs.
- Captures semantic relationships between words.  
*(Zhang et al., 2020)*

**BLEURT** 
 
- Fine-tuned regression model based on BERT.
- Outputs a score indicating grammar and meaning similarity with reference text.  
*(Sellam et al., 2020)*

**Reference Quality Matters**

Evaluation scores are only as good as the reference they compare against.

- A flawed reference can mislead metrics like ROUGE.
- Expert-written references lead to better correlation with human judgments of faithfulness.


---

## References & Further Reading



[16] Huyen, C. (2024). *AI Engineering: Building Applications with Foundation Models*. O‚ÄôReilly Media.  

[17] Alammar, J., & Grootendorst, M. (2023). *Hands-On Large Language Models: Language Understanding and Generation*. O‚ÄôReilly Media. 


 