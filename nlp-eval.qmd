---
title: "NLP Model Evaluation"
format: html
---

## Introduction:

**What is NLP Evaluation?**

NLP evaluation measures how well language models actually work. It answers basic questions: Is this model good enough? Can we trust it with real users?

But here's the challengeâ€”testing language models isn't like testing regular software. When code breaks, it's obvious. When a language model fails, it might write something that sounds perfect but is completely wrong. A model might be 92% accurate overall but fail every time it sees sarcasm. This is what makes NLP evaluation so tricky: we're measuring how well computers understand the messy, complex world of human language.

**Why It Matters Now More Than Ever**

In 2023 alone, we saw an AI chatbot fail to recognize suicide warning signs (leading to a death), lawyers submit fake AI-generated cases to courts, and Air Canada forced to honor a refund policy its bot invented. As Chip Huyen warns: "The more AI is used, the more opportunity there is for catastrophic failure."

**The paradox** â€” The smarter our models get, the harder they become to evaluate. It's easy to check a kid's math homework, but verifying if an AI's medical advice is accurate requires medical expertise. We need good evaluation to build better models, but we need expertise to do good evaluation.

**Two Types of Tasks, Two Different Challenges**

- **Closed-ended tasks** have clear right answers (Is this email spam? What's the sentiment?). We can use traditional metrics like accuracy and precision, but even these "simple" tasks suffer from shortcuts, dataset problems, and human labeling errors.

- **Open-ended tasks** have no single right answer (Write a summary, translate this text, answer this question). Traditional metrics completely fail here. Word-matching might say "Heck no!" is similar to "Heck yes!" because they share words.

**What's Ahead**

This overview covers how to evaluate both closed and open-ended tasks, why current methods fail and what's replacing them, major problems like contamination and bias, and practical solutions for real-world applications.

---

## Closed-Ended Tasks

Closed-ended tasks are characterized by a limited set of correct answersâ€”often just one. This constraint enables automatic, objective evaluation and makes these tasks ideal for benchmarking.

While closed-ended tasks offer clear and objective evaluation metrics, they still present meaningful challenges in dataset construction, evaluation design, and metric selection. Their well-defined output space enables direct comparison across models, but can also obscure subtle model weaknessesâ€”such as shortcut learning or reliance on spurious correlations.

Common metrics such as accuracy, precision, recall, and F1 score are frequently used to evaluate these tasks (see Section 2.3 for a quick reference).


| **Task Type**             | **Description**                                           | **Example**                                                                                                                                     | **Popular Benchmarks**     | **Common Pitfalls**                                        |
|---------------------------|-----------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------|-------------------------------------------------------------|
| **Sentiment Analysis**    | Classify emotional tone (positive/negative/neutral)       | "Read the book, forget the movie!" â†’ **Negative**                                                                                               | SST, IMDB, Yelp             | Fails on sarcasm, irony, cultural context                  |
| **Textual Entailment**    | Does sentence B logically follow from sentence A?         | *Premise:* "A soccer game with multiple males playing"<br>*Hypothesis:* "Some men are playing sport" â†’ **Entailment**                            | SNLI, MultiNLI, RTE         | Shortcut learning (e.g., keyword overlap); see SNLI issues |
| **Named Entity Recognition (NER)** | Identify and classify proper nouns                     | "Apple released the iPhone." â†’ **[Appleâ€“ORG]**, **[iPhoneâ€“PRODUCT]**                                                                             | CoNLL-2003                  | Ambiguity: Apple (fruit vs company)                        |
| **Part-of-Speech Tagging**| Assign grammatical categories to words                    | "The quick brown fox" â†’ **[Theâ€“DET]**, **[quickâ€“ADJ]**, **[brownâ€“ADJ]**, **[foxâ€“NOUN]**                                                           | Penn Treebank (PTB)         | Often used as foundation for parsing & other tasks         |
| **Coreference Resolution**| Determine pronoun references                              | "Mark told Pete lies about himself. He should have been more truthful." â†’ **"He" = Mark**                                                        | WSC, OntoNotes              | Requires deep context or world knowledge                   |
| **Question Answering**    | Extract answers from passage                              | *Context:* "The ESA was passed in 1973."<br>*Q:* "When was it passed?" â†’ **1973**                                                                | SQuAD, SQuAD 2.0            | Models memorize patterns or position, not true reasoning   |

> ðŸ’¡ *These tasks often serve as building blocks for more complex NLP pipelines.*

These individual tasks can also be grouped into multi-task benchmarks that evaluate general language understanding across a range of closed-ended challenges. **SuperGLUE** is one of the most prominent such benchmarks.

### Multi-Task Benchmark: **SuperGLUE**

While the previous section outlined individual closed-ended tasks, real-world evaluation often demands a unified benchmark that spans multiple task types. **SuperGLUE** is a widely adopted **closed-ended multi-task benchmark** created to evaluate broad general language understanding. It builds on its predecessor (GLUE) with harder tasks, more robust metrics, and an emphasis on reasoning.

SuperGLUE combines a diverse set of tasksâ€”ranging from entailment and coreference resolution to causal reasoning and word sense disambiguationâ€”designed to holistically assess a modelâ€™s linguistic and reasoning capabilities across multiple dimensions.

**Tasks in SuperGLUE:**

| Task          | Description                                               | Type                        |
|---------------|-----------------------------------------------------------|-----------------------------|
| **BoolQ**, **MultiRC** | Reading comprehension                              | QA / Inference              |
| **CB**, **RTE**        | Natural language inference                         | Entailment                  |
| **COPA**              | Causal reasoning (cause/effect)                    | Reasoning                   |
| **ReCoRD**            | Reading comprehension with commonsense reasoning  | QA / Commonsense            |
| **WiC**               | Word meaning in context                            | Word Sense Disambiguation   |
| **WSC**               | Coreference resolution                             | Coreference                 |

Together, these tasks go beyond surface-level predictionâ€”testing abilities like logical reasoning, commonsense application, coreference tracking, and contextual understanding.

**Leaderboard Highlights (v2.0):**

To measure and compare model performance on SuperGLUE, an official leaderboard tracks results across all tasks using standardized metrics. The **v2.0 leaderboard** showcases todayâ€™s most advanced modelsâ€”ranging from parameter-efficient **Mixture of Experts (MoE)** systems to massive transformer-based architecturesâ€”offering a clear snapshot of the state of the art in general language understanding.

- **Top models**: Vega v2, ST-MoE-32B, ERNIE, PaLM 540B, T5  
- **Metrics used**: Accuracy, F1 score, Exact Match, Gender Parity, etc.

The leaderboard emphasizes **balanced generalization**, rewarding models that perform consistently well across diverse task typesâ€”not just a few. This makes it a reliable benchmark for tracking progress toward broadly capable language models.


### Domain-Rich Multi-Task Benchmark: MMLU

While SuperGLUE focuses on general linguistic reasoning, **Massive Multitask Language Understanding (MMLU)** shifts the spotlight to **domain knowledge**. It has rapidly become the *de facto* benchmark for evaluating a modelâ€™s grasp of academic and professional subjectsâ€”effectively acting as a proxy for general intelligence in many LLM leaderboards.

**What is MMLU?**

- **57 subjects** spanning elementary math, anatomy, law, philosophy, computer science, US history, and more  
- **Multiple-choice format** (A, B, C, D), with ~100 questions per subject  
- **Balanced question design** that mimics real academic tests and professional licensing exams  
- **Closed-book evaluation** (no retrieval), testing what the model has internalized from pretraining  

MMLU has emerged as a **standardized benchmark** for evaluating foundational knowledge across domains. It allows for **direct accuracy-based comparisons** between models of different sizes and architectures. Performance gains have been dramaticâ€”rising from ~**25% (random guessing)** to over **90% accuracy** in just four years.

> **Note**: MMLU is often treated as a shortcut for measuring "general intelligence," but that can be misleading. What it really tests is how well a model can recall facts and recognize patternsâ€”not necessarily how well it can reason or think abstractly. Weâ€™ll explore these limitations later.

**MMLU in Modern LLM Leaderboards**

Modern LLMsâ€”including GPT-4, Claude 3, Gemini, LLaMA 3, and PaLMâ€”routinely report MMLU scores as a primary metric. As with SuperGLUE, MMLU supports **multi-subject generalization**, but with a stronger emphasis on **world knowledge** rather than linguistic nuance.


**SuperGLUE vs MMLU: A Comparison**

| Aspect | **SuperGLUE** | **MMLU** |
|--------|----------------|----------|
| **Focus** | Language reasoning | Factual subject knowledge |
| **Format** | Varied NLP tasks | Multiple choice |
| **Tasks / Subjects** | 8 tasks | 57 subjects |
| **Primary Skill Tested** | Inference, disambiguation, coreference | Retained domain knowledge |
| **Metric** | Accuracy, F1, etc. | Accuracy only |

> While both are multi-task benchmarks, they evaluate very different capabilitiesâ€”SuperGLUE emphasizes reasoning and understanding, whereas MMLU stresses factual recall across disciplines.


### Challenges in Closed-Ended Evaluation

1. **Metric Selection:** Selecting an appropriate evaluation metric is critical but non-trivial. Different tasks and dataset characteristics (e.g., class imbalance, label skew) demand different metrics. For instance, accuracy may look impressive but can be misleading in imbalanced datasetsâ€”it might just reflect majority-class predictions.

    Instead, precision and recall offer complementary views: precision focuses on correctness (fewer false positives), while recall emphasizes completeness (fewer false negatives).

    The choice of metric shapes how performance is interpreted, compared, and optimized. In multi-class or skewed settings, relying on a single metric may obscure weaknesses.

::: {.callout-note title="Quick Reference: Common Evaluation Metrics"}

- **Accuracy**: $(TP + TN) / (TP + TN + FP + FN)$  
  Best for balanced datasets with equal class distribution.

- **Precision**: $TP / (TP + FP)$  
  High precision means fewer false positives.

- **Recall**: $TP / (TP + FN)$  
  High recall means fewer false negatives.

- **F1 Score**: $2 \cdot (Precision \cdot Recall) / (Precision + Recall)$  
  Useful when you need a balance between precision and recall.

- **Exact Match (EM)**: Binary match with reference output.  
  Best for QA and structured prediction.

- **AUC-ROC**: Measures discrimination across thresholds.  
  Great for binary classification tasks.

:::

2. **Metric Aggregation:** In multi-task benchmarks like SuperGLUE, aggregating scores across tasks is not as simple as averaging. Different tasks often use different metrics (e.g., accuracy, F1 score, correlation), each with its own scale and interpretation. Some metrics reward lower values (e.g., loss), while others reward higher ones (e.g., accuracy). Averaging without normalization or weighting can obscure true performanceâ€”especially on harder tasks or those with imbalanced label distributions.

3. **Label Quality:** Evaluation reliability depends on the consistency and accuracy of ground-truth labels. Poorly defined labels or subjective task setups can introduce ambiguity, reducing the quality of both training and evaluation signals.

4. **Spurious Correlations:** Models sometimes rely on patterns or artifacts in the data that donâ€™t reflect real understanding. This can lead to seemingly correct answers based on shortcuts, not reasoning.

   - **Example** (SNLI):  
     - Premise: â€œThe economy could be still better.â€  
     - Hypothesis: â€œThe economy has *never* been better.â€  
     - Model might infer contradiction simply due to the word *never* rather than actual reasoning

5. **Annotation Artifacts and Dataset Bias**: Evaluation can be skewed by patterns unintentionally introduced during dataset construction. For example, in some QA datasets, answers are often placed in the first few sentences of the context. Models may learn to exploit this structural bias rather than understanding the full passage. Similarly, crowd-sourced datasets like SNLI may contain stylistic cues that correlate with specific labels (e.g., longer or more formal sentences labeled as "entailment"). These artifacts can inflate evaluation scores without reflecting true model understanding.

6. **Adversarial Robustness**: Small input changesâ€”such as adding, removing, or rephrasing a few wordsâ€”can cause models to make incorrect predictions, even when the meaning stays the same. This sensitivity to minor perturbations highlights a weakness in model generalization and exposes brittle behavior. Robust evaluation must test whether models maintain performance under such adversarial or paraphrased variations.

This highlights that even well-defined tasks require evaluation beyond raw accuracy to capture true model behavior and ensure robustness.

---


## Open-Ended Text Generation

Open-ended generation models, such as large language models (LLMs), can produce diverse free-form outputs like summaries, translations, stories, or answers to instructions. Unlike classification models that generate a fixed label or number, open-ended models return entire sequences of textâ€”often with many valid responses for the same input.

This flexibility makes them powerful but difficult to evaluate. There's rarely a single â€œcorrectâ€ output, and even seemingly unrelated completions may still be valid. For example, the prompt _â€œTell me something interesting about the moonâ€_ could yield many accurate and fluent yet different answers.

Evaluating such responses goes beyond simple reference matchingâ€”it requires assessing coherence, fluency, relevance, factual accuracy, style, and semantic alignment. As a result, evaluation of open-ended generation must adopt a more nuanced, multi-faceted approach.

The following sections outline key evaluation methods and challenges.


### Content Overlap Metrics

These methods compute similarity based on surface-level word overlap between the generated text and a human-written reference. They are fast and widely used but fail to capture meaning or paraphrased content.

- **Common metrics:** `BLEU`, `ROUGE`, `METEOR`, `CIDEr`.
- BLEU emphasizes precision; ROUGE emphasizes recall.
- Often used in summarization and translation tasks.

> **Limitation:** N-gram metrics have no concept of meaningâ€”they fail when different words express the same idea.

**Illustrative Example:**

Reference: "Heck yes!"  
Generated â†’ BLEU score:

	- "Yes!" â†’ 67%
	- "You know it!" â†’ low
	- "Yup" â†’ 0% (but semantically correct!)
	- "Heck no!" â†’ 67% (opposite meaning!)

These metrics penalize valid rephrasings and may reward word overlap even when meaning is incorrect.


### Model-Based Metrics

These metrics use learned representations from pre-trained language models to compute **semantic similarity** in embedding spaceâ€”making them more robust to lexical variation.

**Why they matter:**
They go beyond word matching and better capture meaning between generated and reference texts.

**Popular approaches:**

- **BERTScore**  
  - Compares contextual embeddings from BERT using cosine similarity.  
  - Captures pairwise semantic relations.  
  - *Zhang et al., 2020*
- **BLEURT**  
  - Fine-tuned BERT on human rating datasets.  
  - Outputs a single score reflecting grammar, fluency, and semantic accuracy.  
  - *Sellam et al., 2020*

These metrics have become popular in evaluating paraphrased or abstractive generation tasks.

### Human Evaluations

Human evaluation remains the **gold standard**, especially for open-ended tasks.

**What they measure:**

- Fluency
- Coherence
- Factual Accuracy
- Commonsense
- Harmlessness
- Style and grammar
- Redundancy

**Methods:**

- Likert-scale ratings  
- Pairwise comparisons  
- Ranking systems  
- Multi-annotator voting (for inter-rater reliability)

> **Note:** Never compare scores across studiesâ€”evaluation methods, prompts, and annotators vary.

**Challenges:**

- **Expensive** and **time-consuming**
- **Inter-annotator disagreement**: Even simple tasks had only 67% agreement.
- **Intra-annotator inconsistency**
- **Crowdworker incentives**: Speed over care, favoring longer outputs.
- **Precision-only**: Humans can't judge what _could_ have been generated, only what _was_.
- **Reproducibility crisis**: Only ~5% of NLP papers between 2015â€“2020 documented enough to replicate human evals.


### Reference-Free Evaluation

Instead of comparing to human-written references, these approaches judge quality **without a gold standard**.

**Approaches:**

- **Traditional:** Fine-tune BERT to predict quality scores.
- **Modern (LLM-based):** Use GPT-4 to evaluate generated outputs.
  - Benchmarks: `AlpacaEval`, `MT-Bench`, `Chatbot Arena`
  - Surprisingly strong alignment with human judgments
  - Fast and scalableâ€”100Ã— cheaper than human eval

> GPT-4 shows **higher agreement with humans than humans show with each other** due to low variance.


### Side-by-Side Comparison (Arena-style)

Used to compare models directly, especially for chatbots or instruction-following tasks.

**Example:**

- Ask the same question to two models.
- Let humans (or GPT-4) choose the better output.
- Use **Elo ratings** (like chess) to build a leaderboard.

**Tools:**

- **Chatbot Arena**: Crowdsourced, ongoing, 200K+ votes
- **HELM**, **Open LLM Leaderboard**, **VLLM Eval Leaderboard**

**Limitations:**

- User-generated prompts may not be representative.
- Early-stage models don't get enough votes.
- Not usable for training or development, only end-product evaluation.


### Evaluation Pitfalls and Biases

**Reference Quality Matters:**

- Bad references = bad evaluation.
- Studies show ROUGE is uncorrelated with human scores unless references are written by experts.

**Spurious Correlations:**

- **Length bias**: ~70% preference for longer outputs
- **List bias**: Preference for bullet-style responses
- **Position bias**: Left vs right positioning in comparison
- **Self-bias**: GPT-4 mildly favors its own outputs


### Broader Challenges

- **Consistency Issues**: Different prompt styles or decoding methods yield very different results (e.g., MMLU scores varied by 15%).
- **Contamination**: Models trained on test data (e.g., GPT-4 aces pre-2021 Codeforces, flunks post-2021).
- **Overfitting**: Datasets saturate quickly; benchmark usefulness decays.
- **Monoculture**: 70% of ACL 2021 papers were English-only; many ignore bias or efficiency.
- **Single Metric Fallacy**: Oversimplifies model performance. E.g., accuracy alone misses bias, latency, and fairness tradeoffs.


### Key Takeaways

- Open-ended evaluation is **complex**â€”no one-size-fits-all.
- Traditional metrics (BLEU, ROUGE) are useful but **insufficient**.
- Model-based metrics and human evals are better but expensive or slow.
- LLM-based eval is promisingâ€”**fast**, **scalable**, **well-correlated**.
- Always **manually inspect outputs**â€”numbers alone can be misleading.



## References & Further Reading



[16] Huyen, C. (2024). *AI Engineering: Building Applications with Foundation Models*. Oâ€™Reilly Media.  

[17] Alammar, J., & Grootendorst, M. (2023). *Hands-On Large Language Models: Language Understanding and Generation*. Oâ€™Reilly Media. 


 