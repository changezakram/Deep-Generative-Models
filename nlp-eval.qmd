---
title: "NLP Model Evaluation"
format: html
---

## Introduction:

NLP evaluation measures how well language models actually work. It answers basic questions: Is this model good enough? Can we trust it with real users? But here's the challenge—testing language models isn't like testing regular software. When code breaks, it's obvious. When a language model fails, it might write something that sounds perfect but is completely wrong. A model might be 92% accurate overall but fail every time it sees sarcasm. This is what makes NLP evaluation so tricky: we're measuring how well computers understand the messy, complex world of human language.

**Why It Matters Now More Than Ever**

In 2023 alone, we saw an AI chatbot fail to recognize suicide warning signs (leading to a death), lawyers submit fake AI-generated cases to courts, and Air Canada forced to honor a refund policy its bot invented. As Chip Huyen warns: "The more AI is used, the more opportunity there is for catastrophic failure." The smarter our models get, the harder they become to evaluate. It's easy to check a kid's math homework, but verifying if an AI's medical advice is accurate requires medical expertise. We need good evaluation to build better models, but we need expertise to do good evaluation.

**Two Types of Tasks, Two Different Challenges**

- **Closed-ended tasks** have clear right answers (Is this email spam? What's the sentiment?). We can use traditional metrics like accuracy and precision, but even these "simple" tasks suffer from shortcuts, dataset problems, and human labeling errors.

- **Open-ended tasks** have no single right answer (Write a summary, translate this text, answer this question). Traditional metrics completely fail here. Word-matching might say "Heck no!" is similar to "Heck yes!" because they share words.

**What's Ahead**

This overview covers how to evaluate both closed and open-ended tasks, why current methods fail and what's replacing them, major problems like contamination and bias, and practical solutions for real-world applications.

---

## Closed-Ended Tasks

Closed-ended tasks are widely used in NLP evaluation because they provide clear right or wrong answers. Since the model's output is limited to a small set of predefined choices—often fewer than ten—they make it easier to compare models using standard metrics like accuracy, precision, recall, and F1-score. This structure allows for objective evaluation, consistent benchmarking, and easier tracking of progress over time.

**Bounded Output Space:** Unlike text generation where models can produce any sequence of tokens, closed-ended tasks constrain outputs to predefined categories or structured formats.    
**Objective Evaluation:** Success can be measured automatically using established metrics from classical machine learning—accuracy, precision, recall, and F1-score.    
**Reproducible Benchmarks:** Standard datasets enable fair comparison across models and over time.    
**Systematic Progress Tracking:** Clear metrics allow the field to monitor advancement and identify when models have genuinely improved.

While this constraint makes evaluation more straightforward than open-ended generation, closed-ended tasks still present significant challenges that can mislead researchers and practitioners. Understanding these problems is important because closed-ended evaluation is often the first test of model quality, affecting research focus and decisions about which models to deploy.

### Evaluation Metrics for Closed-Ended Tasks

Understanding how to measure performance is fundamental to closed-ended evaluation. Different metrics serve different purposes and can lead to very different conclusions about model quality. Following metrics evaluate performance on individual labeled tasks:

- **Accuracy:** Percentage of correct predictions (simple but can be misleading with imbalanced data).    
- **Precision:** Of all positive predictions, how many were correct? (important when false alarms are costly).    
- **Recall:** Of all actual positive cases, how many were found? (important when missing cases is costly).    
- **F1-Score:** Harmonic mean of precision and recall (balances both concerns).    

These metrics are typically applied to standardized datasets called **benchmarks**, which we'll explore in the next section. Like standardized tests for AI, benchmarks enable fair comparison between models from different research groups and track progress over time.

### Types of Closed-Ended Tasks


| **Task Type**             | **Description**                                           | **Example**                                                                                                                                     | **Popular Benchmarks**     | **Common Pitfalls**                                        |
|---------------------------|-----------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------|-------------------------------------------------------------|
| **Sentiment Analysis**    | Classify emotional tone (positive/negative/neutral)       | "Read the book, forget the movie!" → **Negative**                                                                                               | SST, IMDB, Yelp             | Fails on sarcasm, irony, cultural context                  |
| **Textual Entailment**    | Does sentence B logically follow from sentence A?         | *Premise:* "A soccer game with multiple males playing"<br>*Hypothesis:* "Some men are playing sport" → **Entailment**                            | SNLI, MultiNLI, RTE         | Shortcut learning (e.g., keyword overlap); see SNLI issues |
| **Named Entity Recognition (NER)** | Identify and classify proper nouns                     | "Apple released the iPhone." → **[Apple–ORG]**, **[iPhone–PRODUCT]**                                                                             | CoNLL-2003                  | Ambiguity: Apple (fruit vs company)                        |
| **Part-of-Speech Tagging**| Assign grammatical categories to words                    | "The quick brown fox" → **[The–DET]**, **[quick–ADJ]**, **[brown–ADJ]**, **[fox–NOUN]**                                                           | Penn Treebank (PTB)         | Often used as foundation for parsing & other tasks         |
| **Coreference Resolution**| Determine pronoun references                              | "Mark told Pete lies about himself. He should have been more truthful." → **"He" = Mark**                                                        | WSC, OntoNotes              | Requires deep context or world knowledge                   |
| **Question Answering**    | Extract answers from passage                              | *Context:* "The ESA was passed in 1973."<br>*Q:* "When was it passed?" → **1973**                                                                | SQuAD, SQuAD 2.0            | Models memorize patterns or position, not true reasoning   |

These individual tasks can also be grouped into multi-task benchmarks that evaluate general language understanding across a range of closed-ended challenges. **SuperGLUE** is one of the most prominent such benchmarks.

### Multi-Task Benchmark: **SuperGLUE**

While the previous section outlined individual closed-ended tasks, real-world evaluation often demands a unified benchmark that spans multiple task types. **SuperGLUE** is a widely adopted **closed-ended multi-task benchmark** created to evaluate broad general language understanding. It builds on its predecessor (GLUE) with harder tasks, more robust metrics, and an emphasis on reasoning.

SuperGLUE combines a diverse set of tasks—ranging from entailment and coreference resolution to causal reasoning and word sense disambiguation—designed to holistically assess a model’s linguistic and reasoning capabilities across multiple dimensions.

**Tasks in SuperGLUE:**

| Task          | Description                                               | Type                        |
|---------------|-----------------------------------------------------------|-----------------------------|
| **BoolQ**, **MultiRC** | Reading comprehension                              | QA / Inference              |
| **CB**, **RTE**        | Natural language inference                         | Entailment                  |
| **COPA**              | Causal reasoning (cause/effect)                    | Reasoning                   |
| **ReCoRD**            | Reading comprehension with commonsense reasoning  | QA / Commonsense            |
| **WiC**               | Word meaning in context                            | Word Sense Disambiguation   |
| **WSC**               | Coreference resolution                             | Coreference                 |

Together, these tasks go beyond surface-level prediction—testing abilities like logical reasoning, commonsense application, coreference tracking, and contextual understanding.

**Leaderboard Highlights (v2.0):**

To measure and compare model performance on SuperGLUE, an official leaderboard tracks results across all tasks using standardized metrics. The **v2.0 leaderboard** showcases most advanced models—ranging from parameter-efficient **Mixture of Experts (MoE)** systems to massive transformer-based architectures—offering a clear snapshot of the state of the art in general language understanding.

- **Top models**: Vega v2, ST-MoE-32B, ERNIE, PaLM 540B, T5  
- **Metrics used**: Accuracy, F1 score, Exact Match, Gender Parity, etc.

The leaderboard emphasizes **balanced generalization**, rewarding models that perform consistently well across diverse task types—not just a few. This makes it a reliable benchmark for tracking progress toward broadly capable language models.


### Domain-Rich Multi-Task Benchmark: MMLU

While SuperGLUE focuses on general linguistic reasoning, **Massive Multitask Language Understanding (MMLU)** shifts the spotlight to **domain knowledge**. It has rapidly become the *de facto* benchmark for evaluating a model’s grasp of academic and professional subjects—effectively acting as a proxy for general intelligence in many LLM leaderboards.

**What is MMLU?**

- **57 subjects** spanning elementary math, anatomy, law, philosophy, computer science, US history, and more  
- **Multiple-choice format** (A, B, C, D), with ~100 questions per subject  
- **Balanced question design** that mimics real academic tests and professional licensing exams  
- **Closed-book evaluation** testing what the model has internalized from pretraining  

MMLU has emerged as a **standardized benchmark** for evaluating foundational knowledge across domains. It allows for **direct accuracy-based comparisons** between models of different sizes and architectures. Performance gains have been dramatic—rising from ~**25% (random guessing)** to over **90% accuracy** in just four years.

> **Note**: MMLU is often treated as a shortcut for measuring "general intelligence," but that can be misleading. What it really tests is how well a model can recall facts and recognize patterns—not necessarily how well it can reason or think abstractly. We’ll explore these limitations later.

**MMLU in Modern LLM Leaderboards**

Modern LLMs—including GPT-4, Claude 3, Gemini, LLaMA 3, and PaLM—routinely report MMLU scores as a primary metric. As with SuperGLUE, MMLU supports **multi-subject generalization**, but with a stronger emphasis on **world knowledge** rather than linguistic nuance.


**SuperGLUE vs MMLU: A Comparison**

| Aspect | **SuperGLUE** | **MMLU** |
|--------|----------------|----------|
| **Focus** | Language reasoning | Factual subject knowledge |
| **Format** | Varied NLP tasks | Multiple choice |
| **Tasks / Subjects** | 8 tasks | 57 subjects |
| **Primary Skill Tested** | Inference, disambiguation, coreference | Retained domain knowledge |
| **Metric** | Accuracy, F1, etc. | Accuracy only |

> While both are multi-task benchmarks, they evaluate very different capabilities—SuperGLUE emphasizes reasoning and understanding, whereas MMLU stresses factual recall across disciplines.


### Challenges in Closed-Ended Evaluation

1. **Metric Selection:** Different metrics highlight different aspects of model performance. For example: Accuracy can be misleading on imbalanced datasets—it may just reflect majority-class predictions. Precision measures correctness (fewer false positives), while Recall measures completeness (fewer false negatives). Using a single metric in isolation—especially on skewed tasks—can hide a model’s weaknesses.

2. **Metric Aggregation:** Benchmarks like SuperGLUE combine many tasks, each with its own metric (e.g., F1, accuracy, loss). Simply averaging scores can give an incomplete picture. Some tasks are easier than others, and their metrics scale differently. Without proper weighting or normalization, overall scores may not reflect true performance.

3. **Label Quality:** Poorly defined or inconsistent labels can introduce noise into both training and evaluation. This reduces reliability and makes it hard to tell if performance differences are meaningful or just due to annotation issues.

4. **Spurious Correlations:** Models may rely on patterns or keywords rather than real understanding.

   - **Example** (SNLI):  
     - Premise: “The economy could be still better.”  
     - Hypothesis: “The economy has *never* been better.”  
     - Model might infer contradiction simply due to the word *never* rather than actual reasoning

5. **Annotation Artifacts and Dataset Bias**: Some datasets contain structural biases—for example, QA answers often appear at the start of a passage. Models may exploit these shortcuts without real comprehension. Stylistic cues (e.g., sentence length or formality) may also correlate with certain labels, inflating scores without reflecting true understanding.

6. **Adversarial Robustness**: Small tweaks—like changing a word or rephrasing a sentence—can confuse models even if the meaning stays the same. This shows a lack of generalization. Robust evaluation should test whether models maintain performance under such paraphrased or adversarial inputs.

This highlights that even well-defined tasks require evaluation beyond raw accuracy to capture true model behavior and ensure robustness.

---


## Open-Ended Text Generation

Open-ended generation models, such as large language models (LLMs), can produce diverse free-form outputs like summaries, translations, stories, or answers to instructions. Unlike classification models that generate a fixed label or number, open-ended models return entire sequences of text—often with many valid responses for the same input.

This flexibility makes them powerful but difficult to evaluate. There's rarely a single “correct” output, and even seemingly unrelated completions may still be valid. For example, the prompt _“Tell me something interesting about the moon”_ could yield many accurate and fluent yet different answers.

Evaluating such responses goes beyond simple reference matching—it requires assessing coherence, fluency, relevance, factual accuracy, style, and semantic alignment. As a result, evaluation of open-ended generation must adopt a more nuanced, multi-faceted approach.

The following sections outline key evaluation methods and challenges.


### Content Overlap Metrics

These methods compute similarity based on surface-level word overlap between the generated text and a human-written reference. They are fast and widely used but fail to capture meaning or paraphrased content.

- **Common metrics:** `BLEU`, `ROUGE`, `METEOR`, `CIDEr`.
- BLEU emphasizes precision; ROUGE emphasizes recall.
- Often used in summarization and translation tasks.

> **Limitation:** N-gram metrics have no concept of meaning—they fail when different words express the same idea.

**Illustrative Example:**

Reference: "Heck yes!"  
Generated → BLEU score:

	- "Yes!" → 67%
	- "You know it!" → low
	- "Yup" → 0% (but semantically correct!)
	- "Heck no!" → 67% (opposite meaning!)

These metrics penalize valid rephrasings and may reward word overlap even when meaning is incorrect.


### Model-Based Metrics

As NLP models generate increasingly fluent and diverse outputs, traditional word-overlap metrics like BLEU and ROUGE struggle to capture true semantic quality. To address this, researchers have developed **model-based metrics** that leverage the power of pre-trained language models to assess meaning, fluency, and naturalness. Two widely adopted examples are **BERTScore** and **BLEURT**.

#### BERTScore: Semantic Matching with Contextual Embeddings 

BERTScore compares generated and reference texts using contextual token embeddings from BERT. Instead of matching exact words, it evaluates how semantically similar they are. 

**Step-by-Step: How BERTScore Works**

1. **Token Embedding:** Both candidate and reference sentences are passed through BERT to get contextualized embeddings for each token.
2. **Similarity Matrix:** A cosine similarity matrix is computed between every token in the candidate and reference.
3. **Greedy Token Matching**  
   - **Precision**: Each candidate token is matched to its most similar reference token.  
   - **Recall**: Each reference token is matched to its most similar candidate token.  
   - **F1 Score**: Harmonic mean of precision and recall gives the final BERTScore.   
4. **Optional Enhancements**  
   - **IDF Weighting** to highlight rare, informative words  
   - **Baseline Rescaling** to improve comparability across datasets

**Example**:     
- *Reference*: "The weather is cold today"  
- *Candidate*: "It is freezing today"  

BLEU would score this low due to minimal overlap. BERTScore identifies that “cold” and “freezing” are semantically similar, and “it” can contextually align with “weather,” resulting in a high score.

**Comparison with Traditional Metrics**

| Aspect                  | **BERTScore**                                      | **BLEU / ROUGE**                        |
|-------------------------|----------------------------------------------------|-----------------------------------------|
| **Basis**               | Contextual semantic similarity                     | N-gram word overlap                     |
| **Paraphrase Handling** | Robust: understands rewordings                     | Weak: penalizes even accurate paraphrases |
| **Sensitivity**         | Sensitive to semantic errors                       | Overly sensitive to word order/style |
| **Human Alignment**     | Strong correlation with human ratings              | Weaker alignment                     |


**Real-World Use Cases**    
- **Machine Translation**: Meaningful quality evaluation across phrasing differences
- **Text Summarization**: Measures fidelity to the source even if words differ
- **Dialogue Systems**: Checks relevance of model responses
- **Paraphrase Generation**: Verifies semantic alignment

**Key Advantages**      
- **Semantic Awareness**: Captures meaning beyond exact matches
- **Context Sensitivity**: Embeddings reflect true contextual meaning
- **Better Human Alignment**: Matches human quality judgments more closely
- **Robust to Variation**: Tolerates stylistic and structural differences

**Limitations**        
- **Computational Overhead**: Requires expensive BERT inference
- **Reference-Dependent**: Needs gold-standard responses
- **Bias Inheritance**: Carries biases from pre-trained models
- **Domain Sensitivity**: May underperform in out-of-domain settings


#### BLEURT: Learned Quality Estimation

BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) is a learned metric trained to mimic human judgments of text quality.

**How It Works**:

1. **Pre-training:** BLEURT starts with a pre-trained BERT model and further trains it using synthetic sentence pairs—created by automatically altering clean reference sentences (e.g., adding noise, paraphrasing, or inserting errors).
2. **Fine-Tuning:** The model is then fine-tuned on a smaller dataset with **human-annotated quality scores**, teaching it to mimic human evaluations. This dual-stage training makes BLEURT robust and accurate.
3. **Scoring:** To evaluate a model-generated sentence, BLEURT compares it to a reference and outputs a **single scalar score**—typically between -1 and 1—indicating semantic similarity and overall quality.


**Why BLEURT Outperforms Traditional Metrics**

| Feature                  | **BLEURT**                                         | **BLEU / ROUGE**                        |
|--------------------------|---------------------------------------------------|-----------------------------------------|
| **Learning-Based**       | Trained to predict human ratings               | Rule-based n-gram matching            |
| **Paraphrasing Support** | Recognizes semantically similar variations     | Penalizes even minor wording changes  |
| **Fluency Awareness**    | Understands natural phrasing and grammar       | Ignores fluency                       |
| **Human Alignment**      | Strong correlation with human scores           | Weaker and inconsistent               |
| **Generalization**       | Works well even out-of-domain or with limited data | Domain-sensitive                      |


**Real-World Applications**

BLEURT is commonly used to evaluate generation tasks where **semantic fidelity** and **natural language quality** are crucial:    
- **Machine Translation**: Measures fluency and meaning even when translations use different phrasing.
- **Text Summarization**: Evaluates whether summaries retain key points while sounding natural.
- **Caption Generation**: Scores image captions by how well they match reference captions in meaning and readability.
- **Response Generation**: Used in dialogue and chatbot systems to assess how human-like the responses sound.


**Advantages of BLEURT**    
- **Trained on Human Ratings**: Directly optimized to match real human preferences.
- **Sensitive to Meaning and Fluency**: Goes beyond surface-level matching to assess semantic equivalence and naturalness.
- **Robust Across Domains**: Performs well even when applied to tasks or topics outside its training data.
- **Single Unified Score**: Simplifies evaluation with a clear, continuous score that reflects quality.


**Limitations**    
- **Computational Cost**: Requires GPU inference for fast scoring, especially at scale.
- **Reference-Based**: Like most metrics, BLEURT still needs a reference sentence for comparison.
- **Opacity**: Being a neural model, it's harder to interpret than transparent metrics like BLEU.


#### Summary: When to Use Which?

| Metric     | Strengths                                 | Limitations                               |
|------------|--------------------------------------------|--------------------------------------------|
| **BERTScore** | Fast, interpretable, uses contextual embeddings | Requires alignment heuristics (greedy match), less fluent-aware |
| **BLEURT**    | Trained to match human ratings, fluency-aware  | Heavier compute, less transparent          |

Both metrics mark a shift toward evaluation methods that better reflect **semantic correctness**, **paraphrasing**, and **natural language generation goals**.


### Human Evaluations

Human evaluation remains the **gold standard**, especially for open-ended tasks.

**What they measure:**

- Fluency
- Coherence
- Factual Accuracy
- Commonsense
- Harmlessness
- Style and grammar
- Redundancy

**Methods:**

- Likert-scale ratings  
- Pairwise comparisons  
- Ranking systems  
- Multi-annotator voting (for inter-rater reliability)

> **Note:** Never compare scores across studies—evaluation methods, prompts, and annotators vary.

**Challenges:**

- **Expensive** and **time-consuming**
- **Inter-annotator disagreement**: Even simple tasks had only 67% agreement.
- **Intra-annotator inconsistency**
- **Crowdworker incentives**: Speed over care, favoring longer outputs.
- **Precision-only**: Humans can't judge what _could_ have been generated, only what _was_.
- **Reproducibility crisis**: Only ~5% of NLP papers between 2015–2020 documented enough to replicate human evals.


### Reference-Free Evaluation

Instead of comparing to human-written references, these approaches judge quality **without a gold standard**.

**Approaches:**

- **Traditional:** Fine-tune BERT to predict quality scores.
- **Modern (LLM-based):** Use GPT-4 to evaluate generated outputs.
  - Benchmarks: `AlpacaEval`, `MT-Bench`, `Chatbot Arena`
  - Surprisingly strong alignment with human judgments
  - Fast and scalable—100× cheaper than human eval

> GPT-4 shows **higher agreement with humans than humans show with each other** due to low variance.


### Side-by-Side Comparison (Arena-style)

Used to compare models directly, especially for chatbots or instruction-following tasks.

**Example:**

- Ask the same question to two models.
- Let humans (or GPT-4) choose the better output.
- Use **Elo ratings** (like chess) to build a leaderboard.

**Tools:**

- **Chatbot Arena**: Crowdsourced, ongoing, 200K+ votes
- **HELM**, **Open LLM Leaderboard**, **VLLM Eval Leaderboard**

**Limitations:**

- User-generated prompts may not be representative.
- Early-stage models don't get enough votes.
- Not usable for training or development, only end-product evaluation.


### Evaluation Pitfalls and Biases

**Reference Quality Matters:**

- Bad references = bad evaluation.
- Studies show ROUGE is uncorrelated with human scores unless references are written by experts.

**Spurious Correlations:**

- **Length bias**: ~70% preference for longer outputs
- **List bias**: Preference for bullet-style responses
- **Position bias**: Left vs right positioning in comparison
- **Self-bias**: GPT-4 mildly favors its own outputs


### Broader Challenges

- **Consistency Issues**: Different prompt styles or decoding methods yield very different results (e.g., MMLU scores varied by 15%).
- **Contamination**: Models trained on test data (e.g., GPT-4 aces pre-2021 Codeforces, flunks post-2021).
- **Overfitting**: Datasets saturate quickly; benchmark usefulness decays.
- **Monoculture**: 70% of ACL 2021 papers were English-only; many ignore bias or efficiency.
- **Single Metric Fallacy**: Oversimplifies model performance. E.g., accuracy alone misses bias, latency, and fairness tradeoffs.


### Key Takeaways

- Open-ended evaluation is **complex**—no one-size-fits-all.
- Traditional metrics (BLEU, ROUGE) are useful but **insufficient**.
- Model-based metrics and human evals are better but expensive or slow.
- LLM-based eval is promising—**fast**, **scalable**, **well-correlated**.
- Always **manually inspect outputs**—numbers alone can be misleading.



## References & Further Reading



[16] Huyen, C. (2024). *AI Engineering: Building Applications with Foundation Models*. O’Reilly Media.  

[17] Alammar, J., & Grootendorst, M. (2023). *Hands-On Large Language Models: Language Understanding and Generation*. O’Reilly Media. 


 