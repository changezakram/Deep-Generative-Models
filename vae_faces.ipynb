{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c58859",
   "metadata": {},
   "source": [
    "# Face Generation with Convolutional VAE\n",
    "\n",
    "This notebook implements a convolutional variational autoencoder (VAE) trained on the CelebA face dataset using PyTorch.\n",
    "\n",
    "It uses convolutional layers to encode and decode 64x64 face images, and demonstrates generation by sampling from the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7b955",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84626d1",
   "metadata": {},
   "source": [
    "## 2. Load CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download CelebA dataset (you must accept the license the first time)\n",
    "celeba = torchvision.datasets.CelebA(root=\"./data\", split=\"train\", download=True, transform=transform)\n",
    "dataloader = DataLoader(celeba, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d088a7",
   "metadata": {},
   "source": [
    "## 3. Define the Convolutional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca09c0",
   "metadata": {},
   "source": [
    "### VAE Architecture Explained\n",
    "- **Encoder**: uses 4 convolutional layers to compress the image into a latent vector.\n",
    "- **Latent Space**: the model learns a distribution (mean and variance) over latent variables.\n",
    "- **Reparameterization**: samples from this distribution to make training differentiable.\n",
    "- **Decoder**: mirrors the encoder with transposed convolutions to reconstruct the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),  # [B, 32, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # [B, 64, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # [B, 128, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), # [B, 256, 4, 4]\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # [B, 128, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # [B, 64, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # [B, 32, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),     # [B, 3, 64, 64]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z).view(-1, 256, 4, 4)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823063f9",
   "metadata": {},
   "source": [
    "## 4. Define ELBO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8824f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')  # MSE for real-valued images\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e6b36",
   "metadata": {},
   "source": [
    "## 5. Generate New Faces\n",
    "Now that the VAE is trained, we can sample new faces by drawing $z \\sim \\mathcal{N}(0, I)$ and decoding it.\n",
    "\n",
    "This works because the decoder has learned how to map points in latent space back to realistic face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7524ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize generated images\n",
    "def show_generated_images(samples, nrow=8):\n",
    "    grid = make_grid(samples, nrow=nrow)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Generated Faces\")\n",
    "    plt.show()\n",
    "\n",
    "# Sample z ~ N(0, I)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, model.latent_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    generated = model.decode(z)\n",
    "\n",
    "# Show the generated images\n",
    "show_generated_images(generated)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
