{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c58859",
   "metadata": {},
   "source": [
    "# Face Generation with Convolutional VAE\n",
    "\n",
    "This notebook implements a convolutional variational autoencoder (VAE) trained on the CelebA face dataset using PyTorch.\n",
    "\n",
    "It uses convolutional layers to encode and decode 64x64 face images, and demonstrates generation by sampling from the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7b955",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84626d1",
   "metadata": {},
   "source": [
    "## 2. Load CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download CelebA dataset (you must accept the license the first time)\n",
    "celeba = torchvision.datasets.CelebA(root=\"./data\", split=\"train\", download=True, transform=transform)\n",
    "dataloader = DataLoader(celeba, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d088a7",
   "metadata": {},
   "source": [
    "## 3. Define the Convolutional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca09c0",
   "metadata": {},
   "source": [
    "### VAE Architecture Explained\n",
    "- **Encoder**: uses 4 convolutional layers to compress the image into a latent vector.\n",
    "- **Latent Space**: the model learns a distribution (mean and variance) over latent variables.\n",
    "- **Reparameterization**: samples from this distribution to make training differentiable.\n",
    "- **Decoder**: mirrors the encoder with transposed convolutions to reconstruct the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, 2, 1),  # [B, 32, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # [B, 64, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # [B, 128, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), # [B, 256, 4, 4]\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # [B, 128, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),   # [B, 64, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),    # [B, 32, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),     # [B, 3, 64, 64]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z).view(-1, 256, 4, 4)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823063f9",
   "metadata": {},
   "source": [
    "## 4. Define ELBO Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8824f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')  # MSE for real-valued images\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac26bc",
   "metadata": {},
   "source": [
    "## 5. Train the VAE\n",
    "This section trains the convolutional variational autoencoder (VAE) on the CelebA dataset using ELBO loss. After training, we will sample from the latent space to generate new face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6cabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvVAE(latent_dim=100).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 5  # Adjust for better quality\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for x, _ in dataloader:\n",
    "        x = x.to(device)\n",
    "        recon_x, mu, logvar = model(x)\n",
    "        loss = elbo_loss(recon_x, x, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1782b",
   "metadata": {},
   "source": [
    "## 6. Generate New Faces\n",
    "In this section, we sample random latent vectors from a standard normal distribution and pass them through the decoder to generate new face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155fc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize generated images\n",
    "def show_generated_images(samples, nrow=8):\n",
    "    grid = make_grid(samples, nrow=nrow)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Faces\")\n",
    "    plt.show()\n",
    "\n",
    "# Sample z ~ N(0, I) and generate faces\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, model.latent_dim).to(device)\n",
    "    generated = model.decode(z)\n",
    "\n",
    "# Display\n",
    "show_generated_images(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be4f9d4",
   "metadata": {},
   "source": [
    "## 7. Reconstruction Comparison\n",
    "This section compares original images with their reconstructions to show how well the VAE preserves features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Take a small batch\n",
    "    x, _ = next(iter(dataloader))\n",
    "    x = x.to(device)\n",
    "    recon_x, _, _ = model(x)\n",
    "\n",
    "    # Convert to CPU and reshape for plotting\n",
    "    x = x[:8].cpu()\n",
    "    recon_x = recon_x[:8].cpu()\n",
    "\n",
    "    from torchvision.utils import make_grid\n",
    "\n",
    "    def show_grid(images, title):\n",
    "        grid = make_grid(images, nrow=8)\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.imshow(grid.permute(1, 2, 0))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    show_grid(x, \"Original Images\")\n",
    "    show_grid(recon_x, \"Reconstructed Images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ccdad",
   "metadata": {},
   "source": [
    "## 8. Visualize Latent Space (for latent_dim = 2)\n",
    "If your model uses a 2D latent space, this plot shows how different digits (or faces) are embedded. Note: this is best interpreted with a 2D latent space and labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686dae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.latent_dim == 2:\n",
    "    import seaborn as sns\n",
    "    zs = []\n",
    "    ys = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            mu, _ = model.encode(x)\n",
    "            zs.append(mu.cpu())\n",
    "            ys.append(y)\n",
    "\n",
    "    z = torch.cat(zs)\n",
    "    labels = torch.cat(ys)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=z[:, 0], y=z[:, 1], hue=labels, palette=\"tab10\", s=15, legend=False)\n",
    "    plt.title(\"Latent Space Visualization (Î¼)\")\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Latent space visualization only works for latent_dim = 2\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
