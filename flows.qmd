---
title: "Normalizing Flow Models"
format: html
---

## Introduction

In generative modeling, the objective is to learn a probability distribution over data that allows us to both **generate new examples** and **evaluate the likelihood** of observed ones. For a model to be practically useful, it must support **efficient sampling** and enable **exact or tractable likelihood computation** during training.

Traditional generative models each offer distinct advantages but also come with limitations. **Autoregressive models** allow for exact likelihood estimation by factorizing the joint distribution into a sequence of conditional probabilities. However, they lack a **latent representation**, which means they do not explicitly capture high-level features or structure in the data.

**Variational Autoencoders (VAEs)** address this limitation by introducing latent variables $z$, enabling the model to learn compressed, structured representations of the data. While VAEs are flexible and powerful, they face a key challenge: the marginal likelihood $p(x)$ — which integrates over all possible latent variables $z$ — is intractable

To overcome this, VAEs use **variational inference** and optimize the **Evidence Lower Bound (ELBO)**, a tractable surrogate objective. Combined with the **reparameterization trick**, this allows for efficient training despite the intractability of the exact likelihood.

**Normalizing Flows** present an elegant solution to these challenges. They define the data distribution through a **deterministic and invertible transformation** of a simple base distribution. In this setup, a data point $x$ is generated via a function $x = f(z)$, and the corresponding latent variable can be recovered exactly using the inverse function $z = f^{-1}(x)$.

This structure offers several advantages. First, **each $x$ maps to a unique $z$** — eliminating the need to integrate over latent variables. Second, the **change-of-variables formula** allows for **exact computation of the likelihood**. Third, **sampling is straightforward**: we simply draw $z \sim p_Z(z)$ (e.g., from a standard Gaussian) and transform it via $x = f(z)$.

However, despite their strengths, Normalizing Flows are not without limitations. Unlike VAEs, which learn **lower-dimensional latent representations** by compressing the data, flows require the latent and data spaces to have **equal dimensionality** to preserve invertibility. This means flow models do **not inherently perform dimensionality reduction**, which can be a disadvantage in tasks where compact representations are desired.


## Math Review: Change of Variables in 1D

Suppose we have a **random variable** $z$ with a known distribution $p_Z(z)$, and we define a new variable:

$$
x = f(z)
$$

where $f$ is a **monotonic, differentiable** function with an inverse:

$$
z = f^{-1}(x) = h(x)
$$

Our goal is to compute the probability density function (PDF) of $x$, denoted $p_X(x)$, in terms of the known PDF $p_Z(z)$.

### Step 1: Cumulative Distribution Function (CDF)

We begin with the cumulative distribution function of $x$:

$$
F_X(x) = P(X \leq x) = P(f(Z) \leq x)
$$

Since $f$ is monotonic and invertible, this becomes:

$$
P(f(Z) \leq x) = P(Z \leq f^{-1}(x)) = F_Z(h(x))
$$

### Step 2: Deriving the PDF via Chain Rule

To obtain the PDF, we differentiate the CDF:

$$
p_X(x) = \frac{d}{dx} F_X(x) = \frac{d}{dx} F_Z(h(x))
$$

Applying the chain rule:

$$
p_X(x) = F_Z'(h(x)) \cdot h'(x) = p_Z(h(x)) \cdot h'(x)
$$

### Step 3: Rewrite in Terms of $z$

From the previous step:

$$
p_X(x) = p_Z(h(x)) \cdot h'(x)
$$

Since $z = h(x)$, we can rewrite:

$$
p_X(x) = p_Z(z) \cdot h'(x)
$$

Now, using the **inverse function theorem**, we express $h'(x)$ as:

$$
h'(x) = \frac{d}{dx} f^{-1}(x) = \frac{1}{f'(z)}
$$

So the final expression becomes:

$$
p_X(x) = p_Z(z) \cdot \left| \frac{1}{f'(z)} \right|
$$

The **absolute value** ensures the density remains non-negative, as required for any valid probability distribution.

### Final Result (1D Case)

The final result of the change-of-variables formula in 1D is:

$$
p_X(x) = p_Z(z) \cdot \left| \frac{1}{f'(z)} \right|, \quad \text{where } z = f^{-1}(x)
$$

This is the fundamental concept Normalizing Flows use to model complex distributions by transforming simple ones.


## Geometry: Determinants and Volume Changes

To understand the multivariate change-of-variable formula, it's helpful to first explore how transformations affect **volume** in high-dimensional spaces.

Let $\mathbf{Z}$ be a random vector uniformly distributed in the unit cube $[0,1]^n$, and let $\mathbf{X} = A\mathbf{Z}$ where $A$ is a square, invertible matrix. Geometrically, the matrix $A$ maps the unit hypercube to a **parallelogram** (in 2D) or a **parallelotope** (in higher dimensions).

The **determinant** of a square matrix tells us how that matrix **scales volume** in space. For example, if the determinant of a $2 \times 2$ matrix is 3, then applying that matrix to a region will stretch its area by a factor of 3. A negative determinant also indicates a **reflection** (orientation reversal). The **absolute value** of the determinant is what we care about when measuring volume.

The **volume** of this parallelotope is equal to the **absolute value of the determinant** of matrix $A$:

$$
\text{Volume} = |\det(A)|
$$

This tells us how much the transformation $A$ **scales space**. For example, if $|\det(A)| = 2$, the transformation doubles the volume.

## Determinants and Probability Density

Now suppose we apply the linear transformation $\mathbf{X} = A\mathbf{Z}$ to a random vector $\mathbf{Z}$ with a known density $p_Z$. The new density after transformation is:

$$
p_X(\mathbf{x}) = p_Z(W\mathbf{x}) \cdot |\det(W)| \quad \text{where} \quad W = A^{-1}
$$

This is directly analogous to the 1D change-of-variable rule:

$$
p_X(x) = p_Z(h(x)) \cdot |h'(x)|
$$

but now in multiple dimensions using the determinant of the **inverse transformation**.

## Generalizing to Nonlinear Transformations

For **nonlinear** transformations $\mathbf{x} = f(\mathbf{z})$, the idea is similar. But instead of a constant matrix $A$, we now consider the **Jacobian matrix** of the function $f$:

$$
J_f(\mathbf{z}) = \frac{\partial f}{\partial \mathbf{z}}
$$

This Jacobian tells us how the function scales and rotates space **locally** around a point $\mathbf{z}$. The **determinant of the Jacobian** gives the volume change factor at that point.

## Multivariate Change-of-Variable Formula

Given an invertible transformation $\mathbf{x} = f(\mathbf{z})$, the probability density transforms as:

$$
p_X(\mathbf{x}) = p_Z(f^{-1}(\mathbf{x})) \cdot \left| \det \left( \frac{\partial f^{-1}(\mathbf{x})}{\partial \mathbf{x}} \right) \right|
$$

Alternatively, in the **forward form** (often used during training):

$$
p_X(\mathbf{x}) = p_Z(\mathbf{z}) \cdot \left| \det \left( \frac{\partial f(\mathbf{z})}{\partial \mathbf{z}} \right) \right|^{-1}
$$

This generalizes the 1D rule and enables us to compute **exact likelihoods** for complex distributions as long as the transformation is invertible and differentiable.

