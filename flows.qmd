---
title: "Normalizing Flow Models"
format: html
---

## Introduction

In generative modeling, the objective is to learn a probability distribution over data that allows us to both **generate new examples** and **evaluate the likelihood** of observed ones. For a model to be practically useful, it must support **efficient sampling** and enable **exact or tractable likelihood computation** during training.

Traditional generative models each offer distinct advantages but also come with limitations. **Autoregressive models** allow for exact likelihood estimation by factorizing the joint distribution into a sequence of conditional probabilities. However, they lack a **latent representation**, which means they do not explicitly capture high-level features or structure in the data.

**Variational Autoencoders (VAEs)** address this limitation by introducing latent variables $z$, enabling the model to learn compressed, structured representations of the data. While VAEs are flexible and powerful, they face a key challenge: the marginal likelihood $p(x)$ — which integrates over all possible latent variables $z$ — is intractable

To overcome this, VAEs use **variational inference** and optimize the **Evidence Lower Bound (ELBO)**, a tractable surrogate objective. Combined with the **reparameterization trick**, this allows for efficient training despite the intractability of the exact likelihood.

**Normalizing Flows** present an elegant solution to these challenges. They define the data distribution through a **deterministic and invertible transformation** of a simple base distribution. In this setup, a data point $x$ is generated via a function $x = f(z)$, and the corresponding latent variable can be recovered exactly using the inverse function $z = f^{-1}(x)$.

This structure offers several advantages. First, **each $x$ maps to a unique $z$** — eliminating the need to integrate over latent variables. Second, the **change-of-variables formula** allows for **exact computation of the likelihood**. Third, **sampling is straightforward**: we simply draw $z \sim p_Z(z)$ (e.g., from a standard Gaussian) and transform it via $x = f(z)$.

However, despite their strengths, Normalizing Flows are not without limitations. Unlike VAEs, which learn **lower-dimensional latent representations** by compressing the data, flows require the latent and data spaces to have **equal dimensionality** to preserve invertibility. This means flow models do **not inherently perform dimensionality reduction**, which can be a disadvantage in tasks where compact representations are desired.


## Math Review: Change of Variables in 1D

Suppose we have a **random variable** $z$ with a known distribution $p_Z(z)$, and we define a new variable:

$$
x = f(z)
$$

where $f$ is a **monotonic, differentiable** function with an inverse:

$$
z = f^{-1}(x) = h(x)
$$

Our goal is to compute the probability density function (PDF) of $x$, denoted $p_X(x)$, in terms of the known PDF $p_Z(z)$.

### Step 1: Cumulative Distribution Function (CDF)

We begin with the cumulative distribution function of $x$:

$$
F_X(x) = P(X \leq x) = P(f(Z) \leq x)
$$

Since $f$ is monotonic and invertible, this becomes:

$$
P(f(Z) \leq x) = P(Z \leq f^{-1}(x)) = F_Z(h(x))
$$

### Step 2: Deriving the PDF via Chain Rule

To obtain the PDF, we differentiate the CDF:

$$
p_X(x) = \frac{d}{dx} F_X(x) = \frac{d}{dx} F_Z(h(x))
$$

Applying the chain rule:

$$
p_X(x) = F_Z'(h(x)) \cdot h'(x) = p_Z(h(x)) \cdot h'(x)
$$

### Step 3: Rewrite in Terms of $z$

From the previous step:

$$
p_X(x) = p_Z(h(x)) \cdot h'(x)
$$

Since $z = h(x)$, we can rewrite:

$$
p_X(x) = p_Z(z) \cdot h'(x)
$$

Now, using the **inverse function theorem**, we express $h'(x)$ as:

$$
h'(x) = \frac{d}{dx} f^{-1}(x) = \frac{1}{f'(z)}
$$

So the final expression becomes:

$$
p_X(x) = p_Z(z) \cdot \left| \frac{1}{f'(z)} \right|
$$

The **absolute value** ensures the density remains non-negative, as required for any valid probability distribution.

### Final Result (1D Case)

The final result of the change-of-variables formula in 1D is:

$$
p_X(x) = p_Z(z) \cdot \left| \frac{1}{f'(z)} \right|, \quad \text{where } z = f^{-1}(x)
$$

This is the fundamental tool Normalizing Flows use to model complex distributions by transforming simple ones.
